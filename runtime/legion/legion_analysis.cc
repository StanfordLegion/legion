/* Copyright 2023 Stanford University, NVIDIA Corporation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <cmath>
#include "legion.h"
#include "legion/runtime.h"
#include "legion/legion_ops.h"
#include "legion/legion_tasks.h"
#include "legion/region_tree.h"
#include "legion/legion_spy.h"
#include "legion/legion_trace.h"
#include "legion/legion_profiling.h"
#include "legion/legion_instances.h"
#include "legion/legion_views.h"
#include "legion/legion_analysis.h"
#include "legion/legion_context.h"
#include "legion/legion_replication.h"

namespace Legion {
  namespace Internal {

    LEGION_EXTERN_LOGGER_DECLARATIONS

    /////////////////////////////////////////////////////////////
    // Users and Info 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    LogicalUser::LogicalUser(Operation *o, unsigned id, const RegionUsage &u,
                             ProjectionSummary *p, unsigned internal)
      : Collectable(), usage(u), op(o), ctx_index(op->get_ctx_index()),
        internal_idx(internal), idx(id), gen(o->get_generation()),
        shard_proj(p), timeout(0)
#ifdef LEGION_SPY
        , uid(o->get_unique_op_id())
#endif
    //--------------------------------------------------------------------------
    {
      if (op != NULL)
        op->add_mapping_reference(gen);
      if (shard_proj != NULL)
        shard_proj->add_reference();
    }

    //--------------------------------------------------------------------------
    LogicalUser::~LogicalUser(void)
    //--------------------------------------------------------------------------
    {
      if (op != NULL)
        op->remove_mapping_reference(gen);
      if ((shard_proj != NULL) && shard_proj->remove_reference())
        delete shard_proj;
    }

    //--------------------------------------------------------------------------
    PhysicalUser::PhysicalUser(const RegionUsage &u, IndexSpaceExpression *e,
                               UniqueID id, unsigned x, bool cpy, bool cov)
      : usage(u), expr(e), op_id(id), index(x), copy_user(cpy), covers(cov)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(expr != NULL);
#endif
      expr->add_base_expression_reference(PHYSICAL_USER_REF);
    }

    //--------------------------------------------------------------------------
    PhysicalUser::PhysicalUser(const PhysicalUser &rhs) 
      : usage(rhs.usage), expr(rhs.expr), op_id(rhs.op_id), index(rhs.index),
        copy_user(rhs.copy_user), covers(rhs.covers)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    PhysicalUser::~PhysicalUser(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(expr != NULL);
#endif
      if (expr->remove_base_expression_reference(PHYSICAL_USER_REF))
        delete expr;
    }

    //--------------------------------------------------------------------------
    PhysicalUser& PhysicalUser::operator=(const PhysicalUser &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void PhysicalUser::pack_user(Serializer &rez, 
                                 const AddressSpaceID target) const
    //--------------------------------------------------------------------------
    {
      RezCheck z(rez);
      rez.serialize(usage);
      expr->pack_expression(rez, target);
      rez.serialize(op_id);
      rez.serialize(index);
      rez.serialize<bool>(copy_user);
      rez.serialize<bool>(covers);
    }

    //--------------------------------------------------------------------------
    /*static*/ PhysicalUser* PhysicalUser::unpack_user(Deserializer &derez,
                          RegionTreeForest *forest, const AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      RegionUsage usage;
      derez.deserialize(usage);
      IndexSpaceExpression *expr = 
        IndexSpaceExpression::unpack_expression(derez, forest, source);
      UniqueID op_id;
      derez.deserialize(op_id);
      unsigned index;
      derez.deserialize(index);
      bool copy_user, covers;
      derez.deserialize<bool>(copy_user);
      derez.deserialize<bool>(covers);
      return new PhysicalUser(usage, expr, op_id, index, copy_user, covers);
    }

    /////////////////////////////////////////////////////////////
    // VersionInfo 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    VersionInfo::VersionInfo(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    VersionInfo::VersionInfo(const VersionInfo &rhs)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(equivalence_sets.empty());
      assert(rhs.equivalence_sets.empty());
#endif
    }

    //--------------------------------------------------------------------------
    VersionInfo::~VersionInfo(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    VersionInfo& VersionInfo::operator=(const VersionInfo &rhs)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(equivalence_sets.empty());
      assert(rhs.equivalence_sets.empty());
#endif
      return *this;
    }

    //--------------------------------------------------------------------------
    void VersionInfo::pack_equivalence_sets(Serializer &rez) const
    //--------------------------------------------------------------------------
    {
      rez.serialize<size_t>(equivalence_sets.size());
      for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
            equivalence_sets.begin(); it != equivalence_sets.end(); it++)
      {
        rez.serialize(it->first->did);
        rez.serialize(it->second);
      }
    }

    //--------------------------------------------------------------------------
    void VersionInfo::unpack_equivalence_sets(Deserializer &derez, 
                              Runtime *runtime, std::set<RtEvent> &ready_events)
    //--------------------------------------------------------------------------
    {
      size_t num_sets;
      derez.deserialize(num_sets);
      for (unsigned idx = 0; idx < num_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        FieldMask mask;
        derez.deserialize(mask);
        RtEvent ready_event;
        EquivalenceSet *set = 
          runtime->find_or_request_equivalence_set(did, ready_event);
        equivalence_sets.insert(set, mask);
        if (ready_event.exists())
          ready_events.insert(ready_event);
      }
    }

    //--------------------------------------------------------------------------
    void VersionInfo::record_equivalence_set(EquivalenceSet *set,
                                             const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      equivalence_sets.insert(set, mask);
    }

    //--------------------------------------------------------------------------
    void VersionInfo::clear(void)
    //--------------------------------------------------------------------------
    {
      equivalence_sets.clear();
    }

    /////////////////////////////////////////////////////////////
    // LogicalTraceInfo 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    LogicalTraceInfo::LogicalTraceInfo(Operation *op, unsigned idx, 
                                       const RegionRequirement &r)
      : trace(op->get_trace()), req_idx(idx), req(r),
        skip_analysis((trace != NULL) && 
                       trace->skip_analysis(r.parent.get_tree_id()))
    //--------------------------------------------------------------------------
    {
    }

    /////////////////////////////////////////////////////////////
    // Unique Instance
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    UniqueInst::UniqueInst(void)
      : inst_did(0), view_did(0), analysis_space(0)
    //--------------------------------------------------------------------------
    {
#ifdef LEGION_SPY
      tid = 0;
#endif
    }

    //--------------------------------------------------------------------------
    UniqueInst::UniqueInst(IndividualView *view)
      : inst_did(view->get_manager()->did), view_did(view->did),
        analysis_space(view->get_analysis_space(view->get_manager()))
    //--------------------------------------------------------------------------
    {
#ifdef LEGION_SPY
      tid = view->get_manager()->tree_id;
#endif
    }

    //--------------------------------------------------------------------------
    void UniqueInst::serialize(Serializer &rez) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(view_did != 0);
      assert(inst_did != 0);
#endif
      rez.serialize(view_did);
      rez.serialize(inst_did);
      rez.serialize(analysis_space);
#ifdef LEGION_SPY
      rez.serialize(tid);
#endif
    }

    //--------------------------------------------------------------------------
    void UniqueInst::deserialize(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      derez.deserialize(view_did);
      derez.deserialize(inst_did);
      derez.deserialize(analysis_space);
#ifdef LEGION_SPY
      derez.deserialize(tid);
#endif
    }

    //--------------------------------------------------------------------------
    AddressSpaceID UniqueInst::get_analysis_space(void) const
    //--------------------------------------------------------------------------
    {
      return analysis_space;
    }

    /////////////////////////////////////////////////////////////
    // Remote Trace Recorder
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RemoteTraceRecorder::RemoteTraceRecorder(Runtime *rt, AddressSpaceID origin,
                                 AddressSpaceID local, const TraceLocalID &tlid,
                                 PhysicalTemplate *tpl, RtUserEvent applied)
      : runtime(rt), origin_space(origin), local_space(local),
        remote_tpl(tpl), applied_event(applied)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(remote_tpl != NULL);
#endif
    }

    //--------------------------------------------------------------------------
    RemoteTraceRecorder::~RemoteTraceRecorder(void)
    //--------------------------------------------------------------------------
    {
      if (!applied_events.empty())
        Runtime::trigger_event(applied_event, 
            Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(applied_event);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::add_recorder_reference(void)
    //--------------------------------------------------------------------------
    {
      add_reference();
    }

    //--------------------------------------------------------------------------
    bool RemoteTraceRecorder::remove_recorder_reference(void)
    //--------------------------------------------------------------------------
    {
      return remove_reference();
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::pack_recorder(Serializer &rez,
                                            std::set<RtEvent> &external_applied)
    //--------------------------------------------------------------------------
    {
      rez.serialize(origin_space);
      rez.serialize(remote_tpl);
      RtUserEvent remote_applied = Runtime::create_rt_user_event();
      rez.serialize(remote_applied);
      // Only need to store this one locally since we already hooked our whole 
      // chain of events into the operations applied set on the origin node
      // See PhysicalTemplate::pack_recorder
      AutoLock a_lock(applied_lock);
      applied_events.insert(remote_applied);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_completion_event(ApEvent lhs,
                                     unsigned op_kind, const TraceLocalID &tlid)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent applied = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_RECORD_COMPLETION_EVENT);
          rez.serialize(applied);
          rez.serialize(lhs);
          rez.serialize(op_kind);
          tlid.serialize(rez);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        AutoLock a_lock(applied_lock);
        applied_events.insert(applied);
      }
      else
        remote_tpl->record_completion_event(lhs, op_kind, tlid);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_replay_mapping(ApEvent lhs,
                 unsigned op_kind, const TraceLocalID &tlid, bool register_memo)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent applied = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_RECORD_REPLAY_MAPPING);
          rez.serialize(applied);
          rez.serialize(lhs);
          rez.serialize(op_kind);
          tlid.serialize(rez);
          rez.serialize<bool>(register_memo);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        AutoLock a_lock(applied_lock);
        applied_events.insert(applied);
      }
      else
        remote_tpl->record_replay_mapping(lhs, op_kind, tlid, register_memo);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::request_term_event(ApUserEvent &term_event)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!term_event.exists() || term_event.has_triggered_faultignorant());
#endif
      if (local_space != origin_space)
      {
        RtUserEvent ready = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_REQUEST_TERM_EVENT);
          rez.serialize(&term_event);
          rez.serialize(ready);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        // Wait for the result to be set
        ready.wait();
      }
      else
        remote_tpl->request_term_event(term_event);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_create_ap_user_event(
                                     ApUserEvent &lhs, const TraceLocalID &tlid)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent done = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_CREATE_USER_EVENT);
          rez.serialize(done);
          rez.serialize(&lhs);
          tlid.serialize(rez);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        // Need this to be done before returning because we need to ensure
        // that this event is recorded before anyone tries to trigger it
        done.wait();
      }
      else
        remote_tpl->record_create_ap_user_event(lhs, tlid);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_trigger_event(ApUserEvent lhs, ApEvent rhs,
                                                   const TraceLocalID &tlid)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent applied = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_TRIGGER_EVENT);
          rez.serialize(applied);
          rez.serialize(lhs);
          rez.serialize(rhs);
          tlid.serialize(rez);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        AutoLock a_lock(applied_lock);
        applied_events.insert(applied);
      }
      else
        remote_tpl->record_trigger_event(lhs, rhs, tlid);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_merge_events(ApEvent &lhs, ApEvent rhs,
                                                  const TraceLocalID &tlid)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        std::set<ApEvent> rhs_events;
        rhs_events.insert(rhs);
        record_merge_events(lhs, rhs_events, tlid);
      }
      else
        remote_tpl->record_merge_events(lhs, rhs, tlid);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_merge_events(ApEvent &lhs, ApEvent e1,
                                           ApEvent e2, const TraceLocalID &tlid)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        std::set<ApEvent> rhs_events;
        rhs_events.insert(e1);
        rhs_events.insert(e2);
        record_merge_events(lhs, rhs_events, tlid);
      }
      else
        remote_tpl->record_merge_events(lhs, e1, e2, tlid);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_merge_events(ApEvent &lhs, ApEvent e1,
                                                  ApEvent e2, ApEvent e3,
                                                  const TraceLocalID &tlid)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        std::set<ApEvent> rhs_events;
        rhs_events.insert(e1);
        rhs_events.insert(e2);
        rhs_events.insert(e3);
        record_merge_events(lhs, rhs_events, tlid);
      }
      else
        remote_tpl->record_merge_events(lhs, e1, e2, e3, tlid);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_merge_events(ApEvent &lhs,
                                                  const std::set<ApEvent>& rhs,
                                                  const TraceLocalID &tlid)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent done = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_MERGE_EVENTS);
          rez.serialize(done);
          rez.serialize(&lhs);
          rez.serialize(lhs);
          tlid.serialize(rez);
          rez.serialize<size_t>(rhs.size());
          for (std::set<ApEvent>::const_iterator it = 
                rhs.begin(); it != rhs.end(); it++)
            rez.serialize(*it);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        // Wait to see if lhs changes
        done.wait();
      }
      else
        remote_tpl->record_merge_events(lhs, rhs, tlid);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_merge_events(ApEvent &lhs,
                                                const std::vector<ApEvent>& rhs,
                                                const TraceLocalID &tlid)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent done = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_MERGE_EVENTS);
          rez.serialize(done);
          rez.serialize(&lhs);
          rez.serialize(lhs);
          tlid.serialize(rez);
          rez.serialize<size_t>(rhs.size());
          for (std::vector<ApEvent>::const_iterator it = 
                rhs.begin(); it != rhs.end(); it++)
            rez.serialize(*it);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        // Wait to see if lhs changes
        done.wait();
      }
      else
        remote_tpl->record_merge_events(lhs, rhs, tlid);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_merge_events(PredEvent &lhs,
                           PredEvent e1, PredEvent e2, const TraceLocalID &tlid)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent done = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_MERGE_PRED_EVENTS);
          rez.serialize(done);
          rez.serialize(&lhs);
          rez.serialize(lhs);
          rez.serialize(e1);
          rez.serialize(e2);
          tlid.serialize(rez);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        // Wait to see if lhs changes
        done.wait();
      }
      else
        remote_tpl->record_merge_events(lhs, e1, e2, tlid);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_collective_barrier(ApBarrier bar, 
              ApEvent pre, const std::pair<size_t,size_t> &key, size_t arrivals)
    //--------------------------------------------------------------------------
    {
      // Should be no cases where this is called remotely
      assert(false);
    }

    //--------------------------------------------------------------------------
    ShardID RemoteTraceRecorder::record_managed_barrier(ApBarrier bar,
                                                        size_t total_arrivals)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(bar.exists());
#endif
      if (local_space != origin_space)
      {
        const RtUserEvent done = Runtime::create_rt_user_event();
        std::atomic<ShardID> owner(0);
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_RECORD_BARRIER);
          rez.serialize(done);
          rez.serialize(bar);
          rez.serialize(total_arrivals);
          rez.serialize(&owner);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        done.wait();
        return owner.load();
      }
      else
        return remote_tpl->record_managed_barrier(bar, total_arrivals);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_barrier_arrival(ApBarrier bar, ApEvent pre,
               size_t arrivals, std::set<RtEvent> &applied, ShardID owner_shard)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(bar.exists());
#endif
      if (local_space != origin_space)
      {
        const RtUserEvent done = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_BARRIER_ARRIVAL);
          rez.serialize(done);
          rez.serialize(bar);
          rez.serialize(pre);
          rez.serialize(arrivals);
          rez.serialize(owner_shard);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        applied.insert(done);
      }
      else
        remote_tpl->record_barrier_arrival(bar, pre, arrivals,
                                           applied, owner_shard);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_issue_copy(const TraceLocalID &tlid,
                                 ApEvent &lhs, IndexSpaceExpression *expr,
                                 const std::vector<CopySrcDstField>& src_fields,
                                 const std::vector<CopySrcDstField>& dst_fields,
                                 const std::vector<Reservation> &reservations,
#ifdef LEGION_SPY
                                             RegionTreeID src_tree_id,
                                             RegionTreeID dst_tree_id,
#endif
                                             ApEvent precondition, 
                                             PredEvent pred_guard,
                                             LgEvent src_unique,
                                             LgEvent dst_unique,
                                             int priority,
                                             CollectiveKind collective)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent done = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_ISSUE_COPY);
          rez.serialize(done);
          tlid.serialize(rez);
          rez.serialize(&lhs);
          rez.serialize(lhs);
          expr->pack_expression(rez, origin_space);
#ifdef DEBUG_LEGION
          assert(src_fields.size() == dst_fields.size());
#endif
          rez.serialize<size_t>(src_fields.size());
          for (unsigned idx = 0; idx < src_fields.size(); idx++)
          {
            pack_src_dst_field(rez, src_fields[idx]);
            pack_src_dst_field(rez, dst_fields[idx]);
          }
          rez.serialize<size_t>(reservations.size());
          for (unsigned idx = 0; idx < reservations.size(); idx++)
            rez.serialize(reservations[idx]);
#ifdef LEGION_SPY
          rez.serialize(src_tree_id);
          rez.serialize(dst_tree_id);
#endif
          rez.serialize(precondition);
          rez.serialize(pred_guard);
          rez.serialize(src_unique);
          rez.serialize(dst_unique);
          rez.serialize(priority);
          rez.serialize(collective);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        // Wait to see if lhs changes
        done.wait();
      }
      else
        remote_tpl->record_issue_copy(tlid, lhs, expr, src_fields,
                              dst_fields, reservations,
#ifdef LEGION_SPY
                              src_tree_id, dst_tree_id,
#endif
                              precondition, pred_guard,
                              src_unique, dst_unique, priority, collective);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_copy_insts(ApEvent lhs, 
                                              const TraceLocalID &tlid,
                                              unsigned src_idx,unsigned dst_idx,
                                              IndexSpaceExpression *expr,
                                              const UniqueInst &src_inst,
                                              const UniqueInst &dst_inst,
                                              const FieldMask &src_mask,
                                              const FieldMask &dst_mask,
                                              PrivilegeMode src_mode,
                                              PrivilegeMode dst_mode,
                                              ReductionOpID redop,
                                              std::set<RtEvent> &applied)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        const RtUserEvent done = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_COPY_INSTS);
          rez.serialize(done);
          tlid.serialize(rez);
          rez.serialize(lhs);
          rez.serialize(src_idx);
          rez.serialize(dst_idx);
          rez.serialize(src_mode);
          rez.serialize(dst_mode);
          expr->pack_expression(rez, origin_space);
          src_inst.serialize(rez);
          dst_inst.serialize(rez);
          rez.serialize(src_mask);
          rez.serialize(dst_mask);
          rez.serialize(redop);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        applied.insert(done);
      }
      else
        remote_tpl->record_copy_insts(lhs, tlid, src_idx, dst_idx, expr,
                                 src_inst, dst_inst, src_mask, dst_mask,
                                 src_mode, dst_mode, redop, applied);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_issue_across(const TraceLocalID &tlid,
                                              ApEvent &lhs,
                                              ApEvent collective_precondition,
                                              ApEvent copy_precondition,
                                              ApEvent src_indirect_precondition,
                                              ApEvent dst_indirect_precondition,
                                              CopyAcrossExecutor *executor)
    //--------------------------------------------------------------------------
    {
      // We should never get a call to record a remote indirection
      assert(false);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_across_insts(ApEvent lhs, 
                                 const TraceLocalID &tlid,
                                 unsigned src_idx, unsigned dst_idx,
                                 IndexSpaceExpression *expr,
                                 const AcrossInsts &src_insts,
                                 const AcrossInsts &dst_insts,
                                 PrivilegeMode src_mode, PrivilegeMode dst_mode,
                                 bool src_indirect, bool dst_indirect,
                                 std::set<RtEvent> &applied)
    //--------------------------------------------------------------------------
    {
      // We should never get a call to record a remote across
      assert(false);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_indirect_insts(ApEvent indirect_done,
                                                    ApEvent all_done,
                                                    IndexSpaceExpression *expr,
                                                    const AcrossInsts &insts,
                                                    std::set<RtEvent> &applied,
                                                    PrivilegeMode privilege)
    //--------------------------------------------------------------------------
    {
      // We should never get a call to record a remote indirection
      assert(false);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_issue_fill(const TraceLocalID &tlid,
                                 ApEvent &lhs, IndexSpaceExpression *expr,
                                 const std::vector<CopySrcDstField> &fields,
                                             const void *fill_value, 
                                             size_t fill_size,
#ifdef LEGION_SPY
                                             UniqueID fill_uid,
                                             FieldSpace handle,
                                             RegionTreeID tree_id,
#endif
                                             ApEvent precondition,
                                             PredEvent pred_guard,
                                             LgEvent unique_event,
                                             int priority,
                                             CollectiveKind collective)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent done = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_ISSUE_FILL);
          rez.serialize(done);
          tlid.serialize(rez);
          rez.serialize(&lhs);
          rez.serialize(lhs);
          expr->pack_expression(rez, origin_space);
          rez.serialize<size_t>(fields.size());
          for (unsigned idx = 0; idx < fields.size(); idx++)
            pack_src_dst_field(rez, fields[idx]);
          rez.serialize(fill_size);
          rez.serialize(fill_value, fill_size);
#ifdef LEGION_SPY
          rez.serialize(fill_uid);
          rez.serialize(handle);
          rez.serialize(tree_id);
#endif
          rez.serialize(precondition);
          rez.serialize(pred_guard);  
          rez.serialize(unique_event);
          rez.serialize(priority);
          rez.serialize(collective);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        // Wait to see if lhs changes
        done.wait();
      }
      else
        remote_tpl->record_issue_fill(tlid, lhs, expr, fields, 
                                      fill_value, fill_size, 
#ifdef LEGION_SPY
                                      fill_uid, handle, tree_id,
#endif
                                      precondition, pred_guard,
                                      unique_event, priority, collective);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_fill_inst(ApEvent lhs,
                                 IndexSpaceExpression *expr, 
                                 const UniqueInst &inst,
                                 const FieldMask &inst_mask,
                                 std::set<RtEvent> &applied_events,
                                 const bool reduction_initialization)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        const RtUserEvent done = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_FILL_INST);
          rez.serialize(done);
          rez.serialize(lhs);
          expr->pack_expression(rez, origin_space);
          inst.serialize(rez);
          rez.serialize(inst_mask);
          rez.serialize<bool>(reduction_initialization);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        applied_events.insert(done);
      }
      else
        remote_tpl->record_fill_inst(lhs, expr, inst, inst_mask,
                                     applied_events, reduction_initialization);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_op_inst(const TraceLocalID &tlid,
                                             unsigned parent_req_index,
                                             const UniqueInst &inst,
                                             RegionNode *node,
                                             const RegionUsage &usage,
                                             const FieldMask &user_mask,
                                             bool update_validity,
                                             std::set<RtEvent> &effects)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent applied = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_RECORD_OP_INST);
          rez.serialize(applied);
          tlid.serialize(rez);
          rez.serialize(parent_req_index);
          inst.serialize(rez);
          rez.serialize(node->handle);
          rez.serialize(usage);
          rez.serialize(user_mask);
          rez.serialize<bool>(update_validity);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        AutoLock a_lock(applied_lock);
        applied_events.insert(applied);
      }
      else
        remote_tpl->record_op_inst(tlid, parent_req_index, inst, node, usage,
                                   user_mask, update_validity, effects);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_set_op_sync_event(ApEvent &lhs, 
                                                       const TraceLocalID &tlid)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent done = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_SET_OP_SYNC);
          rez.serialize(done);
          tlid.serialize(rez);
          rez.serialize(&lhs);
          rez.serialize(lhs);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        // wait to see if lhs changes
        done.wait();
      }
      else
        remote_tpl->record_set_op_sync_event(lhs, tlid);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_mapper_output(const TraceLocalID &tlid,
                              const Mapper::MapTaskOutput &output,
                              const std::deque<InstanceSet> &physical_instances,
                              const std::vector<size_t> &future_size_bounds,
                              const std::vector<TaskTreeCoordinates> &coords,
                              std::set<RtEvent> &external_applied)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(output.future_locations.size() == future_size_bounds.size());
      assert(coords.size() == future_size_bounds.size());
#endif
      if (local_space != origin_space)
      {
        RtUserEvent applied = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_RECORD_MAPPER_OUTPUT);
          rez.serialize(applied);
          tlid.serialize(rez);
          // We actually only need a few things here  
          rez.serialize<size_t>(output.target_procs.size());
          for (unsigned idx = 0; idx < output.target_procs.size(); idx++)
            rez.serialize(output.target_procs[idx]);
          rez.serialize<size_t>(output.future_locations.size());
          for (unsigned idx = 0; idx < output.future_locations.size(); idx++)
            rez.serialize(output.future_locations[idx]);
          // Same size as the future locations
          for (unsigned idx = 0; idx < future_size_bounds.size(); idx++)
            rez.serialize(future_size_bounds[idx]);
          // Same size as the future locations
          for (unsigned idx = 0; idx < coords.size(); idx++)
          {
            const TaskTreeCoordinates &future_coordinates = coords[idx];
            rez.serialize<size_t>(future_coordinates.size());
            for (TaskTreeCoordinates::const_iterator it =
                  future_coordinates.begin(); it !=
                  future_coordinates.end(); it++)
              it->serialize(rez);
          }
          rez.serialize(output.chosen_variant);
          rez.serialize(output.task_priority);
          rez.serialize<bool>(output.postmap_task);
          rez.serialize<size_t>(physical_instances.size());
          for (std::deque<InstanceSet>::const_iterator it = 
               physical_instances.begin(); it != physical_instances.end(); it++)
            it->pack_references(rez);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        AutoLock a_lock(applied_lock);
        applied_events.insert(applied);
      }
      else
        remote_tpl->record_mapper_output(tlid, output, physical_instances,
                            future_size_bounds, coords, external_applied);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_complete_replay(const TraceLocalID &tlid, 
                    ApEvent pre, ApEvent post, std::set<RtEvent> &local_applied)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent applied = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_COMPLETE_REPLAY);
          rez.serialize(applied);
          tlid.serialize(rez);
          rez.serialize(pre);
          rez.serialize(post);
        }
        runtime->send_remote_trace_update(origin_space, rez);
        // Don't use the applied_events!
        local_applied.insert(applied);
      }
      else
        remote_tpl->record_complete_replay(tlid, pre, post, local_applied);
    }

    //--------------------------------------------------------------------------
    void RemoteTraceRecorder::record_reservations(const TraceLocalID &tlid,
                                 const std::map<Reservation,bool> &reservations,
                                 std::set<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      if (local_space != origin_space)
      {
        RtUserEvent done = Runtime::create_rt_user_event(); 
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(remote_tpl);
          rez.serialize(REMOTE_TRACE_ACQUIRE_RELEASE);
          rez.serialize(done);
          tlid.serialize(rez);
          rez.serialize<size_t>(reservations.size());
          for (std::map<Reservation,bool>::const_iterator it =
                reservations.begin(); it != reservations.end(); it++)
          {
            rez.serialize(it->first);
            rez.serialize<bool>(it->second);
          }
        }
        runtime->send_remote_trace_update(origin_space, rez);
        applied_events.insert(done);
      }
      else
        remote_tpl->record_reservations(tlid, reservations, applied_events); 
    }

    //--------------------------------------------------------------------------
    /*static*/ RemoteTraceRecorder* RemoteTraceRecorder::unpack_remote_recorder(
                Deserializer &derez, Runtime *runtime, const TraceLocalID &tlid)
    //--------------------------------------------------------------------------
    {
      AddressSpaceID origin_space;
      derez.deserialize(origin_space);
      PhysicalTemplate *remote_tpl;
      derez.deserialize(remote_tpl);
      RtUserEvent applied_event;
      derez.deserialize(applied_event);
      return new RemoteTraceRecorder(runtime, origin_space, 
                                     runtime->address_space, tlid,
                                     remote_tpl, applied_event);
    }

    //--------------------------------------------------------------------------
    /*static*/ void RemoteTraceRecorder::handle_remote_update(
                   Deserializer &derez, Runtime *runtime, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      PhysicalTemplate *tpl;
      derez.deserialize(tpl);
      RemoteTraceKind kind;
      derez.deserialize(kind);
      switch (kind)
      {
        case REMOTE_TRACE_RECORD_COMPLETION_EVENT:
          {
            RtUserEvent applied;
            derez.deserialize(applied);
            ApEvent lhs;
            derez.deserialize(lhs);
            unsigned op_kind;
            derez.deserialize(op_kind);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            tpl->record_completion_event(lhs, op_kind, tlid);
            Runtime::trigger_event(applied);
            break;
          }
        case REMOTE_TRACE_RECORD_REPLAY_MAPPING:
          {
            RtUserEvent applied;
            derez.deserialize(applied);
            ApEvent lhs;
            derez.deserialize(lhs);
            unsigned op_kind;
            derez.deserialize(op_kind);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            bool register_memo;
            derez.deserialize<bool>(register_memo);
            tpl->record_replay_mapping(lhs, op_kind, tlid, register_memo);
            Runtime::trigger_event(applied);
            break;
          }
        case REMOTE_TRACE_REQUEST_TERM_EVENT:
          {
            ApUserEvent *target;
            derez.deserialize(target);
            RtUserEvent ready;
            derez.deserialize(ready);
            ApUserEvent result;
            tpl->request_term_event(result);
#ifdef DEBUG_LEGION
            assert(result.exists());
#endif
            Serializer rez;
            {
              RezCheck z2(rez);
              rez.serialize(REMOTE_TRACE_REQUEST_TERM_EVENT);
              rez.serialize(target);
              rez.serialize(result);
              rez.serialize(ready);
            }
            runtime->send_remote_trace_response(source, rez);
            break;
          }
        case REMOTE_TRACE_CREATE_USER_EVENT:
          {
            RtUserEvent applied;
            derez.deserialize(applied);
            ApUserEvent *target;
            derez.deserialize(target);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            ApUserEvent result;
            tpl->record_create_ap_user_event(result, tlid);
#ifdef DEBUG_LEGION
            assert(result.exists());
#endif
            Serializer rez;
            {
              RezCheck z2(rez);
              rez.serialize(REMOTE_TRACE_CREATE_USER_EVENT);
              rez.serialize(target);
              rez.serialize(result);
              rez.serialize(applied);
            }
            runtime->send_remote_trace_response(source, rez);
            break;
          }
        case REMOTE_TRACE_TRIGGER_EVENT:
          {
            RtUserEvent applied;
            derez.deserialize(applied);
            ApUserEvent lhs;
            derez.deserialize(lhs);
            ApEvent rhs;
            derez.deserialize(rhs);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            tpl->record_trigger_event(lhs, rhs, tlid);
            Runtime::trigger_event(applied);
            break;
          }
        case REMOTE_TRACE_MERGE_EVENTS:
          {
            RtUserEvent done;
            derez.deserialize(done);
            ApEvent *event_ptr;
            derez.deserialize(event_ptr);
            ApEvent lhs;
            derez.deserialize(lhs);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            size_t num_rhs;
            derez.deserialize(num_rhs);
            const ApEvent lhs_copy = lhs;
            if (num_rhs == 2)
            {
              ApEvent e1, e2;
              derez.deserialize(e1);
              derez.deserialize(e2);
              tpl->record_merge_events(lhs, e1, e2, tlid);
            }
            else if (num_rhs == 3)
            {
              ApEvent e1, e2, e3;
              derez.deserialize(e1);
              derez.deserialize(e2);
              derez.deserialize(e3);
              tpl->record_merge_events(lhs, e1, e2, e3, tlid);
            }
            else
            {
              std::vector<ApEvent> rhs_events(num_rhs);
              for (unsigned idx = 0; idx < num_rhs; idx++)
              {
                ApEvent event;
                derez.deserialize(rhs_events[idx]);
              }
              tpl->record_merge_events(lhs, rhs_events, tlid);
            }
            if (lhs != lhs_copy)
            {
              Serializer rez;
              {
                RezCheck z2(rez);
                rez.serialize(REMOTE_TRACE_MERGE_EVENTS);
                rez.serialize(event_ptr);
                rez.serialize(lhs);
                rez.serialize(done);
              }
              runtime->send_remote_trace_response(source, rez);
            }
            else // didn't change so just trigger
              Runtime::trigger_event(done);
            break;
          }
        case REMOTE_TRACE_MERGE_PRED_EVENTS:
          {
            RtUserEvent done;
            derez.deserialize(done);
            PredEvent *event_ptr;
            derez.deserialize(event_ptr);
            PredEvent lhs, e1, e2;
            derez.deserialize(lhs);
            derez.deserialize(e1);
            derez.deserialize(e2);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            PredEvent lhs_copy = lhs;
            tpl->record_merge_events(lhs_copy, e1, e2, tlid);
            if (lhs != lhs_copy)
            {
              Serializer rez;
              {
                RezCheck z2(rez);
                rez.serialize(REMOTE_TRACE_MERGE_PRED_EVENTS);
                rez.serialize(event_ptr);
                rez.serialize(lhs);
                rez.serialize(done);
              }
              runtime->send_remote_trace_response(source, rez);
            }
            else // didn't change so just trigger
              Runtime::trigger_event(done);
            break;
          }
        case REMOTE_TRACE_ISSUE_COPY:
          {
            RtUserEvent done;
            derez.deserialize(done);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            ApUserEvent *lhs_ptr;
            derez.deserialize(lhs_ptr);
            ApUserEvent lhs;
            derez.deserialize(lhs);
            RegionTreeForest *forest = runtime->forest;
            IndexSpaceExpression *expr = 
              IndexSpaceExpression::unpack_expression(derez, forest, source);
            size_t num_fields;
            derez.deserialize(num_fields);
            std::vector<CopySrcDstField> src_fields(num_fields);
            std::vector<CopySrcDstField> dst_fields(num_fields);
            for (unsigned idx = 0; idx < num_fields; idx++)
            {
              unpack_src_dst_field(derez, src_fields[idx]);
              unpack_src_dst_field(derez, dst_fields[idx]);
            }
            size_t num_reservations;
            derez.deserialize(num_reservations);
            std::vector<Reservation> reservations(num_reservations);
            for (unsigned idx = 0; idx < num_reservations; idx++)
              derez.deserialize(reservations[idx]);
#ifdef LEGION_SPY
            RegionTreeID src_tree_id, dst_tree_id;
            derez.deserialize(src_tree_id);
            derez.deserialize(dst_tree_id);
#endif
            ApEvent precondition;
            derez.deserialize(precondition);
            PredEvent pred_guard;
            derez.deserialize(pred_guard);
            LgEvent src_unique, dst_unique;
            derez.deserialize(src_unique);
            derez.deserialize(dst_unique);
            int priority;
            derez.deserialize(priority);
            CollectiveKind collective;
            derez.deserialize(collective);
            // Use this to track if lhs changes
            const ApUserEvent lhs_copy = lhs;
            // Do the base call
            tpl->record_issue_copy(tlid, lhs, expr,
                                   src_fields, dst_fields, reservations,
#ifdef LEGION_SPY
                                   src_tree_id, dst_tree_id,
#endif
                                   precondition, pred_guard,
                                   src_unique, dst_unique,
                                   priority, collective);
            if (lhs != lhs_copy)
            {
              Serializer rez;
              {
                RezCheck z2(rez);
                rez.serialize(REMOTE_TRACE_ISSUE_COPY);
                rez.serialize(lhs_ptr);
                rez.serialize(lhs);
                rez.serialize(done);
              }
              runtime->send_remote_trace_response(source, rez);
            }
            else // lhs was unchanged
              Runtime::trigger_event(done);
            break;
          }
        case REMOTE_TRACE_COPY_INSTS:
          {
            RtUserEvent done;
            derez.deserialize(done);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            ApUserEvent lhs;
            derez.deserialize(lhs);
            unsigned src_idx, dst_idx;
            derez.deserialize(src_idx);
            derez.deserialize(dst_idx);
            PrivilegeMode src_mode, dst_mode;
            derez.deserialize(src_mode);
            derez.deserialize(dst_mode);
            RegionTreeForest *forest = runtime->forest;
            IndexSpaceExpression *expr =
              IndexSpaceExpression::unpack_expression(derez, forest, source);
            FieldMaskSet<InstanceView> tracing_srcs, tracing_dsts;
            UniqueInst src_inst, dst_inst;
            src_inst.deserialize(derez);
            dst_inst.deserialize(derez);
            FieldMask src_mask, dst_mask;
            derez.deserialize(src_mask);
            derez.deserialize(dst_mask);
            ReductionOpID redop;
            derez.deserialize(redop);
            std::set<RtEvent> ready_events;
            tpl->record_copy_insts(lhs, tlid, src_idx, dst_idx, expr,
                                   src_inst, dst_inst, src_mask, dst_mask,
                                   src_mode, dst_mode, redop, ready_events);
            if (!ready_events.empty())
              Runtime::trigger_event(done, Runtime::merge_events(ready_events));
            else
              Runtime::trigger_event(done);
            break;
          }
        case REMOTE_TRACE_ISSUE_FILL:
          {
            RtUserEvent done;
            derez.deserialize(done);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            ApUserEvent *lhs_ptr;
            derez.deserialize(lhs_ptr);
            ApUserEvent lhs;
            derez.deserialize(lhs);
            RegionTreeForest *forest = runtime->forest;
            IndexSpaceExpression *expr = 
              IndexSpaceExpression::unpack_expression(derez, forest, source);
            size_t num_fields;
            derez.deserialize(num_fields);
            std::vector<CopySrcDstField> fields(num_fields);
            for (unsigned idx = 0; idx < num_fields; idx++)
              unpack_src_dst_field(derez, fields[idx]);
            size_t fill_size;
            derez.deserialize(fill_size);
            const void *fill_value = derez.get_current_pointer();
            derez.advance_pointer(fill_size);
#ifdef LEGION_SPY
            UniqueID fill_uid;
            derez.deserialize(fill_uid);
            FieldSpace handle;
            derez.deserialize(handle);
            RegionTreeID tree_id;
            derez.deserialize(tree_id);
#endif
            ApEvent precondition;
            derez.deserialize(precondition);
            PredEvent pred_guard;
            derez.deserialize(pred_guard);
            LgEvent unique_event;
            derez.deserialize(unique_event);
            int priority;
            derez.deserialize(priority);
            CollectiveKind collective;
            derez.deserialize(collective);
            // Use this to track if lhs changes
            const ApUserEvent lhs_copy = lhs; 
            // Do the base call
            tpl->record_issue_fill(tlid, lhs, expr, fields,
                                   fill_value, fill_size,
#ifdef LEGION_SPY
                                   fill_uid, handle, tree_id,
#endif
                                   precondition, pred_guard,
                                   unique_event, priority, collective);
            if (lhs != lhs_copy)
            {
              Serializer rez;
              {
                RezCheck z2(rez);
                rez.serialize(REMOTE_TRACE_ISSUE_FILL);
                rez.serialize(lhs_ptr);
                rez.serialize(lhs);
                rez.serialize(done);
              }
              runtime->send_remote_trace_response(source, rez);
            }
            else // lhs was unchanged
              Runtime::trigger_event(done);
            break;
          }
        case REMOTE_TRACE_FILL_INST:
          {
            RtUserEvent done;
            derez.deserialize(done);
            ApUserEvent lhs;
            derez.deserialize(lhs);
            RegionTreeForest *forest = runtime->forest;
            IndexSpaceExpression *expr = 
              IndexSpaceExpression::unpack_expression(derez, forest, source);
            UniqueInst inst;
            inst.deserialize(derez);
            FieldMask inst_mask;
            derez.deserialize(inst_mask);
            bool reduction_initialization;
            derez.deserialize<bool>(reduction_initialization);
            std::set<RtEvent> ready_events;
            tpl->record_fill_inst(lhs, expr, inst, inst_mask,
                                  ready_events, reduction_initialization);
            if (!ready_events.empty())
              Runtime::trigger_event(done, Runtime::merge_events(ready_events));
            else
              Runtime::trigger_event(done);
            break;
          }
        case REMOTE_TRACE_RECORD_OP_INST:
          {
            RtUserEvent applied;
            derez.deserialize(applied);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            unsigned index;
            derez.deserialize(index);
            UniqueInst inst;
            inst.deserialize(derez);
            LogicalRegion handle;
            derez.deserialize(handle);
            RegionUsage usage;
            derez.deserialize(usage);
            FieldMask user_mask;
            derez.deserialize(user_mask);
            bool update_validity;
            derez.deserialize<bool>(update_validity);
            RegionNode *node = runtime->forest->get_node(handle);
            std::set<RtEvent> effects;
            tpl->record_op_inst(tlid, index, inst, node, usage,
                                user_mask, update_validity, effects);
            if (!effects.empty())
              Runtime::trigger_event(applied, Runtime::merge_events(effects));
            else
              Runtime::trigger_event(applied);
            break;
          }
        case REMOTE_TRACE_SET_OP_SYNC:
          {
            RtUserEvent done;
            derez.deserialize(done);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            ApUserEvent *lhs_ptr;
            derez.deserialize(lhs_ptr);
            ApUserEvent lhs;
            derez.deserialize(lhs);
            const ApUserEvent lhs_copy = lhs;
            tpl->record_set_op_sync_event(lhs, tlid);
            if (lhs != lhs_copy)
            {
              Serializer rez;
              {
                RezCheck z2(rez);
                rez.serialize(REMOTE_TRACE_SET_OP_SYNC);
                rez.serialize(lhs_ptr);
                rez.serialize(lhs);
                rez.serialize(done);
              }
              runtime->send_remote_trace_response(source, rez);
            }
            else // lhs didn't change
              Runtime::trigger_event(done);
            break;
          }
        case REMOTE_TRACE_RECORD_MAPPER_OUTPUT:
          {
            RtUserEvent applied;
            derez.deserialize(applied);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            size_t num_target_processors;
            derez.deserialize(num_target_processors);
            Mapper::MapTaskOutput output;
            output.target_procs.resize(num_target_processors);
            for (unsigned idx = 0; idx < num_target_processors; idx++)
              derez.deserialize(output.target_procs[idx]);
            size_t num_future_locations;
            derez.deserialize(num_future_locations);
            std::vector<size_t> future_size_bounds(num_future_locations);
            std::vector<TaskTreeCoordinates> coordinates(num_future_locations);
            if (num_future_locations > 0)
            {
              output.future_locations.resize(num_future_locations);
              for (unsigned idx = 0; idx < num_future_locations; idx++)
                derez.deserialize(output.future_locations[idx]);
              for (unsigned idx = 0; idx < num_future_locations; idx++)
                derez.deserialize(future_size_bounds[idx]);
              for (unsigned idx = 0; idx < num_future_locations; idx++)
              {
                TaskTreeCoordinates &coords = coordinates[idx];
                size_t num_coords;
                derez.deserialize(num_coords);
                coords.resize(num_coords);
                for (unsigned idx2 = 0; idx2 < num_coords; idx2++)
                  coords[idx2].deserialize(derez);
              }
            }
            derez.deserialize(output.chosen_variant);
            derez.deserialize(output.task_priority);
            derez.deserialize<bool>(output.postmap_task);
            size_t num_phy_instances;
            derez.deserialize(num_phy_instances);
            std::deque<InstanceSet> physical_instances(num_phy_instances);
            std::set<RtEvent> ready_events;
            for (unsigned idx = 0; idx < num_phy_instances; idx++)
              physical_instances[idx].unpack_references(runtime, derez,
                                                        ready_events);
            if (!ready_events.empty())
            {
              const RtEvent wait_on = Runtime::merge_events(ready_events);
              if (wait_on.exists() && !wait_on.has_triggered())
                wait_on.wait();
            }
            std::set<RtEvent> applied_events;
            tpl->record_mapper_output(tlid, output, physical_instances,
                      future_size_bounds, coordinates, applied_events);
            if (!applied_events.empty())
              Runtime::trigger_event(applied, 
                  Runtime::merge_events(applied_events));
            else
              Runtime::trigger_event(applied);
            break;
          }
        case REMOTE_TRACE_COMPLETE_REPLAY:
          {
            RtUserEvent applied;
            derez.deserialize(applied);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            ApEvent pre, post;
            derez.deserialize(pre);
            derez.deserialize(post);
            std::set<RtEvent> applied_events;
            tpl->record_complete_replay(tlid, pre, post, applied_events);
            if (!applied_events.empty())
              Runtime::trigger_event(applied,
                  Runtime::merge_events(applied_events));
            else
              Runtime::trigger_event(applied);
            break;
          }
        case REMOTE_TRACE_ACQUIRE_RELEASE:
          {
            RtUserEvent applied;
            derez.deserialize(applied);
            TraceLocalID tlid;
            tlid.deserialize(derez);
            size_t num_reservations;
            derez.deserialize(num_reservations);
            std::map<Reservation,bool> reservations;
            for (unsigned idx = 0; idx < num_reservations; idx++)
            {
              Reservation reservation;
              derez.deserialize(reservation);
              derez.deserialize<bool>(reservations[reservation]);
            }
            std::set<RtEvent> applied_events;
            tpl->record_reservations(tlid, reservations, applied_events);
            if (!applied_events.empty())
              Runtime::trigger_event(applied, 
                  Runtime::merge_events(applied_events));
            else
              Runtime::trigger_event(applied);
            break;
          }
        case REMOTE_TRACE_RECORD_BARRIER:
          {
            RtUserEvent done_event;
            derez.deserialize(done_event);
            ApBarrier barrier;
            derez.deserialize(barrier);
            size_t arrivals;
            derez.deserialize(arrivals);
            std::atomic<ShardID> *target;
            derez.deserialize(target);
            ShardID owner = tpl->record_managed_barrier(barrier, arrivals);
            Serializer rez;
            {
              RezCheck z2(rez);
              rez.serialize(REMOTE_TRACE_RECORD_BARRIER);
              rez.serialize(target);
              rez.serialize(owner);
              rez.serialize(done_event);
            }
            runtime->send_remote_trace_response(source, rez);
            break;
          }
        case REMOTE_TRACE_BARRIER_ARRIVAL:
          {
            RtUserEvent done_event;
            derez.deserialize(done_event);
            ApBarrier barrier;
            derez.deserialize(barrier);
            ApEvent pre;
            derez.deserialize(pre);
            size_t arrivals;
            derez.deserialize(arrivals);
            ShardID owner;
            derez.deserialize(owner);
            std::set<RtEvent> applied;
            tpl->record_barrier_arrival(barrier, pre, arrivals, applied, owner);
            if (!applied.empty())
              Runtime::trigger_event(done_event,
                  Runtime::merge_events(applied));
            else
              Runtime::trigger_event(done_event);
            break;
          }
        default:
          assert(false);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void RemoteTraceRecorder::handle_remote_response(
                                                            Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      RemoteTraceKind kind;
      derez.deserialize(kind);
      switch (kind)
      {
        case REMOTE_TRACE_REQUEST_TERM_EVENT:
        case REMOTE_TRACE_CREATE_USER_EVENT:
          {
            ApUserEvent *event_ptr;
            derez.deserialize(event_ptr);
            derez.deserialize(*event_ptr);
            RtUserEvent done;
            derez.deserialize(done);
            Runtime::trigger_event(done);
            break;
          }
        case REMOTE_TRACE_MERGE_EVENTS:
        case REMOTE_TRACE_ISSUE_COPY:
        case REMOTE_TRACE_ISSUE_FILL:
        case REMOTE_TRACE_SET_OP_SYNC:
          {
            ApEvent *event_ptr;
            derez.deserialize(event_ptr);
            derez.deserialize(*event_ptr);
            RtUserEvent done;
            derez.deserialize(done);
            Runtime::trigger_event(done);
            break;
          }
        case REMOTE_TRACE_MERGE_PRED_EVENTS:
          {
            PredEvent *event_ptr;
            derez.deserialize(event_ptr);
            derez.deserialize(*event_ptr);
            RtUserEvent done;
            derez.deserialize(done);
            Runtime::trigger_event(done);
            break;
          }
        case REMOTE_TRACE_RECORD_BARRIER:
          {
            std::atomic<ShardID> *target;
            derez.deserialize(target);
            ShardID owner;
            derez.deserialize(owner);
            RtUserEvent done;
            derez.deserialize(done);
            target->store(owner);
            Runtime::trigger_event(done);
            break;
          }
        default:
          assert(false);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void RemoteTraceRecorder::pack_src_dst_field(
                                  Serializer &rez, const CopySrcDstField &field)
    //--------------------------------------------------------------------------
    {
      RezCheck z(rez);
      rez.serialize(field.inst);
      rez.serialize(field.field_id);
      rez.serialize(field.size);
      rez.serialize(field.redop_id);
      rez.serialize<bool>(field.red_fold);
      rez.serialize(field.serdez_id);
      rez.serialize(field.subfield_offset);
      rez.serialize(field.indirect_index);
      rez.serialize(field.fill_data.indirect);
    }

    //--------------------------------------------------------------------------
    /*static*/ void RemoteTraceRecorder::unpack_src_dst_field(
                                    Deserializer &derez, CopySrcDstField &field)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      derez.deserialize(field.inst);
      derez.deserialize(field.field_id);
      derez.deserialize(field.size);
      derez.deserialize(field.redop_id);
      derez.deserialize<bool>(field.red_fold);
      derez.deserialize(field.serdez_id);
      derez.deserialize(field.subfield_offset);
      derez.deserialize(field.indirect_index);
      derez.deserialize(field.fill_data.indirect);
    }

    /////////////////////////////////////////////////////////////
    // TraceInfo
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    TraceInfo::TraceInfo(Operation *op)
      : rec(init_recorder(op)), tlid(init_tlid(op)),
        recording((rec == NULL) ? false : rec->is_recording())
    //--------------------------------------------------------------------------
    {
      if (rec != NULL)
        rec->add_recorder_reference();
    }

    //--------------------------------------------------------------------------
    /*static*/ PhysicalTraceRecorder* TraceInfo::init_recorder(Operation *op)
    //--------------------------------------------------------------------------
    {
      if (op == NULL)
        return NULL;
      MemoizableOp *memo = op->get_memoizable();
      if (memo == NULL)
        return NULL;
      return memo->get_template();
    }

    //--------------------------------------------------------------------------
    /*static*/ TraceLocalID TraceInfo::init_tlid(Operation *op)
    //--------------------------------------------------------------------------
    {
      if (op == NULL)
        return TraceLocalID();
      MemoizableOp *memo = op->get_memoizable();
      if (memo == NULL)
        return TraceLocalID();
      return memo->get_trace_local_id();
    }

    //--------------------------------------------------------------------------
    TraceInfo::TraceInfo(SingleTask *task, RemoteTraceRecorder *r)
      : rec(r), tlid(task->get_trace_local_id()), recording(rec != NULL)
    //--------------------------------------------------------------------------
    {
      if (recording)
        rec->add_recorder_reference();
    }

    //--------------------------------------------------------------------------
    TraceInfo::TraceInfo(const TraceInfo &rhs)
      : rec(rhs.rec), tlid(rhs.tlid), recording(rhs.recording)
    //--------------------------------------------------------------------------
    {
      if (rec != NULL)
        rec->add_recorder_reference();
    }

   //--------------------------------------------------------------------------
    TraceInfo::TraceInfo(PhysicalTraceRecorder *r, const TraceLocalID &tld)
      : rec(r), tlid(tld), recording((r != NULL) && r->is_recording())
    //--------------------------------------------------------------------------
    {
      if (rec != NULL)
        rec->add_recorder_reference();
    }

    //--------------------------------------------------------------------------
    TraceInfo::~TraceInfo(void)
    //--------------------------------------------------------------------------
    {
      if ((rec != NULL) && rec->remove_recorder_reference())
        delete rec;
    }

    /////////////////////////////////////////////////////////////
    // PhysicalTraceInfo
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PhysicalTraceInfo::PhysicalTraceInfo(Operation *o, unsigned idx)
      : TraceInfo(o), index(idx), dst_index(idx), update_validity(true)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PhysicalTraceInfo::PhysicalTraceInfo(const TraceInfo &info, 
                                         unsigned idx, bool update/*=true*/)
      : TraceInfo(info), index(idx), dst_index(idx), update_validity(update)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PhysicalTraceInfo::PhysicalTraceInfo(unsigned src_idx, 
                                         const TraceInfo &info,unsigned dst_idx)
      : TraceInfo(info), index(src_idx), dst_index(dst_idx), 
        update_validity(true)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PhysicalTraceInfo::PhysicalTraceInfo(const PhysicalTraceInfo &rhs)
      : TraceInfo(rhs), index(rhs.index), dst_index(rhs.dst_index),
        update_validity(rhs.update_validity)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PhysicalTraceInfo::PhysicalTraceInfo(const TraceLocalID &tlid,
                                         unsigned src_idx, unsigned dst_idx,
                                         bool update, PhysicalTraceRecorder *r)
      : TraceInfo(r, tlid), index(src_idx), dst_index(dst_idx),
        update_validity(update)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    void PhysicalTraceInfo::record_op_inst(const RegionUsage &usage,
                                           const FieldMask &user_mask,
                                           const UniqueInst &inst,
                                           RegionNode *node, Operation *op,
                                           std::set<RtEvent> &applied) const
    //--------------------------------------------------------------------------
    {
      sanity_check();
      rec->record_op_inst(tlid, op->find_parent_index(index), inst, node,
                          usage, user_mask, update_validity, applied);
    }

    //--------------------------------------------------------------------------
    void PhysicalTraceInfo::pack_trace_info(Serializer &rez,
                                            std::set<RtEvent> &applied) const 
    //--------------------------------------------------------------------------
    {
      rez.serialize<bool>(recording);
      if (recording)
      {
#ifdef DEBUG_LEGION
        assert(rec != NULL);
#endif
        tlid.serialize(rez);
        rez.serialize(index);
        rez.serialize(dst_index);
        rez.serialize<bool>(update_validity);
        rec->pack_recorder(rez, applied); 
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ PhysicalTraceInfo PhysicalTraceInfo::unpack_trace_info(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      bool recording;
      derez.deserialize<bool>(recording);
      if (recording)
      {
        TraceLocalID tlid;
        tlid.deserialize(derez);
        unsigned index, dst_index;
        derez.deserialize(index);
        derez.deserialize(dst_index);
        bool update_validity;
        derez.deserialize(update_validity);
        RemoteTraceRecorder *recorder = 
          RemoteTraceRecorder::unpack_remote_recorder(derez, runtime, tlid);
        return PhysicalTraceInfo(tlid, index, dst_index,
                                 update_validity, recorder);
      }
      else
        return PhysicalTraceInfo(NULL, -1U);
    }

    /////////////////////////////////////////////////////////////
    // ProjectionInfo
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ProjectionInfo::ProjectionInfo(Runtime *runtime, 
                                   const RegionRequirement *req,
                                   IndexSpaceNode *launch_space, 
                                   ShardingFunction *func,
                                   IndexSpace shard_space)
      : projection(nullptr),
        projection_type(LEGION_SINGULAR_PROJECTION),
        projection_space(nullptr),
        sharding_function(func),
        sharding_space(nullptr)
    //--------------------------------------------------------------------------
    {
      if (launch_space == nullptr)
        return;

      if (shard_space.exists())
        sharding_space = runtime->forest->get_node(shard_space);
      else if (func != nullptr) {
        sharding_space = launch_space;
      }

      // There is special logic here to handle the case of singular region 
      // requirements with sharding functions which we still want to treat as
      // projection region requirements for the logical analysis
#ifdef DEBUG_LEGION
      // Should always have a launch space with a sharding function
      assert((func == NULL) || (launch_space != NULL));
#endif
      if (req->handle_type == LEGION_SINGULAR_PROJECTION)
      {
        if (func != NULL)
        {
          // Treat single region requirements with sharding functions
          // as projections with the identity functor
          projection = runtime->find_projection_function(0/*identity*/);
          projection_type = LEGION_REGION_PROJECTION;
          projection_space = launch_space;
        }
        else
        {
          projection = NULL;
          projection_type = req->handle_type;
          projection_space = NULL;
        }
      }
      else
      {
        projection = runtime->find_projection_function(req->projection);
        projection_type = req->handle_type;
        projection_space = launch_space;
      }
    }

    //--------------------------------------------------------------------------
    bool ProjectionInfo::is_complete_projection(RegionTreeNode *node,
                                                const LogicalUser &user) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(is_projecting());
#endif
      return projection->is_complete(node, user.op, user.idx, projection_space);
    }

#if 0
    //--------------------------------------------------------------------------
    bool ProjectionInfo::can_elide_close_operation_symbolic(
                                      RegionTreeNode *node, LogicalState &state,
                                      const ProjectionSummary *previous) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      // should be in a projection mode
      assert(is_projecting() && is_sharding());
#endif
      // In order for us to prove that it safe to elide symbolically then 
      // we need to know that both projections are using projection id 0
      // which is one that we know how to interpret symbolically
      if ((projection->projection_id != 0) || 
          (previous->projection->projection_id != 0))
        return false;
      if (node->is_region())
      {
        // Both projections have all their points using the same logical
        // They only way to the close is if both projections have a single
        // point and the single point for each one maps to the same shard
        if (projection_space->get_volume() != 1)
          return false;
        if (previous->domain->get_volume() != 1)
          return false;
        Domain prev_domain, next_domain;
        previous->domain->get_launch_space_domain(prev_domain);
        projection_space->get_launch_space_domain(next_domain);
#ifdef DEBUG_LEGION
        assert(prev_domain.lo() == prev_domain.hi());
        assert(next_domain.lo() == next_domain.hi());
#endif
        Domain prev_sharding, next_sharding;
        if (previous->domain != previous->sharding_domain)
          previous->sharding_domain->get_launch_space_domain(prev_sharding);
        else
          prev_sharding = prev_domain;
        if (sharding_space != projection_space)
          sharding_space->get_launch_space_domain(next_sharding);
        else
          next_sharding = next_domain;
        const ShardID prev_shard =
          previous->sharding->find_owner(prev_domain.lo(), prev_sharding);
        const ShardID next_shard =
          sharding_function->find_owner(next_domain.lo(), next_sharding);
        return (prev_shard == next_shard);
      }
      else if (node->as_partition_node()->row_source->is_disjoint(false/*app*/))
      {
        // This is a disjoint partition, therefore the only way for points
        // to alias is if they are literally the same point since there is
        // a one-to-one mapping between points and subregions.
        // The first thing to check is if the sharding functions are the
        // same, if they are then we know that all the points that are
        // the same in each projection will map to the same shard and
        // therefore no fence will be required
        if (sharding_function == previous->sharding)
          return true;
        // Next check for any overlapping points between the launch
        // spaces, if there aren't any overlapping points then clearly
        // there will be no points that will alias with each other
        IndexSpaceExpression *overlap = node->context->intersect_index_spaces(
                                          projection_space, previous->domain);
        if (overlap->is_empty())
          return true;
        // Before doing the expensive test, see if we memoized this result
        bool result = true;
        ProjectionSummary summary(*this);
        if (state.find_symbolic_elide_close_result(*previous, summary, result))
          return result;
        // See if all the overlapping points shard the same way
        ApEvent ready;
        const Domain overlap_domain = overlap->get_domain(ready, true/*tight*/);
#ifdef DEBUG_LEGION
        assert(!ready.exists());
#endif
        Domain shard_domain_prev, shard_domain_next;
        sharding_space->get_launch_space_domain(shard_domain_next);
        previous->sharding_domain->get_launch_space_domain(shard_domain_prev);
        for (Domain::DomainPointIterator itr(overlap_domain); itr; itr++)
        {
          const ShardID prev = 
            previous->sharding->find_owner(*itr, shard_domain_prev);
          const ShardID next =
            sharding_function->find_owner(*itr, shard_domain_next);
          // If any points map to different shards that is game over
          if (prev == next)
            continue;
          result = false;
          break;
        }
        // Save the result with the state
        state.record_symbolic_elide_close_result(*previous, summary, result);
        // All the overlapping points map to the same shard so we're done
        return result;
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool ProjectionInfo::expensive_elide_test(RegionTreeNode *node,
                                              LogicalUser &user,
       const FieldMaskSet<LogicalUser> &prev_users, FieldMask &close_mask) const
    //--------------------------------------------------------------------------
    {
 #ifdef DEBUG_LEGION
      assert(!close_mask);
      assert(is_sharding());
      assert(is_projecting());
      ReplicateContext *repl_ctx = 
        dynamic_cast<ReplicateContext*>(user.op->get_context()); 
      assert(repl_ctx != NULL);
#else
      ReplicateContext *repl_ctx = 
        static_cast<ReplicateContext*>(user.op->get_context());
#endif
      const ShardID local_shard = repl_ctx->owner_shard->shard_id;
      // First build the projection tree for the new projection and then 
      // exchange it with all the other shards that are participating in
      // this logical analysis
      ProjectionTree *next = 
        projection->construct_projection_tree(user.op, user.idx, local_shard,
            node, projection_space, sharding_function, sharding_space);
      ElideCloseExchange exchange(next, repl_ctx, 
       repl_ctx->get_next_collective_index(COLLECTIVE_LOC_90, true/*logical*/));
      exchange.perform_collective_async();
      // While we're waiting for the exchange, build the projection trees
      // for all of our local previous projections
      // Sort the previous users into their field sets, then build a 
      // projection tree for each one of the field sets
      LegionList<FieldSet<LogicalUser*> > field_users;
      prev_users.compute_field_sets(FieldMask(), field_users);
      FieldMaskSet<ProjectionTree> projection_trees;
      IndexTreeNode *root_source = node->get_row_source();
      for (LegionList<FieldSet<LogicalUser*> >::const_iterator fit =
            field_users.begin(); fit != field_users.end(); fit++)
      {
        ProjectionTree *prev = new ProjectionTree(root_source);
        std::map<IndexTreeNode*,ProjectionTree*> node_map;
        node_map[root_source] = prev; 
        for (std::set<LogicalUser*>::const_iterator it =
              fit->elements.begin(); it != fit->elements.end(); it++)
        {
#ifdef DEBUG_LEGION
          assert((*it)->shard_proj != NULL);
#endif
          ProjectionSummary *summary = (*it)->shard_proj;
          (*it)->shard_proj->projection->construct_projection_tree(user.op,
              user.idx, local_shard, node, summary->domain, summary->sharding,
              summary->sharding_domain, node_map);
        }
        projection_trees.insert(prev, fit->set_mask);
      }
      MaskExchange<true/*union*/> close_mask_exchange(close_mask, repl_ctx,
       repl_ctx->get_next_collective_index(COLLECTIVE_LOC_93, true/*logical*/));
      // Get the result and check the invariants for all the new projection
      // points in the index space launch
      // We need to prove one of two things for every point
      // 1. It is disjoint from all our local projections, if that is
      // the case then it can't interfere with any local points. Note that
      // our analysis for disjointness is sound, but not precise since we
      // only do this analysis symbolically based on knowledge of the region
      // tree and not by testing all the regions for intersection. Therefore
      // we might still infer dependences where they don't need to be, but
      // that's ok because this is logical analysis. We're mainly just trying
      // to catch the cases here where we points use the same regions but
      // are found with different projection and sharding functions.
      // 2. If it is potentially interfering then it must come from the
      // same shard to ensure that mapping dependences are abided.
      exchange.perform_collective_wait(); 
      for (FieldMaskSet<ProjectionTree>::const_iterator it =
            projection_trees.begin(); it != projection_trees.end(); it++)
        if (next->interferes(it->first, local_shard))
          close_mask |= it->second;
      // Exchange the field mask between the shards
      close_mask_exchange.perform_collective_async();
      // Clean-up the projection trees while we're waiting for the
      // exchange to happen
      for (FieldMaskSet<ProjectionTree>::const_iterator it =
            projection_trees.begin(); it != projection_trees.end(); it++)
        delete it->first;
      delete next;
      close_mask_exchange.perform_collective_wait();
      return !close_mask;
    }
#endif

    /////////////////////////////////////////////////////////////
    // PathTraverser 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PathTraverser::PathTraverser(RegionTreePath &p)
      : path(p)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PathTraverser::PathTraverser(const PathTraverser &rhs)
      : path(rhs.path)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    PathTraverser::~PathTraverser(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PathTraverser& PathTraverser::operator=(const PathTraverser &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    bool PathTraverser::traverse(RegionTreeNode *node)
    //--------------------------------------------------------------------------
    {
      // Continue visiting nodes and then finding their children
      // until we have traversed the entire path.
      while (true)
      {
#ifdef DEBUG_LEGION
        assert(node != NULL);
#endif
        depth = node->get_depth();
        has_child = path.has_child(depth);
        if (has_child)
          next_child = path.get_child(depth);
        bool continue_traversal = node->visit_node(this);
        if (!continue_traversal)
          return false;
        if (!has_child)
          break;
        node = node->get_tree_child(next_child);
      }
      return true;
    }

    /////////////////////////////////////////////////////////////
    // CurrentInitializer 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    CurrentInitializer::CurrentInitializer(ContextID c)
      : ctx(c)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    CurrentInitializer::CurrentInitializer(const CurrentInitializer &rhs)
      : ctx(0)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    CurrentInitializer::~CurrentInitializer(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    CurrentInitializer& CurrentInitializer::operator=(
                                                  const CurrentInitializer &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    bool CurrentInitializer::visit_only_valid(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    bool CurrentInitializer::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      node->initialize_current_state(ctx); 
      return true;
    }

    //--------------------------------------------------------------------------
    bool CurrentInitializer::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      node->initialize_current_state(ctx);
      return true;
    }

    /////////////////////////////////////////////////////////////
    // CurrentInvalidator
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    CurrentInvalidator::CurrentInvalidator(ContextID c)
      : ctx(c)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    CurrentInvalidator::~CurrentInvalidator(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    bool CurrentInvalidator::visit_only_valid(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    bool CurrentInvalidator::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      node->invalidate_current_state(ctx); 
      return true;
    }

    //--------------------------------------------------------------------------
    bool CurrentInvalidator::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      node->invalidate_current_state(ctx);
      return true;
    }

    /////////////////////////////////////////////////////////////
    // DeletionInvalidator 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    DeletionInvalidator::DeletionInvalidator(ContextID c, const FieldMask &dm)
      : ctx(c), deletion_mask(dm)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    DeletionInvalidator::~DeletionInvalidator(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    bool DeletionInvalidator::visit_only_valid(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    bool DeletionInvalidator::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      node->invalidate_deleted_state(ctx, deletion_mask); 
      return true;
    }

    //--------------------------------------------------------------------------
    bool DeletionInvalidator::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      node->invalidate_deleted_state(ctx, deletion_mask);
      return true;
    }

    /////////////////////////////////////////////////////////////
    // ProjectionNode
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    void ProjectionNode::IntervalTree::add_child(LegionColor color)
    //--------------------------------------------------------------------------
    {
      add_range(color, color+1);
    }

    //--------------------------------------------------------------------------
    void ProjectionNode::IntervalTree::remove_child(LegionColor color)
    //--------------------------------------------------------------------------
    {
      // Don't contain the color
      if (ranges.empty())
        return;
      std::map<LegionColor,LegionColor>::iterator finder =
        ranges.upper_bound(color);
      // Don't contain the color
      if (finder == ranges.begin())
        return;
      // Get the interval right before
      finder = std::prev(finder);
      // Don't have the color in the previous interval
      if (finder->second <= color)
        return;
      // Start a new range if this wasn't the last color in the range
      if ((color+1) < finder->second)
        ranges[color+1] = finder->second;
      // See if it is at the start of the range in which case this is easy
      if (finder->first == color)
        // Remove the old range
        ranges.erase(finder);
      else
        // Update the range to end at the color which was removed
        finder->second = color;
    }

    //--------------------------------------------------------------------------
    void ProjectionNode::IntervalTree::add_range(LegionColor start,
                                                 LegionColor stop)
    //--------------------------------------------------------------------------
    {
      if (!ranges.empty())
      {
        // Find the first range that might contain the start point
        std::map<LegionColor,LegionColor>::iterator next = 
          ranges.upper_bound(start);
        if (next != ranges.begin())
        {
          std::map<LegionColor,LegionColor>::iterator prev = std::prev(next);
          // Note that prev->first <= start because of upper_bound
          // Start is contained in this interval so we can merge it
          if (start < prev->second)
          {
            // Interval contained in already existing range
            if (stop <= prev->second)
              return;
            start = prev->first;
            ranges.erase(prev);
          }
        }
        // Now merge forwards with any future ranges
        while ((next != ranges.end()) && (next->first <= stop))
        {
          if (stop < next->second)
            stop = next->second;
          std::map<LegionColor,LegionColor>::iterator to_delete = next++;
          ranges.erase(to_delete);
        }
      }
      // Add this range to the tree
      ranges[start] = stop;
    }

    //--------------------------------------------------------------------------
    bool ProjectionNode::IntervalTree::has_child(LegionColor color) const
    //--------------------------------------------------------------------------
    {
      if (ranges.empty())
        return false;
      // Find the first interval that starts after the color
      std::map<LegionColor,LegionColor>::const_iterator finder =
        ranges.upper_bound(color);
      // If it's the first interval, then we can't contain the color
      if (finder == ranges.begin())
        return false;
      // Otherwise step backwards to get the interval right before
      finder = std::prev(finder);
      // See if that interval includes it or not
      // Note that (finder->first <= color) is already implied by upper_bound
      return (color < finder->second);
    }

    //--------------------------------------------------------------------------
    void ProjectionNode::IntervalTree::serialize(Serializer &rez) const
    //--------------------------------------------------------------------------
    {
      rez.serialize<size_t>(ranges.size());
      for (std::map<LegionColor,LegionColor>::const_iterator it =
            ranges.begin(); it != ranges.end(); it++)
      {
        rez.serialize(it->first);
        rez.serialize(it->second);
      }
    }

    //--------------------------------------------------------------------------
    void ProjectionNode::IntervalTree::deserialize(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      size_t num_ranges;
      derez.deserialize(num_ranges);
      for (unsigned idx = 0; idx < num_ranges; idx++)
      {
        LegionColor start, end;
        derez.deserialize(start);
        derez.deserialize(end);
        add_range(start, end);
      }
    }

    //--------------------------------------------------------------------------
    ProjectionNode::ShardSet::ShardSet(void)
      : size(0), max(MAX_VALUES)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    ProjectionNode::ShardSet::~ShardSet(void)
    //--------------------------------------------------------------------------
    {
      if (max > MAX_VALUES)
        free(set.buffer);
    }

    //--------------------------------------------------------------------------
    void ProjectionNode::ShardSet::insert(ShardID shard, unsigned total_shards)
    //--------------------------------------------------------------------------
    {
      if (max == MAX_VALUES)
      {
        if (size == 0)
          set.values[size++] = shard;
        else if (!std::binary_search(&set.values[0], 
              &set.values[size], shard, std::less<ShardID>()))
        {
          if (size == MAX_VALUES)
          {
            // Need to increase to intermediate size
            unsigned new_max = max*2;
            unsigned bit_max = 
              (((total_shards + 7)/8) + sizeof(ShardID) - 1) / sizeof(ShardID);
            if (new_max < bit_max)
            {
              // Going to sparse buffer representation
              max = new_max;
              ShardID *buffer = (ShardID*)malloc(new_max * sizeof(ShardID));
              for (unsigned idx = 0; idx < MAX_VALUES; idx++)
                buffer[idx] = set.values[idx];
              buffer[size++] = shard;
              std::sort(buffer, buffer+size, std::less<ShardID>());
              set.buffer = buffer;
            }
            else
            {
              size = bit_max;
              max = total_shards + 1;
              ShardID *buffer = (ShardID*)malloc(bit_max * sizeof(ShardID));
              constexpr size_t power = STATIC_LOG2(sizeof(ShardID));
              for (unsigned idx = 0; idx < MAX_VALUES; idx++)
              {
                unsigned index = set.values[idx] >> power;
                buffer[idx] |= 
                  (1U << (set.values[index] & ((1U << power) - 1)));
              }
              unsigned index = shard >> power;
              buffer[index] |= (1U << (shard & ((1U << power) - 1)));
              set.buffer = buffer;
            }
          }
          else
          {
            set.values[size++] = shard;
            std::sort(&set.values[0], &set.values[size], std::less<ShardID>());
          }
        }
      }
      else if (total_shards < max)
      {
        // We're already in a bitmask mode
        constexpr size_t power = STATIC_LOG2(sizeof(ShardID));
        unsigned index = shard >> power;
        set.buffer[index] |= (1U << (shard & ((1U << power) - 1)));
      }
      else if (!std::binary_search(&set.buffer[0], &set.buffer[size],
                                   shard, std::less<ShardID>()))
      {
        if (size < max)
        {
          set.buffer[size++] = shard;
          std::sort(set.buffer, set.buffer+size, std::less<ShardID>());
        }
        else
        {
          unsigned new_max = max*2;
          unsigned bit_max = 
            (((total_shards + 7)/8) + sizeof(ShardID) - 1) / sizeof(ShardID);
          if (new_max < bit_max)
          {
            // Going to sparse buffer representation
            max = new_max;
            ShardID *buffer = (ShardID*)malloc(new_max * sizeof(ShardID));
            for (unsigned idx = 0; idx < size; idx++)
              buffer[idx] = set.buffer[idx];
            buffer[size++] = shard;
            std::sort(buffer, buffer+size, std::less<ShardID>());
            free(set.buffer);
            set.buffer = buffer;
          }
          else
          {
            max = total_shards + 1;
            ShardID *buffer = (ShardID*)malloc(bit_max * sizeof(ShardID));
            constexpr size_t power = STATIC_LOG2(sizeof(ShardID));
            for (unsigned idx = 0; idx < size; idx++)
            {
              unsigned index = set.buffer[idx] >> power;
              buffer[index] |= (1U << (set.buffer[idx] & ((1U << power) - 1)));
            }
            unsigned index = shard >> power;
            buffer[index] |= (1U << (shard & ((1U << power) - 1)));
            free(set.buffer);
            set.buffer = buffer;
            size = bit_max;
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    ShardID ProjectionNode::ShardSet::find_nearest_shard(ShardID local,
                                                    unsigned total_shards) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(size > 0);
#endif
      if (max == MAX_VALUES)
      {
        if (size == 1)
          return set.values[0];
        return find_nearest(local, total_shards, &set.values[0], size);
      }
      else if (total_shards < max)
      {
        constexpr size_t power = STATIC_LOG2(sizeof(ShardID));
        const unsigned bit_max =
          (((total_shards + 7)/8) + sizeof(ShardID) - 1) / sizeof(ShardID);
        // Find the next set bit both above and below
        ShardID lower = 0;
        {
          int index = local >> power;
          int offset = local & ((1U << power) - 1);
#ifdef DEBUG_LEGION
          // Shouldn't exist in the set
          assert(!(set.buffer[index] & (1U << offset)));
#endif
          offset--;
          while (true)
          {
            while (offset >= 0)
            {
              if (set.buffer[index] & (1U << offset))
              {
                lower = (index << power) + offset; 
                break;
              }
              else
                offset--;
            }
            if (offset < 0)
            {
              offset = ((1U << power) - 1);
              // Handle wrap around case
              if (--index < 0)
                index = bit_max - 1;
            }
            else
              break;
          }
        }
        ShardID upper = 0;
        {
          unsigned index = local >> power;
          unsigned offset = (local & ((1U << power) - 1)) + 1;
          while (true)
          {
            while (offset < (1U << power))
            {
              if (set.buffer[index] & (1U << offset))
              {
                upper = (index << power) + offset;
                break;
              }
              else
                offset++;
            }
            if (offset == (1U << power))
            {
              offset = 0;
              // Handle wrap around case
              if (++index == bit_max)
                index = 0;
            }
            else
              break;
          }
        }
        unsigned lower_dist = find_distance(lower, local, total_shards);
        unsigned upper_dist = find_distance(local, upper, total_shards);
        if (lower_dist < upper_dist)
          return lower;
        else
          return upper;
      }
      else
        return find_nearest(local, total_shards, set.buffer, size);
    }

    //--------------------------------------------------------------------------
    ShardID ProjectionNode::ShardSet::find_nearest(ShardID local,
       unsigned total_shards, const ShardID *buffer, unsigned buffer_size) const
    //--------------------------------------------------------------------------
    {
      // Find the upper bound if it exists
      unsigned upper = 0;
      unsigned count = buffer_size;
      while (count > 0)
      {
        unsigned step = count / 2;
        if (local >= buffer[upper+step])
        {
          upper = step + 1;
          count -= step + 1;
        }
        else
          count = step;
      }
#ifdef DEBUG_LEGION
      assert(upper <= buffer_size);
#endif
      // Check to see if the upper bound exists or not
      unsigned lower = 0;
      if (upper == buffer_size)
      {
#ifdef DEBUG_LEGION
        assert(buffer[upper-1] < local);
#endif
        lower = buffer_size-1;
        upper = 0;
      }
      else if (upper == 0)
      {
#ifdef DEBUG_LEGION
        assert(local < buffer[0]);
#endif
        lower = buffer_size-1;
      }
      else
      {
        lower = buffer[upper-1];  
#ifdef DEBUG_LEGION
        assert(buffer[lower] < local);
        assert(local < buffer[upper]);
#endif
      }
      // Figure out which of the two is closer
      unsigned lower_dist = find_distance(buffer[lower], local, total_shards);
      unsigned upper_dist = find_distance(local, buffer[upper], total_shards);
      if (lower_dist < upper_dist)
        return buffer[lower];
      else
        return buffer[upper];
    }

    //--------------------------------------------------------------------------
    /*static*/ unsigned ProjectionNode::ShardSet::find_distance(
                                ShardID one, ShardID two, unsigned total_shards)
    //--------------------------------------------------------------------------
    {
      unsigned abs_diff = (one < two) ? (two - one) : (one - two);
      // closest distance with wrap around
      if (abs_diff < (total_shards / 2))
        return abs_diff;
      else
        return (total_shards - abs_diff);
    }

    //--------------------------------------------------------------------------
    void ProjectionNode::ShardSet::serialize(Serializer &rez,
                                             unsigned total_shards) const
    //--------------------------------------------------------------------------
    {
      rez.serialize(max);
      if (max == MAX_VALUES)
      {
        rez.serialize(size);
        for (unsigned idx = 0; idx < size; idx++)
          rez.serialize(set.values[idx]);
      }
      else if (total_shards < max)
      {
        for (unsigned idx = 0; idx < size; idx++)
          rez.serialize(set.buffer[idx]);
      }
      else
      {
        rez.serialize(size);
        for (unsigned idx = 0; idx < size; idx++)
          rez.serialize(set.buffer[idx]);
      }
    }

    //--------------------------------------------------------------------------
    void ProjectionNode::ShardSet::deserialize(Deserializer &derez,
                                               unsigned total_shards)
    //--------------------------------------------------------------------------
    {
      unsigned dmax;
      derez.deserialize(dmax);
      if (dmax == MAX_VALUES)
      {
        unsigned dsize;
        derez.deserialize(dsize);
        for (unsigned idx = 0; idx < dsize; idx++)
        {
          ShardID shard;
          derez.deserialize(shard);
          insert(shard, total_shards);
        }
      }
      else if (total_shards < dmax)
      {
        if (total_shards < max)
        {
          for (unsigned idx = 0; idx < size; idx++)
          {
            ShardID bits;
            derez.deserialize(bits);
            set.buffer[idx] |= bits;
          }
        }
        else
        {
          unsigned bit_max = 
            (((total_shards + 7)/8) + sizeof(ShardID) - 1) / sizeof(ShardID);
          ShardID *buffer = (ShardID*)malloc(bit_max * sizeof(ShardID));
          for (unsigned idx = 0; idx < bit_max; idx++)
            derez.deserialize(buffer[idx]);
#ifdef DEBUG_LEGION
          assert(max < bit_max);
#endif
          constexpr size_t power = STATIC_LOG2(sizeof(ShardID));
          if (max == MAX_VALUES)
          {
            for (unsigned idx = 0; idx < size; idx++)
            {
              unsigned index = set.values[idx] >> power;
              buffer[index] |= (1U << (set.values[idx] & ((1U << power) - 1)));
            }
          }
          else
          {
            for (unsigned idx = 0; idx < size; idx++)
            {
              unsigned index = set.buffer[idx] >> power;
              buffer[index] |= (1U << (set.buffer[idx] & ((1U << power) - 1)));
            }
            free(set.buffer);
          }
          max = total_shards + 1;
          size = bit_max;
          set.buffer = buffer;
        }
      }
      else
      {
        unsigned dsize;
        derez.deserialize(dsize);
        for (unsigned idx = 0; idx < dsize; idx++)
        {
          ShardID shard;
          derez.deserialize(shard);
          insert(shard, total_shards);
        }
      }
    }

    /////////////////////////////////////////////////////////////
    // ProjectionRegion
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ProjectionRegion::ProjectionRegion(RegionNode *node)
      : region(node)
    //--------------------------------------------------------------------------
    {
      region->add_base_gc_ref(PROJECTION_REF);
    }

    //--------------------------------------------------------------------------
    ProjectionRegion::~ProjectionRegion(void)
    //--------------------------------------------------------------------------
    {
      for (std::unordered_map<LegionColor,ProjectionPartition*>::const_iterator
            it = local_children.begin(); it != local_children.end(); it++)
        if (it->second->remove_reference())
          delete it->second;
      if (region->remove_base_gc_ref(PROJECTION_REF))
        delete region;
    }

    //--------------------------------------------------------------------------
    bool ProjectionRegion::is_disjoint(void) const
    //--------------------------------------------------------------------------
    {
      if (local_children.size() > 1)
        return false;
      if (!local_children.empty() && 
          !local_children.begin()->second->is_disjoint())
        return false;
      return true;
    }

#if 0
    //--------------------------------------------------------------------------
    bool ProjectionRegion::is_complete(void) const
    //--------------------------------------------------------------------------
    {
      for (std::unordered_map<LegionColor,ProjectionPartition*>::const_iterator
            it = local_children.begin(); it != local_children.end(); it++)
        if (!it->second->is_complete())
          return false;
      return true;
    }
#endif

    //--------------------------------------------------------------------------
    bool ProjectionRegion::is_leaves_only(void) const
    //--------------------------------------------------------------------------
    {
      if (shard_users.empty())
      {
        for (std::unordered_map<LegionColor,
                                ProjectionPartition*>::const_iterator
              it = local_children.begin(); it != local_children.end(); it++)
          if (!it->second->is_leaves_only())
            return false;
        return true;
      }
      else
        return local_children.empty();
    }

    //--------------------------------------------------------------------------
    bool ProjectionRegion::is_unique_shards(void) const
    //--------------------------------------------------------------------------
    {
      if (shard_users.size() > 1)
        return false;
      for (std::unordered_map<LegionColor,ProjectionPartition*>::const_iterator
            it = local_children.begin(); it != local_children.end(); it++)
        if (!it->second->is_unique_shards())
          return false;
      return true;
    }

    //--------------------------------------------------------------------------
    bool ProjectionRegion::interferes(ProjectionNode *other,
                                      ShardID local_shard) const 
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      ProjectionRegion *rhs = dynamic_cast<ProjectionRegion*>(other);
      assert(rhs != NULL);
      assert(region == rhs->region);
      return has_interference(rhs, local_shard);
#else
      return has_interference(static_cast<ProjectionRegion*>(other),
                              local_shard);
#endif
    }

    //--------------------------------------------------------------------------
    void ProjectionRegion::extract_shard_summaries(bool supports_name_based,
        ShardID local_shard, size_t total_shards,
        std::map<LogicalRegion,RegionSummary> &region_summaries,
        std::map<LogicalPartition,PartitionSummary> &partition_summaries) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(region_summaries.find(region->handle) == region_summaries.end());
#endif
      RegionSummary &summary = region_summaries[region->handle];
      summary.users = shard_users;
      for (std::unordered_map<LegionColor,ProjectionPartition*>::const_iterator
            it = local_children.begin(); it != local_children.end(); it++)
      {
        summary.children.add_child(it->first);
        it->second->extract_shard_summaries(supports_name_based, local_shard,
            total_shards, region_summaries, partition_summaries);
      }
    }

    //--------------------------------------------------------------------------
    void ProjectionRegion::update_shard_summaries(bool supports_name_based, 
        ShardID local_shard, size_t total_shards,
        std::map<LogicalRegion,RegionSummary> &region_summaries,
        std::map<LogicalPartition,PartitionSummary> &partition_summaries)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(region_summaries.find(region->handle) != region_summaries.end());
#endif
      RegionSummary &summary = region_summaries[region->handle];
      shard_users.swap(summary.users);
      shard_children.swap(summary.children);
      if (supports_name_based && !shard_children.empty())
      {
        // Check to see if we have a child but haven't recorded a local
        // one in which case we need to make a child for the partition
        // locally and unpack it so we can have the knowledge of which
        // shards know about the subregions of the partition
#ifdef DEBUG_LEGION
        assert(shard_children.ranges.size() == 1);
#endif
        std::map<LegionColor,LegionColor>::const_iterator it =
          shard_children.ranges.begin();
#ifdef DEBUG_LEGION
        // Should only be one color
        assert((it->first + 1) == it->second);
#endif
        if (local_children.empty())
          local_children[it->first] =
            new ProjectionPartition(region->get_child(it->first));
#ifdef DEBUG_LEGION
        else
          assert(local_children.find(it->first) != local_children.end());
        assert(local_children.size() == 1);
#endif
      }
      // Remove all our local children from the shard children
      for (std::unordered_map<LegionColor,ProjectionPartition*>::const_iterator
            it = local_children.begin(); it != local_children.end(); it++)
      {
        shard_children.remove_child(it->first);
        it->second->update_shard_summaries(supports_name_based, local_shard,
            total_shards, region_summaries, partition_summaries);
      }
    }

    //--------------------------------------------------------------------------
    bool ProjectionRegion::has_interference(ProjectionRegion *other,
                                            ShardID local_shard) const
    //--------------------------------------------------------------------------
    {
      // If either one has more than one shard ID then we're done
      if ((shard_users.size() > 1) || (other->shard_users.size() > 1))
        return true;
      if (!shard_users.empty() && (shard_users.back() != local_shard))
        return true;
      if (!other->shard_users.empty() &&
          (other->shard_users.back() != local_shard))
        return true;
      // If either has any shard children we're immediately interfering 
      if (!shard_children.empty() || !other->shard_children.empty())
        return true;
      // If we have different numbers of partitions then we are definitely
      // going ot be interfering on something
      if (local_children.size() != other->local_children.size())
        return true;
      for (std::unordered_map<LegionColor,ProjectionPartition*>::const_iterator
            it = local_children.begin(); it != local_children.end(); it++)
      {
        std::unordered_map<LegionColor,ProjectionPartition*>::const_iterator
          finder = other->local_children.find(it->first);
        if (finder == other->local_children.end())
          return true;
        if (it->second->has_interference(finder->second, local_shard))
          return true;
      }
      return false;
    }

    //--------------------------------------------------------------------------
    void ProjectionRegion::add_user(ShardID user)
    //--------------------------------------------------------------------------
    {
      if (std::binary_search(shard_users.begin(), shard_users.end(), user))
        return;
      shard_users.push_back(user);
      std::sort(shard_users.begin(), shard_users.end());
    }

    //--------------------------------------------------------------------------
    void ProjectionRegion::add_child(ProjectionPartition *child)
    //--------------------------------------------------------------------------
    {
      LegionColor color = child->partition->row_source->color;
      if (local_children.insert(std::make_pair(color, child)).second)
        child->add_reference();
    }

#if 0
    //--------------------------------------------------------------------------
    RefinementNode* ProjectionRegion::create_refinement(void) const
    //--------------------------------------------------------------------------
    {
      if (!local_children.empty())
      {
#ifdef DEBUG_LEGION
        // Should only have one child right now
        assert(local_children.size() == 1);
#endif
        std::unordered_map<LegionColor,ProjectionPartition*>::const_iterator
          child = local_children.begin();
        return new RegionRefinementNode(region,
            child->second->create_refinement()->as_partition_refinement());
      }
      else
      {
        RegionRefinementNode *result = new RegionRefinementNode(region);
        result->refining_shards = shard_users;
        return result;
      }
    }

    //--------------------------------------------------------------------------
    void ProjectionRegion::convert(
                             const std::vector<ShardID> *shard_to_shard_mapping)
    //--------------------------------------------------------------------------
    {
      // If we're converting, we don't need the shard users anymore
      if (!shard_users.empty())
        shard_users.clear();
      for (std::unordered_map<LegionColor,ProjectionPartition*>::const_iterator
            it = local_children.begin(); it != local_children.end(); it++)
        it->second->convert(shard_to_shard_mapping);
    }

    //--------------------------------------------------------------------------
    void ProjectionRegion::merge(ProjectionRegion *rhs,
                             const std::vector<ShardID> *shard_to_shard_mapping)
    //--------------------------------------------------------------------------
    {
      for (std::unordered_map<LegionColor,ProjectionPartition*>::const_iterator
            it = rhs->local_children.begin(); 
            it != rhs->local_children.end(); it++)
      {
        std::unordered_map<LegionColor,ProjectionPartition*>::const_iterator
          finder = local_children.find(it->first);
        if (finder == local_children.end())
        {
          // Do the conversion over
          it->second->convert(shard_to_shard_mapping);
          add_child(it->second);
        }
        else
          finder->second->merge(it->second, shard_to_shard_mapping);
      }
#ifdef DEBUG_LEGION
      // Should already have been converted and cleared this
      assert(shard_users.empty());
#endif
      // No need to merge over the shard users since we don't need them
    }

    //--------------------------------------------------------------------------
    void ProjectionRegion::pack(Serializer &rez) const
    //--------------------------------------------------------------------------
    {
      rez.serialize<size_t>(local_children.size());
      for (std::unordered_map<LegionColor,ProjectionPartition*>::const_iterator
            it = local_children.begin(); it != local_children.end(); it++)
      {
        rez.serialize(it->first);
        it->second->pack(rez);
      }
      rez.serialize<size_t>(shard_users.size());
      for (unsigned idx = 0; idx < shard_users.size(); idx++)
        rez.serialize(shard_users[idx]);
    }

    //--------------------------------------------------------------------------
    void ProjectionRegion::unpack(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      size_t num_children;
      derez.deserialize(num_children);
      for (unsigned idx = 0; idx < num_children; idx++)
      {
        LegionColor color;
        derez.deserialize(color);
        PartitionNode *partition = region->get_child(color);
        ProjectionPartition *child = new ProjectionPartition(partition);
        child->unpack(derez);
        local_children[color] = child;
        child->add_reference();
      }
      size_t num_users;
      derez.deserialize(num_users);
      shard_users.resize(num_users);
      for (unsigned idx = 0; idx < num_users; idx++)
        derez.deserialize(shard_users[idx]);
    }
#endif

    /////////////////////////////////////////////////////////////
    // ProjectionPartition
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ProjectionPartition::ProjectionPartition(PartitionNode *node, 
                                             ShardedColorMap *map)
      : partition(node), name_based_children_shards(map)
    //--------------------------------------------------------------------------
    {
      partition->add_base_gc_ref(PROJECTION_REF);
      if (name_based_children_shards != NULL)
        name_based_children_shards->add_reference();
    }

    //--------------------------------------------------------------------------
    ProjectionPartition::~ProjectionPartition(void)
    //--------------------------------------------------------------------------
    {
      for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator 
            it = local_children.begin(); it != local_children.end(); it++)
        if (it->second->remove_reference())
          delete it->second;
      if (partition->remove_base_gc_ref(PROJECTION_REF))
        delete partition;
      if ((name_based_children_shards != NULL) &&
          name_based_children_shards->remove_reference())
        delete name_based_children_shards;
    }

    //--------------------------------------------------------------------------
    bool ProjectionPartition::is_disjoint(void) const
    //--------------------------------------------------------------------------
    {
      if (!partition->row_source->is_disjoint(false/*from app*/))
        return false;
      for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator 
            it = local_children.begin(); it != local_children.end(); it++)
        if (!it->second->is_disjoint())
          return false;
      return true;
    }

#if 0
    //--------------------------------------------------------------------------
    bool ProjectionPartition::is_complete(void) const
    //--------------------------------------------------------------------------
    {
      if (!partition->row_source->is_complete(false/*from app*/))
        return false;
      for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator 
            it = local_children.begin(); it != local_children.end(); it++)
        if (!it->second->is_complete())
          return false;
      return true;
    }
#endif

    //--------------------------------------------------------------------------
    bool ProjectionPartition::is_leaves_only(void) const
    //--------------------------------------------------------------------------
    {
      for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator 
            it = local_children.begin(); it != local_children.end(); it++)
        if (!it->second->is_leaves_only())
          return false;
      return true;
    }

    //--------------------------------------------------------------------------
    bool ProjectionPartition::is_unique_shards(void) const
    //--------------------------------------------------------------------------
    {
      for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator 
            it = local_children.begin(); it != local_children.end(); it++)
        if (!it->second->is_unique_shards())
          return false;
      return true;
    }

    //--------------------------------------------------------------------------
    bool ProjectionPartition::interferes(ProjectionNode *other,
                                         ShardID local_shard) const 
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      ProjectionPartition *rhs = dynamic_cast<ProjectionPartition*>(other);
      assert(rhs != NULL);
      assert(partition == rhs->partition);
      return has_interference(rhs, local_shard);
#else
      return has_interference(static_cast<ProjectionPartition*>(other),
                              local_shard);
#endif
    }

    //--------------------------------------------------------------------------
    void ProjectionPartition::extract_shard_summaries(bool supports_name_based,
        ShardID local_shard, size_t total_shards,
        std::map<LogicalRegion,RegionSummary> &region_summaries,
        std::map<LogicalPartition,PartitionSummary> &partition_summaries) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(partition_summaries.find(partition->handle) ==
              partition_summaries.end());
#endif
      PartitionSummary &summary = partition_summaries[partition->handle];
      for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator
            it = local_children.begin(); it != local_children.end(); it++)
      {
        summary.children.add_child(it->first);
        it->second->extract_shard_summaries(supports_name_based, local_shard,
            total_shards, region_summaries, partition_summaries);
      }
      if (supports_name_based)
      {
        // Record that we know about all of our local children
        for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator
              it = local_children.begin(); it != local_children.end(); it++)
          summary.disjoint_complete_child_shards[it->first].insert(local_shard, 
                                                                  total_shards);
      }
    }

    //--------------------------------------------------------------------------
    void ProjectionPartition::update_shard_summaries(bool supports_name_based,
        ShardID local_shard, size_t total_shards,
        std::map<LogicalRegion,RegionSummary> &region_summaries,
        std::map<LogicalPartition,PartitionSummary> &partition_summaries)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(partition_summaries.find(partition->handle) != 
              partition_summaries.end());
#endif
      PartitionSummary &summary = partition_summaries[partition->handle];
      shard_children.swap(summary.children);
      // Remove all our local children from the shard children
      for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator 
            it = local_children.begin(); it != local_children.end(); it++)
      {
        shard_children.remove_child(it->first);
        it->second->update_shard_summaries(supports_name_based, local_shard,
            total_shards, region_summaries, partition_summaries);
      }
      if (supports_name_based &&
          (local_children.size() < partition->get_num_children()))
      {
        // If necessary create a sharded color map for any children
        // which we don't know about locally so we know the nearest
        // shard which does have some information about it
        std::unordered_map<LegionColor,ShardID> nearest_shards;  
        for (std::unordered_map<LegionColor,ShardSet>::const_iterator it =
              summary.disjoint_complete_child_shards.begin(); it !=
              summary.disjoint_complete_child_shards.end(); it++)
        {
          if (local_children.find(it->first) != local_children.end())
            continue;
          nearest_shards[it->first] = 
            it->second.find_nearest_shard(local_shard, total_shards);
        }
#if 0
        std::multimap<LegionColor,ShardID>::const_iterator lower =
          summary.disjoint_complete_child_shards.begin();
        while (lower != summary.disjoint_complete_child_shards.end())
        {
          std::multimap<LegionColor,ShardID>::const_iterator  upper = lower;
          while ((upper != summary.disjoint_complete_child_shards.end()) &&
                  (lower->first == upper->first))
            upper++;
#ifdef DEBUG_LEGION
          assert(lower != upper);
#endif
          // Skip if we already have a local child for it
          if (local_children.find(lower->first) == local_children.end())
          {
            // Find the shard closest to our local shard including wrap around
            ShardID closest_shard = 0;
            size_t closest_distance = total_shards;
            for (std::multimap<LegionColor,ShardID>::const_iterator it =
                  lower; it != upper; it++)
            {
#ifdef DEBUG_LEGION
              assert(it->second != local_shard);
#endif
              size_t abs_diff = (local_shard < it->second) ? 
                (it->second - local_shard) : (local_shard - it->second);
              // closest distance by shard ID with wrap around
              size_t distance = (abs_diff < (total_shards/2)) ? 
                abs_diff : total_shards - abs_diff;
              if (distance < closest_distance)
              {
                closest_shard = it->second;
                closest_distance = distance;
              }
            }
#ifdef DEBUG_LEGION
            assert(closest_distance != total_shards);
#endif
            nearest_shards[lower->first] = closest_shard;
          }
          lower = upper;
        }
#endif
        // Now we can make our ShardedColorMap and save it
#ifdef DEBUG_LEGION
        assert(name_based_children_shards == NULL);
#endif
        name_based_children_shards =
          new ShardedColorMap(std::move(nearest_shards));
        name_based_children_shards->add_reference();
      }
    }

    //--------------------------------------------------------------------------
    bool ProjectionPartition::has_interference(ProjectionPartition *other,
                                               ShardID local_shard) const
    //--------------------------------------------------------------------------
    {
      if (partition->row_source->is_disjoint(false/*from app*/))
      {
        // Disjoint partition, check all the children against each other
        for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator
              it = local_children.begin(); it != local_children.end(); it++)
        {
          std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator
            finder = other->local_children.find(it->first);
          if (finder == other->local_children.end())
          {
            // Check to see if there is a remote shard with that child
            if (other->shard_children.has_child(it->first))
              return true;
          }
          else if (it->second->has_interference(finder->second, local_shard))
            return true;
        }
        // Check in the opposite direction too
        for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator
              it = other->local_children.begin();
              it != other->local_children.end(); it++)
        {
          std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator
            finder = local_children.find(it->first);
          if (finder == local_children.end())
          {
            if (shard_children.has_child(it->first))
              return true;
          }
          // Else we've already done interferences with ourself locally
        }
        return false;
      }
      else
      {
        // Aliased partition
        // If either has any shard children we're immediately interfering 
        if (!shard_children.empty() || !other->shard_children.empty())
          return true;
        // If we have different numbers of partitions then we are definitely
        // going to be interfering on something
        if (local_children.size() != other->local_children.size())
          return true;
        for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator
              it = local_children.begin(); it != local_children.end(); it++)
        {
          std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator
            finder = other->local_children.find(it->first);
          if (finder == other->local_children.end())
            return true;
          if (it->second->has_interference(finder->second, local_shard))
            return true;
        }
        return false;
      }
    }

    //--------------------------------------------------------------------------
    void ProjectionPartition::add_child(ProjectionRegion *child)
    //--------------------------------------------------------------------------
    {
      LegionColor color = child->region->row_source->color;
      if (local_children.insert(std::make_pair(color, child)).second)
        child->add_reference();
    }

#if 0
    //--------------------------------------------------------------------------
    RefinementNode* ProjectionPartition::create_refinement(void) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(partition->row_source->is_disjoint(false/*from app*/));
      assert(partition->row_source->is_complete(false/*from app*/));
      assert((local_children.size() + disjoint_complete_children_shards->size())
          == partition->get_num_children());
#endif
      PartitionRefinementNode *refinement = 
       new PartitionRefinementNode(partition,disjoint_complete_children_shards);
      for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator 
            it = local_children.begin(); it != local_children.end(); it++)
        refinement->children[it->first] =
          it->second->create_refinement()->as_region_refinement();
      return refinement;
    }

    //--------------------------------------------------------------------------
    void ProjectionPartition::convert(
                             const std::vector<ShardID> *shard_to_shard_mapping)
    //--------------------------------------------------------------------------
    {
      if (disjoint_complete_children_shards != NULL)
      {
        if ((shard_to_shard_mapping == NULL) || 
            !shard_to_shard_mapping->empty())
        {
          std::unordered_map<LegionColor,ShardID> new_mapping;
          if (shard_to_shard_mapping != NULL)
          {
            // Make a new mapping
            for (std::unordered_map<LegionColor,ShardID>::const_iterator it =
                  disjoint_complete_children_shards->color_shards.begin(); it !=
                  disjoint_complete_children_shards->color_shards.end(); it++)
            {
#ifdef DEBUG_LEGION
              assert(it->second < shard_to_shard_mapping->size());
#endif
              new_mapping[it->first] = shard_to_shard_mapping->at(it->second);
            }
          }
          if (disjoint_complete_children_shards->remove_reference())
            delete disjoint_complete_children_shards;
          if (!new_mapping.empty())
          {
            disjoint_complete_children_shards = 
              new ShardedColorMap(std::move(new_mapping)); 
            disjoint_complete_children_shards->add_reference();
          }
          else
            disjoint_complete_children_shards = NULL;
        }
        // else the identity case means we don't need to do anything
      }
      for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator
            it = local_children.begin(); it != local_children.end(); it++)
        it->second->convert(shard_to_shard_mapping);
    }

    //--------------------------------------------------------------------------
    void ProjectionPartition::merge(ProjectionPartition *rhs,
                             const std::vector<ShardID> *shard_to_shard_mapping)
    //--------------------------------------------------------------------------
    {
      std::unordered_map<LegionColor,ShardID> new_mapping;
      for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator it
           = rhs->local_children.begin(); it != rhs->local_children.end(); it++)
      {
        std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator
          finder = local_children.find(it->first);
        if (finder == local_children.end())
        {
          it->second->convert(shard_to_shard_mapping);
          add_child(it->second);
          // If we have a disjoint complete children shards then remove the
          // entry from the mapping
          if (disjoint_complete_children_shards != NULL)
          {
            if (new_mapping.empty())
              new_mapping = disjoint_complete_children_shards->color_shards;
            std::unordered_map<LegionColor,ShardID>::iterator color_finder =
              new_mapping.find(it->first);
#ifdef DEBUG_LEGION
            assert(color_finder != new_mapping.end());
#endif
            new_mapping.erase(color_finder);
            if (new_mapping.empty())
            {
              if (disjoint_complete_children_shards->remove_reference())
                delete disjoint_complete_children_shards;
              disjoint_complete_children_shards = NULL;
            }
          }
        }
        else
          finder->second->merge(it->second, shard_to_shard_mapping);
      }
      if (!new_mapping.empty())
      {
        if (disjoint_complete_children_shards->remove_reference())
          delete disjoint_complete_children_shards;
        disjoint_complete_children_shards =
          new ShardedColorMap(std::move(new_mapping));
        disjoint_complete_children_shards->add_reference();
      }
    }

    //--------------------------------------------------------------------------
    void ProjectionPartition::pack(Serializer &rez) const
    //--------------------------------------------------------------------------
    {
      rez.serialize<size_t>(local_children.size());
      for (std::unordered_map<LegionColor,ProjectionRegion*>::const_iterator 
            it = local_children.begin(); it != local_children.end(); it++)
      {
        rez.serialize(it->first);
        it->second->pack(rez);
      }
      if (disjoint_complete_children_shards != NULL)
        disjoint_complete_children_shards->pack(rez);
      else
        ShardedColorMap::pack_empty(rez);
    }

    //--------------------------------------------------------------------------
    void ProjectionPartition::unpack(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      size_t num_children;
      derez.deserialize(num_children);
      for (unsigned idx = 0; idx < num_children; idx++)
      {
        LegionColor color;
        derez.deserialize(color);
        RegionNode *region = partition->get_child(color);
        ProjectionRegion *child = new ProjectionRegion(region);
        child->unpack(derez);
        local_children[color] = child;
        child->add_reference();
      }
      disjoint_complete_children_shards = ShardedColorMap::unpack(derez);
      if (disjoint_complete_children_shards != NULL)
        disjoint_complete_children_shards->add_reference();
    }
#endif

    /////////////////////////////////////////////////////////////
    // ShardedColorMap
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ShardID ShardedColorMap::at(LegionColor color) const
    //--------------------------------------------------------------------------
    {
      std::unordered_map<LegionColor,ShardID>::const_iterator finder =
        color_shards.find(color);
#ifdef DEBUG_LEGION
      assert(finder != color_shards.end());
#endif
      return finder->second;
    }

    //--------------------------------------------------------------------------
    void ShardedColorMap::pack(Serializer &rez)
    //--------------------------------------------------------------------------
    {
      rez.serialize<size_t>(color_shards.size());
      for (std::unordered_map<LegionColor,ShardID>::const_iterator it =
            color_shards.begin(); it != color_shards.end(); it++)
      {
        rez.serialize(it->first);
        rez.serialize(it->second);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void ShardedColorMap::pack_empty(Serializer &rez)
    //--------------------------------------------------------------------------
    {
      rez.serialize<size_t>(0);
    }

    //--------------------------------------------------------------------------
    /*static*/ ShardedColorMap* ShardedColorMap::unpack(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      size_t num_colors;
      derez.deserialize(num_colors);
      if (num_colors == 0)
        return NULL;
      std::unordered_map<LegionColor,ShardID> color_shards;
      for (unsigned idx = 0; idx < num_colors; idx++)
      {
        LegionColor color;
        derez.deserialize(color);
        derez.deserialize(color_shards[color]);
      }
      return new ShardedColorMap(std::move(color_shards));
    }

#if 0
    /////////////////////////////////////////////////////////////
    // ProjectionTree
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ProjectionTree::ProjectionTree(bool all_disjoint)
      : all_children_disjoint(all_disjoint)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    ProjectionTree::~ProjectionTree(void)
    //--------------------------------------------------------------------------
    {
      for (std::map<LegionColor,ProjectionTree*>::const_iterator it =
            children.begin(); it != children.end(); it++)
        delete it->second;
    }

    //--------------------------------------------------------------------------
    bool ProjectionTree::interferes(const ProjectionTree *other,
                                    ShardID other_shard) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(all_children_disjoint == other->all_children_disjoint);
#endif
      // Check all our uses here first
      if (!users.empty())
      {
        for (std::set<ShardID>::const_iterator it = 
              users.begin(); it != users.end(); it++)
        {
          if ((*it) == other_shard)
            continue;
          return true;
        }
      }
      if (all_children_disjoint)
      {
        for (std::map<LegionColor,ProjectionTree*>::const_iterator it =
              children.begin(); it != children.end(); it++)
        {
          // All disjoint, so just need to do analysis on any children we find
          std::map<LegionColor,ProjectionTree*>::const_iterator finder =
            other->children.find(it->first);
          if (finder == other->children.end())
            continue;
          if (it->second->interferes(finder->second, other_shard))
            return true;
        }
      }
      else
      {
        for (std::map<LegionColor,ProjectionTree*>::const_iterator it =
              children.begin(); it != children.end(); it++)
        {
          std::map<LegionColor,ProjectionTree*>::const_iterator finder =
            other->children.find(it->first);
          if (finder == other->children.end())
          {
            if (it->second->uses_shard(other_shard))
              return true;
          }
          else if (it->second->interferes(finder->second, other_shard))
            return true;
        }
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool ProjectionTree::uses_shard(ShardID other_shard) const
    //--------------------------------------------------------------------------
    {
      if (users.find(other_shard) != users.end())
        return true;
      for (std::map<LegionColor,ProjectionTree*>::const_iterator it =
            children.begin(); it != children.end(); it++)
        if (it->second->uses_shard(other_shard))
          return true;
      return false;
    }

    //--------------------------------------------------------------------------
    void ProjectionTree::serialize(Serializer &rez) const
    //--------------------------------------------------------------------------
    {
      rez.serialize<size_t>(users.size());
      for (std::set<ShardID>::const_iterator it =
            users.begin(); it != users.end(); it++)
        rez.serialize(*it);
      rez.serialize<size_t>(children.size());
      for (std::map<LegionColor,ProjectionTree*>::const_iterator it =
            children.begin(); it != children.end(); it++)
      {
        rez.serialize(it->first);
        rez.serialize<bool>(it->second->all_children_disjoint);
        it->second->serialize(rez);
      }
    }

    //--------------------------------------------------------------------------
    void ProjectionTree::deserialize(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      size_t num_users;
      derez.deserialize(num_users);
      for (unsigned idx = 0; idx < num_users; idx++)
      {
        ShardID user;
        derez.deserialize(user);
        users.insert(user);
      }
      size_t num_children;
      derez.deserialize(num_children);
      for (unsigned idx = 0; idx < num_children; idx++)
      {
        LegionColor color;
        derez.deserialize(color);
        bool children_complete;
        derez.deserialize(children_complete);
        std::map<LegionColor,ProjectionTree*>::const_iterator finder =
          children.find(color);
        if (finder == children.end())
        {
          ProjectionTree *child = new ProjectionTree(children_complete);
          child->deserialize(derez);
          children[color] = child;
        }
        else
          finder->second->deserialize(derez);
      }
    }

    /////////////////////////////////////////////////////////////
    // RefinementNode
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RefinementNode::RefinementNode(RegionTreeNode *n)
      : node(n)
    //--------------------------------------------------------------------------
    {
      node->add_base_gc_ref(DISJOINT_COMPLETE_REF);
    }

    //--------------------------------------------------------------------------
    RefinementNode::~RefinementNode(void)
    //--------------------------------------------------------------------------
    {
      if (node->remove_base_gc_ref(DISJOINT_COMPLETE_REF))
        delete node;
      for (std::map<LegionColor,RefinementNode*>::const_iterator it =
            children.begin(); it != children.end(); it++)
        if (it->second->remove_reference())
          delete it->second;
    }

    //--------------------------------------------------------------------------
    FieldMask RefinementNode::increment_touches(const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      FieldMask result;
      for (LegionMap<unsigned,FieldMask>::reverse_iterator it =
            touches.rbegin(); it != touches.rend(); /*nothing*/)
      {
        const FieldMask overlap = it->second & mask;
        if (!overlap)
          continue;
        const unsigned next = it->first + 1;
        if (next == REFINEMENT_CHANGE_COUNT)
          result = overlap;
        LegionMap<unsigned,FieldMask>::iterator finder = touches.find(next);
        if (finder == touches.end())
          touches[next] = overlap;
        else
          finder->second |= overlap;
        it->second -= overlap;
        if (!it->second)
        {
          it++;
          touches.erase(it.base());
        }
        else
          it++;
      }
      return result;
    }

    //--------------------------------------------------------------------------
    FieldMask RefinementNode::dominates_touches(const FieldMask &mask,
                                                const RefinementNode *rhs) const
    //--------------------------------------------------------------------------
    {
      FieldMask result;
      for (LegionMap<unsigned,FieldMask>::const_iterator it =
            touches.begin(); it != touches.end(); it++)
      {
        FieldMask overlap = it->second & mask;
        if (!overlap)
          continue;
        for (LegionMap<unsigned,FieldMask>::const_reverse_iterator rit =
              rhs->touches.rbegin(); rit != rhs->touches.rend(); rit++)
        {
          // As soon as the right hand side count is less than the left
          // hand side count then we can stop filtering
          if (rit->first < it->first)
            break;
          overlap -= rit->second;
          if (!overlap)
            break;
        }
        result |= overlap;
      }
      return result;
    }

    //--------------------------------------------------------------------------
    void RefinementNode::filter_touches(const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      for (LegionMap<unsigned,FieldMask>::iterator it =
            touches.begin(); it != touches.end(); /*nothing*/)
      {
        it->second -= mask;
        if (!it->second)
        {
          LegionMap<unsigned,FieldMask>::iterator to_delete = it++;
          touches.erase(to_delete);
        }
        else
          it++;
      }
    }

    //--------------------------------------------------------------------------
    void RefinementNode::record_refinement_tree(ContextID ctx, 
                                                const FieldMask &mask) const
    //--------------------------------------------------------------------------
    {
      std::vector<RegionTreeNode*> to_record;
      to_record.reserve(children.size());
      for (std::map<LegionColor,RefinementNode*>::const_iterator it =
            children.begin(); it != children.end(); it++)
        to_record.push_back(it->second->node);
      node->record_refinement_tree(ctx, mask, to_record);
    }

    //--------------------------------------------------------------------------
    bool RefinementNode::is_mostly_complete(void) const
    //--------------------------------------------------------------------------
    {
      if (node->is_region())
        return true;
      // This is a heuristic but we'll consider this node to be complete
      // once we get an indication that more than half of its children
      // have been touched and can therefore be refined
      PartitionNode *partition = node->as_partition_node();
      return ((partition->get_num_children()/2) < children.size());
    }

    //--------------------------------------------------------------------------
    bool RefinementNode::matches(const RefinementNode *sibling) const
    //--------------------------------------------------------------------------
    {
      if (children.size() != sibling->children.size())
        return false;
      for (std::map<LegionColor,RefinementNode*>::const_iterator it =
            children.begin(); it != children.end(); it++)
      {
        std::map<LegionColor,RefinementNode*>::const_iterator finder =
          sibling->children.find(it->first);
        if (finder == sibling->children.end())
          return false;
        if (it->second != finder->second)
          return false;
      }
      return true;
    }

    //--------------------------------------------------------------------------
    bool RefinementNode::matches_child(LegionColor color, 
                                       RefinementNode *child) const
    //--------------------------------------------------------------------------
    {
      std::map<LegionColor,RefinementNode*>::const_iterator finder =
        children.find(color);
      if (finder == children.end())
        return false;
      return (child == finder->second);
    }

    //--------------------------------------------------------------------------
    void RefinementNode::update_child(LegionColor color, RefinementNode *child)
    //--------------------------------------------------------------------------
    {
      std::map<LegionColor,RefinementNode*>::iterator finder =
        children.find(color);
      if (finder != children.end())
      {
        if (finder->second == child)
          return;
        if (finder->second->remove_reference())
          delete finder->second;
        finder->second = child;
      }
      else
        children[color] = child;
      child->add_reference();
    }

    //--------------------------------------------------------------------------
    RefinementNode* RefinementNode::clone(void) const
    //--------------------------------------------------------------------------
    {
      RefinementNode *copy = new RefinementNode(node);  
      for (std::map<LegionColor,RefinementNode*>::const_iterator it =
            children.begin(); it != children.end(); it++)
        copy->update_child(it->first, it->second);
      copy->touches = touches;
      return copy;
    }

    /////////////////////////////////////////////////////////////
    // RegionRefinementNode
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RegionRefinementNode::RegionRefinementNode(RegionNode *n,
                                               PartitionRefinementNode *ch)
      : node(n), child(ch)
    //--------------------------------------------------------------------------
    {
      node->add_base_gc_ref(DISJOINT_COMPLETE_REF);
    }

    //--------------------------------------------------------------------------
    RegionRefinementNode::~RegionRefinementNode(void)
    //--------------------------------------------------------------------------
    {
      if (child != NULL)
        delete child;
      if (node->remove_base_gc_ref(DISJOINT_COMPLETE_REF))
        delete node;
    }

    //--------------------------------------------------------------------------
    RegionTreeNode* RegionRefinementNode::get_region_tree_node(void) const
    //--------------------------------------------------------------------------
    {
      return node;
    }

    //--------------------------------------------------------------------------
    RefinementNode* RegionRefinementNode::clone(void) const
    //--------------------------------------------------------------------------
    {
      if (child == NULL)
      {
        RegionRefinementNode *result = new RegionRefinementNode(node);
        if (!refining_shards.empty())
          result->refining_shards = refining_shards;
        return result;
      }
      else
        return new RegionRefinementNode(node, 
          child->clone()->as_partition_refinement());
    }

    //--------------------------------------------------------------------------
    bool RegionRefinementNode::incorporate(RefinementNode *refinement)
    //--------------------------------------------------------------------------
    {
      if (child == NULL)
      {
        RegionTreeNode *child_node = refinement->get_region_tree_node();
        if (child_node->get_parent() == node)
        {
          child = refinement->as_partition_refinement();
#ifdef DEBUG_LEGION
          assert(child != NULL);
#endif
          return true;
        }
        else
          return false;
      }
      else
        return child->incorporate(refinement);
    }

    //--------------------------------------------------------------------------
    void RegionRefinementNode::perform_versioning_analysis(ContextID ctx,
        InnerContext *context, const FieldMask &refinement_mask,
        LegionMap<RegionNode*,VersionInfo> &version_infos,
        UniqueID op_id, std::set<RtEvent> &ready_events) const
    //--------------------------------------------------------------------------
    {
      if (child == NULL)
      {
#ifdef DEBUG_LEGION
        assert(version_infos.find(node) == version_infos.end());
#endif
        VersionInfo &version_info = version_infos[node];
        node->perform_versioning_analysis(ctx, context, &version_info,
            refinement_mask, op_id, ready_events);
      }
      else
        child->perform_versioning_analysis(ctx, context, refinement_mask,
            version_infos, op_id, ready_events);
    }

    //--------------------------------------------------------------------------
    void RegionRefinementNode::register_refinement(ContextID ctx, 
        const FieldMask &refinement_mask, InnerContext *context,
        size_t op_ctx_index, unsigned refinement_number,
        unsigned parent_req_index, std::set<RtEvent> &applied_events,
        const LegionMap<RegionNode*,VersionInfo> &version_infos) const
    //--------------------------------------------------------------------------
    {
      if (child == NULL)
      {
        // A small optimization here, check to see if any of the existing
        // equivalence sets can be reused because they're already for the
        // region for this node so we don't need to make a new one
        LegionMap<RegionNode*,VersionInfo>::const_iterator finder =
          version_infos.find(node);
#ifdef DEBUG_LEGION
        assert(finder != version_infos.end());
#endif
        const FieldMaskSet<EquivalenceSet> &old_sets = 
          finder->second.get_equivalence_sets();
#ifdef DEBUG_LEGION
        assert(old_sets.get_valid_mask() == refinement_mask);
#endif
        FieldMask create_mask = refinement_mask;
        for (FieldMaskSet<EquivalenceSet>::const_iterator it =
              old_sets.begin(); it != old_sets.end(); it++) 
        {
          if (it->first->region_node != node)
            continue;
          node->record_refinement(ctx, it->first, it->second);
          create_mask -= it->second;
          if (!create_mask)
            break;
        }
        if (!!create_mask)
        {
          EquivalenceSet *new_set = 
            context->create_equivalence_set(node, op_ctx_index, refining_shards,
                                       create_mask, old_sets, refinement_number,
                                       parent_req_index, applied_events);
          node->record_refinement(ctx, new_set, create_mask);
          // Remove the reference added by context when it made the 
          // equivalence set now that it has been recorded
          if (new_set->remove_base_gc_ref(CONTEXT_REF))
            delete new_set;
        }
      }
      else
        child->register_refinement(ctx, refinement_mask, context, op_ctx_index,
            refinement_number, parent_req_index, applied_events, version_infos);
    }

    /////////////////////////////////////////////////////////////
    // PartitionRefinementNode
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PartitionRefinementNode::PartitionRefinementNode(PartitionNode *n,
                                                     ShardedColorMap *map)
      : node(n), children_shards(map)
    //--------------------------------------------------------------------------
    {
      node->add_base_gc_ref(DISJOINT_COMPLETE_REF);
      if (children_shards != NULL)
        children_shards->add_reference();
    }

    //--------------------------------------------------------------------------
    PartitionRefinementNode::~PartitionRefinementNode(void)
    //--------------------------------------------------------------------------
    {
      for (std::unordered_map<LegionColor,RegionRefinementNode*>::const_iterator
            it = children.begin(); it != children.end(); it++)
        delete it->second;
      if (node->remove_base_gc_ref(DISJOINT_COMPLETE_REF))
        delete node;
      if ((children_shards != NULL) && children_shards->remove_reference())
        delete children_shards;
    }

    //--------------------------------------------------------------------------
    RegionTreeNode* PartitionRefinementNode::get_region_tree_node(void) const
    //--------------------------------------------------------------------------
    {
      return node;
    }

    //--------------------------------------------------------------------------
    RefinementNode* PartitionRefinementNode::clone(void) const
    //--------------------------------------------------------------------------
    {
      PartitionRefinementNode *result = 
        new PartitionRefinementNode(node, children_shards);
      for (std::unordered_map<LegionColor,RegionRefinementNode*>::const_iterator
            it = children.begin(); it != children.end(); it++)
        result->children[it->first] =
          it->second->clone()->as_region_refinement();
      return result;
    }

    //--------------------------------------------------------------------------
    bool PartitionRefinementNode::incorporate(RefinementNode *refinement)
    //--------------------------------------------------------------------------
    {
      RegionTreeNode *child = refinement->get_region_tree_node();
      if (child->get_parent() == node)
      {
        LegionColor child_color = child->get_color();
        RegionRefinementNode *child = refinement->as_region_refinement();
#ifdef DEBUG_LEGION
        assert(child != NULL);
        assert(children.find(child_color) == children.end());
#endif
        children[child_color] = child;
        return true;
      }
      else
      {
        for (std::unordered_map<LegionColor,
                                RegionRefinementNode*>::const_iterator
              it = children.begin(); it != children.end(); it++)
          if (it->second->incorporate(refinement))
            return true;
        return false;
      }
    }

    //--------------------------------------------------------------------------
    void PartitionRefinementNode::perform_versioning_analysis(ContextID ctx,
        InnerContext *context, const FieldMask &refinement_mask,
        LegionMap<RegionNode*,VersionInfo> &version_infos,
        UniqueID op_id, std::set<RtEvent> &ready_events) const
    //--------------------------------------------------------------------------
    {
      for (std::unordered_map<LegionColor,RegionRefinementNode*>::const_iterator
            it = children.begin(); it != children.end(); it++)
        it->second->perform_versioning_analysis(ctx, context, refinement_mask,
                                          version_infos, op_id, ready_events);
    }

    //--------------------------------------------------------------------------
    void PartitionRefinementNode::register_refinement(ContextID ctx, 
        const FieldMask &refinement_mask, InnerContext *context,
        size_t op_ctx_index, unsigned refinement_number,
        unsigned parent_req_index, std::set<RtEvent> &ready_events,
        const LegionMap<RegionNode*,VersionInfo> &version_infos) const
    //--------------------------------------------------------------------------
    {
      if (children_shards != NULL)
        node->record_refinement(ctx, children_shards, refinement_mask);
      for (std::unordered_map<LegionColor,RegionRefinementNode*>::const_iterator
            it = children.begin(); it != children.end(); it++)
        it->second->register_refinement(ctx, refinement_mask, context,
                    op_ctx_index, refinement_number, parent_req_index,
                    ready_events, version_infos);
    }
#endif

    /////////////////////////////////////////////////////////////
    // ProjectionSummary
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ProjectionSummary::ProjectionSummary(const ProjectionInfo &proj_info, 
                           ProjectionNode *node, Operation *op, unsigned index,
                           const RegionRequirement &req, LogicalState *state)
      : owner(state), domain(proj_info.projection_space),
        projection(proj_info.projection), sharding(proj_info.sharding_function),
        sharding_domain(proj_info.sharding_space), 
        arglen(req.projection_args_size), 
        args((arglen > 0) ? malloc(arglen) : NULL), tree(node), exchange(NULL), 
        // Special case here: if we can't prove its disjoint by the region tree
        // but we know that all the regions are writing and the projection
        // function is not invertible then the user is guaranteeing use that all
        // the point in this projection function are disjoint from each other
        disjoint((IS_WRITE(req) && !proj_info.projection->is_invertible) ||
                  tree->is_disjoint()),
        complete(proj_info.projection->is_complete(state->owner, op, index,
                                                   proj_info.projection_space)),
        permits_name_based_self_analysis(disjoint && tree->is_leaves_only()),
        unique_shard_users(true)/* no shard in non-control-replicated context */
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(proj_info.is_projecting());
      assert(tree != NULL);
#endif
      if (domain != NULL)
        domain->add_base_gc_ref(PROJECTION_REF);
      if (sharding_domain != NULL)
        sharding_domain->add_base_gc_ref(PROJECTION_REF);
      tree->add_reference();
      if (arglen > 0)
        memcpy(args, req.projection_args, arglen);
    }

    //--------------------------------------------------------------------------
    ProjectionSummary::ProjectionSummary(const ProjectionInfo &proj_info, 
                           ProjectionNode *node, Operation *op, unsigned index,
                           const RegionRequirement &req, LogicalState *state,
                           bool dis, bool unique)
      : owner(state), domain(proj_info.projection_space),
        projection(proj_info.projection), sharding(proj_info.sharding_function),
        sharding_domain(proj_info.sharding_space),
        arglen(req.projection_args_size), 
        args((arglen > 0) ? malloc(arglen) : NULL), tree(node), exchange(NULL),
        // Special case here: if we can't prove its disjoint by the region tree
        // but we know that all the regions are writing and the projection
        // function is not invertible then the user is guaranteeing use that all
        // the point in this projection function are disjoint from each other
        disjoint(dis || (IS_WRITE(req) &&
              !proj_info.projection->is_invertible)),
        complete(proj_info.projection->is_complete(state->owner, op, index,
                                                   proj_info.projection_space)),
        permits_name_based_self_analysis(disjoint), unique_shard_users(unique)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(proj_info.is_projecting());
      assert(tree != NULL);
#endif
      if (domain != NULL)
        domain->add_base_gc_ref(PROJECTION_REF);
      if (sharding_domain != NULL)
        sharding_domain->add_base_gc_ref(PROJECTION_REF);
      tree->add_reference();
      if (arglen > 0)
        memcpy(args, req.projection_args, arglen);
    }

    //--------------------------------------------------------------------------
    ProjectionSummary::ProjectionSummary(const ProjectionInfo &proj_info, 
                           ProjectionNode *node, Operation *op, unsigned index,
                           const RegionRequirement &req, LogicalState *state,
                           ReplicateContext *context)
      : owner(state), domain(proj_info.projection_space),
        projection(proj_info.projection), sharding(proj_info.sharding_function),
        sharding_domain(proj_info.sharding_space),
        arglen(req.projection_args_size), 
        args((arglen > 0) ? malloc(arglen) : NULL), tree(node),
        exchange(new ProjectionTreeExchange(tree, context, COLLECTIVE_LOC_50,
              disjoint, permits_name_based_self_analysis, unique_shard_users)),
        // Special case here: if we can't prove its disjoint by the region tree
        // but we know that all the regions are writing and the projection
        // function is not invertible then the user is guaranteeing use that all
        // the point in this projection function are disjoint from each other
        disjoint((IS_WRITE(req) && !proj_info.projection->is_invertible) || 
                  tree->is_disjoint()),
        complete(proj_info.projection->is_complete(state->owner, op, index,
                                                   proj_info.projection_space)),
        permits_name_based_self_analysis(true), 
        unique_shard_users(tree->is_unique_shards())
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(proj_info.is_projecting());
      assert(tree != NULL);
#endif
      exchange->perform_collective_async();
      if (domain != NULL)
        domain->add_base_gc_ref(PROJECTION_REF);
      if (sharding_domain != NULL)
        sharding_domain->add_base_gc_ref(PROJECTION_REF);
      tree->add_reference();
      if (arglen > 0)
        memcpy(args, req.projection_args, arglen);
    }

    //--------------------------------------------------------------------------
    ProjectionSummary::~ProjectionSummary(void)
    //--------------------------------------------------------------------------
    {
      owner->remove_projection_summary(this);
      if ((domain != NULL) && domain->remove_base_gc_ref(PROJECTION_REF))
        delete domain;
      if ((sharding_domain != NULL) && 
          sharding_domain->remove_base_gc_ref(PROJECTION_REF))
        delete sharding_domain;
      if (exchange != NULL)
      {
        exchange->perform_collective_wait(true/*block*/);  
        delete exchange;
      }
      if (tree->remove_reference())
        delete tree;
      if (args != NULL)
        free(args);
    }

    //--------------------------------------------------------------------------
    bool ProjectionSummary::matches(const ProjectionInfo &rhs,
                                    const RegionRequirement &req) const
    //--------------------------------------------------------------------------
    {
      if (domain != rhs.projection_space)
        return false;
      if (projection != rhs.projection)
        return false;
      if (sharding != rhs.sharding_function)
        return false;
      if (sharding_domain != rhs.sharding_space)
        return false;
      size_t proj_arglen;
      const void *projection_args = req.get_projection_args(&proj_arglen);
      if (arglen != proj_arglen)
        return false;
      if ((arglen > 0) && (memcmp(args, projection_args, arglen) != 0))
        return false;
      return true;
    }

    //--------------------------------------------------------------------------
    bool ProjectionSummary::is_disjoint(void)
    //--------------------------------------------------------------------------
    {
      if (exchange != NULL)
      {
        exchange->perform_collective_wait(true/*block*/);
        delete exchange;
        exchange = NULL;
      }
      return disjoint;
    }

    //--------------------------------------------------------------------------
    bool ProjectionSummary::can_perform_name_based_self_analysis(void)
    //--------------------------------------------------------------------------
    {
      if (exchange != NULL)
      {
        exchange->perform_collective_wait(true/*block*/);
        delete exchange;
        exchange = NULL;
      }
      return permits_name_based_self_analysis;
    }

    //--------------------------------------------------------------------------
    bool ProjectionSummary::has_unique_shard_users(void)
    //--------------------------------------------------------------------------
    {
      if (exchange != NULL)
      {
        exchange->perform_collective_wait(true/*block*/);
        delete exchange;
        exchange = NULL;
      }
      return unique_shard_users;
    }

    //--------------------------------------------------------------------------
    ProjectionNode* ProjectionSummary::get_tree(void)
    //--------------------------------------------------------------------------
    {
      if (exchange != NULL)
      {
        exchange->perform_collective_wait(true/*block*/);
        delete exchange;
        exchange = NULL;
      }
      return tree;
    }

    /////////////////////////////////////////////////////////////
    // RegionRefinementTracker
    /////////////////////////////////////////////////////////////

    /*static*/ constexpr uint64_t RefinementTracker::MAX_INCOMPLETE_WRITES;

    //--------------------------------------------------------------------------
    RegionRefinementTracker::RegionRefinementTracker(RegionNode *node)
      : region(node), refinement_state(UNREFINED_STATE), refined_child(NULL),
        refined_projection(NULL), total_traversals(0), return_timeout(0)
    //--------------------------------------------------------------------------
    {
      region->add_base_resource_ref(REFINEMENT_REF);
    }

    //--------------------------------------------------------------------------
    RegionRefinementTracker::~RegionRefinementTracker(void)
    //--------------------------------------------------------------------------
    {
      if ((refined_child != NULL) && 
          refined_child->remove_base_resource_ref(REFINEMENT_REF))
        delete refined_child;
      if ((refined_projection != NULL) && 
            refined_projection->remove_reference())
        delete refined_projection;
      for (std::unordered_map<PartitionNode*,
                std::pair<double,uint64_t> >::const_iterator it =
            candidate_partitions.begin(); it != 
            candidate_partitions.end(); it++)
        if (it->first->remove_base_resource_ref(REFINEMENT_REF))
          delete it->first;
      for (std::unordered_map<ProjectionRegion*,
                std::pair<double,uint64_t> >::const_iterator it =
            candidate_projections.begin(); it != 
            candidate_projections.end(); it++)
        if (it->first->remove_reference())
          delete it->first;
      if (region->remove_base_resource_ref(REFINEMENT_REF))
        delete region;
    }

    //--------------------------------------------------------------------------
    RefinementTracker* RegionRefinementTracker::clone(void) const
    //--------------------------------------------------------------------------
    {
      RegionRefinementTracker *tracker = new RegionRefinementTracker(region);
      tracker->refinement_state = refinement_state;
      if (refined_child != NULL)
      {
        tracker->refined_child = refined_child;
        refined_child->add_base_resource_ref(REFINEMENT_REF);
      }
      if (refined_projection != NULL)
      {
        tracker->refined_projection = refined_projection;
        refined_projection->add_reference();
      }
      for (std::unordered_map<PartitionNode*,
                std::pair<double,uint64_t> >::const_iterator it =
            candidate_partitions.begin(); it != 
            candidate_partitions.end(); it++)
      {
        it->first->add_base_resource_ref(REFINEMENT_REF);
        tracker->candidate_partitions.insert(*it);
      }
      for (std::unordered_map<ProjectionRegion*,
                std::pair<double,uint64_t> >::const_iterator it =
            candidate_projections.begin(); it != 
            candidate_projections.end(); it++)
      {
        it->first->add_reference();
        tracker->candidate_projections.insert(*it);
      }
      tracker->total_traversals = total_traversals;
      tracker->return_timeout = return_timeout;
      return tracker;
    }

    //--------------------------------------------------------------------------
    void RegionRefinementTracker::initialize_already_refined(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(refined_child == NULL);
      assert(refinement_state == UNREFINED_STATE);
#endif
      refinement_state = NO_REFINEMENT_STATE;
    }

    //--------------------------------------------------------------------------
    bool RegionRefinementTracker::update_child(RegionTreeNode *node,
                               const RegionUsage &usage, bool &allow_refinement)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!allow_refinement);
      assert(!node->is_region());
#endif
      PartitionNode *child = node->as_partition_node();
      switch (refinement_state)
      {
        case UNREFINED_STATE:
          {
#ifdef DEBUG_LEGION
            assert(refined_child == NULL);
            assert(refined_projection == NULL);
#endif
            // If we don't have any refinements, we'll always allow them
            if (child->row_source->is_complete())
              refinement_state = IS_WRITE(usage) ? 
                COMPLETE_WRITE_REFINED_STATE : COMPLETE_NONWRITE_REFINED_STATE;
            else
              refinement_state = IS_WRITE(usage) ?
                INCOMPLETE_WRITE_REFINED_STATE : 
                INCOMPLETE_NONWRITE_REFINED_STATE;
            child->add_base_resource_ref(REFINEMENT_REF);
            refined_child = child;
            break;
          }
        case COMPLETE_NONWRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              // Switch over to write refinements as soon as we see one
              // We can delete this tracker and make a new one to get
              // any kind of field coalescing happening
              return true;
            }
            // Don't bother refining for other kinds of partitions 
            break;
          }
        case INCOMPLETE_NONWRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              // Switch over to write refinements as soon as we see one
              // We can delete this tracker and make a new one to get
              // any kind of field coalescing happening
              return true;
            }
            if (child->row_source->is_complete())
            {
              // Swith over to non-write complete
              // We can delete this tracker and make a new one to
              // get any kind of field coalescing happening
              return true;
            }
            break;
          }
        case COMPLETE_WRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage) && child->row_source->is_complete())
            {
              // Comparing complete writes against each other
              // Step the clock for a new traversal
              total_traversals++;
              // Check to see if we've observed this refinement before
              std::unordered_map<PartitionNode*,
                std::pair<double,uint64_t> >::iterator finder =
                  candidate_partitions.find(child);
              if (finder != candidate_partitions.end())
              {
                // Update the score using exponentially weighted moving average
                finder->second.first = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
                    (total_traversals - finder->second.second)) *
                      finder->second.first + 1.0;
                // Since CHANGE_REFINEMENT_RETURN_COUNT is a power of 2 the 
                // compiler should be able to optimize integer division to a 
                // basic bit shift
                const uint64_t previous_epoch = 
                  finder->second.second / CHANGE_REFINEMENT_RETURN_COUNT;
                const uint64_t current_epoch = 
                  total_traversals / CHANGE_REFINEMENT_RETURN_COUNT;
                finder->second.second = total_traversals;
                // Reset the timeout since we saw a return
                return_timeout = 0;
                // If the last time we saw this child was in a previous epoch 
                // then we can check to see if this is now the dominant child
                if ((previous_epoch != current_epoch) &&
                    is_dominant_candidate(finder->second.first,
                                          (child == refined_child)))
                {
                  // If we're current refinement we don't switch but just end
                  // invalidating all the other candidates so they can start again
                  if (child == refined_child)
                    invalidate_unused_candidates();
                  else
                    return true;
                }
              }
              else if (child == refined_child)
              {
                // This counts as a return too since we're refined this way
                candidate_partitions[child] = 
                  std::pair<double,uint64_t>(1.0, total_traversals);
                child->add_base_resource_ref(REFINEMENT_REF);
                // Reset the timeout since we saw a return
                return_timeout = 0;
                // No need to switch, we're already the refinement
              }
              else
              {
                if (++return_timeout == CHANGE_REFINEMENT_TIMEOUT)
                {
                  invalidate_unused_candidates();
                  return_timeout = 0;
                }
                child->add_base_resource_ref(REFINEMENT_REF);
                candidate_partitions[child] = 
                  std::pair<double,uint64_t>(0.0, total_traversals);
              }
            }
            break;
          }
        case INCOMPLETE_WRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              if (child->row_source->is_complete())
              {
                // Swith over to write complete
                // We can delete this tracker and make a new one to
                // get any kind of field coalescing happening
                return true;
              }
              else
              {
                // Always allow refinements down writing incomplete children
                allow_refinement = true;
                // Don't both deleting this, we can stay in this mode as long
                // as there are more incomplete writes
                return false;
              }
            }
            break;
          }
        case NO_REFINEMENT_STATE:
          // Don't allow anything to happen here
          break;
        default:
          assert(false); // should never hit this
      }
      // Allow alternative refinements of our current child
      allow_refinement = (child == refined_child);
      return false;
    }

    //--------------------------------------------------------------------------
    bool RegionRefinementTracker::update_projection(ProjectionSummary *summary,
                               const RegionUsage &usage, bool &allow_refinement)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!allow_refinement);
#endif
      switch (refinement_state)
      {
        case UNREFINED_STATE:
          {
#ifdef DEBUG_LEGION
            assert(refined_child == NULL);
            assert(refined_projection == NULL);
#endif
            // If we don't have any refinements, we'll always allow them
            allow_refinement = true;
            if (summary->is_complete())
              refinement_state = IS_WRITE(usage) ? 
                COMPLETE_WRITE_REFINED_STATE : COMPLETE_NONWRITE_REFINED_STATE;
            else
              refinement_state = IS_WRITE(usage) ?
                INCOMPLETE_WRITE_REFINED_STATE : 
                INCOMPLETE_NONWRITE_REFINED_STATE;
            refined_projection = summary->get_tree()->as_region_projection();
            refined_projection->add_reference();
            break;
          }
        case COMPLETE_NONWRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              // Switch over to write refinements as soon as we see one
              // We can delete this tracker and make a new one to get
              // any kind of field coalescing happening
              return true;
            }
            // Don't bother refining for other kinds of partitions 
            break;
          }
        case INCOMPLETE_NONWRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              // Switch over to write refinements as soon as we see one
              // We can delete this tracker and make a new one to get
              // any kind of field coalescing happening
              return true;
            }
            if (summary->is_complete())
            {
              // Swith over to non-write complete
              // We can delete this tracker and make a new one to
              // get any kind of field coalescing happening
              return true;
            }
            break;
          }
        case COMPLETE_WRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage) && summary->is_complete())
            {
              ProjectionRegion *projection = 
                summary->get_tree()->as_region_projection();
              // Step the clock for a new traversal
              total_traversals++;
              // Check to see if we observed this refinement before
              std::unordered_map<ProjectionRegion*,
                std::pair<double,uint64_t> >::iterator finder =
                  candidate_projections.find(projection);
              if (finder != candidate_projections.end())
              {
                // Update the score using exponentially weighted moving average
                finder->second.first = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
                    (total_traversals - finder->second.second)) *
                  finder->second.first + 1.0;
                const uint64_t previous_epoch = 
                  finder->second.second / CHANGE_REFINEMENT_RETURN_COUNT;
                const uint64_t current_epoch = 
                  total_traversals / CHANGE_REFINEMENT_RETURN_COUNT;
                finder->second.second = total_traversals;
                // Reset the timeout since we saw a return
                return_timeout = 0;
                // If the last time we saw this projection was in a previous epoch 
                // then we can check to see if this is now the dominant child
                if ((previous_epoch != current_epoch) &&
                    is_dominant_candidate(finder->second.first, 
                                          (projection == refined_projection)))
                {
                  // If we're current refinement we don't switch but just end
                  // invalidating all the other candidates so they can start again
                  if (projection == refined_projection)
                    invalidate_unused_candidates();
                  else
                    return true;
                }
              }
              else if (projection == refined_projection)
              {
                // This counts as a return too since we're refined this way
                candidate_projections[projection] =
                  std::pair<double,uint64_t>(1.0, total_traversals);
                projection->add_reference();
                // Reset the timeout since we saw a return
                return_timeout = 0;
                // No need to switch, we're already using the refinement
              }
              else
              {
                if (++return_timeout == CHANGE_REFINEMENT_TIMEOUT)
                {
                  invalidate_unused_candidates();
                  return_timeout = 0;
                }
                candidate_projections[projection] = 
                  std::pair<double,uint64_t>(0.0, total_traversals);
                projection->add_reference();
              }
            }
            break;
          }
        case INCOMPLETE_WRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              if (summary->is_complete())
              {
                // If this is a complete write then we're going to switch to it
                return true;
              }
              // Otherwise we're only going to do a refinement for this 
              // projection if it is the "first" time we've seen it. In 
              // practice it's unreasonable to keep a memory of all prior
              // disjoint and complete partitions, so we track a set of 
              // the most recent MAX_INCOMPLETE_WRITES
              ProjectionRegion *projection = 
                summary->get_tree()->as_region_projection();
              std::unordered_map<ProjectionRegion*,
                std::pair<double,uint64_t> >::iterator finder =
                  candidate_projections.find(projection);
              if (finder == candidate_projections.end())
              {
                // Didn't find it so decrement all the prior entries TTLs 
                for (std::unordered_map<ProjectionRegion*,
                      std::pair<double,uint64_t> >::iterator it =
                      candidate_projections.begin(); it != 
                      candidate_projections.end(); /*nothing*/)
                {
                  if (--it->second.second == 0)
                  {
                    std::unordered_map<ProjectionRegion*,
                      std::pair<double,uint64_t> >::iterator delete_it = it++;
                    candidate_projections.erase(delete_it);
                  }
                  else
                    it++;
                }
                projection->add_reference();
                candidate_projections.emplace(std::make_pair(projection,
                  std::pair<double,uint64_t>(0.0, MAX_INCOMPLETE_WRITES)));
                allow_refinement = true;
              }
              else
              {
                // We already had it, so decrement all TTLs of everything that
                // came before it and then add it back with a new TTL
                for (std::unordered_map<ProjectionRegion*,
                      std::pair<double,uint64_t> >::iterator it =
                      candidate_projections.begin(); it != 
                      candidate_projections.end(); it++)
                  if (finder->second.second < it->second.second)
                    it->second.second--;
                finder->second.second = MAX_INCOMPLETE_WRITES;
              }
            }
            break;
          }
        case NO_REFINEMENT_STATE:
          // Don't allow anything to happen here
          break;
        default:
          assert(false); // should never hit this
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool RegionRefinementTracker::update_arrival(const RegionUsage &usage)
    //--------------------------------------------------------------------------
    {
      // We don't change the state because of an arrival right now
      return false;
    }

    //--------------------------------------------------------------------------
    void RegionRefinementTracker::invalidate_refinement(ContextID ctx,
                                             const FieldMask &invalidation_mask)
    //--------------------------------------------------------------------------
    {
      if (refined_child != NULL)
        refined_child->invalidate_logical_refinement(ctx, invalidation_mask);
    }

    //--------------------------------------------------------------------------
    bool RegionRefinementTracker::is_dominant_candidate(double score, 
                                                        bool is_current)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert((refined_child != NULL) || (refined_projection != NULL));
#endif
      // Has to have the most returns
      for (std::unordered_map<PartitionNode*,
                              std::pair<double,uint64_t> >::iterator it =
            candidate_partitions.begin(); it !=
            candidate_partitions.end(); it++)
      {
        // Skip ourselves
        if (it->second.second == total_traversals)
          continue;
        // Recompute the score before comparing
        it->second.first = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
            (total_traversals - it->second.second)) * it->second.first;
        it->second.second = total_traversals;
        if (score < it->second.first)
          return false;
      }
      for (std::unordered_map<ProjectionRegion*,
                              std::pair<double,uint64_t> >::iterator it =
            candidate_projections.begin(); it !=
            candidate_projections.end(); it++)
      {
        // Skip ourselves
        if (it->second.second == total_traversals)
          continue;
        // Recompute the score before comparing
        it->second.first = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
            (total_traversals - it->second.second)) * it->second.first;
        it->second.second = total_traversals;
        if (score < it->second.first)
          return false;
      }
      // If we're the current refinement then just being the largest is enough
      // to indicate that we're the dominant candidate
      if (!is_current)
      {
        // If we're not the current refinement, then we want to make sure its
        // obvious that we should switch and not just ping-pong so we need to
        // have a score that is at least sqrt(total_candidates) more than the
        // current refinement's score
        if (refined_child != NULL)
        {
          std::unordered_map<PartitionNode*,
            std::pair<double,uint64_t> >::const_iterator finder =
              candidate_partitions.find(refined_child);
          // Current refinement is never observed
          if (finder == candidate_partitions.end())
            return true;
          double total_candidates =
            candidate_partitions.size() + candidate_projections.size();
          double hysteresis = std::sqrt(total_candidates);
          return ((finder->second.first * hysteresis) < score);
        }
        else
        {
#ifdef DEBUG_LEGION
          assert(refined_projection != NULL);
#endif
          std::unordered_map<ProjectionRegion*,
            std::pair<double,uint64_t> >::const_iterator
              finder = candidate_projections.find(refined_projection);
          // Current refinement is never observed
          if (finder == candidate_projections.end())
            return true;
          double total_candidates =
            candidate_partitions.size() + candidate_projections.size();
          double hysteresis = std::sqrt(total_candidates);
          return ((finder->second.first * hysteresis) < score);
        }
      }
      return true;
    }

    //--------------------------------------------------------------------------
    void RegionRefinementTracker::invalidate_unused_candidates(void)
    //--------------------------------------------------------------------------
    {
      // Remove any candidates that have gone too long without being observed
      if (!candidate_partitions.empty())
      {
        for (std::unordered_map<PartitionNode*,
                            std::pair<double,uint64_t> >::iterator it =
              candidate_partitions.begin(); it !=
              candidate_partitions.end(); /*nothing*/)
        {
          uint64_t last_observation = total_traversals - it->second.second;
          if (CHANGE_REFINEMENT_TIMEOUT < last_observation)
          {
            if (it->first->remove_base_resource_ref(REFINEMENT_REF))
              delete it->first;
            std::unordered_map<PartitionNode*,
              std::pair<double,uint64_t> >::iterator to_delete = it++;
            candidate_partitions.erase(to_delete);
          }
          else
            it++;
        }
      }
      if (!candidate_projections.empty())
      {
        for (std::unordered_map<ProjectionRegion*,
                            std::pair<double,uint64_t> >::iterator it =
              candidate_projections.begin(); it !=
              candidate_projections.end(); /*nothing*/)
        {
          uint64_t last_observation = total_traversals - it->second.second;
          if (CHANGE_REFINEMENT_TIMEOUT < last_observation)
          {
            if (it->first->remove_reference())
              delete it->first;
            std::unordered_map<ProjectionRegion*,
              std::pair<double,uint64_t> >::iterator to_delete = it++;
            candidate_projections.erase(to_delete);
          }
          else
            it++;
        }
      }
    }

#if 0
    //--------------------------------------------------------------------------
    RegionRefinementTracker::RegionRefinementTracker(RegionNode *node,
                                                     bool current)
      : region(node), refined_child(NULL), refined_projection(NULL),
        current_refinement(current), total_traversals(0), return_timeout(0)
    //--------------------------------------------------------------------------
    {
      region->add_base_gc_ref(REFINEMENT_REF);
    }

    //--------------------------------------------------------------------------
    RegionRefinementTracker::RegionRefinementTracker(RegionNode *node,
                                                     RegionTreeNode *child)
      : region(node), refined_child(child->as_partition_node()),
        refined_projection(NULL), current_refinement(true), total_traversals(0),
        return_timeout(0)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!child->is_region());
#endif
      region->add_base_gc_ref(REFINEMENT_REF);
      refined_child->add_base_gc_ref(REFINEMENT_REF);
    }

    //--------------------------------------------------------------------------
    RegionRefinementTracker::RegionRefinementTracker(RegionNode *node,
                                                     ProjectionRegion *proj)
      : region(node), refined_child(NULL), refined_projection(proj),
        current_refinement(true), total_traversals(0), return_timeout(0)
    //--------------------------------------------------------------------------
    {
      region->add_base_gc_ref(REFINEMENT_REF);
      refined_projection->add_reference();
    }

    //--------------------------------------------------------------------------
    RegionRefinementTracker::~RegionRefinementTracker(void)
    //--------------------------------------------------------------------------
    {
      for (std::unordered_map<PartitionNode*,
                              std::pair<double,uint64_t> >::const_iterator it =
            candidate_partitions.begin(); it != 
            candidate_partitions.end(); it++)
        if (it->first->remove_base_gc_ref(REFINEMENT_REF))
          delete it->first;
      for (std::unordered_map<ProjectionRegion*,
                              std::pair<double,uint64_t> >::const_iterator it =
            candidate_projections.begin(); it !=
            candidate_projections.end(); it++)
        if (it->first->remove_reference())
          delete it->first;
      if ((refined_child != NULL) && 
          refined_child->remove_base_gc_ref(REFINEMENT_REF))
        delete refined_child;
      if ((refined_projection != NULL) &&
          refined_projection->remove_reference())
        delete refined_projection;
      if (region->remove_base_gc_ref(REFINEMENT_REF))
        delete region;
    }

    //--------------------------------------------------------------------------
    RefinementTracker* RegionRefinementTracker::clone(void) const
    //--------------------------------------------------------------------------
    {
      RegionRefinementTracker *result = (refined_child != NULL) ?
        new RegionRefinementTracker(region, refined_child) :
        (refined_projection != NULL) ? 
        new RegionRefinementTracker(region, refined_projection) :
        new RegionRefinementTracker(region, current_refinement);
      for (std::unordered_map<PartitionNode*,
                              std::pair<double,uint64_t> >::const_iterator it =
            candidate_partitions.begin(); it != 
            candidate_partitions.end(); it++)
      {
        it->first->add_base_gc_ref(REFINEMENT_REF);
        result->candidate_partitions.insert(*it);
      }
      for (std::unordered_map<ProjectionRegion*,
                              std::pair<double,uint64_t> >::const_iterator it =
            candidate_projections.begin(); it !=
            candidate_projections.end(); it++)
      {
        it->first->add_reference();
        result->candidate_projections.insert(*it);
      }
      result->total_traversals = total_traversals;
      result->return_timeout = return_timeout;
      return result;
    }

    //--------------------------------------------------------------------------
    bool RegionRefinementTracker::is_disjoint_complete(void) const
    //--------------------------------------------------------------------------
    {
      // Region nodes are always disjoint and complete regardless of 
      // whether they have and refinements or not
      return true;
    }

    //--------------------------------------------------------------------------
    bool RegionRefinementTracker::needs_fallback_refinement(
                                          const ProjectionInfo &proj_info) const
    //--------------------------------------------------------------------------
    {
      if (!proj_info.is_projecting())
        return false;
      if (!current_refinement)
        return false;
      if (refined_child != NULL)
        return false;
      if (refined_projection != NULL)
        return false;
      return true;
    }

    //--------------------------------------------------------------------------
    bool RegionRefinementTracker::update_refinement_child(RegionTreeNode *child)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!child->is_region());
#endif
      // Step the clock for a new traversal
      total_traversals++;
      PartitionNode *child_node = child->as_partition_node();
      // Check to see if we've observed this refinement before
      std::unordered_map<PartitionNode*,std::pair<double,uint64_t> >::iterator
        finder = candidate_partitions.find(child_node);
      if (finder != candidate_partitions.end())
      {
        // Update the score using exponentially weighted moving average
        finder->second.first = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
         (total_traversals - finder->second.second))*finder->second.first + 1.0;
        // Since CHANGE_REFINEMENT_RETURN_COUNT is a power of 2 the compiler
        // should be able to optimize integer division to a basic bit shift
        const uint64_t previous_epoch = 
          finder->second.second / CHANGE_REFINEMENT_RETURN_COUNT;
        const uint64_t current_epoch = 
          total_traversals / CHANGE_REFINEMENT_RETURN_COUNT;
        finder->second.second = total_traversals;
        // Reset the timeout since we saw a return
        return_timeout = 0;
        // If the last time we saw this child was in a previous epoch 
        // then we can check to see if this is now the dominant child
        if (current_refinement && (previous_epoch != current_epoch) &&
            is_dominant_candidate(finder->second.first,
                                  (child_node == refined_child)))
        {
          // If we're current refinement we don't switch but just end
          // invalidating all the other candidates so they can start again
          if (child_node == refined_child)
            invalidate_unused_candidates();
          else
            return true;
        }
      }
      else if (child_node == refined_child)
      {
#ifdef DEBUG_LEGION
        assert(current_refinement);
#endif
        // This counts as a return too since we're refined this way
        candidate_partitions[child_node] = 
          std::pair<double,uint64_t>(1.0, total_traversals);
        child_node->add_base_gc_ref(REFINEMENT_REF);
        // Reset the timeout since we saw a return
        return_timeout = 0;
        // No need to switch, we're already the refinement
      }
      else
      {
        // If this is the first time we've seen a disjoint-complete child
        // for something that is refined but not below this region then we
        // want to switch to using it immediately because something needs it
        if (current_refinement && (refined_child == NULL) &&
            (refined_projection == NULL) && candidate_partitions.empty() &&
            candidate_projections.empty())
          return true;
        if (++return_timeout == CHANGE_REFINEMENT_TIMEOUT)
        {
          invalidate_unused_candidates();
          return_timeout = 0;
        }
        candidate_partitions[child_node] = 
          std::pair<double,uint64_t>(0.0, total_traversals);
        child_node->add_base_gc_ref(REFINEMENT_REF);
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool RegionRefinementTracker::update_refinement_projection(
                                                           ProjectionNode *proj)
    //--------------------------------------------------------------------------
    {
      ProjectionRegion *projection = proj->as_region_projection();
#ifdef DEBUG_LEGION
      assert(projection != NULL);
#endif
      // Step the clock for a new traversal
      total_traversals++;
      // Check to see if we observed this refinement before
      std::unordered_map<ProjectionRegion*,
        std::pair<double,uint64_t> >::iterator finder =
          candidate_projections.find(projection);
      if (finder != candidate_projections.end())
      {
        // Update the score using exponentially weighted moving average
        finder->second.first = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
         (total_traversals - finder->second.second))*finder->second.first + 1.0;
        const uint64_t previous_epoch = 
          finder->second.second / CHANGE_REFINEMENT_RETURN_COUNT;
        const uint64_t current_epoch = 
          total_traversals / CHANGE_REFINEMENT_RETURN_COUNT;
        finder->second.second = total_traversals;
        // Reset the timeout since we saw a return
        return_timeout = 0;
        // If the last time we saw this projection was in a previous epoch 
        // then we can check to see if this is now the dominant child
        if (current_refinement && (previous_epoch != current_epoch) &&
            is_dominant_candidate(finder->second.first, 
                                  (projection == refined_projection)))
        {
          // If we're current refinement we don't switch but just end
          // invalidating all the other candidates so they can start again
          if (projection == refined_projection)
            invalidate_unused_candidates();
          else
            return true;
        }
      }
      else if (projection == refined_projection)
      {
#ifdef DEBUG_LEGION
        assert(current_refinement);
#endif
        // This counts as a return too since we're refined this way
        candidate_projections[projection] =
          std::pair<double,uint64_t>(1.0, total_traversals);
        projection->add_reference();
        // Reset the timeout since we saw a return
        return_timeout = 0;
        // No need to switch, we're already using the refinement
      }
      else
      {
        // If this is the first time we've seen a disjoint-complete child
        // for something that is refined but not below this region then we
        // want to switch to using it immediately because something needs it
        if (current_refinement && (refined_child == NULL) &&
            (refined_projection == NULL) && candidate_partitions.empty() &&
            candidate_projections.empty())
          return true;
        if (++return_timeout == CHANGE_REFINEMENT_TIMEOUT)
        {
          invalidate_unused_candidates();
          return_timeout = 0;
        }
        candidate_projections[projection] = 
          std::pair<double,uint64_t>(0.0, total_traversals);
        projection->add_reference();
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool RegionRefinementTracker::is_dominant_candidate(double score, 
                                                        bool is_current)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(current_refinement);
#endif
      // Has to have the most returns
      for (std::unordered_map<PartitionNode*,
                              std::pair<double,uint64_t> >::iterator it =
            candidate_partitions.begin(); it !=
            candidate_partitions.end(); it++)
      {
        // Skip ourselves
        if (it->second.second == total_traversals)
          continue;
        // Recompute the score before comparing
        it->second.first = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
            (total_traversals - it->second.second)) * it->second.first;
        it->second.second = total_traversals;
        if (score < it->second.first)
          return false;
      }
      for (std::unordered_map<ProjectionRegion*,
                              std::pair<double,uint64_t> >::iterator it =
            candidate_projections.begin(); it !=
            candidate_projections.end(); it++)
      {
        // Skip ourselves
        if (it->second.second == total_traversals)
          continue;
        // Recompute the score before comparing
        it->second.first = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
            (total_traversals - it->second.second)) * it->second.first;
        it->second.second = total_traversals;
        if (score < it->second.first)
          return false;
      }
      // If we're the current refinement then just being the largest is enough
      // to indicate that we're the dominant candidate
      if (!is_current)
      {
        // If we're not the current refinement, then we want to make sure its
        // obvious that we should switch and not just ping-pong so we need to
        // have a score that is at least sqrt(total_candidates) more than the
        // current refinement's score
        if (refined_child != NULL)
        {
          std::unordered_map<PartitionNode*,
            std::pair<double,uint64_t> >::const_iterator finder =
              candidate_partitions.find(refined_child);
          // Current refinement is never observed
          if (finder == candidate_partitions.end())
            return true;
          double total_candidates =
            candidate_partitions.size() + candidate_projections.size();
          double hysteresis = std::sqrt(total_candidates);
          return ((finder->second.first * hysteresis) < score);
        }
        else
        {
#ifdef DEBUG_LEGION
          assert(refined_projection != NULL);
#endif
          std::unordered_map<ProjectionRegion*,
            std::pair<double,uint64_t> >::const_iterator
              finder = candidate_projections.find(refined_projection);
          // Current refinement is never observed
          if (finder == candidate_projections.end())
            return true;
          double total_candidates =
            candidate_partitions.size() + candidate_projections.size();
          double hysteresis = std::sqrt(total_candidates);
          return ((finder->second.first * hysteresis) < score);
        }
      }
      return true;
    }

    //--------------------------------------------------------------------------
    void RegionRefinementTracker::invalidate_unused_candidates(void)
    //--------------------------------------------------------------------------
    {
      // Remove any candidates that have gone too long without being observed
      if (!candidate_partitions.empty())
      {
        for (std::unordered_map<PartitionNode*,
                            std::pair<double,uint64_t> >::iterator it =
              candidate_partitions.begin(); it !=
              candidate_partitions.end(); /*nothing*/)
        {
          uint64_t last_observation = total_traversals - it->second.second;
          if (CHANGE_REFINEMENT_TIMEOUT < last_observation)
          {
            if (it->first->remove_base_gc_ref(REFINEMENT_REF))
              delete it->first;
            std::unordered_map<PartitionNode*,
              std::pair<double,uint64_t> >::iterator to_delete = it++;
            candidate_partitions.erase(to_delete);
          }
          else
            it++;
        }
      }
      if (!candidate_projections.empty())
      {
        for (std::unordered_map<ProjectionRegion*,
                            std::pair<double,uint64_t> >::iterator it =
              candidate_projections.begin(); it !=
              candidate_projections.end(); /*nothing*/)
        {
          uint64_t last_observation = total_traversals - it->second.second;
          if (CHANGE_REFINEMENT_TIMEOUT < last_observation)
          {
            if (it->first->remove_reference())
              delete it->first;
            std::unordered_map<ProjectionRegion*,
              std::pair<double,uint64_t> >::iterator to_delete = it++;
            candidate_projections.erase(to_delete);
          }
          else
            it++;
        }
      }
    }

    //--------------------------------------------------------------------------
    void RegionRefinementTracker::change_refinements(ContextID ctx,
                                 const FieldMask &refinement_mask,
                                 FieldMaskSet<RegionTreeNode> &new_children,
                                 FieldMaskSet<ProjectionNode> &new_projections)
    //--------------------------------------------------------------------------
    {
      // If we had a current refinement then we need to issue an invalidation
      if (refined_child != NULL)
        refined_child->invalidate_logical_refinement(ctx, refinement_mask);
      // Now pick the best refinement option between the candidates
      double max_score = 0;
      PartitionNode *best_child = NULL;
      for (std::unordered_map<PartitionNode*,
                              std::pair<double,uint64_t> >::const_iterator it =
            candidate_partitions.begin(); it != 
            candidate_partitions.end(); it++)
      {
        if (max_score < it->second.first)
        {
          max_score = it->second.first;
          best_child = it->first;
        }
      }
      ProjectionRegion *best_projection = NULL;
      for (std::unordered_map<ProjectionRegion*,
                              std::pair<double,uint64_t> >::const_iterator it =
            candidate_projections.begin(); it !=
            candidate_projections.end(); it++)
      {
        if (max_score < it->second.first)
        {
          max_score = it->second.first;
          best_projection = it->first;
          best_child = NULL;
        }
      }
      if (best_projection != NULL)
        new_projections.insert(best_projection, refinement_mask);
      else if (best_child != NULL)
        new_children.insert(best_child, refinement_mask);
    }

    //--------------------------------------------------------------------------
    void RegionRefinementTracker::invalidate_refinement(ContextID ctx,
                                             const FieldMask &invalidation_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(current_refinement);
#endif
      if (refined_child != NULL)
        refined_child->invalidate_logical_refinement(ctx, invalidation_mask);
    }

    //--------------------------------------------------------------------------
    void RegionRefinementTracker::find_child_refinements(
                                      std::set<RegionTreeNode*> &children) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(current_refinement);
#endif
      if (refined_child != NULL)
        children.insert(refined_child);
    }

    //--------------------------------------------------------------------------
    void RegionRefinementTracker::convert(
                             const std::vector<ShardID> *shard_to_shard_mapping)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(current_refinement);
#endif
      if (refined_projection != NULL)
        refined_projection->convert(shard_to_shard_mapping);
    }

    //--------------------------------------------------------------------------
    void RegionRefinementTracker::merge(RefinementTracker *other,
                             const std::vector<ShardID> *shard_to_shard_mapping)
    //--------------------------------------------------------------------------
    {
      RegionRefinementTracker *rhs = other->as_region_tracker();
#ifdef DEBUG_LEGION
      assert(rhs != NULL);
      assert(region == rhs->region);
      assert(current_refinement);
      assert(rhs->current_refinement);
      assert(refined_child == rhs->refined_child);
      assert((refined_projection != NULL) == (rhs->refined_projection != NULL));
#endif
      if (refined_projection != NULL)
        refined_projection->merge(rhs->refined_projection, 
                                  shard_to_shard_mapping);
    }

    //--------------------------------------------------------------------------
    void RegionRefinementTracker::pack(Serializer &rez,
                             std::map<LegionColor,RegionTreeNode*> &to_traverse)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(current_refinement);
#endif
      if (refined_child != NULL)
      {
        rez.serialize<int>(1); // has a partition
        rez.serialize(refined_child->row_source->color);
        to_traverse[refined_child->row_source->color] = refined_child;
      }
      else if (refined_projection != NULL)
      {
        rez.serialize<int>(-1); // has a projection
        refined_projection->pack(rez);
      }
      else
        rez.serialize<int>(0); // just a leaf refinement
    }

    //--------------------------------------------------------------------------
    /*static*/ RegionRefinementTracker* RegionRefinementTracker::unpack(
        RegionNode *region, Deserializer &derez,
        std::map<LegionColor,RegionTreeNode*> &to_traverse)
    //--------------------------------------------------------------------------
    {
      int kind;
      derez.deserialize(kind);
      if (kind > 0)
      {
        LegionColor child_color;
        derez.deserialize(child_color);
        PartitionNode *child = region->get_child(child_color);
        to_traverse[child->row_source->color] = child;
        return new RegionRefinementTracker(region, child); 
      }
      else if (kind < 0)
      {
        ProjectionRegion *projection = new ProjectionRegion(region);
        projection->unpack(derez);
        return new RegionRefinementTracker(region, projection);
      }
      else
        return new RegionRefinementTracker(region, true/*current*/);
    }
#endif

    /////////////////////////////////////////////////////////////
    // PartitionRefinementTracker
    ///////////////////////////////////////////////////////////// 

    //--------------------------------------------------------------------------
    PartitionRefinementTracker::PartitionRefinementTracker(PartitionNode *node)
      : partition(node), refined_projection(NULL),
        refinement_state(UNREFINED_STATE), children_score(-1.0),
        children_last(0), total_traversals(0), return_timeout(0)
    //--------------------------------------------------------------------------
    {
      partition->add_base_resource_ref(REFINEMENT_REF);
    }

    //--------------------------------------------------------------------------
    PartitionRefinementTracker::~PartitionRefinementTracker(void)
    //--------------------------------------------------------------------------
    {
      if ((refined_projection != NULL) && 
          refined_projection->remove_reference())
        delete refined_projection;
      for (std::vector<RegionNode*>::const_iterator it =
            children.begin(); it != children.end(); it++)
        if ((*it)->remove_base_resource_ref(REFINEMENT_REF))
          delete (*it);
      for (std::unordered_map<ProjectionPartition*,
                std::pair<double,uint64_t> >::const_iterator it =
            candidate_projections.begin(); it != 
            candidate_projections.end(); it++)
        if (it->first->remove_reference())
          delete it->first;
      if (partition->remove_base_resource_ref(REFINEMENT_REF))
        delete partition;
    }

    //--------------------------------------------------------------------------
    RefinementTracker* PartitionRefinementTracker::clone(void) const
    //--------------------------------------------------------------------------
    {
      PartitionRefinementTracker *tracker = 
        new PartitionRefinementTracker(partition);
      if (refined_projection != NULL)
      {
        tracker->refined_projection = refined_projection;
        refined_projection->add_reference();
      }
      tracker->refinement_state = refinement_state; 
      if (!children.empty())
      {
        tracker->children = children;
        for (std::vector<RegionNode*>::const_iterator it =
            children.begin(); it != children.end(); it++)
          (*it)->add_base_resource_ref(REFINEMENT_REF);
      }
      for (std::unordered_map<ProjectionPartition*,
                std::pair<double,uint64_t> >::const_iterator it =
            candidate_projections.begin(); it != 
            candidate_projections.end(); it++)
      {
        it->first->add_reference();
        tracker->candidate_projections.insert(*it);
      }
      tracker->children_score = children_score;
      tracker->children_last = children_last;
      tracker->total_traversals = total_traversals;
      tracker->return_timeout = return_timeout;
      return tracker;
    }

    //--------------------------------------------------------------------------
    void PartitionRefinementTracker::initialize_already_refined(void)
    //--------------------------------------------------------------------------
    {
      assert(false); // this should never be called
    }

    //--------------------------------------------------------------------------
    bool PartitionRefinementTracker::update_child(RegionTreeNode *node,
                               const RegionUsage &usage, bool &allow_refinement)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(node->is_region());
      assert(!allow_refinement);
#endif
      RegionNode *child = node->as_region_node();
      switch (refinement_state)
      {
        case UNREFINED_STATE:
          {
#ifdef DEBUG_LEGION
            assert(children.empty());
            assert(refined_projection == NULL);
#endif
            allow_refinement = true;
            child->add_base_resource_ref(REFINEMENT_REF);
            children.push_back(child);
            if (((uint64_t)partition->row_source->total_children) <= 
                CHANGE_REFINEMENT_PARTITION_FRACTION)
              refinement_state = IS_WRITE(usage) ?
                COMPLETE_WRITE_REFINED_STATE : COMPLETE_NONWRITE_REFINED_STATE;
            else
              refinement_state = IS_WRITE(usage) ?
                INCOMPLETE_WRITE_REFINED_STATE :
                INCOMPLETE_NONWRITE_REFINED_STATE;
            break;
          }
        case COMPLETE_NONWRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              // Switch over to write refinements as soon as we see one
              // We can delete this tracker and make a new one to get
              // any kind of field coalescing happening
              return true;
            }
            // Only track non-write children if we're complete because of
            // children and not because of a complete projection refinement
            if (refined_projection == NULL)
            {
              allow_refinement = true;
              if (!std::binary_search(children.begin(), children.end(), child))
              {
                child->add_base_resource_ref(REFINEMENT_REF);
                children.push_back(child);
                std::sort(children.begin(), children.end());
              }
            }
            break;
          }
        case INCOMPLETE_NONWRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              // Switch over to write refinements as soon as we see one
              // We can delete this tracker and make a new one to get
              // any kind of field coalescing happening
              return true;
            }
            allow_refinement = true;
            if (!std::binary_search(children.begin(), children.end(), child))
            {
              child->add_base_resource_ref(REFINEMENT_REF);
              children.push_back(child);
              std::sort(children.begin(), children.end());
              if (((uint64_t)partition->row_source->total_children) <= 
                  (children.size() * CHANGE_REFINEMENT_PARTITION_FRACTION))
              {
                refinement_state = COMPLETE_NONWRITE_REFINED_STATE;
                // Remove any refined projections that we've done
                if (refined_projection != NULL)
                {
                  if (refined_projection->remove_reference())
                    delete refined_projection;
                  refined_projection = NULL;
                }
              }
            }
            break;
          }
        case COMPLETE_WRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              allow_refinement = true;
              // Increment the count for total traversals
              children_last = ++total_traversals;
              if (!std::binary_search(children.begin(), children.end(), child))
              {
                child->add_base_resource_ref(REFINEMENT_REF);
                children.push_back(child);
                std::sort(children.begin(), children.end());
                // See if we dominate the projection at this point
                if ((refined_projection != NULL) && 
                    (((uint64_t)partition->row_source->total_children) <=
                     (children.size() * CHANGE_REFINEMENT_PARTITION_FRACTION)))
                {
                  if (refined_projection->remove_reference())
                    delete refined_projection;
                  refined_projection = NULL;
                }
              }
            }
            break;
          }
        case INCOMPLETE_WRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              allow_refinement = true;
              if (!std::binary_search(children.begin(), children.end(), child))
              {
                child->add_base_resource_ref(REFINEMENT_REF);
                children.push_back(child);
                std::sort(children.begin(), children.end());
                if (((uint64_t)partition->row_source->total_children) <=
                    (children.size() * CHANGE_REFINEMENT_PARTITION_FRACTION))
                {
                  refinement_state = COMPLETE_WRITE_REFINED_STATE;
                  // Remove any refined projections that we've done
                  if (refined_projection != NULL)
                  {
                    if (refined_projection->remove_reference())
                      delete refined_projection;
                    refined_projection = NULL;
                  }
                }
              }
            }
            break;
          }
        default:
          assert(false);
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool PartitionRefinementTracker::update_projection(
                                                    ProjectionSummary *summary, 
                                                    const RegionUsage &usage, 
                                                    bool &allow_refinement)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!allow_refinement);
#endif
      switch (refinement_state)
      {
        case UNREFINED_STATE:
          {
#ifdef DEBUG_LEGION
            assert(children.empty());
            assert(refined_projection == NULL);
#endif
            allow_refinement = true;
            refined_projection = summary->get_tree()->as_partition_projection();
            refined_projection->add_reference();
            if (summary->is_complete())
              refinement_state = IS_WRITE(usage) ?
                COMPLETE_WRITE_REFINED_STATE : COMPLETE_NONWRITE_REFINED_STATE;
            else
              refinement_state = IS_WRITE(usage) ?
                INCOMPLETE_WRITE_REFINED_STATE :
                INCOMPLETE_NONWRITE_REFINED_STATE;
            break;
          }
        case COMPLETE_NONWRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              // Switch over to write refinements as soon as we see one
              // We can delete this tracker and make a new one to get
              // any kind of field coalescing happening
              return true;
            }
            // If we already have a non-write complete refinement
            // then there is nothing more for us to do here
            break;
          }
        case INCOMPLETE_NONWRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              // Switch over to write refinements as soon as we see one
              // We can delete this tracker and make a new one to get
              // any kind of field coalescing happening
              return true;
            }
            // Check to see if we're a complete projection
            if (summary->is_complete())
            {
              // We're a new complete non-write projection so 
              // swtich to that
              return true;
            }
            // Otherwise we don't bother switching
            break;
          }
        case COMPLETE_WRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage) && summary->is_complete())
            {
              // Testing against other writing-complete projections
              // and/or the group of direct children
              // Step the clock for a new traversal
              total_traversals++;
              ProjectionPartition *projection =
                summary->get_tree()->as_partition_projection();
              std::unordered_map<ProjectionPartition*,
                std::pair<double,uint64_t> >::iterator finder =
                  candidate_projections.find(projection);
              if (finder != candidate_projections.end())
              {
                // Update the score using exponentially weighted moving average
                finder->second.first = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
                    (total_traversals - finder->second.second)) *
                  finder->second.first + 1.0;
                // Since CHANGE_REFINEMENT_RETURN_COUNT is a power of 2 the compiler
                // should be able to optimize integer division to a basic bit shift
                const uint64_t previous_epoch = 
                  finder->second.second / CHANGE_REFINEMENT_RETURN_COUNT;
                const uint64_t current_epoch = 
                  total_traversals / CHANGE_REFINEMENT_RETURN_COUNT;
                finder->second.second = total_traversals;
                // Reset the timeout since we saw a return
                return_timeout = 0;
                // If the last time we saw this projection was in a previous epoch 
                // then we can check to see if this is now the dominant child
                if ((previous_epoch != current_epoch) &&
                    is_dominant_candidate(finder->second.first, 
                                          (projection == refined_projection)))
                {
                  // If we're current refinement we don't switch but just end
                  // invalidating all the other candidates so they can start again
                  if (projection == refined_projection)
                    invalidate_unused_candidates();
                  else
                    return true;
                }
              }
              else if (projection == refined_projection)
              {
                // This counts as a return too since we're refined this way
                candidate_projections[projection] = 
                  std::pair<double,uint64_t>(1.0, total_traversals);
                projection->add_reference();
                // Reset the timeout since we saw a return
                return_timeout = 0;
                // No need to switch, we're already using the refinement
              }
              else
              {
                if (++return_timeout == CHANGE_REFINEMENT_TIMEOUT)
                {
                  invalidate_unused_candidates();
                  return_timeout = 0;
                }
                candidate_projections[projection] = 
                  std::pair<double,uint64_t>(0.0, total_traversals);
                projection->add_reference();
              }
            }
            break;
          }
        case INCOMPLETE_WRITE_REFINED_STATE:
          {
            if (IS_WRITE(usage))
            {
              // Check to see if we're a complete projection
              if (summary->is_complete())
              {
                // We're a new complete writing projection so 
                // swtich to that
                return true; 
              }
              // Otherwise we're only going to do a refinement for this 
              // projection if it is the "first" time we've seen it. In 
              // practice it's unreasonable to keep a memory of all prior
              // disjoint and complete partitions, so we track a set of 
              // the most recent MAX_INCOMPLETE_WRITES
              ProjectionPartition *projection = 
                summary->get_tree()->as_partition_projection();
              std::unordered_map<ProjectionPartition*,
                std::pair<double,uint64_t> >::iterator finder =
                  candidate_projections.find(projection);
              if (finder == candidate_projections.end())
              {
                // Didn't find it so decrement all the prior entries TTLs 
                for (std::unordered_map<ProjectionPartition*,
                      std::pair<double,uint64_t> >::iterator it =
                      candidate_projections.begin(); it != 
                      candidate_projections.end(); /*nothing*/)
                {
                  if (--it->second.second == 0)
                  {
                    std::unordered_map<ProjectionPartition*,
                      std::pair<double,uint64_t> >::iterator delete_it = it++;
                    candidate_projections.erase(delete_it);
                  }
                  else
                    it++;
                }
                projection->add_reference();
                candidate_projections.emplace(std::make_pair(projection,
                  std::pair<double,uint64_t>(0.0, MAX_INCOMPLETE_WRITES)));
                allow_refinement = true;
              }
              else
              {
                // We already had it, so decrement all TTLs of everything that
                // came before it and then add it back with a new TTL
                for (std::unordered_map<ProjectionPartition*,
                      std::pair<double,uint64_t> >::iterator it =
                      candidate_projections.begin(); it != 
                      candidate_projections.end(); it++)
                  if (finder->second.second < it->second.second)
                    it->second.second--;
                finder->second.second = MAX_INCOMPLETE_WRITES;
              }
            }
            break;
          }
        default:
          assert(false);
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool PartitionRefinementTracker::update_arrival(const RegionUsage &usage)
    //--------------------------------------------------------------------------
    {
      assert(false); // should never arrive on a partition without a projection 
      return false;
    }

    //--------------------------------------------------------------------------
    void PartitionRefinementTracker::invalidate_refinement(ContextID ctx,
                                             const FieldMask &invalidation_mask)
    //--------------------------------------------------------------------------
    {
      for (std::vector<RegionNode*>::const_iterator it =
            children.begin(); it != children.end(); it++)
        (*it)->invalidate_logical_refinement(ctx, invalidation_mask);
    }

    //--------------------------------------------------------------------------
    bool PartitionRefinementTracker::is_dominant_candidate(double score,
                                                          bool is_current)
    //--------------------------------------------------------------------------
    {
      if (((uint64_t)partition->row_source->total_children) <=
          (children.size() * CHANGE_REFINEMENT_PARTITION_FRACTION))
      {
        // Recompute the children score and compare it
        children_score = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
            (total_traversals - children_last)) * children_score;
        children_last = total_traversals;
        if (score < children_score)
          return false;
      }
      for (std::unordered_map<ProjectionPartition*,
                              std::pair<double,uint64_t> >::iterator it =
            candidate_projections.begin(); it !=
            candidate_projections.end(); it++) 
      {
        // Skip ourselves
        if (it->second.second == total_traversals)
          continue;
        // Recompute the score before comparing
        it->second.first = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
            (total_traversals - it->second.second)) * it->second.first;
        it->second.second = total_traversals;
        if (score < it->second.first)
          return false;
      }
      if (!is_current)
      {
        if (refined_projection != NULL)
        {
          // If we're not the current refinement, then we want to make sure its
          // obvious that we should switch and not just ping-pong so we need to
          // have a score that is at least sqrt(total_candidates) more than the
          // current refinement's score
          std::unordered_map<ProjectionPartition*,
              std::pair<double,uint64_t> >::const_iterator 
                finder = candidate_projections.find(refined_projection);
          // Current refinement is never observed
          if (finder == candidate_projections.end())
            return true;
          double total_candidates =
            (children_score >= 0.0) ? 1 : 0 + candidate_projections.size();
          double hysteresis = std::sqrt(total_candidates);
          return ((finder->second.first * hysteresis) < score);
        }
        else
        {
          // The children are refined so compute the total candidates
          if (children_score < 0.0)
            return true;
          double total_candidates = candidate_projections.size() + 1;
          double hysteresis = std::sqrt(total_candidates);
          return ((children_score * hysteresis) < score);
        }
      }
      return true;
    }

    //--------------------------------------------------------------------------
    void PartitionRefinementTracker::invalidate_unused_candidates(void)
    //--------------------------------------------------------------------------
    {
      if (children_score >= 0.0)
      {
        uint64_t last_observation = total_traversals - children_last;
        if (CHANGE_REFINEMENT_TIMEOUT < last_observation)
          children_score = -1.0;
      }
      if (!candidate_projections.empty())
      {
        for (std::unordered_map<ProjectionPartition*,
                            std::pair<double,uint64_t> >::iterator it =
              candidate_projections.begin(); it !=
              candidate_projections.end(); /*nothing*/)
        {
          uint64_t last_observation = total_traversals - it->second.second;
          if (CHANGE_REFINEMENT_TIMEOUT < last_observation)
          {
            if (it->first->remove_reference())
              delete it->first;
            std::unordered_map<ProjectionPartition*,
              std::pair<double,uint64_t> >::iterator to_delete = it++;
            candidate_projections.erase(to_delete);
          }
          else
            it++;
        }
      }
    }

#if 0
    //--------------------------------------------------------------------------
    PartitionRefinementTracker::PartitionRefinementTracker(PartitionNode *node,
                                                           bool current)
      : partition(node), refined_projection(NULL), current_refinement(current),
        children_score(0.0), children_last(0), total_traversals(0),
        return_timeout(0)
    //--------------------------------------------------------------------------
    {
      partition->add_base_gc_ref(REFINEMENT_REF);
    }

    //--------------------------------------------------------------------------
    PartitionRefinementTracker::PartitionRefinementTracker(PartitionNode *node,
                                                      ProjectionPartition *proj) 
      : partition(node), refined_projection(proj), current_refinement(true),
        children_score(-1.0), children_last(0), total_traversals(0),
        return_timeout(0)
    //--------------------------------------------------------------------------
    {
      partition->add_base_gc_ref(REFINEMENT_REF);
      refined_projection->add_reference();
    }

    //--------------------------------------------------------------------------
    PartitionRefinementTracker::~PartitionRefinementTracker(void)
    //--------------------------------------------------------------------------
    {
      for (std::unordered_map<ProjectionPartition*,
                              std::pair<double,uint64_t> >::const_iterator it =
            candidate_projections.begin(); it !=
            candidate_projections.end(); it++)
        if (it->first->remove_reference())
          delete it->first;
      if ((refined_projection != NULL) && 
          refined_projection->remove_reference())
        delete refined_projection;
      if (partition->remove_base_gc_ref(REFINEMENT_REF))
        delete partition;
    }

    //--------------------------------------------------------------------------
    RefinementTracker* PartitionRefinementTracker::clone(void) const
    //--------------------------------------------------------------------------
    {
      PartitionRefinementTracker *result = (refined_projection == NULL) ?
        new PartitionRefinementTracker(partition, current_refinement) :
        new PartitionRefinementTracker(partition, refined_projection);
      for (std::unordered_map<ProjectionPartition*,
                              std::pair<double,uint64_t> >::const_iterator it =
            candidate_projections.begin(); it !=
            candidate_projections.end(); it++)
      {
        it->first->add_reference();
        result->candidate_projections.insert(*it);
      }
      result->candidate_children = candidate_children;
      result->children_score = children_score;
      result->children_last = children_last;
      result->total_traversals = total_traversals;
      result->return_timeout = return_timeout;
      return result;
    }

    //--------------------------------------------------------------------------
    bool PartitionRefinementTracker::is_disjoint_complete(void) const
    //--------------------------------------------------------------------------
    {
      if (current_refinement)
        return true;
      if (!candidate_projections.empty())
        return true;
      return (partition->get_num_children() <=
          (candidate_children.size() * CHANGE_REFINEMENT_PARTITION_FRACTION));
    }

    //--------------------------------------------------------------------------
    bool PartitionRefinementTracker::needs_fallback_refinement(
                                          const ProjectionInfo &proj_info) const
    //--------------------------------------------------------------------------
    {
      // Never need to check for a fallback refinement on a disjoint 
      // and complete partition because it can be part of its own tree
      return false;
    }

    //--------------------------------------------------------------------------
    bool PartitionRefinementTracker::update_refinement_child(
                                                          RegionTreeNode *child)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(child->is_region());
#endif
      RegionNode *child_node = child->as_region_node();
      candidate_children.insert(child_node);
      if ((candidate_children.size() * CHANGE_REFINEMENT_PARTITION_FRACTION) >=
          partition->get_num_children())
      {
        // Reset the children and bump their return count
        candidate_children.clear();
        // This counts as a tick of the traversal clock
        total_traversals++;
        // Update the children score and the children last 
        if (children_score >= 0.0)
        {
          // Recompute the score for the children and see if we want to update
          children_score = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
              (total_traversals - children_last)) * children_score + 1.0;
          const uint64_t previous_epoch = 
            children_last / CHANGE_REFINEMENT_RETURN_COUNT;
          const uint64_t current_epoch = 
            total_traversals / CHANGE_REFINEMENT_RETURN_COUNT;
          children_last = total_traversals;
          // Reset the timeout since we saw a return
          return_timeout = 0;
          // If the last time we saw this child was in a previous epoch 
          // then we can check to see if this is now the dominant child
          if (current_refinement && (previous_epoch != current_epoch) &&
              is_dominant_candidate(children_score,
                                    (refined_projection == NULL)))
          {
            // See if we're sticking with refined children or whether we're 
            // switching to them from a refined projection
            if (refined_projection == NULL)
              invalidate_unused_candidates();
            else
              return true;
          }
        }
        else
        {
          // first time we've considered all the children so initialize them
          if (++return_timeout == CHANGE_REFINEMENT_TIMEOUT)
          {
            invalidate_unused_candidates();
            return_timeout = 0;
          }
          children_score = 0.0;
          children_last = total_traversals;
        }
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool PartitionRefinementTracker::update_refinement_projection(
                                                           ProjectionNode *proj)
    //--------------------------------------------------------------------------
    {
      ProjectionPartition *projection = proj->as_partition_projection();
#ifdef DEBUG_LEGION
      assert(projection != NULL);
#endif
      // Step the clock for a new traversal
      total_traversals++;
      std::unordered_map<ProjectionPartition*,
        std::pair<double,uint64_t> >::iterator finder =
          candidate_projections.find(projection);
      if (finder != candidate_projections.end())
      {
        // Update the score using exponentially weighted moving average
        finder->second.first = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
         (total_traversals - finder->second.second))*finder->second.first + 1.0;
        // Since CHANGE_REFINEMENT_RETURN_COUNT is a power of 2 the compiler
        // should be able to optimize integer division to a basic bit shift
        const uint64_t previous_epoch = 
          finder->second.second / CHANGE_REFINEMENT_RETURN_COUNT;
        const uint64_t current_epoch = 
          total_traversals / CHANGE_REFINEMENT_RETURN_COUNT;
        finder->second.second = total_traversals;
        // Reset the timeout since we saw a return
        return_timeout = 0;
        // If the last time we saw this projection was in a previous epoch 
        // then we can check to see if this is now the dominant child
        if (current_refinement && (previous_epoch != current_epoch) &&
            is_dominant_candidate(finder->second.first, 
                                  (projection == refined_projection)))
        {
          // If we're current refinement we don't switch but just end
          // invalidating all the other candidates so they can start again
          if (projection == refined_projection)
            invalidate_unused_candidates();
          else
            return true;
        }
      }
      else if (projection == refined_projection)
      {
#ifdef DEBUG_LEGION
        assert(current_refinement);
#endif
        // This counts as a return too since we're refined this way
        candidate_projections[projection] = 
          std::pair<double,uint64_t>(1.0, total_traversals);
        projection->add_reference();
        // Reset the timeout since we saw a return
        return_timeout = 0;
        // No need to switch, we're already using the refinement
      }
      else
      {
        if (++return_timeout == CHANGE_REFINEMENT_TIMEOUT)
        {
          invalidate_unused_candidates();
          return_timeout = 0;
        }
        candidate_projections[projection] = 
          std::pair<double,uint64_t>(0.0, total_traversals);
        projection->add_reference();
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool PartitionRefinementTracker::is_dominant_candidate(double score,
                                                          bool is_current)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(current_refinement);
#endif
      if (children_last != total_traversals)
      {
        // Recompute the children score and compare it
        children_score = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
            (total_traversals - children_last)) * children_score;
        children_last = total_traversals;
        if (score < children_score)
          return false;
      }
      for (std::unordered_map<ProjectionPartition*,
                              std::pair<double,uint64_t> >::iterator it =
            candidate_projections.begin(); it !=
            candidate_projections.end(); it++) 
      {
        // Skip ourselves
        if (it->second.second == total_traversals)
          continue;
        // Recompute the score before comparing
        it->second.first = std::pow(CHANGE_REFINEMENT_RETURN_WEIGHT,
            (total_traversals - it->second.second)) * it->second.first;
        it->second.second = total_traversals;
        if (score < it->second.first)
          return false;
      }
      if (!is_current)
      {
        if (refined_projection != NULL)
        {
          // If we're not the current refinement, then we want to make sure its
          // obvious that we should switch and not just ping-pong so we need to
          // have a score that is at least sqrt(total_candidates) more than the
          // current refinement's score
          std::unordered_map<ProjectionPartition*,
              std::pair<double,uint64_t> >::const_iterator 
                finder = candidate_projections.find(refined_projection);
          // Current refinement is never observed
          if (finder == candidate_projections.end())
            return true;
          double total_candidates =
            (children_score >= 0.0) ? 1 : 0 + candidate_projections.size();
          double hysteresis = std::sqrt(total_candidates);
          return ((finder->second.first * hysteresis) < score);
        }
        else
        {
          // The children are refined so compute the total candidates
          if (children_score < 0.0)
            return true;
          double total_candidates = candidate_projections.size() + 1;
          double hysteresis = std::sqrt(total_candidates);
          return ((children_score * hysteresis) < score);
        }
      }
      return true;
    }

    //--------------------------------------------------------------------------
    void PartitionRefinementTracker::invalidate_unused_candidates(void)
    //--------------------------------------------------------------------------
    {
      if (children_score >= 0.0)
      {
        uint64_t last_observation = total_traversals - children_last;
        if (CHANGE_REFINEMENT_TIMEOUT < last_observation)
          children_score = -1.0;
      }
      if (!candidate_projections.empty())
      {
        for (std::unordered_map<ProjectionPartition*,
                            std::pair<double,uint64_t> >::iterator it =
              candidate_projections.begin(); it !=
              candidate_projections.end(); /*nothing*/)
        {
          uint64_t last_observation = total_traversals - it->second.second;
          if (CHANGE_REFINEMENT_TIMEOUT < last_observation)
          {
            if (it->first->remove_reference())
              delete it->first;
            std::unordered_map<ProjectionPartition*,
              std::pair<double,uint64_t> >::iterator to_delete = it++;
            candidate_projections.erase(to_delete);
          }
          else
            it++;
        }
      }
    }

    //--------------------------------------------------------------------------
    void PartitionRefinementTracker::change_refinements(ContextID ctx,
                               const FieldMask &refinement_mask,
                               FieldMaskSet<RegionTreeNode> &new_children,
                               FieldMaskSet<ProjectionNode> &new_projections)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      // Should never get this call for a current refinement on a partition
      // unless it was a projection based one
      assert(!current_refinement || (refined_projection != NULL));
#endif
      double max_score = children_score;
      ProjectionNode *best_projection = NULL;
      for (std::unordered_map<ProjectionPartition*,
                              std::pair<double,uint64_t> >::const_iterator it =
            candidate_projections.begin(); it !=
            candidate_projections.end(); it++)
      {
        if (max_score < it->second.first)
        {
          max_score = it->second.first;
          best_projection = it->first;
        }
      }
#ifdef DEBUG_LEGION
      assert(max_score > 0.0);
#endif
      if (best_projection == NULL)
      {
        // Refining the children is the best option
        for (ColorSpaceIterator itr(partition->row_source); itr; itr++)
          new_children.insert(partition->get_child(*itr), refinement_mask);
      }
      else // There was a new best projection to use
        new_projections.insert(best_projection, refinement_mask);
    }

    //--------------------------------------------------------------------------
    void PartitionRefinementTracker::invalidate_refinement(ContextID ctx,
                                             const FieldMask &invalidation_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(current_refinement);
#endif
      if (refined_projection == NULL)
      {
        for (ColorSpaceIterator itr(partition->row_source); itr; itr++)
        {
          RegionNode *child = partition->get_child(*itr);
          child->invalidate_logical_refinement(ctx, invalidation_mask);
        }
      }
    }

    //--------------------------------------------------------------------------
    void PartitionRefinementTracker::find_child_refinements(
                                      std::set<RegionTreeNode*> &children) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(current_refinement);
#endif
      if (refined_projection == NULL)
      {
        for (ColorSpaceIterator itr(partition->row_source); itr; itr++)
          children.insert(partition->get_child(*itr));
      }
    }

    //--------------------------------------------------------------------------
    void PartitionRefinementTracker::convert(
                             const std::vector<ShardID> *shard_to_shard_mapping)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(current_refinement);
#endif
      if (refined_projection != NULL)
        refined_projection->convert(shard_to_shard_mapping);
    }

    //--------------------------------------------------------------------------
    void PartitionRefinementTracker::merge(RefinementTracker *rhs,
                             const std::vector<ShardID> *shard_to_shard_mapping)
    //--------------------------------------------------------------------------
    {
      PartitionRefinementTracker *other = rhs->as_partition_tracker();
#ifdef DEBUG_LEGION
      assert(other != NULL);
      assert(partition == other->partition);
      assert(current_refinement);
      assert(other->current_refinement);
      assert((refined_projection != NULL) ==
          (other->refined_projection != NULL));
#endif
      if (refined_projection != NULL)
        refined_projection->merge(other->refined_projection,
                                  shard_to_shard_mapping);
    }

    //--------------------------------------------------------------------------
    void PartitionRefinementTracker::pack(Serializer &rez,
                             std::map<LegionColor,RegionTreeNode*> &to_traverse)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(current_refinement);
#endif
      if (refined_projection == NULL)
      {
        rez.serialize<bool>(true); // has children
        for (ColorSpaceIterator itr(partition->row_source); itr; itr++)
        {
          RegionNode *child = partition->get_child(*itr);
          to_traverse[child->row_source->color] = child;
        }
      }
      else
      {
        rez.serialize<bool>(false); // has children
        refined_projection->pack(rez);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ PartitionRefinementTracker* PartitionRefinementTracker::unpack(
        PartitionNode *partition, Deserializer &derez,
        std::map<LegionColor,RegionTreeNode*> &to_traverse)
    //--------------------------------------------------------------------------
    {
      bool has_children;
      derez.deserialize(has_children);
      if (has_children)
      {
        for (ColorSpaceIterator itr(partition->row_source); itr; itr++)
        {
          RegionNode *child = partition->get_child(*itr);
          to_traverse[child->row_source->color] = child;
        }
        return new PartitionRefinementTracker(partition, true/*current*/);
      }
      else
      {
        ProjectionPartition *projection = new ProjectionPartition(partition); 
        projection->unpack(derez);
        return new PartitionRefinementTracker(partition, projection);
      }
    }
#endif

    /////////////////////////////////////////////////////////////
    // LogicalState 
    ///////////////////////////////////////////////////////////// 

    //--------------------------------------------------------------------------
    LogicalState::LogicalState(RegionTreeNode *node, ContextID c)
      : owner(node)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalState::~LogicalState(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    void LogicalState::check_init(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(field_states.empty());
      assert(curr_epoch_users.empty());
      assert(prev_epoch_users.empty());
      assert(refinement_trackers.empty());
      assert(projection_summary_cache.empty());
      assert(interfering_shards.empty());
#endif
    }

#ifdef DEBUG_LEGION
    //--------------------------------------------------------------------------
    void LogicalState::sanity_check(void) const
    //--------------------------------------------------------------------------
    {
      // For every child and every field, it should only be open in one mode
      FieldMaskSet<RegionTreeNode> previous_children;
      for (std::list<FieldState>::const_iterator fit = 
            field_states.begin(); fit != field_states.end(); fit++)
      {
        FieldMask actually_valid;
        for (FieldState::OrderedFieldMaskChildren::const_iterator it =
              fit->open_children.begin(); it != 
              fit->open_children.end(); it++)
        {
          actually_valid |= it->second;
          FieldMaskSet<RegionTreeNode>::iterator finder = 
            previous_children.find(it->first);
          if (finder != previous_children.end())
          {
            assert(!(finder->second & it->second));
            finder.merge(it->second);
          }
          else
            previous_children.insert(it->first, it->second);
        }
        // Actually valid should be greater than or equal
        assert(!(actually_valid - fit->valid_fields()));
      }
#if 0
      // Also check that for each field it is either only open in one mode
      // or two children in different modes are disjoint
      // Note this is no longer a valid check since we are just
      // tracking which sub-trees have open users in them and we
      // might have two projection region requirements from the same 
      // operation that appear interferring based on which sub-trees
      // they are using but really aren't interfering
      for (std::list<FieldState>::const_iterator it1 = 
            field_states.begin(); it1 != field_states.end(); it1++)
      {
        for (std::list<FieldState>::const_iterator it2 = 
              field_states.begin(); it2 != field_states.end(); it2++)
        {
          // No need to do comparisons if they are the same field state
          if (it1 == it2) 
            continue;
          const FieldState &f1 = *it1;
          const FieldState &f2 = *it2;
          for (FieldState::OrderedFieldMaskChildren::const_iterator cit1 = 
                f1.open_children.begin(); cit1 != 
                f1.open_children.end(); cit1++)
          {
            for (FieldState::OrderedFieldMaskChildren::const_iterator cit2 =
                  f2.open_children.begin(); cit2 != 
                  f2.open_children.end(); cit2++)
            {
              
              // Disjointness check on fields
              if (cit1->second * cit2->second)
                continue;
#ifndef NDEBUG
              LegionColor c1 = cit1->first->get_color();
              LegionColor c2 = cit2->first->get_color();
#endif
              // Some aliasing in the fields, so do the check 
              // for child disjointness
              assert(c1 != c2);
              assert(owner->are_children_disjoint(c1, c2));
            }
          }
        }
      }
#endif
      // Make sure that each refinement has a disjoint set of fields
      if (!refinement_trackers.empty())
      {
        FieldMask disjoint_refinements;
        for (FieldMaskSet<RefinementTracker>::const_iterator it =
              refinement_trackers.begin(); it != 
              refinement_trackers.end(); it++)
        {
          assert(disjoint_refinements * it->second);
          disjoint_refinements |= it->second;
        }
      }
    }
#endif

    //--------------------------------------------------------------------------
    void LogicalState::clear(void)
    //--------------------------------------------------------------------------
    {
      field_states.clear();
      if (!curr_epoch_users.empty())
      {
        for (OrderedFieldMaskUsers::const_iterator it =
              curr_epoch_users.begin(); it != curr_epoch_users.end(); it++)
          if (it->first->remove_reference())
            delete it->first;
        curr_epoch_users.clear();
      }
      if (!prev_epoch_users.empty())
      {
        for (OrderedFieldMaskUsers::const_iterator it =
              prev_epoch_users.begin(); it != prev_epoch_users.end(); it++)
          if (it->first->remove_reference())
            delete it->first;
        prev_epoch_users.clear();
      }
      if (!refinement_trackers.empty())
      {
        for (FieldMaskSet<RefinementTracker>::const_iterator it =
              refinement_trackers.begin(); it !=
              refinement_trackers.end(); it++)
          delete it->first;
        refinement_trackers.clear();
      }
      while (!projection_summary_cache.empty())
      {
        ProjectionSummary *summary = projection_summary_cache.back();
        projection_summary_cache.pop_back();
        if ((projection_summary_cache.size() < PROJECTION_CACHE_SIZE) &&
            summary->remove_reference())
          delete summary;
      }
    }

    //--------------------------------------------------------------------------
    void LogicalState::clear_deleted_state(ContextID ctx,
                                           const FieldMask &deleted_mask)
    //--------------------------------------------------------------------------
    {
      for (LegionList<FieldState>::iterator it = field_states.begin();
            it != field_states.end(); /*nothing*/)
      {
        if (it->filter(deleted_mask))
          it = field_states.erase(it);
        else
          it++;
      }
      if (!curr_epoch_users.empty() && 
          !(deleted_mask * curr_epoch_users.get_valid_mask()))
      {
        std::vector<LogicalUser*> to_delete;
        for (OrderedFieldMaskUsers::iterator it =
              curr_epoch_users.begin(); it != curr_epoch_users.end(); it++)
        {
          it.filter(deleted_mask);
          if (!it->second)
            to_delete.push_back(it->first);
        }
        for (std::vector<LogicalUser*>::const_iterator it =
              to_delete.begin(); it != to_delete.end(); it++)
        {
          curr_epoch_users.erase(*it);
          if ((*it)->remove_reference())
            delete (*it);
        }
        curr_epoch_users.tighten_valid_mask();
      }
      if (!prev_epoch_users.empty() &&
          !(deleted_mask * prev_epoch_users.get_valid_mask()))
      {
        std::vector<LogicalUser*> to_delete;
        for (OrderedFieldMaskUsers::iterator it =
              prev_epoch_users.begin(); it != prev_epoch_users.end(); it++)
        {
          it.filter(deleted_mask);
          if (!it->second)
            to_delete.push_back(it->first);
        }
        for (std::vector<LogicalUser*>::const_iterator it =
              to_delete.begin(); it != to_delete.end(); it++)
        {
          prev_epoch_users.erase(*it);
          if ((*it)->remove_reference())
            delete (*it);
        }
        prev_epoch_users.tighten_valid_mask();
      }
      if (!refinement_trackers.empty() &&
          !(deleted_mask * refinement_trackers.get_valid_mask()))
      {
        std::vector<RefinementTracker*> to_delete;
        for (FieldMaskSet<RefinementTracker>::iterator it =
              refinement_trackers.begin(); it != 
              refinement_trackers.end(); it++)
        {
          const FieldMask overlap = deleted_mask & it->second;
          if (!it->second)
            continue;
          it->first->invalidate_refinement(ctx, overlap);
          it.filter(overlap);
          if (!it->second)
            to_delete.push_back(it->first);
        }
        for (std::vector<RefinementTracker*>::const_iterator it =
              to_delete.begin(); it != to_delete.end(); it++)
        {
          refinement_trackers.erase(*it);
          delete (*it);
        }
        refinement_trackers.tighten_valid_mask();
      }
    }

#if 0
    //--------------------------------------------------------------------------
    void LogicalState::pack_refinements(Serializer &rez, 
                       std::map<LegionColor,RegionTreeNode*> &to_traverse) const
    //--------------------------------------------------------------------------
    {
      RezCheck z(rez);
      rez.serialize<size_t>(refinement_trackers.size());
      for (FieldMaskSet<RefinementTracker>::const_iterator it =
            refinement_trackers.begin(); it != refinement_trackers.end(); it++)
      {
        it->first->pack(rez, to_traverse);
        it->second.serialize(rez);
      }
    }

    //--------------------------------------------------------------------------
    void LogicalState::unpack_refinements(Deserializer &derez,
                             std::map<LegionColor,RegionTreeNode*> &to_traverse)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      size_t num_refinements;
      derez.deserialize(num_refinements);
      for (unsigned idx = 0; idx < num_refinements; idx++)
      {
        RefinementTracker *tracker = 
          owner->unpack_refinement(derez, to_traverse);
        FieldMask mask;
        mask.deserialize(derez);
        refinement_trackers.insert(tracker, mask);
      }
    }

    //--------------------------------------------------------------------------
    void LogicalState::merge_refinements(LogicalState &src, 
                             const std::vector<ShardID> *shard_to_shard_mapping,
                             std::set<RegionTreeNode*> &to_traverse)
    //--------------------------------------------------------------------------
    {
      // Just need to merge the refinement state
      if (!src.refinement_trackers.empty())
      {
        // We're only migrating the non-projected parts of the
        // refinement since projections don't translate between
        // contexts so we only need to move these over once
        for (FieldMaskSet<RefinementTracker>::iterator src_it =
              src.refinement_trackers.begin(); src_it !=
              src.refinement_trackers.end(); src_it++)
        {
          src_it->first->find_child_refinements(to_traverse);
          for (FieldMaskSet<RefinementTracker>::iterator dst_it =
                refinement_trackers.begin(); dst_it !=
                refinement_trackers.end(); dst_it++)
          {
            const FieldMask overlap = dst_it->second & src_it->second;
            if (!overlap)
              continue;
            dst_it->first->merge(src_it->first, shard_to_shard_mapping);
            src_it.filter(overlap);
            if (!src_it->second)
              break;
          }
          if (!!src_it->second)
          {
            src_it->first->convert(shard_to_shard_mapping);
            refinement_trackers.insert(src_it->first, src_it->second);
          }
          else
            delete src_it->first;
        }
        src.refinement_trackers.clear();
      }
      src.reset();
    }

    //--------------------------------------------------------------------------
    void LogicalState::convert_refinements(LogicalState &src,
        const std::vector<ShardID> *shard_to_shard_mapping,
        std::set<RegionTreeNode*> &to_traverse)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      check_init();
#endif
      refinement_trackers.swap(src.refinement_trackers);
      for (FieldMaskSet<RefinementTracker>::const_iterator it =
            refinement_trackers.begin(); it != 
            refinement_trackers.end(); it++)
      {
        it->first->find_child_refinements(to_traverse);
        it->first->convert(shard_to_shard_mapping);
      }
      src.reset();
    }
#endif

    //--------------------------------------------------------------------------
    ProjectionSummary* LogicalState::find_or_create_projection_summary(
                   Operation *op, unsigned index, const RegionRequirement &req,
                   LogicalAnalysis &analysis, const ProjectionInfo &proj_info)
    //--------------------------------------------------------------------------
    {
      // Check to see if the projection functor is functional
      if (proj_info.projection->is_functional)
      {
        // Check to see if we can find this in the cache
        unsigned index = 0;
        ProjectionSummary *invalidated = NULL;
        for (std::list<ProjectionSummary*>::iterator it =
              projection_summary_cache.begin(); it !=
              projection_summary_cache.end(); it++, index++)
        {
          if ((*it)->matches(proj_info, req))
          {
            ProjectionSummary *result = *it;
            // Move it to the front of the list if it wasn't already there
            if (it != projection_summary_cache.begin())
            {
              projection_summary_cache.splice(projection_summary_cache.begin(),
                  projection_summary_cache, it);
              // If this wasn't already in the cache range with a reference
              // then we need to update the reference counts
              if (PROJECTION_CACHE_SIZE <= index)
              {
                // Add a reference to the result
                result->add_reference();
#ifdef DEBUG_LEGION
                assert(invalidated != NULL);
#endif
                if (invalidated->remove_reference())
                  delete invalidated;
              }
            }
            return result;
          }
          if (index == (PROJECTION_CACHE_SIZE - 1))
          {
            // If we make it here then this is the last entry in the
            // cache currenty and we're either going to match on an
            // entry outside the cache and move it into the cache
            // or we're going to make a new entry so this summary
            // is going to be invalidated either way, so save it
            // so we can do the invalidation after we're done 
            // traversing the list when we won't mess with the iterator
            invalidated = (*it);
          }
        }
        if ((invalidated != NULL) && invalidated->remove_reference())
          delete invalidated;
      }
      ProjectionSummary *result =
        analysis.context->construct_projection_summary(op, index, req,
                                                       this, proj_info);
      // If the projection functor is functional then we can save it for
      // the future and evict the least recently used projection
      if (proj_info.projection->is_functional)
      {
        result->add_reference();
        projection_summary_cache.push_front(result);
      }
      return result;
    } 

    //--------------------------------------------------------------------------
    void LogicalState::remove_projection_summary(ProjectionSummary *summary)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(summary->owner == this);
#endif
      if (summary->projection->is_functional &&
          (PROJECTION_CACHE_SIZE <= projection_summary_cache.size()))
      {
        // Only need to filter from things not in the cache so start
        // from the back and only go up to the last element not in the cache
        // Note: handle the off-by-one case where we're deleting the last
        // thing in the cache because we're about to add a new summary
        const unsigned stop = 
          (projection_summary_cache.size() - PROJECTION_CACHE_SIZE) + 1;
        std::list<ProjectionSummary*>::reverse_iterator finder =
          projection_summary_cache.rbegin();
        for (unsigned idx = 0; idx < stop; idx++)
        {
          if ((*finder) == summary)
          {
            // Reverse iterators are stupid
            projection_summary_cache.erase(std::next(finder).base());
            break;
          }
          finder++;
        }
      }
      std::unordered_map<ProjectionSummary*,
        std::unordered_map<ProjectionSummary*,bool> >::iterator finder =
          interfering_shards.find(summary);
      if (finder != interfering_shards.end())
      {
        for (std::unordered_map<ProjectionSummary*,bool>::const_iterator it =
              finder->second.begin(); it != finder->second.end(); it++)
          interfering_shards[it->first].erase(summary);
        interfering_shards.erase(finder);
      }
    }

    //--------------------------------------------------------------------------
    bool LogicalState::has_interfering_shards(LogicalAnalysis &analysis,
                                 ProjectionSummary *one, ProjectionSummary *two)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(one->owner == this);
      assert(two->owner == this);
#endif
      if (one == two)
        // We can elide the close operation here if we can prove that
        // all the regions used a disjoint from each other and they all
        // have exactly one kind of shard user
        return (!one->can_perform_name_based_self_analysis() ||
                !one->has_unique_shard_users());
      std::unordered_map<ProjectionSummary*,
        std::unordered_map<ProjectionSummary*,bool> >::const_iterator
          one_finder = interfering_shards.find(one);
      if (one_finder != interfering_shards.end())
      {
        std::unordered_map<ProjectionSummary*,bool>::const_iterator
          two_finder = one_finder->second.find(two);
        if (two_finder != one_finder->second.end())
          return two_finder->second;
      }
      // Do the test and save the results for later
      const bool result = analysis.context->has_interfering_shards(one,two);
      interfering_shards[one][two] = result;
      interfering_shards[two][one] = result;
      return result;
    }

#if 0
    //--------------------------------------------------------------------------
    ProjectionNode* LogicalState::find_or_create_fallback_refinement(
                             InnerContext *context, IndexSpaceNode *color_space)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(owner->is_region());
#endif
      if (fallback_refinement == NULL)
      {
        fallback_refinement = context->compute_fallback_refinement(
            owner->as_region_node(), color_space);
        fallback_refinement->add_reference();
      }
      return fallback_refinement;
    }
#endif

    //--------------------------------------------------------------------------
    void LogicalState::initialize_refined_fields(const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(owner->is_region());
      assert(mask * refinement_trackers.get_valid_mask());
#endif
      RefinementTracker *new_tracker = owner->create_refinement_tracker();
      new_tracker->initialize_already_refined();
      refinement_trackers.insert(new_tracker, mask);
    }

    //--------------------------------------------------------------------------
    void LogicalState::update_refinement_child(ContextID ctx,
                                               RegionTreeNode *child,
                                               const RegionUsage &usage,
                                               FieldMask &refinement_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!!refinement_mask);
#endif
      // This function filters through the refinement trackers and updates
      // them as necessary. Since the refinement trackers are not field aware,
      // this function will clone them and delete them as necessary to make
      // sure that each field is accurately represented
      FieldMask need_tracker = refinement_mask;
      if (!(refinement_mask * refinement_trackers.get_valid_mask()))
      {
        FieldMaskSet<RefinementTracker> to_add;
        std::vector<RefinementTracker*> to_delete;
        for (FieldMaskSet<RefinementTracker>::iterator it =
             refinement_trackers.begin(); it != refinement_trackers.end(); it++)
        {
          const FieldMask overlap = refinement_mask & it->second;
          if (!overlap)
            continue;
          if (overlap != it->second)
          {
            RefinementTracker *diff = it->first->clone();
            FieldMask diff_mask = it->second - overlap;
            to_add.insert(diff, diff_mask);
            it.filter(diff_mask);
          }
          bool allow_refinement = false;
          if (it->first->update_child(child, usage, allow_refinement))
          {
            it->first->invalidate_refinement(ctx, overlap);
            to_delete.push_back(it->first);
          }
          else 
          {
            if (!allow_refinement)
              refinement_mask -= overlap;
            need_tracker -= overlap;
            if (!need_tracker)
              break;
          }
        }
        // Remove old entries
        for (std::vector<RefinementTracker*>::const_iterator it =
              to_delete.begin(); it != to_delete.end(); it++)
        {
          refinement_trackers.erase(*it);
          delete (*it);
        }
        // Add new entries
        for (FieldMaskSet<RefinementTracker>::const_iterator it =
              to_add.begin(); it != to_add.end(); it++)
          refinement_trackers.insert(it->first, it->second);
      }
      if (!!need_tracker)
      {
        RefinementTracker *new_tracker = owner->create_refinement_tracker();
        bool allow_refinement = false;
        if (new_tracker->update_child(child, usage, allow_refinement))
          assert(false); // should never get here
        if (!allow_refinement)
          refinement_mask -= need_tracker;
        refinement_trackers.insert(new_tracker, need_tracker);
      }
    }

    //--------------------------------------------------------------------------
    void LogicalState::update_refinement_projection(ContextID ctx,
        ProjectionSummary *summary, const RegionUsage &usage,
        FieldMask &refinement_mask)
    //--------------------------------------------------------------------------
    {
      // This function filters through the refinement trackers and updates
      // them as necessary. Since the refinement trackers are not field aware,
      // this function will clone them and delete them as necessary to make
      // sure that each field is accurately represented
      FieldMask need_tracker = refinement_mask;
      if (!(refinement_mask * refinement_trackers.get_valid_mask()))
      {
        FieldMaskSet<RefinementTracker> to_add;
        std::vector<RefinementTracker*> to_delete;
        for (FieldMaskSet<RefinementTracker>::iterator it =
             refinement_trackers.begin(); it != refinement_trackers.end(); it++)
        {
          const FieldMask overlap = refinement_mask & it->second;
          if (!overlap)
            continue;
          if (overlap != it->second)
          {
            RefinementTracker *diff = it->first->clone();
            FieldMask diff_mask = it->second - overlap;
            to_add.insert(diff, diff_mask);
            it.filter(diff_mask);
          }
          bool allow_refinement = false;
          if (it->first->update_projection(summary, usage, allow_refinement))
          {
            it->first->invalidate_refinement(ctx, overlap);
            to_delete.push_back(it->first);
          }
          else
          {
            if (!allow_refinement)
              refinement_mask -= overlap;
            need_tracker -= overlap;
            if (!need_tracker)
              break;
          }
        }
        // Remove old entries
        for (std::vector<RefinementTracker*>::const_iterator it =
              to_delete.begin(); it != to_delete.end(); it++)
        {
          refinement_trackers.erase(*it);
          delete (*it);
        }
        // Add new entries
        for (FieldMaskSet<RefinementTracker>::const_iterator it =
              to_add.begin(); it != to_add.end(); it++)
          refinement_trackers.insert(it->first, it->second);
      }
      if (!!need_tracker)
      {
        RefinementTracker *new_tracker = owner->create_refinement_tracker();
        bool allow_refinement = false;
        if (new_tracker->update_projection(summary, usage, allow_refinement))
          assert(false); // should never get here
        if (!allow_refinement)
          refinement_mask -= need_tracker;
        refinement_trackers.insert(new_tracker, need_tracker);
      }
    }

    //--------------------------------------------------------------------------
    void LogicalState::update_refinement_arrival(ContextID ctx, 
                           const RegionUsage &usage, FieldMask &refinement_mask)
    //--------------------------------------------------------------------------
    {
      // This function filters through the refinement trackers and updates
      // them as necessary. Since the refinement trackers are not field aware,
      // this function will clone them and delete them as necessary to make
      // sure that each field is accurately represented
      if (!(refinement_mask * refinement_trackers.get_valid_mask()))
      {
        FieldMaskSet<RefinementTracker> to_add;
        std::vector<RefinementTracker*> to_delete;
        for (FieldMaskSet<RefinementTracker>::iterator it =
             refinement_trackers.begin(); it != refinement_trackers.end(); it++)
        {
          const FieldMask overlap = refinement_mask & it->second;
          if (!overlap)
            continue;
          if (overlap != it->second)
          {
            RefinementTracker *diff = it->first->clone();
            FieldMask diff_mask = it->second - overlap;
            to_add.insert(diff, diff_mask);
            it.filter(diff_mask);
          }
          if (it->first->update_arrival(usage))
          {
            it->first->invalidate_refinement(ctx, overlap);
            to_delete.push_back(it->first);
          }
          else
          {
            refinement_mask -= overlap;
            if (!refinement_mask)
              break;
          }
        }
        // Remove old entries
        for (std::vector<RefinementTracker*>::const_iterator it =
              to_delete.begin(); it != to_delete.end(); it++)
        {
          refinement_trackers.erase(*it);
          delete (*it);
        }
        // Add new entries
        for (FieldMaskSet<RefinementTracker>::const_iterator it =
              to_add.begin(); it != to_add.end(); it++)
          refinement_trackers.insert(it->first, it->second);
      }
      if (!!refinement_mask)
      {
        RefinementTracker *new_tracker = owner->create_refinement_tracker();
        if (new_tracker->update_arrival(usage))
          assert(false); // should never get here
        refinement_trackers.insert(new_tracker, refinement_mask);
      }
    }

#if 0
    //--------------------------------------------------------------------------
    void LogicalState::initialize_unrefined_fields(const FieldMask &mask,
                                const unsigned index, LogicalAnalysis &analysis)
    //--------------------------------------------------------------------------
    {
      FieldMask uninitialized = mask - refinement_trackers.get_valid_mask();
      if (!uninitialized)
        return;
      initialize_refined_fields(uninitialized); 
      // Record that the analysis is responsibile for issuing at least some
      // kind of refinement operation for these fields to initialize them
      analysis.record_unrefined_fields(owner->as_region_node(),
                                       index, uninitialized);
    }

    //--------------------------------------------------------------------------
    void LogicalState::update_refinement_child(
           FieldMask &disjoint_complete_mask, FieldMask traversal_mask,
           RegionTreeNode *next_child, FieldMask child_disjoint_complete_mask,
           const ProjectionInfo &info, LogicalAnalysis &analysis,
           ContextID ctx, LogicalRegion privilege, unsigned req_index,
           FieldMaskSet<RefinementOp> &refinement_operations)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!!child_disjoint_complete_mask);
      assert(!(child_disjoint_complete_mask - traversal_mask));
#endif
      // child_refine is for when we decide to refine the child
      // fallback_refine is for any fields that aren't refined for the child
      // and therefore we need to check to see if we need a fallback refinement
      // at this level for any projections with large fan-outs
      FieldMask child_refine, fallback_refine;
      // First we record the child refinements for any fields for which
      // the next child is considered disjoint and complete
      if (!!child_disjoint_complete_mask)
      {
        FieldMaskSet<RefinementTracker> to_add;
        for (FieldMaskSet<RefinementTracker>::iterator it =
              refinement_trackers.begin(); it !=
              refinement_trackers.end(); it++)
        {
          const FieldMask overlap = child_disjoint_complete_mask & it->second;
          if (!overlap)
            continue;
          if (overlap != it->second)
          {
            RefinementTracker *diff = it->first->clone();
            FieldMask diff_mask = it->second - overlap;
            to_add.insert(diff, diff_mask);
            it.filter(diff_mask);
          }
          if (it->first->update_refinement_child(next_child))
          {
            child_refine |= overlap;
            disjoint_complete_mask |= overlap;
            // Don't delete this yet, it will get replaced
            // when we update the refinements
          }
          else if (it->first->is_disjoint_complete())
            disjoint_complete_mask |= overlap;
          traversal_mask -= overlap;
          child_disjoint_complete_mask -= overlap;
          if (!child_disjoint_complete_mask)
            break;
        }
        // If we still have child disjoint complete fields then we can
        // make a new refinement tracker for them
        if (!!child_disjoint_complete_mask)
        {
          RefinementTracker *new_tracker =
            owner->create_refinement_tracker(false/*current refinement*/);
          if (new_tracker->update_refinement_child(next_child))
          {
            child_refine |= child_disjoint_complete_mask;
            disjoint_complete_mask |= child_disjoint_complete_mask;
            delete new_tracker;
          }
          else
          {
            refinement_trackers.insert(new_tracker,
                                       child_disjoint_complete_mask);
            if (new_tracker->is_disjoint_complete())
              disjoint_complete_mask |= child_disjoint_complete_mask;
          }
        }
        // Add new entries
        for (FieldMaskSet<RefinementTracker>::const_iterator it =
              to_add.begin(); it != to_add.end(); it++)
          refinement_trackers.insert(it->first, it->second);
      }
      // If we still have fields for which the child wasn't considered
      // disjoint and complete then we still need to check to see if 
      // any of current refinement trackers for those other fields can
      // be considered disjoint and complete or whether they need a 
      // fallback refinement for projections to ensure there are enough
      // equivalence sets for all the points in the projection
      if (!!traversal_mask)
      {
        std::vector<RefinementTracker*> to_delete;
        for (FieldMaskSet<RefinementTracker>::iterator it =
              refinement_trackers.begin(); it !=
              refinement_trackers.end(); it++)
        {
          const FieldMask overlap = traversal_mask & it->second;
          if (!overlap)
            continue;
          if (it->first->needs_fallback_refinement(info))
          {
            fallback_refine |= overlap;
            disjoint_complete_mask |= overlap;
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
          }
          else if (it->first->is_disjoint_complete())
            disjoint_complete_mask |= overlap;
          traversal_mask -= overlap;
          if (!traversal_mask)
            break;
        }
        // Remove old entries
        for (std::vector<RefinementTracker*>::const_iterator it =
              to_delete.begin(); it != to_delete.end(); it++)
        {
          refinement_trackers.erase(*it);
          delete (*it);
        }
      }
      // If we're updating the refinement then we can do that now
      if (!!child_refine)
      {
        // Starting at this node update the new subtrees to tell them
        // that they are refined and then produce refinement trees
        FieldMaskSet<RefinementNode> refinements;
        owner->update_logical_refinement(ctx, 
            analysis.context->get_total_shards(), child_refine, refinements);
#ifdef DEBUG_LEGION
        assert(refinements.get_valid_mask() == child_refine);
#endif
        // Record the refinement tree with the analysis  
        for (FieldMaskSet<RefinementNode>::const_iterator it =
              refinements.begin(); it != refinements.end(); it++)
          analysis.record_pending_refinement(privilege, req_index,
                    it->first, it->second, refinement_operations);
      }
      if (!!fallback_refine)
      {
        // Create a projection summary to represent the fallback refinement
        // subtree to use for equivalence sets
        ProjectionNode *fallback = find_or_create_fallback_refinement(
            analysis.context, info.projection_space);
        // Create a new refinement for the fallback fields
        RefinementTracker *tracker = owner->create_refinement_tracker(fallback);
        refinement_trackers.insert(tracker, fallback_refine);
        // Create a refinement tree and record it with the analysis
        RefinementNode *refinement = fallback->create_refinement();
        analysis.record_pending_refinement(privilege, req_index,
            refinement, fallback_refine, refinement_operations);
      }
    }

    //--------------------------------------------------------------------------
    void LogicalState::update_refinement_projection(
              FieldMask &disjoint_complete_mask, 
              FieldMask traversal_mask, ProjectionSummary *projection,
              LogicalAnalysis &logical_analysis, ContextID ctx,
              LogicalRegion privilege, unsigned req_index,
              FieldMaskSet<RefinementOp> &refinement_operations)
    //--------------------------------------------------------------------------
    {
      if (projection->result->is_disjoint() && 
          projection->result->is_complete())
      {
        // The projection is disjoint and complete so we can record it
        FieldMask refine_now;
        FieldMaskSet<RefinementTracker> to_add;
        std::vector<RefinementTracker*> to_delete;
        for (FieldMaskSet<RefinementTracker>::iterator it =
              refinement_trackers.begin(); it !=
              refinement_trackers.end(); it++)
        {
          const FieldMask overlap = traversal_mask & it->second;
          if (!overlap)
            continue;
          if (overlap != it->second)
          {
            // Need to split out the state for the unmatched fields
            RefinementTracker *diff = it->first->clone();
            FieldMask diff_mask = it->second - overlap;
            to_add.insert(diff, diff_mask);
            it.filter(diff_mask);
          }
          if (it->first->update_refinement_projection(projection->result))
          {
            refine_now |= overlap;
            disjoint_complete_mask |= overlap;
            it->first->invalidate_refinement(ctx, overlap);
            to_delete.push_back(it->first);
          }
          else if (it->first->is_disjoint_complete())
            disjoint_complete_mask |= overlap;
          traversal_mask -= overlap;
          if (!traversal_mask)
            break;
        }
        if (!!traversal_mask)
        {
          RefinementTracker *new_tracker =
            owner->create_refinement_tracker(false/*current refinement*/);
          if (new_tracker->update_refinement_projection(projection->result))
          {
            refine_now |= traversal_mask;
            disjoint_complete_mask |= traversal_mask;
            delete new_tracker;
          }
          else
          {
            refinement_trackers.insert(new_tracker, traversal_mask); 
            if (new_tracker->is_disjoint_complete())
              disjoint_complete_mask |= traversal_mask;
          }
        }
        // Remove old entries
        for (std::vector<RefinementTracker*>::const_iterator it =
              to_delete.begin(); it != to_delete.end(); it++)
        {
          refinement_trackers.erase(*it);
          delete (*it);
        }
        // Add new entries
        for (FieldMaskSet<RefinementTracker>::const_iterator it =
              to_add.begin(); it != to_add.end(); it++)
          refinement_trackers.insert(it->first, it->second);
        if (!!refine_now)
        {
          // Create a new refinement tracker that knows its the new owner
          RefinementTracker *tracker = 
            owner->create_refinement_tracker(projection);
          refinement_trackers.insert(tracker, refine_now);
          // Inform the subtree that it's now refined
          RefinementNode *refinement = projection->result->create_refinement();
          // Record the refinement tree with the analysis  
          logical_analysis.record_pending_refinement(privilege, req_index,
                            refinement, refine_now, refinement_operations);
        }
      }
      else
      {
        // The projection is not disjoint and complete so we need to 
        // iterate through and see which children are disjoint and complete
        for (FieldMaskSet<RefinementTracker>::const_iterator it =
              refinement_trackers.begin(); it != 
              refinement_trackers.end(); it++)
        {
          const FieldMask overlap = traversal_mask & it->second;
          if (!overlap)
            continue;
          if (it->first->is_disjoint_complete())
            disjoint_complete_mask |= overlap;
          traversal_mask -= overlap;
          if (!traversal_mask)
            break;
        }
        if (!!traversal_mask)
        {
          // These fields are not refined at all and we can't use the
          // projection because its not disjoint and complete so we 
          // need to make a default one to consider
          ProjectionNode *fallback = find_or_create_fallback_refinement(
              logical_analysis.context, projection->domain);
          RefinementTracker *tracker =
            owner->create_refinement_tracker(false/*current refinement*/);
          tracker->update_refinement_projection(fallback);
          refinement_trackers.insert(tracker, traversal_mask);
          // Create a refinement tree and record it with the analysis
          RefinementNode *refinement = fallback->create_refinement();
          logical_analysis.record_pending_refinement(privilege, req_index,
              refinement, traversal_mask, refinement_operations);
          // These fields are now also disjoint and complete
          disjoint_complete_mask |= traversal_mask;
        }
      }
    }

    //--------------------------------------------------------------------------
    void LogicalState::change_refinements(ContextID ctx, size_t total_shards, 
           FieldMask refinement_mask, FieldMaskSet<RefinementNode> &refinements)
    //--------------------------------------------------------------------------
    {
      // Iterate through all of our current refinements and get the
      // children to travers and the projections to be done at this level
      FieldMaskSet<RegionTreeNode> new_children;
      FieldMaskSet<ProjectionNode> new_projections;
      std::vector<RefinementTracker*> to_delete;
      for (FieldMaskSet<RefinementTracker>::iterator it =
            refinement_trackers.begin(); it != refinement_trackers.end(); it++)
      {
        const FieldMask overlap = refinement_mask & it->second;
        if (!overlap)
          continue;
        // Ask it to find the next best refinement to use
        it->first->change_refinements(ctx,overlap,new_children,new_projections);
        it.filter(overlap);
        if (!it->second)
          to_delete.push_back(it->first);
        refinement_mask -= overlap;
        if (!refinement_mask)
          break;
      }
      if (!!refinement_mask)
      {
        // If we have any fields we haven't seen before then we must be
        // at a region node or we wouldn't have reported up the tree
#ifdef DEBUG_LEGION
        assert(owner->is_region());
#endif
        // Record a new empty refinement at this level
        RegionRefinementNode *new_refinement = 
          new RegionRefinementNode(owner->as_region_node());
        // If we have multiple shards then record them all here
        // since all the shards will be refining this node in the tree
        if (total_shards > 1)
        {
          new_refinement->refining_shards.resize(total_shards);
          for (ShardID shard = 0; shard < total_shards; shard++)
            new_refinement->refining_shards[shard] = shard;
        }
        refinements.insert(new_refinement, refinement_mask);
        // Create a new empty tracker at this level
        RefinementTracker *new_tracker = 
          owner->create_refinement_tracker(true/*current*/);
        refinement_trackers.insert(new_tracker, refinement_mask);
      }
#ifdef DEBUG_LEGION
      // There should be no overlapping fields between these things
      assert(new_children.get_valid_mask() * new_projections.get_valid_mask());
#endif
      // Create new refinements for all the children
      if (owner->is_region())
      {
        // For regions, we know all the children are field disjoint with
        // each other so we don't need to do the re-grouping
#ifdef DEBUG_LEGION
        FieldMask disjoint_children;
#endif
        for (FieldMaskSet<RegionTreeNode>::const_iterator cit =
              new_children.begin(); cit != new_children.end(); cit++)
        {
#ifdef DEBUG_LEGION
          assert(disjoint_children * cit->second);
          disjoint_children |= cit->second;
#endif
          FieldMaskSet<RefinementNode> child_refinements; 
          cit->first->update_logical_refinement(ctx, total_shards, cit->second,
                                                child_refinements);
#ifdef DEBUG_LEGION
          assert(child_refinements.get_valid_mask() == cit->second);
#endif
          // Create new refinements at this level for each one
          for (FieldMaskSet<RefinementNode>::const_iterator it =
                child_refinements.begin(); it != child_refinements.end(); it++)
          {
            RegionRefinementNode *new_refinement =
              new RegionRefinementNode(owner->as_region_node(),
                                       it->first->as_partition_refinement());
            refinements.insert(new_refinement, it->second); 
          }
          // Create a new tracker for this child and record it
          RefinementTracker *new_tracker =
            owner->create_refinement_tracker(cit->first);
          refinement_trackers.insert(new_tracker, cit->second);
        }
      }
      else
      {
        // For partitions we might get different sets of children for each
        // field so we need to group accordingly
        FieldMaskSet<RefinementNode> child_refinements;
        for (FieldMaskSet<RegionTreeNode>::const_iterator it =
              new_children.begin(); it != new_children.end(); it++)
        {
#ifdef DEBUG_LEGION
          // All the children should have the same set of fields here
          assert(it->second == new_children.get_valid_mask());
#endif
          it->first->update_logical_refinement(ctx, total_shards, it->second,
                                               child_refinements);
        }
        // Now sort them into field groups and make refinement nodes
        LegionList<FieldSet<RefinementNode*> > field_groups;
        child_refinements.compute_field_sets(FieldMask(), field_groups);
        for (LegionList<FieldSet<RefinementNode*> >::iterator it =
              field_groups.begin(); it != field_groups.end(); it++)
        {
          PartitionRefinementNode *new_refinement = 
            new PartitionRefinementNode(owner->as_partition_node());
          for (std::set<RefinementNode*>::const_iterator cit =
                it->elements.begin(); cit != it->elements.end(); it++)
          {
            RegionRefinementNode *child = (*cit)->as_region_refinement();
#ifdef DEBUG_LEGION
            assert(child != NULL);
#endif
            new_refinement->children[child->node->get_color()] = child;
          }
          refinements.insert(new_refinement, it->set_mask);
        }
        // Create a new tracker recording that all the children are refined
        RefinementTracker *new_tracker = 
          owner->create_refinement_tracker(true/*current*/);
        refinement_trackers.insert(new_tracker, new_children.get_valid_mask());
      }
      // Create new refinements for all the projections
      for (FieldMaskSet<ProjectionNode>::const_iterator it =
            new_projections.begin(); it != new_projections.end(); it++)
      {
        RefinementNode *new_refinement = it->first->create_refinement();
        refinements.insert(new_refinement, it->second);
        // Create a new refinement and save it in the trackers
        RefinementTracker *new_tracker = 
          owner->create_refinement_tracker(it->first);
        refinement_trackers.insert(new_tracker, it->second);
      }
      // Delete all the refinement trackers that are no longer valid
      for (std::vector<RefinementTracker*>::const_iterator it =
            to_delete.begin(); it != to_delete.end(); it++)
      {
        refinement_trackers.erase(*it);
        delete (*it);
      }
    }
#endif

    //--------------------------------------------------------------------------
    void LogicalState::invalidate_refinements(ContextID ctx, 
                                              FieldMask invalidation_mask)
    //--------------------------------------------------------------------------
    {
      std::vector<RefinementTracker*> to_delete;
      for (FieldMaskSet<RefinementTracker>::iterator it =
            refinement_trackers.begin(); it != refinement_trackers.end(); it++) 
      {
        const FieldMask overlap = invalidation_mask & it->second;
        if (!overlap)
          continue;
        it->first->invalidate_refinement(ctx, overlap);
        it.filter(overlap);
        if (!it->second)
          to_delete.push_back(it->first);
        invalidation_mask -= overlap;
        if (!invalidation_mask)
          break;
      }
#ifdef DEBUG_LEGION
      assert(!invalidation_mask); // should have seen all the fields
#endif
      for (std::vector<RefinementTracker*>::const_iterator it =
            to_delete.begin(); it != to_delete.end(); it++)
      {
        refinement_trackers.erase(*it);
        delete (*it);
      }
      refinement_trackers.tighten_valid_mask();
    }

    //--------------------------------------------------------------------------
    void LogicalState::record_refinement_dependences(ContextID ctx,
        const LogicalUser &refinement_user, const FieldMask &refinement_mask,
        const ProjectionInfo &no_proj_info, RegionTreeNode *previous_child,
        LogicalRegion privilege_root, LogicalAnalysis &logical_analysis)
    //--------------------------------------------------------------------------
    {
      FieldMask dummy_open_below;
      // First register dependences on any local users, we can't dominate
      // anything here since we need to leave things here for later
      owner->perform_dependence_checks<false/*track dom*/>(privilege_root,
                       refinement_user, curr_epoch_users, 
                       refinement_mask, dummy_open_below, false/*arrived*/,
                       no_proj_info, *this, logical_analysis);
      owner->perform_dependence_checks<false/*track dom*/>(privilege_root,
                        refinement_user, prev_epoch_users,
                        refinement_mask, dummy_open_below, false/*arrived*/,
                        no_proj_info, *this, logical_analysis);
      // If we have a previous child and all the children are independent
      // then we know we don't need to traverse anything else
      if ((previous_child == NULL) || !owner->are_all_children_disjoint())
      {
        // Now traverse any open children and record dependences on them as well
        for (LegionList<FieldState,LOGICAL_FIELD_STATE_ALLOC>::const_iterator 
              fit = field_states.begin(); fit != field_states.end(); fit++)
        {
          const FieldMask field_overlap = fit->valid_fields() & refinement_mask;
          if (!field_overlap)
            continue;
          for (FieldState::OrderedFieldMaskChildren::const_iterator it =
                fit->open_children.begin(); it != 
                fit->open_children.end(); it++)
          {
            // Can skip the previous child if we've already done it
            if (it->first == previous_child)
              continue;
            const FieldMask overlap = refinement_mask & it->second;
            if (!overlap)
              continue;
            if ((previous_child != NULL) && owner->are_children_disjoint(
                  previous_child->get_color(), it->first->get_color()))
              continue;
            it->first->record_refinement_dependences(ctx, refinement_user,
                overlap, no_proj_info, NULL/*previous child*/,
                privilege_root, logical_analysis);
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void LogicalState::promote_next_child(RegionTreeNode *child, FieldMask mask)
    //--------------------------------------------------------------------------
    {
      // This will promote the child up to a read-write field state
      for (LegionList<FieldState,LOGICAL_FIELD_STATE_ALLOC>::iterator it =
            field_states.begin(); it != field_states.end(); it++)
      {
        const FieldMask overlap = mask & it->valid_fields();
        if (!overlap)
          continue;
        FieldState::OrderedFieldMaskChildren::iterator finder =
          it->open_children.find(child);
        if (finder == it->open_children.end())
          continue;
        if ((it->open_children.size() == 1) && (overlap == finder->second))
        {
          // We can just update this state directly
          it->open_state = OPEN_READ_WRITE;
          it->redop = 0;
          mask -= overlap;
          if (!mask)
            return;
        }
        else
        {
          // Remove the child from the state
          finder.filter(overlap);
          if (!finder->second)
            it->remove_child(child);
          it->open_children.tighten_valid_mask();
        }
      }
#ifdef DEBUG_LEGION
      assert(!!mask);
#endif
      // If we get here we still have fields so we need to introduce a new
      // field state here for this child in read-write mode
      field_states.emplace_back(FieldState(OPEN_READ_WRITE, mask, child));
    }

#if 0
    //--------------------------------------------------------------------------
    bool LogicalState::find_symbolic_elide_close_result(
                                                  const ProjectionSummary &prev,
                                                  const ProjectionSummary &next,
                                                  bool &result) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(prev != next);
#endif
      if (symbolic_elide_close_results == NULL)
        return false;
      if (prev < next)
      {
        for (std::list<SymbolicCacheEntry>::iterator it =
              symbolic_elide_close_results->begin(); it !=
              symbolic_elide_close_results->end(); it++)
        {
          if (it->matches(prev, next))
          {
            result = it->result;
            // Splice it to the front of the list
            symbolic_elide_close_results->splice(
                symbolic_elide_close_results->begin(),
                *symbolic_elide_close_results, it);
            return true;
          }
        }
      }
      else
      {
        for (std::list<SymbolicCacheEntry>::iterator it =
              symbolic_elide_close_results->begin(); it !=
              symbolic_elide_close_results->end(); it++)
        {
          if (it->matches(next, prev))
          {
            result = it->result;
            // Splice it to the front of the list
            symbolic_elide_close_results->splice(
                symbolic_elide_close_results->begin(),
                *symbolic_elide_close_results, it);
            return true;
          }
        }
      }
      return false;
    }

    //--------------------------------------------------------------------------
    void LogicalState::record_symbolic_elide_close_result(
      const ProjectionSummary &prev, const ProjectionSummary &next, bool result)
    //--------------------------------------------------------------------------
    {
      // The maximum size for the cache here
      constexpr size_t SYMBOLIC_CACHE_SIZE = 32;
      if (symbolic_elide_close_results == NULL)
        symbolic_elide_close_results = new std::list<SymbolicCacheEntry>();
      else if (symbolic_elide_close_results->size() == SYMBOLIC_CACHE_SIZE)
        symbolic_elide_close_results->pop_back();
      if (prev < next)
        symbolic_elide_close_results->emplace_front(
            SymbolicCacheEntry(prev, next, result));
      else
        symbolic_elide_close_results->emplace_front(
            SymbolicCacheEntry(next, prev, result));
    }

    /////////////////////////////////////////////////////////////
    // Projection Summary 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ProjectionSummary::ProjectionSummary(void)
      : domain(NULL), projection(NULL), sharding(NULL), sharding_domain(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    ProjectionSummary::ProjectionSummary(IndexSpaceNode *is, 
                                         ProjectionFunction *p, 
                                         ShardingFunction *s,
                                         IndexSpaceNode *sd)
      : domain(is), projection(p), sharding(s), sharding_domain(sd)
    //--------------------------------------------------------------------------
    {
      if (domain != NULL)
        domain->add_base_valid_ref(FIELD_STATE_REF);
      if (sharding_domain != NULL)
        sharding_domain->add_base_valid_ref(FIELD_STATE_REF);
    }

    //--------------------------------------------------------------------------
    ProjectionSummary::ProjectionSummary(const ProjectionInfo &info)
      : domain(info.projection_space), projection(info.projection), 
        sharding(info.sharding_function), sharding_domain(info.sharding_space)
    //--------------------------------------------------------------------------
    {
      if (domain != NULL)
        domain->add_base_valid_ref(FIELD_STATE_REF);
      if (sharding_domain != NULL)
        sharding_domain->add_base_valid_ref(FIELD_STATE_REF);
    }

    //--------------------------------------------------------------------------
    ProjectionSummary::ProjectionSummary(ProjectionSummary &&rhs)
      : domain(rhs.domain), projection(rhs.projection),
        sharding(rhs.sharding), sharding_domain(rhs.sharding_domain)
    //--------------------------------------------------------------------------
    {
      rhs.domain = NULL;
      rhs.projection = NULL;
      rhs.sharding = NULL;
      rhs.sharding_domain = NULL;
    }

    //--------------------------------------------------------------------------
    ProjectionSummary::ProjectionSummary(const ProjectionSummary &rhs)
      : domain(rhs.domain), projection(rhs.projection),
        sharding(rhs.sharding), sharding_domain(rhs.sharding_domain)
    //--------------------------------------------------------------------------
    {
      if (domain != NULL)
        domain->add_base_valid_ref(FIELD_STATE_REF);
      if (sharding_domain != NULL)
        sharding_domain->add_base_valid_ref(FIELD_STATE_REF);
    }

    //--------------------------------------------------------------------------
    ProjectionSummary::~ProjectionSummary(void)
    //--------------------------------------------------------------------------
    {
      if ((domain != NULL) && domain->remove_base_valid_ref(FIELD_STATE_REF))
        delete domain;
      if ((sharding_domain != NULL) && 
            sharding_domain->remove_base_valid_ref(FIELD_STATE_REF))
        delete sharding_domain;
    }

    //--------------------------------------------------------------------------
    ProjectionSummary& ProjectionSummary::operator=(const ProjectionSummary &ps)
    //--------------------------------------------------------------------------
    {
      if ((domain != NULL) && domain->remove_base_valid_ref(FIELD_STATE_REF))
        delete domain;
      if ((sharding_domain != NULL) && 
            sharding_domain->remove_base_valid_ref(FIELD_STATE_REF))
        delete sharding_domain;
      domain = ps.domain;
      projection = ps.projection;
      sharding = ps.sharding;
      sharding_domain = ps.sharding_domain;
      if (domain != NULL)
        domain->add_base_valid_ref(FIELD_STATE_REF);
      if (sharding_domain != NULL)
        sharding_domain->add_base_valid_ref(FIELD_STATE_REF);
      return *this;
    }

    //--------------------------------------------------------------------------
    bool ProjectionSummary::operator<(const ProjectionSummary &rhs) const
    //--------------------------------------------------------------------------
    {
      if (domain->handle < rhs.domain->handle)
        return true;
      else if (domain->handle > rhs.domain->handle)
        return false;
      else if (projection->projection_id < rhs.projection->projection_id)
        return true;
      else if (projection->projection_id > rhs.projection->projection_id)
        return false;
      else if ((sharding == NULL) && (rhs.sharding == NULL))
        return false;
      else if ((sharding == NULL) && (rhs.sharding != NULL))
        return true;
      else if (rhs.sharding == NULL)
        return false;
      else if (sharding->sharding_id < rhs.sharding->sharding_id)
        return true;
      else if (sharding->sharding_id > rhs.sharding->sharding_id)
        return false;
      else
        return sharding_domain->handle < rhs.sharding_domain->handle;
    }

    //--------------------------------------------------------------------------
    bool ProjectionSummary::operator==(const ProjectionSummary &rhs) const
    //--------------------------------------------------------------------------
    {
      if (domain->handle != rhs.domain->handle)
        return false;
      else if (projection->projection_id != rhs.projection->projection_id)
        return false;
      else if ((sharding == NULL) && (rhs.sharding == NULL))
        return true;
      else if ((sharding == NULL) && (rhs.sharding != NULL))
        return false;
      else if (rhs.sharding == NULL)
        return false;
      if (sharding->sharding_id != rhs.sharding->sharding_id)
        return false;
      if (sharding_domain->handle != rhs.sharding_domain->handle)
        return false;
      return true;
    }

    //--------------------------------------------------------------------------
    bool ProjectionSummary::operator!=(const ProjectionSummary &rhs) const
    //--------------------------------------------------------------------------
    {
      return !(*this == rhs);
    }

    //--------------------------------------------------------------------------
    void ProjectionSummary::pack_summary(Serializer &rez) const 
    //--------------------------------------------------------------------------
    {
      rez.serialize(domain->handle);
      rez.serialize(projection->projection_id);
      domain->pack_valid_ref();
    }

    //--------------------------------------------------------------------------
    /*static*/ ProjectionSummary ProjectionSummary::unpack_summary(
                                 Deserializer &derez, RegionTreeForest *context)
    //--------------------------------------------------------------------------
    {
      ProjectionSummary result;
      IndexSpace handle;
      derez.deserialize(handle);
      result.domain = context->get_node(handle);
      result.domain->add_base_valid_ref(FIELD_STATE_REF);
      result.domain->unpack_valid_ref();
      ProjectionID pid;
      derez.deserialize(pid);
      result.projection = context->runtime->find_projection_function(pid);
      return result;
    }

    /////////////////////////////////////////////////////////////
    // RefProjectionSummary
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RefProjectionSummary::RefProjectionSummary(const ProjectionInfo &rhs)
      : ProjectionSummary(rhs), Collectable()
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RefProjectionSummary::RefProjectionSummary(ProjectionSummary &&rhs)
      : ProjectionSummary(rhs), Collectable()
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RefProjectionSummary::RefProjectionSummary(const RefProjectionSummary &rhs)
      : ProjectionSummary(rhs), Collectable()
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    RefProjectionSummary::~RefProjectionSummary(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RefProjectionSummary& RefProjectionSummary::operator=(
                                                const RefProjectionSummary &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void RefProjectionSummary::project_refinement(RegionTreeNode *node,
                                        std::vector<RegionNode*> &regions) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(sharding == NULL);
#endif
      projection->project_refinement(domain, node, regions);
    }

    //--------------------------------------------------------------------------
    void RefProjectionSummary::project_refinement(RegionTreeNode *node,
                      ShardID shard_id, std::vector<RegionNode*> &regions,
                      Provenance *provenance) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(sharding != NULL);
#endif
      // Find the domain of points for this shard
      IndexSpace shard_handle = sharding->find_shard_space(shard_id, domain,
          (sharding_domain != NULL) ? sharding_domain->handle : domain->handle,
          provenance);
      if (shard_handle.exists())
      {
        IndexSpaceNode *shard_domain = node->context->get_node(shard_handle);
        projection->project_refinement(shard_domain, node, regions);
      }
    }
#endif

    /////////////////////////////////////////////////////////////
    // FieldState 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    FieldState::FieldState(void)
      : open_state(NOT_OPEN), redop(0)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    FieldState::FieldState(OpenState state, const FieldMask &m,
                           RegionTreeNode *child)
      : open_state(state), redop(0)
    //--------------------------------------------------------------------------
    {
      if (open_children.insert(child, m))
        child->add_base_gc_ref(FIELD_STATE_REF);
    }

    //--------------------------------------------------------------------------
    FieldState::FieldState(const RegionUsage &usage, const FieldMask &m, 
                           RegionTreeNode *child)
      : redop(0)
    //--------------------------------------------------------------------------
    {
      if (IS_READ_ONLY(usage))
        open_state = OPEN_READ_ONLY;
      else if (IS_WRITE(usage))
        open_state = OPEN_READ_WRITE;
      else if (IS_REDUCE(usage))
      {
        open_state = OPEN_SINGLE_REDUCE;
        redop = usage.redop;
      }
      if (open_children.insert(child, m))
        child->add_base_gc_ref(FIELD_STATE_REF);
    }

    //--------------------------------------------------------------------------
    FieldState::FieldState(const FieldState &rhs)
      : open_state(rhs.open_state), redop(rhs.redop)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(rhs.open_children.empty());
#endif
    }

    //--------------------------------------------------------------------------
    FieldState::FieldState(FieldState &&rhs) noexcept
    //--------------------------------------------------------------------------
    {
      open_children.swap(rhs.open_children);
      open_state = rhs.open_state;
      redop = rhs.redop;
    }

    //--------------------------------------------------------------------------
    FieldState::~FieldState(void)
    //--------------------------------------------------------------------------
    {
      for (OrderedFieldMaskChildren::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
        if (it->first->remove_base_gc_ref(FIELD_STATE_REF))
          delete it->first;
    }

    //--------------------------------------------------------------------------
    FieldState& FieldState::operator=(const FieldState &rhs)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(open_children.empty());
      assert(rhs.open_children.empty());
#endif
      open_state = rhs.open_state;
      redop = rhs.redop;
      return *this;
    }

    //--------------------------------------------------------------------------
    FieldState& FieldState::operator=(FieldState &&rhs) noexcept
    //--------------------------------------------------------------------------
    {
      open_children.swap(rhs.open_children);
      open_state = rhs.open_state;
      redop = rhs.redop;
      return *this;
    }

    //--------------------------------------------------------------------------
    bool FieldState::overlaps(const FieldState &rhs) const
    //--------------------------------------------------------------------------
    {
      if (redop != rhs.redop)
        return false;
      // Now check the privilege states
      if (redop == 0)
        return (open_state == rhs.open_state);
      else
      {
#ifdef DEBUG_LEGION
        assert((open_state == OPEN_SINGLE_REDUCE) ||
               (open_state == OPEN_MULTI_REDUCE));
        assert((rhs.open_state == OPEN_SINGLE_REDUCE) ||
               (rhs.open_state == OPEN_MULTI_REDUCE));
#endif
        // Only support merging reduction fields with exactly the
        // same mask which should be single fields for reductions
        return (valid_fields() == rhs.valid_fields());
      }
    }

    //--------------------------------------------------------------------------
    void FieldState::merge(FieldState &rhs, RegionTreeNode *node)
    //--------------------------------------------------------------------------
    {
      if (!rhs.open_children.empty())
      {
        for (OrderedFieldMaskChildren::const_iterator it = 
              rhs.open_children.begin(); it != rhs.open_children.end(); it++)
          // Remove duplicate references if we already had it
          if (!open_children.insert(it->first, it->second))
            it->first->remove_base_gc_ref(FIELD_STATE_REF);
        rhs.open_children.clear();
      }
      else
        open_children.relax_valid_mask(rhs.open_children.get_valid_mask());
#ifdef DEBUG_LEGION
      assert(redop == rhs.redop);
#endif
      if (redop > 0)
      {
#ifdef DEBUG_LEGION
        assert(!open_children.empty());
#endif
        // For the reductions, handle the case where we need to merge
        // reduction modes, if they are all disjoint, we don't need
        // to distinguish between single and multi reduce
        if (node->are_all_children_disjoint())
        {
          open_state = OPEN_READ_WRITE;
          redop = 0;
        }
        else
        {
          if (open_children.size() == 1)
            open_state = OPEN_SINGLE_REDUCE;
          else
            open_state = OPEN_MULTI_REDUCE;
        }
      }
    }

    //--------------------------------------------------------------------------
    bool FieldState::filter(const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      std::vector<RegionTreeNode*> to_delete;
      for (OrderedFieldMaskChildren::iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        it.filter(mask);
        if (!it->second)
          to_delete.push_back(it->first);
      }
      if (to_delete.size() < open_children.size())
      {
        for (std::vector<RegionTreeNode*>::const_iterator it = 
              to_delete.begin(); it != to_delete.end(); it++)
        {
          open_children.erase(*it);
          if ((*it)->remove_base_gc_ref(FIELD_STATE_REF))
            delete (*it);
        }
      }
      else
      {
        open_children.clear();
        for (std::vector<RegionTreeNode*>::const_iterator it = 
              to_delete.begin(); it != to_delete.end(); it++)
          if ((*it)->remove_base_gc_ref(FIELD_STATE_REF))
            delete (*it);
      }
      open_children.tighten_valid_mask();
      return open_children.empty();
    }

    //--------------------------------------------------------------------------
    void FieldState::add_child(RegionTreeNode *child, const FieldMask &mask) 
    //--------------------------------------------------------------------------
    {
      if (open_children.insert(child, mask))
        child->add_base_gc_ref(FIELD_STATE_REF);
    }

    //--------------------------------------------------------------------------
    void FieldState::remove_child(RegionTreeNode *child)
    //--------------------------------------------------------------------------
    {
      OrderedFieldMaskChildren::iterator finder = 
        open_children.find(child);
#ifdef DEBUG_LEGION
      assert(finder != open_children.end());
      assert(!finder->second);
#endif
      open_children.erase(finder);
      if (child->remove_base_gc_ref(FIELD_STATE_REF))
        delete child;
    }

    //--------------------------------------------------------------------------
    void FieldState::print_state(TreeStateLogger *logger,
                                 const FieldMask &capture_mask,
                                 RegionNode *node) const
    //--------------------------------------------------------------------------
    {
      switch (open_state)
      {
        case NOT_OPEN:
          {
            logger->log("Field State: NOT OPEN (%ld)", 
                        open_children.size());
            break;
          }
        case OPEN_READ_WRITE:
          {
            logger->log("Field State: OPEN READ WRITE (%ld)", 
                        open_children.size());
            break;
          }
        case OPEN_READ_ONLY:
          {
            logger->log("Field State: OPEN READ-ONLY (%ld)", 
                        open_children.size());
            break;
          }
        case OPEN_SINGLE_REDUCE:
          {
            logger->log("Field State: OPEN SINGLE REDUCE Mode %d (%ld)", 
                        redop, open_children.size());
            break;
          }
        case OPEN_MULTI_REDUCE:
          {
            logger->log("Field State: OPEN MULTI REDUCE Mode %d (%ld)", 
                        redop, open_children.size());
            break;
          }
        default:
          assert(false);
      }
      logger->down();
      for (OrderedFieldMaskChildren::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        FieldMask overlap = it->second & capture_mask;
        if (!overlap)
          continue;
        char *mask_buffer = overlap.to_string();
        logger->log("Color %d   Mask %s", it->first->get_color(), mask_buffer);
        free(mask_buffer);
      }
      logger->up();
    }

    //--------------------------------------------------------------------------
    void FieldState::print_state(TreeStateLogger *logger,
                                 const FieldMask &capture_mask,
                                 PartitionNode *node) const
    //--------------------------------------------------------------------------
    {
      switch (open_state)
      {
        case NOT_OPEN:
          {
            logger->log("Field State: NOT OPEN (%ld)", 
                        open_children.size());
            break;
          }
        case OPEN_READ_WRITE:
          {
            logger->log("Field State: OPEN READ WRITE (%ld)", 
                        open_children.size());
            break;
          }
        case OPEN_READ_ONLY:
          {
            logger->log("Field State: OPEN READ-ONLY (%ld)", 
                        open_children.size());
            break;
          }
        case OPEN_SINGLE_REDUCE:
          {
            logger->log("Field State: OPEN SINGLE REDUCE Mode %d (%ld)", 
                        redop, open_children.size());
            break;
          }
        case OPEN_MULTI_REDUCE:
          {
            logger->log("Field State: OPEN MULTI REDUCE Mode %d (%ld)", 
                        redop, open_children.size());
            break;
          }
        default:
          assert(false);
      }
      logger->down();
      for (OrderedFieldMaskChildren::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        IndexSpaceNode *color_space = node->row_source->color_space;
        DomainPoint color =
          color_space->delinearize_color_to_point(it->first->get_color());
        FieldMask overlap = it->second & capture_mask;
        if (!overlap)
          continue;
        char *mask_buffer = overlap.to_string();
        switch (color.get_dim())
        {
          case 1:
            {
              logger->log("Color %d   Mask %s", 
                          color[0], mask_buffer);
              break;
            }
#if LEGION_MAX_DIM >= 2
          case 2:
            {
              logger->log("Color (%d,%d)   Mask %s", 
                          color[0], color[1], mask_buffer);
              break;
            }
#endif
#if LEGION_MAX_DIM >= 3
          case 3:
            {
              logger->log("Color (%d,%d,%d)   Mask %s", 
                          color[0], color[1], color[2], mask_buffer);
              break;
            }
#endif
#if LEGION_MAX_DIM >= 4
          case 4:
            {
              logger->log("Color (%d,%d,%d,%d)   Mask %s", 
                          color[0], color[1], color[2], 
                          color[3], mask_buffer);
              break;
            }
#endif
#if LEGION_MAX_DIM >= 5
          case 5:
            {
              logger->log("Color (%d,%d,%d,%d,%d)   Mask %s", 
                          color[0], color[1], color[2],
                          color[3], color[4], mask_buffer);
              break;
            }
#endif
#if LEGION_MAX_DIM >= 6
          case 6:
            {
              logger->log("Color (%d,%d,%d,%d,%d,%d)   Mask %s", 
                          color[0], color[1], color[2], 
                          color[3], color[4], color[5], mask_buffer);
              break;
            }
#endif
#if LEGION_MAX_DIM >= 7
          case 7:
            {
              logger->log("Color (%d,%d,%d,%d,%d,%d,%d)   Mask %s", 
                          color[0], color[1], color[2], 
                          color[3], color[4], color[5], 
                          color[6], mask_buffer);
              break;
            }
#endif
#if LEGION_MAX_DIM >= 8
          case 8:
            {
              logger->log("Color (%d,%d,%d,%d,%d,%d,%d,%d)   Mask %s",
                          color[0], color[1], color[2], 
                          color[3], color[4], color[5], 
                          color[6], color[7], mask_buffer);
              break;
            }
#endif
#if LEGION_MAX_DIM >= 9
          case 9:
            {
              logger->log("Color (%d,%d,%d,%d,%d,%d,%d,%d,%d)   Mask %s",
                          color[0], color[1], color[2], 
                          color[3], color[4], color[5], 
                          color[6], color[7], color[8], mask_buffer);
              break;
            }
#endif
          default:
            assert(false); // implemenent more dimensions
        }
        free(mask_buffer);
      }
      logger->up();
    }

#if 0
    /////////////////////////////////////////////////////////////
    // Logical Closer 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    LogicalCloser::LogicalCloser(ContextID c, const LogicalUser &u, 
                                 RegionTreeNode *r, bool val)
      : ctx(c), user(u), root_node(r), validates(val),
        tracing(user.op->is_tracing()), close_op(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalCloser::~LogicalCloser(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::record_close_operation(const FieldMask &mask) 
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!!mask);
#endif
      close_mask |= mask;
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::record_closed_user(const LogicalUser &user,
                                           const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      closed_users.push_back(user);
      LogicalUser &closed_user = closed_users.back();
      closed_user.field_mask = mask;
    }

#ifndef LEGION_SPY
    //--------------------------------------------------------------------------
    void LogicalCloser::pop_closed_user(void)
    //--------------------------------------------------------------------------
    {
      closed_users.pop_back();
    }
#endif

    //--------------------------------------------------------------------------
    void LogicalCloser::initialize_close_operations(LogicalState &state, 
                                             Operation *creator,
                                             const LogicalTraceInfo &trace_info,
                                             const bool check_for_refinements,
                                             const bool has_next_child)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      // These sets of fields better be disjoint
      assert(!!close_mask);
      assert(close_op == NULL);
#endif
      // Construct a reigon requirement for this operation
      // All privileges are based on the parent logical region
      RegionRequirement req;
      if (root_node->is_region())
        req = RegionRequirement(root_node->as_region_node()->handle,
            LEGION_READ_WRITE, LEGION_EXCLUSIVE, trace_info.req.parent);
      else
        req = RegionRequirement(root_node->as_partition_node()->handle, 0,
            LEGION_READ_WRITE, LEGION_EXCLUSIVE, trace_info.req.parent);
      InnerContext *context = creator->get_context();
#ifdef DEBUG_LEGION_COLLECTIVES
      close_op = context->get_merge_close_op(user, root_node);
#else
      close_op = context->get_merge_close_op();
#endif
      merge_close_gen = close_op->get_generation();
      req.privilege_fields.clear();
      root_node->column_source->get_field_set(close_mask,
                                             trace_info.req.privilege_fields,
                                             req.privilege_fields);
      close_op->initialize(context, req, trace_info, trace_info.req_idx, 
                           close_mask, creator);
      if (check_for_refinements && !!state.disjoint_complete_tree)
      {
        const FieldMask refinement_mask = 
          close_mask & state.disjoint_complete_tree; 
        if (!!refinement_mask)
        {
          // Record that this close op should make a new equivalence
          // set at this region and invalidate all the ones below
          const bool overwriting = HAS_WRITE_DISCARD(user.usage) &&
                    !has_next_child && !user.op->is_predicated_op() && 
                    !trace_info.recording_trace;
          close_op->record_refinements(refinement_mask, overwriting);
#ifdef DEBUG_LEGION
          assert(state.owner->is_region());
#endif
          // We're closing to a region, so invalidate all the children
          std::vector<RegionTreeNode*> to_delete;
          for (FieldMaskSet<RegionTreeNode>::iterator it = 
                state.disjoint_complete_children.begin(); it !=
                state.disjoint_complete_children.end(); it++)
          {
            const FieldMask overlap = refinement_mask & it->second;
            if (!overlap)
              continue;
            it->first->invalidate_disjoint_complete_tree(ctx, overlap, true);
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
          }
          if (!to_delete.empty())
          {
            for (std::vector<RegionTreeNode*>::const_iterator it = 
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              state.disjoint_complete_children.erase(*it);
              if ((*it)->remove_base_gc_ref(DISJOINT_COMPLETE_REF))
                delete (*it);
            }
            state.disjoint_complete_children.tighten_valid_mask();
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::perform_dependence_analysis(const LogicalUser &current,
                                                    const FieldMask &open_below,
                             LegionList<LogicalUser,CURR_LOGICAL_ALLOC> &cusers,
                             LegionList<LogicalUser,PREV_LOGICAL_ALLOC> &pusers)
    //--------------------------------------------------------------------------
    {
      // We also need to do dependence analysis against all the other operations
      // that this operation recorded dependences on above in the tree so we
      // don't run too early.
      LegionList<LogicalUser,LOGICAL_REC_ALLOC> &above_users = 
                                              current.op->get_logical_records();
      const LogicalUser merge_close_user(close_op, 0/*idx*/, RegionUsage(
            LEGION_READ_WRITE, LEGION_EXCLUSIVE, 0/*redop*/), close_mask);
      register_dependences(close_op, merge_close_user, current, 
          open_below, closed_users, above_users, cusers, pusers);
      // Now we can remove our references on our local users
      for (LegionList<LogicalUser>::const_iterator it = 
            closed_users.begin(); it != closed_users.end(); it++)
      {
        it->op->remove_mapping_reference(it->gen);
      }
    }

    // If you are looking for LogicalCloser::register_dependences it can 
    // be found in region_tree.cc to make sure that templates are instantiated

    //--------------------------------------------------------------------------
    void LogicalCloser::update_state(LogicalState &state)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(state.owner == root_node);
#endif
      root_node->filter_prev_epoch_users(state, close_mask);
      root_node->filter_curr_epoch_users(state, close_mask, tracing);
      root_node->filter_disjoint_complete_accesses(state, close_mask); 
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::register_close_operations(
                              LegionList<LogicalUser,CURR_LOGICAL_ALLOC> &users)
    //--------------------------------------------------------------------------
    {
      // No need to add mapping references, we did that in 
      // Note we also use the cached generation IDs since the close
      // operations have already been kicked off and might be done
      // LogicalCloser::register_dependences
      const LogicalUser close_user(close_op, merge_close_gen,0/*idx*/,
        RegionUsage(LEGION_READ_WRITE, LEGION_EXCLUSIVE,0/*redop*/),close_mask);
      users.push_back(close_user);
    }
#endif

    /////////////////////////////////////////////////////////////
    // Logical Analysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    LogicalAnalysis::LogicalAnalysis(Operation *o, unsigned out_off)
      : op(o), context(op->get_context()), output_region_offset(out_off)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalAnalysis::~LogicalAnalysis(void)
    //--------------------------------------------------------------------------
    {
      // If we have any pending refinements, have them record dependences
      // on any pending closes that were done along their path and then
      // issue the refinements 
      unsigned internal_index = 0;
      for (OrderedRefinements::const_iterator it =
            pending_refinements.begin(); it != pending_refinements.end(); it++)
      {
        RegionTreeNode *node = it->first->get_refinement_node();
        RegionTreeNode *path_node = node;
        while (path_node != NULL)
        {
          std::map<RegionTreeNode*,MergeCloseOp*>::const_iterator finder =
            pending_closes.find(path_node);
          if (finder != pending_closes.end())
          {
            FieldMask overlap = it->second & finder->second->get_close_mask();
            if (!!overlap)
            {
#ifdef LEGION_SPY
              LegionSpy::log_mapping_dependence(context->get_unique_id(),
                  finder->second->get_unique_op_id(), 0/*index*/,
                  it->first->get_unique_op_id(), 0/*index*/,
                  LEGION_TRUE_DEPENDENCE);
#endif
              it->first->register_region_dependence(0/*index*/, finder->second,
                  finder->second->get_generation(), 0/*index*/,
                  LEGION_TRUE_DEPENDENCE, false/*validates*/, overlap);
            }
          }
          path_node = path_node->get_parent();
        }
        it->first->record_refinement_mask(internal_index, it->second);
        issue_internal_operation(node, it->first, it->second, internal_index++);
      }
      // Issue the pending closes
      if (!pending_closes.empty())
      {
        // Need to issue these close operations in order in case we are
        // control replicated and therefore all the shards need to see 
        // the closes in the same order for things to work correctly
        std::map<LogicalRegion,RegionTreeNode*> ordered_region_closes;
        std::map<LogicalPartition,RegionTreeNode*> ordered_partition_closes;
        for (std::map<RegionTreeNode*,MergeCloseOp*>::const_iterator it =
              pending_closes.begin(); it != pending_closes.end(); it++)
        {
          if (it->first->is_region())
          {
            RegionNode *region = it->first->as_region_node();
            ordered_region_closes[region->handle] = it->first;
          }
          else
          {
            PartitionNode *partition = it->first->as_partition_node();
            ordered_partition_closes[partition->handle] = it->first;
          }
        }
        for (std::map<LogicalRegion,RegionTreeNode*>::const_iterator it =
              ordered_region_closes.begin(); it !=
              ordered_region_closes.end(); it++)
        {
          MergeCloseOp *close = pending_closes[it->second];
          issue_internal_operation(it->second, close, close->get_close_mask(),
                                   internal_index++);
        }
        for (std::map<LogicalPartition,RegionTreeNode*>::const_iterator it =
              ordered_partition_closes.begin(); it !=
              ordered_partition_closes.end(); it++)
        {
          MergeCloseOp *close = pending_closes[it->second];
          issue_internal_operation(it->second, close, close->get_close_mask(),
                                   internal_index++);
        }
      }
#if 0
      // If we have any unrefined nodes we can issue close operations
      // to create a refinement for them at the root now
      if (!unrefined_nodes.empty())
      {
        // Sort these by region names so that they are done in the same
        // order across all the shards for control replication
        std::map<LogicalRegion,RegionNode*> ordered_regions;
        for (FieldMaskSet<RegionNode>::const_iterator it =
              unrefined_nodes.begin(); it != unrefined_nodes.end(); it++)
          ordered_regions[it->first->handle] = it->first;
        for (std::map<LogicalRegion,RegionNode*>::const_iterator it =
              ordered_regions.begin(); it != ordered_regions.end(); it++)
        {
          const FieldMask &refinement_mask = unrefined_nodes[it->second];
#ifdef DEBUG_LEGION_COLLECTIVES
          MergeCloseOp *initializer = 
            context->get_merge_close_op(op, it->second);
#else
          MergeCloseOp *initializer = context->get_merge_close_op();
#endif
          RegionRequirement req(it->first, LEGION_WRITE_DISCARD,
              LEGION_EXCLUSIVE, it->first);
          it->second->column_source->get_field_set(refinement_mask,
                                      context, req.privilege_fields);
          initializer->initialize(context, req, 
                                  unrefined_indexes[it->second], op);
          initializer->record_refinements(refinement_mask, true/*overwrite*/);
          initializer->begin_dependence_analysis();
          // These fields are unversioned so there is nothing for 
          // this close operation to depend on
          issue_internal_operation(it->second, initializer, refinement_mask);
        }
      }
      if (!pending_closes.empty())
      {
        // Need to issue these close operations in order in case we are
        // control replicated and therefore all the shards need to see 
        // the closes in the same order for things to work correctly
        for (std::map<LogicalRegion,
              std::map<RegionTreeNode*,PendingClose*> >::const_iterator
              pit = pending_closes.begin(); pit != pending_closes.end(); pit++)
        {
          // Put closes in order by their region/partition handles
          // whichi we know are the same across all the shards
          std::map<LogicalRegion,PendingClose*> ordered_region_closes;
          std::map<LogicalPartition,PendingClose*> ordered_partition_closes;
          for (std::map<RegionTreeNode*,PendingClose*>::const_iterator it =
                pit->second.begin(); it != pit->second.end(); it++)
          {
            if (it->first->is_region())
            {
              RegionNode *region = it->first->as_region_node();
              ordered_region_closes[region->handle] = it->second;
            }
            else
            {
              PartitionNode *partition = it->first->as_partition_node();
              ordered_partition_closes[partition->handle] = it->second;
            }
          }
          for (std::map<LogicalRegion,PendingClose*>::const_iterator it =
                ordered_region_closes.begin(); it !=
                ordered_region_closes.end(); it++)
          {
            issue_close_operation(pit->first, it->second);
            delete it->second;
          }
          for (std::map<LogicalPartition,PendingClose*>::const_iterator it =
                ordered_partition_closes.begin(); it !=
                ordered_partition_closes.end(); it++)
          {
            issue_close_operation(pit->first, it->second);
            delete it->second;
          }
        }
      }
      if (!pending_refinements.empty())
      {
        const RegionTreeContext ctx = context->get_context();
        // Note that we walk these refinements in order for control replication
        for (LegionVector<PendingRefinement>::const_iterator it =
             pending_refinements.begin(); it != pending_refinements.end(); it++)
        {
          // Update the disjoint-complete tree for this refinement
          RegionNode *root = it->partition->parent;
          root->refine_disjoint_complete_tree(ctx.get_id(), it->partition, 
                  it->refinement_op, it->refinement_mask, applied_events);
#ifdef DEBUG_LEGION
          // Sanity check that we recorded refinements for
          // all the fields in the refinement mask
          it->refinement_op->verify_refinement_mask(it->refinement_mask);
#endif
          // Trigger this here so it happens in a determinsitic order
          // for control replication. It has to happen after we've got
          // the list of refinements to make from updating the 
          // disjoint-complete tree
          it->refinement_op->trigger_dependence_analysis();
          // Now we can finish the dependence analysis for this refinement
          // Grab these before we end the dependence analysis which will
          // dump the refinement op into the pipeline
          const GenerationID refinement_gen = 
            it->refinement_op->get_generation();
#ifdef LEGION_SPY
          const UniqueID refinement_uid = it->refinement_op->get_unique_op_id();
#endif
          it->refinement_op->end_dependence_analysis();
          // Record that our operation depends on this refinement
          op->register_region_dependence(it->index, it->refinement_op, 
              refinement_gen, 0/*index*/, LEGION_TRUE_DEPENDENCE,
              false/*validates*/, it->refinement_mask);
#ifdef LEGION_SPY
          LegionSpy::log_mapping_dependence(context->get_unique_id(),
              refinement_uid, 0/*index*/, op->get_unique_op_id(), it->index,
              LEGION_TRUE_DEPENDENCE);
#endif
        }
      }
#endif
    }

#if 0
    //--------------------------------------------------------------------------
    RefinementOp* LogicalAnalysis::create_refinement(const LogicalUser &user,
                     PartitionNode *partition, const FieldMask &refinement_mask,
                     LogicalRegion privilege_root)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(op == user.op);
#endif
      RegionNode *root = partition->parent;
#ifdef DEBUG_LEGION_COLLECTIVES
      RefinementOp *refinement_op = context->get_refinement_op(op, root);
#else
      RefinementOp *refinement_op = context->get_refinement_op();
#endif
      refinement_op->initialize(op, user.idx, root,
                                refinement_mask, privilege_root);
      pending_refinements.emplace_back(
        PendingRefinement(refinement_op, partition, refinement_mask, user.idx));
      // Start the dependence analysis for this refinement now
      // We'll finish the dependence analysis in the destructor when we
      // know all the region requirements are traversed and we can safely
      // update the disjoint-complete tree
      refinement_op->begin_dependence_analysis();
      return refinement_op;
    }

    //--------------------------------------------------------------------------
    bool LogicalAnalysis::deduplicate(PartitionNode *child, FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      for (LegionVector<PendingRefinement>::iterator it =
            pending_refinements.begin(); it != pending_refinements.end(); it++)
      {
        if (it->partition->parent != child->parent)
          continue;
        const FieldMask overlap = it->refinement_mask & mask;
        if (!overlap)
          continue;
        if (it->partition != child)
        {
          if (overlap != it->refinement_mask)
          {
            // Filter off the fields we'll do later
            it->refinement_mask -= overlap;
            continue;
          }
          else
            // We can co-opt this refinement op to do this child instead
            it->partition = child;
        }
        mask -= overlap;
        if (!mask)
          return false;
      }
      return true;
    }

    //--------------------------------------------------------------------------
    void LogicalAnalysis::record_unrefined_fields(RegionNode *node, 
                                     unsigned index, const FieldMask &unrefined)
    //--------------------------------------------------------------------------
    {
      // No need for references here, this data structure isn't going to 
      // live past the end of this meta-task for the enclosing operation 
      // which ensures the liveness of the region tree nodes
      if (unrefined_nodes.insert(node, unrefined))
        unrefined_indexes[node] = index;
    }
#endif

    //--------------------------------------------------------------------------
    void LogicalAnalysis::record_pending_refinement(LogicalRegion privilege,
                                            unsigned req_index,
                                            unsigned parent_req_index,
                                            RegionTreeNode *refinement_node,
                                            const FieldMask &refinement_mask,
                                            FieldMaskSet<RefinementOp> &pending)
    //--------------------------------------------------------------------------
    {
#if 0
      // If we overlap with any unrefined nodes then we can remove them
      if (!unrefined_nodes.empty())
      {
        if (refinement_node->is_region())
        {
          FieldMaskSet<RegionNode>::iterator finder = 
            unrefined_nodes.find(refinement_node->as_region_node());
          if (finder != unrefined_nodes.end())
          {
            finder.filter(refinement_mask);
            if (!finder->second)
            {
              unrefined_indexes.erase(finder->first);
              unrefined_nodes.erase(finder);
            }
          }
        }
      }
#endif
      // Ignore any requests for refinements for output region requirmeents
      if (output_region_offset <= req_index)
        return;
      // See if we already have a refinement for handling this node
      for (OrderedRefinements::iterator it =
            pending_refinements.begin(); it != pending_refinements.end(); it++)
      {
        if (it->first->get_refinement_node() != refinement_node)
          continue;
        it.merge(refinement_mask);
        pending.insert(it->first, refinement_mask);
        return;
      }
#ifdef DEBUG_LEGION_COLLECTIVES
      RefinementOp *refinement_op = context->get_refinement_op(op,
                                                  refinement_node);
#else
      RefinementOp *refinement_op = context->get_refinement_op();
#endif
      refinement_op->initialize(op, req_index, privilege,
                                refinement_node, parent_req_index);
      // Start the dependence analysis for this refinement now
      // We'll finish the dependence analysis in the destructor
      refinement_op->begin_dependence_analysis();
      pending_refinements.insert(refinement_op, refinement_mask);
      pending.insert(refinement_op, refinement_mask);
    }

    //--------------------------------------------------------------------------
    void LogicalAnalysis::record_close_dependence(LogicalRegion parent,
                                 unsigned req_index, RegionTreeNode *path_node,
                                 const LogicalUser *user, const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      std::map<RegionTreeNode*,MergeCloseOp*>::const_iterator finder =
        pending_closes.find(path_node);
      if (finder == pending_closes.end())
      {
        // Start a new close operation for this node in the region tree
        // Construct a reigon requirement for this operation
        // All privileges are based on the parent logical region
        RegionRequirement req;
        if (path_node->is_region())
          req = RegionRequirement(path_node->as_region_node()->handle,
              LEGION_READ_WRITE, LEGION_EXCLUSIVE, parent);
        else
          req = RegionRequirement(path_node->as_partition_node()->handle, 0,
              LEGION_READ_WRITE, LEGION_EXCLUSIVE, parent);
#ifdef DEBUG_LEGION_COLLECTIVES
        MergeCloseOp *close_op = context->get_merge_close_op(op, path_node);
#else
        MergeCloseOp *close_op = context->get_merge_close_op();
#endif
        close_op->initialize(context, req, req_index, op);
        // Mark that we are starting our dependence analysis
        close_op->begin_dependence_analysis();
        finder = pending_closes.insert(
            std::make_pair(path_node, close_op)).first;
      }
#ifdef LEGION_SPY
      LegionSpy::log_mapping_dependence(context->get_unique_id(),
          user->uid, user->idx, finder->second->get_unique_op_id(),
          0/*index*/, LEGION_TRUE_DEPENDENCE);
#endif
      finder->second->register_region_dependence(0/*index*/, user->op,
        user->gen, user->idx, LEGION_TRUE_DEPENDENCE, false/*validates*/, mask);
      finder->second->update_close_mask(mask);
    }

#if 0
    //--------------------------------------------------------------------------
    void LegionAnalysis::record_pending_refinement(RefinementNode *refinement,
      const FieldMask &refinement_mask, FieldMaskSet<RefinementOp> &refinements)
    //--------------------------------------------------------------------------
    {
      // Scan through all the existing refinement operations and see which
      // ones subsume 

    }

    //--------------------------------------------------------------------------
    void LogicalAnalysis::record_close_dependence(LogicalRegion privilege_root,
                   RegionTreeNode *path_node, LogicalUser *user, FieldMask mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      // Should never be recording dependences on ourself
      assert((user->op != op) || (user->gen != op->get_generation()));
#endif
      std::map<RegionTreeNode*,PendingClose*> &closes = 
        pending_closes[privilege_root];
      // See if we already have a close that covers these fields
      std::map<RegionTreeNode*,PendingClose*>::iterator finder =
        closes.find(path_node);
      if (finder != closes.end())
      {
        // Special case, we're the only close so we can safely add to
        // this pending close operation
        if (closes.size() == 1)
        {
          if (finder->second->preconditions.insert(user, mask))
            user->add_reference();
          return;
        }
        else
        {
          const FieldMask overlap = 
            mask & finder->second->preconditions.get_valid_mask();
          if (!!overlap)
          {
            if (finder->second->preconditions.insert(user, overlap))
              user->add_reference();
            mask -= overlap;
            if (!mask)
              return;
          }
        }
      }
      // Otherwise we need to traverse the other close operations to see
      // if we need to merge any close operations
      FieldMaskSet<PendingClose> to_merge;
      for (std::map<RegionTreeNode*,PendingClose*>::const_iterator it =
            closes.begin(); it != closes.end(); it++)
      {
        if (it->first == path_node)
          continue;
        // See if we share any fields
        const FieldMask overlap = 
          it->second->preconditions.get_valid_mask() & mask;
        if (!overlap)
          continue;
        // If there are shared fields, we need to check to see if one
        // of the nodes dominates the other and make sure we have just
        // one close operation for them in that case
        const unsigned prev_depth = it->first->get_depth();
        const unsigned next_depth = path_node->get_depth();
        if (prev_depth < next_depth)
        {
          RegionTreeNode *next = path_node;
          while (prev_depth < next->get_depth())
            next = next->get_parent();
          if (it->first == next)
          {
            // This previous closing node dominates this path node
            // so we can just record this operation here
            if (it->second->preconditions.insert(user, overlap))
              user->add_reference();
            mask -= overlap;
            if (!mask)
              return;
          }
        }
        else if (next_depth < prev_depth)
        {
          RegionTreeNode *prev = it->first;
          while (next_depth < prev->get_depth())
            prev = prev->get_parent();
          if (prev == path_node)
          {
            // This new path node dominates the previous so we need to 
            // split out the users here 
            to_merge.insert(it->second, overlap);
          }
        }
        // Else depth is the same and we already know that they
        // are not the same node so neither can be dominating
      }
      if (!!mask)
      {
        // If we still have remaining fields then start a new pending
        // close operation at this path node
        if (finder != closes.end())
        {
          // If we already had close for this node we can still use it
          if (finder->second->preconditions.insert(user, mask))
            user->add_reference();
        }
        else
        {
          PendingClose *pending = new PendingClose(path_node, user->idx);
          if (pending->preconditions.insert(user, mask))
            user->add_reference();
          finder = closes.insert(std::make_pair(path_node, pending)).first;
        }
        if (!to_merge.empty())
        {
          // Merge all the subsumed ones into our new close operation
          for (FieldMaskSet<PendingClose>::const_iterator pit =
                to_merge.begin(); pit != to_merge.end(); pit++)
          {
            if (pit->second == pit->first->preconditions.get_valid_mask())
            {
              // All the users are migrating over to the new close
              // References flow over unless there are duplicates
              for (FieldMaskSet<LogicalUser>::const_iterator it =
                    pit->first->preconditions.begin(); it !=
                    pit->first->preconditions.end(); it++)
                if (!finder->second->preconditions.insert(it->first,it->second))
                  it->first->remove_reference();
              // Prune it out of the data structure
              closes.erase(pit->first->node);
              delete pit->first;
            }
            else
            {
              // Filter over the users that need to move
              std::vector<LogicalUser*> to_delete;
              for (FieldMaskSet<LogicalUser>::iterator it =
                    pit->first->preconditions.begin(); it !=
                    pit->first->preconditions.end(); it++)
              {
                const FieldMask overlap = pit->second & it->second;
                if (!overlap)
                  continue;
                if (finder->second->preconditions.insert(it->first, overlap))
                  it->first->add_reference();
                it.filter(overlap);
                if (!it->second)
                  to_delete.push_back(it->first);
              }
              for (std::vector<LogicalUser*>::const_iterator it =
                    to_delete.begin(); it != to_delete.end(); it++)
              {
                pit->first->preconditions.erase(*it);
                if ((*it)->remove_reference())
                  delete (*it);
              }
              pit->first->preconditions.tighten_valid_mask();
            }
          }
        }
      }
    }
#endif

    //--------------------------------------------------------------------------
    void LogicalAnalysis::issue_internal_operation(RegionTreeNode *node,
                  InternalOp *internal_op, const FieldMask &internal_mask,
                  const unsigned internal_index) const
    //--------------------------------------------------------------------------
    {
      // Do any other work for the dependence analysis
      internal_op->trigger_dependence_analysis();
      // Record a user for this internal operation in the region tree 
      LogicalUser *user = new LogicalUser(internal_op, 0/*region index*/,
          RegionUsage(LEGION_READ_WRITE, LEGION_EXCLUSIVE, 0/*redop*/),
          NULL/*projection*/, internal_index);
      LogicalState &state = node->get_logical_state(
                context->get_logical_tree_context());
      // This will take ownership of the user
      node->register_local_user(state, *user, internal_mask);
      // Record a dependence on the internal operation for ourself
      op->register_region_dependence(internal_op->get_internal_index(),
          internal_op, internal_op->get_generation(), 0/*internal idx*/,
          LEGION_TRUE_DEPENDENCE, false/*validates*/, internal_mask);
#ifdef LEGION_SPY
      LegionSpy::log_mapping_dependence(context->get_unique_id(),
          internal_op->get_unique_op_id(), 0/*index*/, op->get_unique_op_id(),
          internal_op->get_internal_index(), LEGION_TRUE_DEPENDENCE);
#endif
      // Mark that we are done, this puts the op in the pipeline!
      internal_op->end_dependence_analysis();
    }

    /////////////////////////////////////////////////////////////
    // Copy Fill Guard
    /////////////////////////////////////////////////////////////

#ifndef NON_AGGRESSIVE_AGGREGATORS
    //--------------------------------------------------------------------------
    CopyFillGuard::CopyFillGuard(RtUserEvent post, RtUserEvent applied)
      : guard_postcondition(post), effects_applied(applied),
        releasing_guards(false), read_only_guard(false)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    CopyFillGuard::CopyFillGuard(const CopyFillGuard &rhs)
      : guard_postcondition(rhs.guard_postcondition), 
        effects_applied(rhs.effects_applied)
    //--------------------------------------------------------------------------
    {
      // Should never be called
      assert(false);
    }
#else
    //--------------------------------------------------------------------------
    CopyFillGuard::CopyFillGuard(RtUserEvent applied)
      : effects_applied(applied), releasing_guards(false),read_only_guard(false)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    CopyFillGuard::CopyFillGuard(const CopyFillGuard &rhs)
      : effects_applied(rhs.effects_applied)
    //--------------------------------------------------------------------------
    {
      // Should never be called
      assert(false);
    }
#endif

    //--------------------------------------------------------------------------
    CopyFillGuard::~CopyFillGuard(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(releasing_guards); // should have done a release
      assert(guarded_sets.empty());
      assert(remote_release_events.empty());
#endif
    }

    //--------------------------------------------------------------------------
    CopyFillGuard& CopyFillGuard::operator=(const CopyFillGuard &rhs)
    //--------------------------------------------------------------------------
    {
      // Should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void CopyFillGuard::pack_guard(Serializer &rez)
    //--------------------------------------------------------------------------
    {
      AutoLock g_lock(guard_lock);
      // If we're already releasing a guard then there is no point in sending it 
      if (releasing_guards)
      {
        rez.serialize(RtUserEvent::NO_RT_USER_EVENT);
        return;
      }
#ifdef DEBUG_LEGION
      assert(!guarded_sets.empty());
      assert(effects_applied.exists());
#endif
      // We only ever pack the effects applied event here because once a
      // guard is on a remote node then the guard postcondition is no longer
      // useful since all remote copy fill operations will need to key off
      // the effects applied event to be correct
      rez.serialize(effects_applied);
      rez.serialize<bool>(read_only_guard);
      // Make an event for recording when all the remote events are applied
      RtUserEvent remote_release = Runtime::create_rt_user_event();
      rez.serialize(remote_release);
      remote_release_events.push_back(remote_release);
    }

    //--------------------------------------------------------------------------
    /*static*/ CopyFillGuard* CopyFillGuard::unpack_guard(Deserializer &derez,
                                          Runtime *runtime, EquivalenceSet *set)
    //--------------------------------------------------------------------------
    {
      RtUserEvent effects_applied;
      derez.deserialize(effects_applied);
      if (!effects_applied.exists())
        return NULL;
#ifndef NON_AGGRESSIVE_AGGREGATORS
      // Note we use the effects applied event here twice because all
      // copy-fill aggregators on this node will need to wait for the
      // full effects to be applied of any guards on a remote node
      CopyFillGuard *result = 
        new CopyFillGuard(effects_applied, effects_applied);
#else
      CopyFillGuard *result = new CopyFillGuard(effects_applied);
#endif
      bool read_only_guard;
      derez.deserialize(read_only_guard);
#ifdef DEBUG_LEGION
      if (!result->record_guard_set(set, read_only_guard))
        assert(false);
#else
      result->record_guard_set(set, read_only_guard);
#endif
      RtUserEvent remote_release;
      derez.deserialize(remote_release);
      std::set<RtEvent> release_preconditions;
      result->release_guards(runtime, release_preconditions, true/*defer*/);
      if (!release_preconditions.empty())
        Runtime::trigger_event(remote_release,
            Runtime::merge_events(release_preconditions));
      else
        Runtime::trigger_event(remote_release);
      return result;
    }

    //--------------------------------------------------------------------------
    bool CopyFillGuard::record_guard_set(EquivalenceSet *set, bool read_only)
    //--------------------------------------------------------------------------
    {
      if (releasing_guards)
        return false;
      AutoLock g_lock(guard_lock);
      // Check again after getting the lock to avoid the race
      if (releasing_guards)
        return false;
#ifdef DEBUG_LEGION
      assert(guarded_sets.empty() || (read_only_guard == read_only));
#endif
      guarded_sets.insert(set);
      read_only_guard = read_only;
      return true;
    }

    //--------------------------------------------------------------------------
    bool CopyFillGuard::release_guards(Runtime *rt, std::set<RtEvent> &applied,
                                       bool force_deferral /*=false*/)
    //--------------------------------------------------------------------------
    {
      if (force_deferral || !effects_applied.has_triggered())
      {
        RtUserEvent released = Runtime::create_rt_user_event();
        // Meta-task will take responsibility for deletion
        CopyFillDeletion args(this, implicit_provenance, released);
        rt->issue_runtime_meta_task(args,
            LG_LATENCY_DEFERRED_PRIORITY, effects_applied);
        applied.insert(released);
        return false;
      }
      else
        release_guarded_sets(applied);
      return true;
    }

    //--------------------------------------------------------------------------
    /*static*/ void CopyFillGuard::handle_deletion(const void *args)
    //--------------------------------------------------------------------------
    {
      const CopyFillDeletion *dargs = (const CopyFillDeletion*)args;
      std::set<RtEvent> released_preconditions;
      dargs->guard->release_guarded_sets(released_preconditions);
      if (!released_preconditions.empty())
        Runtime::trigger_event(dargs->released, 
            Runtime::merge_events(released_preconditions));
      else
        Runtime::trigger_event(dargs->released);
      delete dargs->guard;
    }

    //--------------------------------------------------------------------------
    void CopyFillGuard::release_guarded_sets(std::set<RtEvent> &released)
    //--------------------------------------------------------------------------
    {
      std::set<EquivalenceSet*> to_remove;
      {
        AutoLock g_lock(guard_lock);
#ifdef DEBUG_LEGION
        assert(!releasing_guards);
#endif
        releasing_guards = true;
        to_remove.swap(guarded_sets);
        if (!remote_release_events.empty())
        {
          released.insert(remote_release_events.begin(),
                          remote_release_events.end());
          remote_release_events.clear();
        }
      }
      if (!to_remove.empty())
      {
        if (read_only_guard)
        {
          for (std::set<EquivalenceSet*>::const_iterator it = 
                to_remove.begin(); it != to_remove.end(); it++)
            (*it)->remove_read_only_guard(this);
        }
        else
        {
          for (std::set<EquivalenceSet*>::const_iterator it = 
                to_remove.begin(); it != to_remove.end(); it++)
            (*it)->remove_reduction_fill_guard(this);
        }
      }
    }

    /////////////////////////////////////////////////////////////
    // Copy Fill Aggregator
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    CopyFillAggregator::CopyFillAggregator(RegionTreeForest *f, 
                                           PhysicalAnalysis *a, 
                                           CopyFillGuard *previous,
                                           bool t, PredEvent p)
#ifndef NON_AGGRESSIVE_AGGREGATORS
      : CopyFillGuard(Runtime::create_rt_user_event(), 
                      Runtime::create_rt_user_event()),
#else
      :  CopyFillGuard(Runtime::create_rt_user_event()), 
#endif
        forest(f), local_space(f->runtime->address_space), analysis(a), 
        collective_mapping(analysis->get_replicated_mapping()),
        src_index(analysis->index), dst_index(analysis->index),
#ifndef NON_AGGRESSIVE_AGGREGATORS
        guard_precondition((previous == NULL) ? RtEvent::NO_RT_EVENT :
                            RtEvent(previous->guard_postcondition)),
#else
        guard_precondition((previous == NULL) ? RtEvent::NO_RT_EVENT :
                            RtEvent(previous->effects_applied)),
#endif
        predicate_guard(p), track_events(t)
    //--------------------------------------------------------------------------
    {
      analysis->add_reference();
      // Need to transitively chain effects across aggregators since they
      // each need to summarize all the ones that came before
      if (previous != NULL)
        effects.insert(previous->effects_applied);
    }

    //--------------------------------------------------------------------------
    CopyFillAggregator::CopyFillAggregator(RegionTreeForest *f, 
                                PhysicalAnalysis *a,
                                unsigned src_idx, unsigned dst_idx,
                                CopyFillGuard *previous, bool t, PredEvent p, 
                                RtEvent alternative_precondition)
#ifndef NON_AGGRESSIVE_AGGREGATORS
      : CopyFillGuard(Runtime::create_rt_user_event(), 
                      Runtime::create_rt_user_event()),
#else
      : CopyFillGuard(Runtime::create_rt_user_event()),
#endif
        forest(f), local_space(f->runtime->address_space), analysis(a),
        collective_mapping(analysis->get_replicated_mapping()),
        src_index(src_idx), dst_index(dst_idx),
#ifndef NON_AGGRESSIVE_AGGREGATORS
        guard_precondition((previous == NULL) ? alternative_precondition :
                            RtEvent(previous->guard_postcondition)),
#else
        guard_precondition((previous == NULL) ? alternative_precondition:
                            RtEvent(previous->effects_applied)),
#endif
        predicate_guard(p), track_events(t)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert((previous == NULL) || !alternative_precondition.exists());
#endif
      analysis->add_reference();
      // Need to transitively chain effects across aggregators since they
      // each need to summarize all the ones that came before
      if (previous != NULL)
        effects.insert(previous->effects_applied);
    }

    //--------------------------------------------------------------------------
    CopyFillAggregator::~CopyFillAggregator(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
#ifndef NON_AGGRESSIVE_AGGREGATORS
      assert(guard_postcondition.has_triggered());
#endif
      assert(effects_applied.has_triggered());
#endif
      if (analysis->remove_reference())
        delete analysis;
      // Remove references from any views that we have
      for (std::set<LogicalView*>::const_iterator it = 
            all_views.begin(); it != all_views.end(); it++)
        if ((*it)->remove_base_valid_ref(AGGREGATOR_REF))
          delete (*it);
      all_views.clear();
      // Delete all our copy updates
      for (LegionMap<InstanceView*,FieldMaskSet<Update> >::const_iterator
            mit = sources.begin(); mit != sources.end(); mit++)
      {
        for (FieldMaskSet<Update>::const_iterator it = 
              mit->second.begin(); it != mit->second.end(); it++)
          delete it->first;
      }
      for (std::vector<LegionMap<InstanceView*,
                FieldMaskSet<Update> > >::const_iterator rit = 
            reductions.begin(); rit != reductions.end(); rit++)
      {
        for (LegionMap<InstanceView*,FieldMaskSet<Update> >::const_iterator
              mit = rit->begin(); mit != rit->end(); mit++)
        {
          for (FieldMaskSet<Update>::const_iterator it = 
                mit->second.begin(); it != mit->second.end(); it++)
            delete it->first;
        }
      }
    }

    //--------------------------------------------------------------------------
    CopyFillAggregator::Update::Update(IndexSpaceExpression *exp,
                                const FieldMask &mask, CopyAcrossHelper *helper)
      : expr(exp), src_mask(mask), across_helper(helper)
    //--------------------------------------------------------------------------
    {
      expr->add_base_expression_reference(AGGREGATOR_REF);
    }

    //--------------------------------------------------------------------------
    CopyFillAggregator::Update::~Update(void)
    //--------------------------------------------------------------------------
    {
      if (expr->remove_base_expression_reference(AGGREGATOR_REF))
        delete expr;
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::CopyUpdate::record_source_expressions(
                                            InstanceFieldExprs &src_exprs) const
    //--------------------------------------------------------------------------
    {
      FieldMaskSet<IndexSpaceExpression> &exprs = src_exprs[source];  
      FieldMaskSet<IndexSpaceExpression>::iterator finder = 
        exprs.find(expr);
      if (finder == exprs.end())
        exprs.insert(expr, src_mask);
      else
        finder.merge(src_mask);
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::CopyUpdate::sort_updates(
                    std::map<InstanceView*, std::vector<CopyUpdate*> > &copies,
                    std::vector<FillUpdate*> &fills)
    //--------------------------------------------------------------------------
    {
      copies[source].push_back(this);
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::FillUpdate::record_source_expressions(
                                            InstanceFieldExprs &src_exprs) const
    //--------------------------------------------------------------------------
    {
      // Do nothing, we have no source expressions
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::FillUpdate::sort_updates(
                    std::map<InstanceView*, std::vector<CopyUpdate*> > &copies,
                    std::vector<FillUpdate*> &fills)
    //--------------------------------------------------------------------------
    {
      fills.push_back(this);
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::record_update(InstanceView *dst_view,
                                           PhysicalManager *dst_man,
                                           LogicalView *src_view,
                                           const FieldMask &src_mask,
                                           IndexSpaceExpression *expr,
                                           const PhysicalTraceInfo &trace_info,
                                           EquivalenceSet *tracing_eq,
                                           ReductionOpID redop /*=0*/,
                                           CopyAcrossHelper *helper /*=NULL*/)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!!src_mask);
      assert(!expr->is_empty());
#endif
      if (src_view->is_deferred_view())
      {
#ifdef DEBUG_LEGION
        assert(redop == 0);
#endif
        DeferredView *def_view = src_view->as_deferred_view();
        def_view->flatten(*this, dst_view, src_mask, expr, predicate_guard,
                          trace_info, tracing_eq, helper);
      }
      else
      {
        InstanceView *inst_view = src_view->as_instance_view();
        PhysicalManager *src_man = NULL;
        if (inst_view->is_collective_view())
        {
          if (dst_man != NULL)
          {
            std::vector<InstanceView*> src_views(1, inst_view);
            const SelectSourcesResult &result = 
              select_sources(dst_view, dst_man, src_views);
#ifdef DEBUG_LEGION
            assert(result.ranking.size() == 1);
#endif
            std::map<unsigned,PhysicalManager*>::const_iterator finder =
              result.points.find(0);
            if (finder != result.points.end())
              src_man = finder->second;
          }
        }
        else
          src_man = inst_view->as_individual_view()->get_manager();
        record_instance_update(dst_view, inst_view, src_man, src_mask,
                               expr, tracing_eq, redop, helper);
      }
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::record_instance_update(InstanceView *dst_view,
                                                  InstanceView *src_view,
                                                  PhysicalManager *src_man,
                                                  const FieldMask &src_mask,
                                                  IndexSpaceExpression *expr,
                                                  EquivalenceSet *tracing_eq,
                                                  ReductionOpID redop,
                                                  CopyAcrossHelper *helper)
    //--------------------------------------------------------------------------
    {
      update_fields |= src_mask;
      record_view(dst_view);
      record_view(src_view);
      CopyUpdate *update = 
        new CopyUpdate(src_view, src_man, src_mask, expr, redop, helper);
      FieldMaskSet<Update> &updates = sources[dst_view];
      if (helper == NULL)
        updates.insert(update, src_mask);
      else
        updates.insert(update, helper->convert_src_to_dst(src_mask));
      if (tracing_eq != NULL)
        update_tracing_valid_views(tracing_eq, src_view, dst_view, 
                                   src_mask, expr, redop);
    }

    //--------------------------------------------------------------------------
    const CopyFillAggregator::SelectSourcesResult& 
        CopyFillAggregator::select_sources(InstanceView *dst_view, 
                                    PhysicalManager *dst_man,
                                    const std::vector<InstanceView*> &src_views)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(dst_view != NULL);
      assert(dst_man != NULL);
      assert(!src_views.empty());
#endif
      const std::pair<InstanceView*,PhysicalManager*> key(dst_view, dst_man);
      std::map<std::pair<InstanceView*,PhysicalManager*>,
               std::vector<SelectSourcesResult> >::iterator
        finder = mapper_queries.find(key);
      if (finder != mapper_queries.end())
      {
        for (std::vector<SelectSourcesResult>::const_iterator it = 
              finder->second.begin(); it != finder->second.end(); it++)
          if (it->matches(src_views))
            return *it;
      }
      else
        finder = mapper_queries.insert(
            std::make_pair(key, std::vector<SelectSourcesResult>())).first;
      // If we didn't find the query result we need to do it for ourself
      std::vector<unsigned> ranking;
      std::map<unsigned,PhysicalManager*> points;
      // Always use the source index for selecting sources
      analysis->op->select_sources(src_index, dst_man,src_views,ranking,points);
      // Check to make sure that the ranking has sound output
      unsigned count = 0;
      std::vector<bool> unique_indexes(src_views.size(), false);
      for (std::vector<unsigned>::iterator it =
            ranking.begin(); it != ranking.end(); /*nothing*/)
      {
        if (((*it) < unique_indexes.size()) && !unique_indexes[*it])
        {
          unique_indexes[*it] = true;
          count++;
          it++;
        }
        else // remove duplicates and out of bound entries
          it = ranking.erase(it);
      }
      if (count < unique_indexes.size())
      {
        for (unsigned idx = 0; idx < unique_indexes.size(); idx++)
          if (!unique_indexes[idx])
            ranking.push_back(idx);
      }
      // Save the result for the future
      finder->second.emplace_back(SelectSourcesResult(
            std::vector<InstanceView*>(src_views)/*make a copy*/,
            std::move(ranking), std::move(points)));
      return finder->second.back();
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::record_updates(InstanceView *dst_view, 
                                    PhysicalManager *dst_man,
                                    const FieldMaskSet<LogicalView> &src_views,
                                    const FieldMask &src_mask,
                                    IndexSpaceExpression *expr,
                                    const PhysicalTraceInfo &trace_info,
                                    EquivalenceSet *tracing_eq,
                                    ReductionOpID redop /*=0*/,
                                    CopyAcrossHelper *helper /*=NULL*/)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!!src_mask);
      assert(!src_views.empty());
      assert(!expr->is_empty());
#endif
      if (src_views.size() == 1)
      {
        LogicalView *src_view = src_views.begin()->first;
        const FieldMask record_mask = src_views.get_valid_mask() & src_mask;
        record_update(dst_view, dst_man, src_view, record_mask, expr,
                      trace_info, tracing_eq, redop, helper);
      }
      else
      {
        // We have multiple views, so let's sort them
        LegionList<FieldSet<LogicalView*> > view_sets;
        src_views.compute_field_sets(src_mask, view_sets);
        for (LegionList<FieldSet<LogicalView*> >::const_iterator
              vit = view_sets.begin(); vit != view_sets.end(); vit++)
        {
          if (vit->elements.empty())
            continue;
          if (vit->elements.size() == 1)
          {
            // Easy case, just one view so do it  
            LogicalView *src_view = *(vit->elements.begin());
            const FieldMask &record_mask = vit->set_mask;
            record_update(dst_view, dst_man, src_view, record_mask, expr,
                          trace_info, tracing_eq, redop, helper); 
          }
          else
          {
            // Sort the views, prefer deferred  then instances
            DeferredView *deferred = NULL;
            std::vector<InstanceView*> instances;
            for (std::set<LogicalView*>::const_iterator it = 
                  vit->elements.begin(); it != vit->elements.end(); it++)
            {
              if (!(*it)->is_instance_view())
              {
                deferred = (*it)->as_deferred_view();
                // Break out since we found what we're looking for
                break;
              }
              else
                instances.push_back((*it)->as_instance_view());
            }
            if (deferred != NULL)
              deferred->flatten(*this, dst_view, vit->set_mask, expr,
                  predicate_guard, trace_info, tracing_eq, helper);
            else if (!instances.empty())
            {
              if (instances.size() == 1)
              {
                // Easy, just one instance to use and no collective instances
                InstanceView *src_view = instances.back();
                record_update(dst_view, dst_man, src_view, vit->set_mask, expr,
                              trace_info, tracing_eq, redop, helper);
              }
              else
              {
                // Hard, multiple potential sources,
                // ask the mapper which one to use
                const SelectSourcesResult &result = 
                  select_sources(dst_view, dst_man, instances);
#ifdef DEBUG_LEGION
                assert(result.ranking.size() == instances.size());
#endif
                const unsigned first = result.ranking.front();
                InstanceView *src_view = instances[first];
                PhysicalManager *src_man = NULL;
                // Find the source point if it is a collective view 
                if (src_view->is_collective_view())
                {
                  std::map<unsigned,PhysicalManager*>::const_iterator 
                    point_finder = result.points.find(first);
                  if (point_finder != result.points.end())
                    src_man = point_finder->second;
                }
                else
                  src_man = src_view->as_individual_view()->get_manager();
                record_instance_update(dst_view, src_view, src_man,
                    vit->set_mask, expr, tracing_eq, redop, helper);
              }
            }
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::record_partial_updates(InstanceView *dst_view, 
                          PhysicalManager *dst_man,const LegionMap<LogicalView*,
                                 FieldMaskSet<IndexSpaceExpression> >&src_views,
                          const FieldMask &src_mask, IndexSpaceExpression *expr, 
                          const PhysicalTraceInfo &trace_info,
                          EquivalenceSet *tracing_eq, ReductionOpID redop,
                          CopyAcrossHelper *across_helper)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!!src_mask);
      assert(!src_views.empty());
      assert(!expr->is_empty());
#endif
      update_fields |= src_mask;
      record_view(dst_view);
      std::vector<InstanceView*> instances;
      FieldMaskSet<IndexSpaceExpression> remainders;
      remainders.insert(expr, src_mask);
      // Issue deferred immediately, otherwise record instances so that
      // we can ask the mapper what order it wants us to issue copies from
      for (LegionMap<LogicalView*,
            FieldMaskSet<IndexSpaceExpression> >::const_iterator vit =
            src_views.begin(); vit != src_views.end(); vit++)
      {
        FieldMask view_overlap = 
          vit->second.get_valid_mask() & remainders.get_valid_mask();;
        if (!view_overlap)
          continue;
        if (vit->first->is_deferred_view())
        {
          DeferredView *deferred = vit->first->as_deferred_view();
          // Skip any deferred if we're doing a reduction, we only care
          // about valid instances here
          if (redop > 0)
            continue;
          // Join in the fields to see what overlaps
          LegionMap<std::pair<IndexSpaceExpression*,IndexSpaceExpression*>,
            FieldMask> deferred_exprs;
          unique_join_on_field_mask_sets(remainders,vit->second,deferred_exprs);
          bool need_tighten = false;
          for (LegionMap<std::pair<IndexSpaceExpression*,
                IndexSpaceExpression*>,FieldMask>::const_iterator 
                it = deferred_exprs.begin(); it != deferred_exprs.end(); it++)
          {
            IndexSpaceExpression *overlap =
             forest->intersect_index_spaces(it->first.first,it->first.second);
            const size_t overlap_size = overlap->get_volume();
            if (overlap_size == 0)
              continue;
            FieldMaskSet<IndexSpaceExpression>::iterator finder = 
              remainders.find(it->first.first);
#ifdef DEBUG_LEGION
            assert(finder != remainders.end());
#endif
            finder.filter(it->second);
            if (!finder->second)
              remainders.erase(finder);
            if (overlap_size < it->first.first->get_volume())
            {
              if (overlap_size == it->first.second->get_volume())
                deferred->flatten(*this, dst_view, it->second, it->first.second,
                   predicate_guard, trace_info, tracing_eq, across_helper);
              else
                deferred->flatten(*this, dst_view, it->second, overlap,
                   predicate_guard, trace_info, tracing_eq, across_helper);
              // Compute the difference
              IndexSpaceExpression *diff_expr = 
                forest->subtract_index_spaces(it->first.first, overlap); 
              remainders.insert(diff_expr, it->second);
            }
            else // completely covers remainder expression
            {
              deferred->flatten(*this, dst_view, it->second, it->first.first,
                  predicate_guard, trace_info, tracing_eq, across_helper);
              if (remainders.empty())
                return;
              need_tighten = true;
            }
          }
          if (need_tighten)
            remainders.tighten_valid_mask();
        }
        else
          instances.push_back(vit->first->as_instance_view());
      }
      // If we get here, next try to sort the instances into whatever order
      // the mapper wants us to try to issue copies from them
      if (!instances.empty())
      {
        std::vector<unsigned> ranking;
        std::map<unsigned,PhysicalManager*> points;
        // Need to ask the mapper which instances it prefers if there are
        // multiple choices or we have a collective instance to pick from
        if ((instances.size() > 1) || instances.back()->is_collective_view())
        {
          const SelectSourcesResult &result = 
            select_sources(dst_view, dst_man, instances);
          ranking = result.ranking;
          points = result.points;
        }
        else
          ranking.push_back(0);
        for (unsigned idx = 0; idx < ranking.size(); idx++)
        {
          InstanceView *src_view = instances[ranking[idx]];
          PhysicalManager *src_man = NULL;
          // Find the source key if this is a collective instance
          if (src_view->is_collective_view())
          {
            std::map<unsigned,PhysicalManager*>::const_iterator point_finder =
              points.find(ranking[idx]);
            if (point_finder != points.end())
              src_man = point_finder->second;
          }
          else
            src_man = src_view->as_individual_view()->get_manager();
          LegionMap<LogicalView*,FieldMaskSet<IndexSpaceExpression> >::
              const_iterator finder = src_views.find(src_view);
#ifdef DEBUG_LEGION
          assert(finder != src_views.end());
#endif
          LegionMap<std::pair<IndexSpaceExpression*,IndexSpaceExpression*>,
            FieldMask> src_expressions;
          unique_join_on_field_mask_sets(remainders, finder->second,
                                         src_expressions);
          bool need_tighten = false;
          for (LegionMap<std::pair<IndexSpaceExpression*,IndexSpaceExpression*>,
                FieldMask>::const_iterator it = 
                src_expressions.begin(); it != src_expressions.end(); it++)
          {
            IndexSpaceExpression *overlap = 
              forest->intersect_index_spaces(it->first.first, it->first.second);
            const size_t overlap_size = overlap->get_volume();
            if (overlap_size == 0)
              continue;
            FieldMaskSet<IndexSpaceExpression>::iterator finder = 
              remainders.find(it->first.first);
#ifdef DEBUG_LEGION
            assert(finder != remainders.end());
#endif
            finder.filter(it->second);
            if (!finder->second)
              remainders.erase(finder);
            if (overlap_size < it->first.first->get_volume())
            {
              if (overlap_size == it->first.second->get_volume())
                record_instance_update(dst_view, src_view, src_man, it->second,
                    it->first.second, tracing_eq, redop, across_helper);
              else
                record_instance_update(dst_view, src_view, src_man, it->second,
                    overlap, tracing_eq, redop, across_helper);
              // Compute the difference
              IndexSpaceExpression *diff_expr = 
                forest->subtract_index_spaces(it->first.first, overlap);
              remainders.insert(diff_expr, it->second);
            }
            else // completely covers remainder expression
            {
              record_instance_update(dst_view, src_view, src_man, it->second,
                  it->first.first, tracing_eq, redop, across_helper);
              if (remainders.empty())
                return;
              need_tighten = true;
            }
          }
          if (need_tighten)
            remainders.tighten_valid_mask();
        }
      }
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::record_fill(InstanceView *dst_view,
                                         FillView *src_view,
                                         const FieldMask &fill_mask,
                                         IndexSpaceExpression *expr,
                                         const PredEvent fill_guard,
                                         EquivalenceSet *tracing_eq,
                                         CopyAcrossHelper *helper /*=NULL*/)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!!fill_mask);
      assert(!expr->is_empty());
#endif
      update_fields |= fill_mask;
      record_view(src_view);
      record_view(dst_view);
      FillUpdate *update = 
        new FillUpdate(src_view, fill_mask, expr, fill_guard, helper); 
      if (helper == NULL)
        sources[dst_view].insert(update, fill_mask);
      else
        sources[dst_view].insert(update, helper->convert_src_to_dst(fill_mask));
      if (tracing_eq != NULL)
      {
        if (dst_view->is_reduction_kind())
          tracing_eq->update_tracing_anti_views(dst_view, expr, fill_mask);
        else
          update_tracing_valid_views(tracing_eq, src_view, dst_view,
                                     fill_mask, expr, 0/*redop*/);
      }
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::record_reductions(InstanceView *dst_view,
                                               PhysicalManager *dst_man,
                                  const std::list<std::pair<InstanceView*,
                                            IndexSpaceExpression*> > &src_views,
                                  const unsigned src_fidx,
                                  const unsigned dst_fidx,
                                  EquivalenceSet *tracing_eq,
                                  CopyAcrossHelper *across_helper)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!src_views.empty());
#endif 
      update_fields.set_bit(src_fidx);
      record_view(dst_view);
      const std::pair<InstanceView*,unsigned> dst_key(dst_view, dst_fidx);
      std::vector<ReductionOpID> &redop_epochs = reduction_epochs[dst_key];
      FieldMask src_mask, dst_mask;
      src_mask.set_bit(src_fidx);
      dst_mask.set_bit(dst_fidx);
      // Always start scanning from the first redop index
      unsigned redop_index = 0;
      for (std::list<std::pair<InstanceView*,IndexSpaceExpression*> >::
            const_iterator it = src_views.begin(); it != src_views.end(); it++)
      {
#ifdef DEBUG_LEGION
        assert(!it->second->is_empty());
#endif
        record_view(it->first);
        const ReductionOpID redop = it->first->get_redop();
#ifdef DEBUG_LEGION
        assert(redop > 0);
#endif
        PhysicalManager *src_man = NULL;
        if (it->first->is_collective_view())
        {
          if (dst_man != NULL)
          {
            std::vector<InstanceView*> src_views(1, it->first);
            const SelectSourcesResult &result =
              select_sources(dst_view, dst_man, src_views);
#ifdef DEBUG_LEGION
            assert(result.ranking.size() == 1);
#endif
            std::map<unsigned,PhysicalManager*>::const_iterator finder =
              result.points.find(0);
            if (finder != result.points.end())
              src_man = finder->second;
          }
        }
        else
          src_man = it->first->as_individual_view()->get_manager();
        CopyUpdate *update = new CopyUpdate(it->first, src_man, src_mask,
                                            it->second, redop, across_helper);
        // Ignore shadows when tracing, we only care about the normal
        // preconditions and postconditions for the copies
        if (tracing_eq != NULL)
          update_tracing_valid_views(tracing_eq, it->first, dst_view, 
                                     src_mask, it->second, redop);
        // Scan along looking for a reduction op epoch that matches
        while ((redop_index < redop_epochs.size()) &&
                (redop_epochs[redop_index] != redop))
          redop_index++;
        if (redop_index == redop_epochs.size())
        {
#ifdef DEBUG_LEGION
          assert(redop_index <= reductions.size());
#endif
          // Start a new redop epoch if necessary
          redop_epochs.push_back(redop);
          if (reductions.size() == redop_index)
            resize_reductions(redop_index + 1);
        }
        reductions[redop_index][dst_view].insert(update, dst_mask);
      }
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::resize_reductions(size_t new_size)
    //--------------------------------------------------------------------------
    {
      std::vector<LegionMap<InstanceView*,FieldMaskSet<Update> > >
        new_reductions(new_size);
      for (unsigned idx = 0; idx < reductions.size(); idx++)
        new_reductions[idx].swap(reductions[idx]);
      reductions.swap(new_reductions);
    }

    //--------------------------------------------------------------------------
    ApEvent CopyFillAggregator::issue_updates(
                      const PhysicalTraceInfo &trace_info,
                      ApEvent precondition, const bool restricted_output,
                      const bool manage_dst_events,
                      std::map<InstanceView*,std::vector<ApEvent> > *dst_events,
                      int stage)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!sources.empty() || !reductions.empty());
#endif
      if (guard_precondition.exists() && !guard_precondition.has_triggered())
      {
        if (track_events && !summary_event.exists())
          summary_event = Runtime::create_ap_user_event(&trace_info);
        CopyFillAggregation args(this, trace_info, precondition,
                                 manage_dst_events, restricted_output,
                                 analysis->op->get_unique_op_id(), 
                                 stage, dst_events);
        analysis->runtime->issue_runtime_meta_task(args, 
            LG_THROUGHPUT_DEFERRED_PRIORITY, guard_precondition);
        return summary_event;
      }
#ifdef DEBUG_LEGION
      assert(!guard_precondition.exists() || 
              guard_precondition.has_triggered());
#endif
#ifndef NON_AGGRESSIVE_AGGREGATORS
      std::set<RtEvent> recorded_events;
#endif
      // This is really subtle so pay attention: 
      // Copy across aggregators issue copies and fills to destination
      // instances. In some cases those destination instances are filled
      // and copied to multiple times across the initial sources and then
      // reductions of this copy fill aggregator. We need a way to make
      // sure that all the updates to the views for those destination 
      // instances for the copy users are recorded in the right order.
      // To facilitate that we have a fast path and slow path. In the
      // fast path, if all the updates are coming from this node
      // (in the case with no collective analysis) or from all the nodes
      // in the collective analysis then we can issue all those copies
      // back-to-back without interruption because we know that the 
      // messages being sent are all pipelined and run through ordered
      // virtual channels. However, if any copies need to be issued 
      // to a destination that does not share the collective mapping
      // then we don't have that pipelining guarantee so we need to
      // block the issuing of the copies between those stages. Whether
      // we can do this pipelining or not after each stage is tracked
      // by the 'pipeline' variable. This method will defer itself and
      // restart when this pipelining is not possible.
      bool pipeline = true;
      // Perform updates from any sources first
      if (stage < 0)
      {
#ifdef DEBUG_LEGION
        assert(stage == -1);
#endif
        if (!sources.empty())
        {

          pipeline = perform_updates(sources, trace_info, precondition,
#ifdef NON_AGGRESSIVE_AGGREGATORS
              effects,
#else
              recorded_events,
#endif
              stage, manage_dst_events, restricted_output, dst_events);
        }
        stage = 0;
      }
      // Then apply any reductions that we might have
      if (!reductions.empty())
      {
        // Skip any passes that we might have already done
        for (unsigned idx = stage; idx < reductions.size(); idx++)
        {
          if (!pipeline)
          {
#ifdef NON_AGGRESSIVE_AGGREGATORS
            const RtEvent stage_pre = Runtime::merge_events(effects);
            effects.clear();
#else
            const RtEvent stage_pre = Runtime::merge_events(recorded_events);
            recorded_events.clear();
#endif
            if (stage_pre.exists() && !stage_pre.has_triggered())
            {
              // If it's not safe to pipeline the launching of these copies
              // with the copies from the previous stage then we need to 
              // defer launching those copies until the previous ones have
              // run and registered all of their users
              CopyFillAggregation args(this, trace_info, precondition,
                                   manage_dst_events, restricted_output,
                                   analysis->op->get_unique_op_id(), 
                                   idx, dst_events);
              analysis->runtime->issue_runtime_meta_task(args, 
                  LG_THROUGHPUT_DEFERRED_PRIORITY, stage_pre);
              return summary_event;
            }
          }
          perform_updates(reductions[idx], trace_info, precondition,
#ifdef NON_AGGRESSIVE_AGGREGATORS
                          effects,
#else
                          recorded_events,
#endif
                          idx/*redop index*/, manage_dst_events,
                          restricted_output, dst_events);
        }
      }
      // Make sure we do this before we trigger the effects_applied
      // event as it could result in the deletion of this object
      ApEvent summary;
      if (track_events)
      {
        summary = Runtime::merge_events(&trace_info, events);
        if (summary_event.exists())
        {
          Runtime::trigger_event(&trace_info, summary_event, summary);
          // Pull this onto the stack in case the object is deleted
          summary = summary_event;
        }
      }
#ifndef NON_AGGRESSIVE_AGGREGATORS
      if (!recorded_events.empty())
        Runtime::trigger_event(guard_postcondition,
            Runtime::merge_events(recorded_events));
      else
        Runtime::trigger_event(guard_postcondition);
      // Make sure the guard postcondition is chained on the deletion
      if (!effects.empty())
      {
        effects.insert(guard_postcondition);
        Runtime::trigger_event(effects_applied,
            Runtime::merge_events(effects));
      }
      else
        Runtime::trigger_event(effects_applied, guard_postcondition);
#else
      // We can also trigger our effects event once the effects are applied
      if (!effects.empty())
        Runtime::trigger_event(effects_applied,
            Runtime::merge_events(effects));
      else
        Runtime::trigger_event(effects_applied);
#endif
      return summary;
    } 

    //--------------------------------------------------------------------------
    void CopyFillAggregator::record_view(LogicalView *new_view)
    //--------------------------------------------------------------------------
    {
      std::pair<std::set<LogicalView*>::iterator,bool> result = 
        all_views.insert(new_view);
      if (result.second)
        new_view->add_base_valid_ref(AGGREGATOR_REF);
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::update_tracing_valid_views(
                          EquivalenceSet *tracing_eq, LogicalView *src, 
                          LogicalView *dst, const FieldMask &mask, 
                          IndexSpaceExpression *expr, ReductionOpID redop) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(tracing_eq != NULL);
#endif
      const RegionUsage read_usage(LEGION_READ_PRIV, LEGION_EXCLUSIVE, 0);
      tracing_eq->update_tracing_valid_views(src, expr, read_usage, mask, 
                                             false/*invalidates*/);
      // Only record the destination if this is not a copy across
      if (src_index == dst_index)
      {
        const RegionUsage write_usage((redop > 0) ? LEGION_REDUCE_PRIV : 
            LEGION_WRITE_PRIV, LEGION_EXCLUSIVE, redop);
        // If we're doing a reduction, that does need to invalidate 
        // everything that is not being reduced to since it is a kind
        // of a write and this instance has dirty state, otherwise normal
        // copies are just making another copy of the same data
        tracing_eq->update_tracing_valid_views(dst, expr, write_usage, mask,
                                               (redop > 0)/*invalidates*/);
      }
    }

    //--------------------------------------------------------------------------
    bool CopyFillAggregator::perform_updates(
         const LegionMap<InstanceView*,FieldMaskSet<Update> > &updates,
         const PhysicalTraceInfo &trace_info, const ApEvent precondition,
         std::set<RtEvent> &recorded_events, const int redop_index,
         const bool manage_dst_events, const bool restricted_output,
         std::map<InstanceView*,std::vector<ApEvent> > *dst_events)
    //--------------------------------------------------------------------------
    {
      bool pipelined = true;
      std::vector<ApEvent> *target_events = NULL;
      for (LegionMap<InstanceView*,FieldMaskSet<Update> >::const_iterator
            uit = updates.begin(); uit != updates.end(); uit++)
      {
        ApEvent dst_precondition = precondition;
        // In the case where we're not managing destination events
        // then we need to incorporate any event postconditions from
        // previous passes as part of the preconditions for this pass
        if (!manage_dst_events)
        {
#ifdef DEBUG_LEGION
          assert(dst_events != NULL);
#endif
          // This only happens in the case of across copies
          std::map<InstanceView*,std::vector<ApEvent> >::iterator finder =
            dst_events->find(uit->first);
#ifdef DEBUG_LEGION
          assert(finder != dst_events->end());
#endif
          if (!finder->second.empty())
          {
            // Update our precondition to incude the copies from 
            // any previous passes that we performed
            finder->second.push_back(precondition);
            dst_precondition =
              Runtime::merge_events(&trace_info, finder->second);
            // Clear this for the next iteration
            // It's not obvious why this safe, but it is
            // We are guaranteed to issue at least one fill/copy that
            // will depend on this and therefore either test that it
            // has triggered or record itself back in the set of events
            // which gives us a transitive precondition
            finder->second.clear();
          }
          target_events = &finder->second;
        }
        // Group by fields first
        LegionList<FieldSet<Update*> > field_groups;
        uit->second.compute_field_sets(FieldMask(), field_groups);
        for (LegionList<FieldSet<Update*> >::const_iterator fit = 
              field_groups.begin(); fit != field_groups.end(); fit++)
        {
          const FieldMask &dst_mask = fit->set_mask;
          // Now that we have the src mask for these operations group 
          // them into fills and copies
          std::vector<FillUpdate*> fills;
          std::map<InstanceView* /*src*/,std::vector<CopyUpdate*> > copies;
          for (std::set<Update*>::const_iterator it = fit->elements.begin();
                it != fit->elements.end(); it++)
            (*it)->sort_updates(copies, fills);
          // Issue the copies and fills
          if (!fills.empty())
            issue_fills(uit->first, fills, recorded_events, dst_precondition,
                        dst_mask, trace_info, manage_dst_events,
                        restricted_output, target_events);
          if (!copies.empty())
            issue_copies(uit->first, copies, recorded_events, dst_precondition,
                         dst_mask, trace_info, manage_dst_events,
                         restricted_output, target_events);
        }
        // Check whether later stages can be pipelined with respect to this
        // current stage. The requirement for pipelining is that the target
        // view have the same collective 
        if (!pipelined || 
            (collective_mapping == uit->first->collective_mapping))
          continue;
        // If the collective mapping of the target view does not match the
        // collective mapping of the analysis then we cannot pipeline 
        // the view analysis for issuing of copies
        if ((collective_mapping == NULL) ||
            (uit->first->collective_mapping == NULL) ||
            (*collective_mapping != *(uit->first->collective_mapping)))
          pipelined = false;
      }
      return pipelined;
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::issue_fills(InstanceView *target,
                                         const std::vector<FillUpdate*> &fills,
                                         std::set<RtEvent> &recorded_events,
                                         const ApEvent precondition, 
                                         const FieldMask &fill_mask,
                                         const PhysicalTraceInfo &trace_info,
                                         const bool manage_dst_events,
                                         const bool restricted_output,
                                         std::vector<ApEvent> *dst_events)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!fills.empty());
      assert(!!fill_mask); 
      // Should only have across helper on across copies
      assert((fills[0]->across_helper == NULL) || !manage_dst_events);
      assert((dst_events == NULL) || track_events);
#endif
      const IndexSpaceID match_space = analysis->get_collective_match_space();
      if (fills.size() == 1)
      {
        FillUpdate *update = fills[0];
#ifdef DEBUG_LEGION
#ifndef NDEBUG
        // Should cover all the fields
        if (fills[0]->across_helper != NULL)
        {
          const FieldMask src_mask =
            fills[0]->across_helper->convert_dst_to_src(fill_mask);
          assert(!(src_mask - update->src_mask));
        }
        else
        {
          assert(!(fill_mask - update->src_mask));
        }
#endif
#endif
        IndexSpaceExpression *fill_expr = update->expr;
        FillView *fill_view = update->source;
        const ApEvent result = target->fill_from(fill_view,
                                                 precondition,
                                                 update->fill_guard, fill_expr,
                                                 analysis->op, dst_index,
                                                 match_space,
                                                 fill_mask, trace_info,
                                                 recorded_events, effects,
                                                 fills[0]->across_helper,
                                                 manage_dst_events,
                                                 restricted_output,
                                                 track_events);
        if (result.exists())
        {
          if (track_events)
            events.push_back(result);
          if (dst_events != NULL)
            dst_events->push_back(result);
        }
      }
      else
      {
#ifdef DEBUG_LEGION
#ifndef NDEBUG
        FieldMask src_mask;
        if (fills[0]->across_helper != NULL)
          src_mask = fills[0]->across_helper->convert_dst_to_src(fill_mask);
        else
          src_mask = fill_mask;
        // These should all have had the same across helper
        for (unsigned idx = 1; idx < fills.size(); idx++)
          assert(fills[idx]->across_helper == fills[0]->across_helper);
#endif
#endif
        // Fills can have different predicates because of nested predicated fills
        // so we can only merge across fills with the same predicates
        std::map<std::pair<FillView*,PredEvent>,
                 std::set<IndexSpaceExpression*> > exprs;
        for (std::vector<FillUpdate*>::const_iterator it = 
              fills.begin(); it != fills.end(); it++)
        {
#ifdef DEBUG_LEGION
          // Should cover all the fields
          assert(!(src_mask - (*it)->src_mask));
          // Should also have the same across helper as the first one
          assert(fills[0]->across_helper == (*it)->across_helper);
#endif
          std::pair<FillView*,PredEvent> key((*it)->source, (*it)->fill_guard);
          exprs[key].insert((*it)->expr);
        }
        for (std::map<std::pair<FillView*,PredEvent>,
                      std::set<IndexSpaceExpression*> >::const_iterator it = 
              exprs.begin(); it != exprs.end(); it++)
        {
          IndexSpaceExpression *fill_expr = (it->second.size() == 1) ?
            *(it->second.begin()) : forest->union_index_spaces(it->second);
          // See if we have any work to do for tracing
          const ApEvent result = target->fill_from(it->first.first,
                                                   precondition,
                                                   it->first.second, fill_expr,
                                                   analysis->op, dst_index,
                                                   match_space,
                                                   fill_mask, trace_info,
                                                   recorded_events, effects, 
                                                   fills[0]->across_helper,
                                                   manage_dst_events,
                                                   restricted_output,
                                                   track_events);
          if (result.exists())
          {
            if (track_events)
              events.push_back(result);
            if (dst_events != NULL)
              dst_events->push_back(result);
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void CopyFillAggregator::issue_copies(InstanceView *target, 
               std::map<InstanceView*,std::vector<CopyUpdate*> > &copies,
               std::set<RtEvent> &recorded_events,
               const ApEvent precondition, const FieldMask &copy_mask,
               const PhysicalTraceInfo &trace_info,
               const bool manage_dst_events, const bool restricted_output,
               std::vector<ApEvent> *dst_events)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!copies.empty());
      assert(!!copy_mask);
      assert((src_index == dst_index) || !manage_dst_events);
      assert((dst_events == NULL) || track_events);
#endif
      const IndexSpaceID match_space = analysis->get_collective_match_space();
      // We'll also look for an interesting optimization case here 
      // that was identified by cuNumeric tensor contractions, in 
      // some cases we'll have a group of individual views in the
      // source that are all going to be copied into the same 
      // collective destination view. In this case, what we can 
      // copy all the sources to one instance in the collective
      // view and then we can fuse the broadcast result.
      if (target->is_collective_view())
      {
        std::map<IndividualView*,std::vector<CopyUpdate*> > fused_gather_copies;
        for (std::map<InstanceView*,std::vector<CopyUpdate*> >::iterator
              cit = copies.begin(); cit != copies.end(); /*nothing*/)
        {
          if (!cit->first->is_individual_view())
          {
            cit++;
            continue;
          }
          IndividualView *source = cit->first->as_individual_view();
          for (std::vector<CopyUpdate*>::iterator it =
                cit->second.begin(); it != cit->second.end(); /*nothing*/)
          {
#ifdef DEBUG_LEGION
            assert((*it)->across_helper == NULL);
            assert(!(copy_mask - (*it)->src_mask));
#endif
            if ((*it)->redop == 0)
            {
              fused_gather_copies[source].push_back(*it);
              it = cit->second.erase(it);
            }
            else
              it++;
          }
          if (cit->second.empty())
          {
            std::map<InstanceView*,std::vector<CopyUpdate*> >::iterator
              to_delete = cit++;
            copies.erase(to_delete);
          }
          else
            cit++;
        }
        if (fused_gather_copies.size() > 1)
        {
#ifdef DEBUG_LEGION
          assert(manage_dst_events);
#endif
          CollectiveView *target_collective = target->as_collective_view();
          std::map<IndividualView*,IndexSpaceExpression*> view_exprs;
          for (std::map<IndividualView*,std::vector<CopyUpdate*> >::iterator
                cit = fused_gather_copies.begin(); 
                cit != fused_gather_copies.end(); cit++)
          {
            if (cit->second.size() > 1)
            {
              std::set<IndexSpaceExpression*> union_exprs;
              for (std::vector<CopyUpdate*>::const_iterator it =
                    cit->second.begin(); it != cit->second.end(); it++)
                union_exprs.insert((*it)->expr);
              view_exprs[cit->first] = forest->union_index_spaces(union_exprs); 
            }
            else
              view_exprs[cit->first] = (*(cit->second.begin()))->expr;
          }
          const ApEvent result = target_collective->collective_fuse_gather(
              view_exprs, precondition, predicate_guard, analysis->op,
              dst_index, match_space, copy_mask, trace_info, recorded_events,
              effects, restricted_output, track_events);
          if (result.exists())
          {
            if (track_events)
              events.push_back(result);
            if (dst_events != NULL)
              dst_events->push_back(result);
          }
        }
        else if (!fused_gather_copies.empty())
        {
          // Only one view so we can put them back onto the original set
          // of copies since we're just going to perform a normal fusion
          std::map<IndividualView*,std::vector<CopyUpdate*> >::iterator next =
            fused_gather_copies.begin();
          std::vector<CopyUpdate*> &original = copies[next->first];
          if (original.empty())
            original.swap(next->second);
          else
            original.insert(original.end(), next->second.begin(), next->second.end());
        }
      }
      for (std::map<InstanceView*,std::vector<CopyUpdate*> >::const_iterator
            cit = copies.begin(); cit != copies.end(); cit++)
      {
#ifdef DEBUG_LEGION
        assert(!cit->second.empty());
        // Should only have across helpers for across copies
        assert((cit->second[0]->across_helper == NULL) || !manage_dst_events);
#endif
        if (cit->second.size() == 1)
        {
          // Easy case of a single update copy
          CopyUpdate *update = cit->second[0];
#ifdef DEBUG_LEGION
#ifndef NDEBUG
          if (cit->second[0]->across_helper != NULL)
          {
            const FieldMask src_mask =
              cit->second[0]->across_helper->convert_dst_to_src(copy_mask);
            assert(!(src_mask - update->src_mask));
          }
          else
          {
            // Should cover all the fields
            assert(!(copy_mask - update->src_mask));
          }
#endif
#endif
          InstanceView *source = update->source;
          IndexSpaceExpression *copy_expr = update->expr;
          const ApEvent result = target->copy_from(source, precondition,
                                    predicate_guard, update->redop, copy_expr,
                                    analysis->op, manage_dst_events ? dst_index
                                      : src_index, match_space, copy_mask,
                                    update->src_man, trace_info, 
                                    recorded_events, effects, 
                                    cit->second[0]->across_helper, 
                                    manage_dst_events, restricted_output,
                                    track_events);
          if (result.exists())
          {
            if (track_events)
              events.push_back(result);
            if (dst_events != NULL)
              dst_events->push_back(result);
          }
        }
        else
        {
#ifdef DEBUG_LEGION
#ifndef NDEBUG
          FieldMask src_mask;
          if (cit->second[0]->across_helper != NULL)
            src_mask = 
              cit->second[0]->across_helper->convert_dst_to_src(copy_mask);
          else
            src_mask = copy_mask;
#endif
#endif
          // Have to group by source instances in order to merge together
          // different index space expressions for the same copy
          // For collective instances we also need to group by the source
          // point of the collective instance to use
          std::map<std::pair<InstanceView*,PhysicalManager*>,
                   std::set<IndexSpaceExpression*> > fused_exprs;
          const ReductionOpID redop = cit->second[0]->redop;
          std::map<IndividualView*,IndexSpaceExpression*> collective_gather;
          for (std::vector<CopyUpdate*>::const_iterator it = 
                cit->second.begin(); it != cit->second.end(); it++)
          {
#ifdef DEBUG_LEGION
            // Should cover all the fields
            assert(!(src_mask - (*it)->src_mask));
            // Should have the same redop
            assert(redop == (*it)->redop);
            // Should also have the same across helper as the first one
            assert(cit->second[0]->across_helper == (*it)->across_helper);
#endif
            const std::pair<InstanceView*,PhysicalManager*>
              key((*it)->source, (*it)->src_man);
            fused_exprs[key].insert((*it)->expr);
          }
          for (std::map<std::pair<InstanceView*,PhysicalManager*>,
                        std::set<IndexSpaceExpression*> >::const_iterator it =
               fused_exprs.begin(); it != fused_exprs.end(); it++)
          {
            IndexSpaceExpression *copy_expr = (it->second.size() == 1) ?
                *(it->second.begin()) : forest->union_index_spaces(it->second);
            const ApEvent result = target->copy_from(it->first.first, 
                                    precondition, predicate_guard, redop,
                                    copy_expr, analysis->op, manage_dst_events ?
                                      dst_index : src_index, match_space,
                                    copy_mask, it->first.second, trace_info, 
                                    recorded_events, effects,
                                    cit->second[0]->across_helper,
                                    manage_dst_events, restricted_output,
                                    track_events);
            if (result.exists())
            {
              if (track_events)
                events.push_back(result);
              if (dst_events != NULL)
                dst_events->push_back(result);
            }
          }
        }
      }
    } 

    //--------------------------------------------------------------------------
    /*static*/ void CopyFillAggregator::handle_aggregation(const void *args)
    //--------------------------------------------------------------------------
    {
      const CopyFillAggregation *cfargs = (const CopyFillAggregation*)args;
      cfargs->aggregator->issue_updates(*cfargs, cfargs->pre, 
          cfargs->restricted_output, cfargs->manage_dst_events,
          cfargs->dst_events, cfargs->stage);
      cfargs->remove_recorder_reference();
      if (cfargs->dst_events != NULL)
        delete cfargs->dst_events;
    } 

    /////////////////////////////////////////////////////////////
    // Physical Analysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PhysicalAnalysis::PhysicalAnalysis(Runtime *rt, Operation *o, unsigned idx, 
                                      IndexSpaceExpression *e, bool h, bool im,
                                      bool ex, CollectiveMapping *m, bool first)
      : previous(rt->address_space), original_source(rt->address_space),
        runtime(rt), analysis_expr(e), op(o), index(idx),
        owns_op(false), on_heap(h), exclusive(ex), immutable(im),
        collective_first_local(first), parallel_traversals(false),
        restricted(false), recorded_instances(NULL), collective_mapping(m)
    //--------------------------------------------------------------------------
    {
      analysis_expr->add_base_expression_reference(PHYSICAL_ANALYSIS_REF);
      if (collective_mapping != NULL)
        collective_mapping->add_reference();
    }

    //--------------------------------------------------------------------------
    PhysicalAnalysis::PhysicalAnalysis(Runtime *rt, AddressSpaceID source, 
                               AddressSpaceID prev, Operation *o, unsigned idx,
                               IndexSpaceExpression *e, bool h, bool im,
                               CollectiveMapping *mapping, bool ex, bool first)
      : previous(prev), original_source(source), runtime(rt), analysis_expr(e),
        op(o), index(idx), owns_op(true), on_heap(h), exclusive(ex),
        immutable(im), collective_first_local(first), 
        parallel_traversals(false), restricted(false), recorded_instances(NULL),
        collective_mapping(mapping)
    //--------------------------------------------------------------------------
    {
      analysis_expr->add_base_expression_reference(PHYSICAL_ANALYSIS_REF);
      if (collective_mapping != NULL)
        collective_mapping->add_reference();
    }

    //--------------------------------------------------------------------------
    PhysicalAnalysis::~PhysicalAnalysis(void)
    //--------------------------------------------------------------------------
    {
      if (deferred_applied_event.exists())
      {
        if (!deferred_applied_events.empty())
        {
          Runtime::trigger_event(deferred_applied_event,
              Runtime::merge_events(deferred_applied_events));
#ifdef DEBUG_LEGION
          deferred_applied_events.clear();
#endif
        }
        else
          Runtime::trigger_event(deferred_applied_event);
      }
#ifdef DEBUG_LEGION
      assert(deferred_applied_events.empty());
#endif
      if (analysis_expr->remove_base_expression_reference(
                                    PHYSICAL_ANALYSIS_REF))
        delete analysis_expr;
      if ((collective_mapping != NULL) && 
          collective_mapping->remove_reference())
        delete collective_mapping;
      if (recorded_instances != NULL)
        delete recorded_instances;
      if (owns_op && (op != NULL))
        delete op;
    } 

    //--------------------------------------------------------------------------
    void PhysicalAnalysis::analyze(EquivalenceSet *set,
                                   const FieldMask &mask,
                                   std::set<RtEvent> &deferral_events,
                                   std::set<RtEvent> &applied_events,
                                   RtEvent precondition/*= NO_EVENT*/,
                                   const bool already_deferred /* = false*/)
    //--------------------------------------------------------------------------
    {
      if (!precondition.exists() || precondition.has_triggered())
      {
#ifdef LEGION_SPY_EQUIVALENCE_SETS
        LegionSpy::log_equivalence_set_use(set->did, 
            op->get_unique_op_id(), index);
#endif
        if (set->set_expr == analysis_expr)
          set->analyze(*this, analysis_expr, true/*covers*/, mask,
                       deferral_events, applied_events, already_deferred);
        else if (!set->set_expr->is_empty())
        {
          IndexSpaceExpression *expr = 
           runtime->forest->intersect_index_spaces(set->set_expr,analysis_expr);
          if (expr->is_empty())
            return;
          // Check to see this expression covers the equivalence set
          // If it does then we can use original set expression
          if (expr->get_volume() == set->set_expr->get_volume())
            set->analyze(*this, set->set_expr, true/*covers*/, mask,
                         deferral_events, applied_events, already_deferred);
          else
            set->analyze(*this, expr, false/*covers*/, mask,
                         deferral_events, applied_events, already_deferred);
        }
        else
          set->analyze(*this, set->set_expr, true/*covers*/, mask,
                       deferral_events, applied_events, already_deferred);
      }
      else
        // This has to be the first time through and isn't really
        // a deferral of an the traversal since we haven't even
        // started the traversal yet
        defer_analysis(precondition, set, mask, deferral_events,
            applied_events, RtUserEvent::NO_RT_USER_EVENT, already_deferred);
    }

    //--------------------------------------------------------------------------
    RtEvent PhysicalAnalysis::defer_traversal(RtEvent precondition,
                                              const VersionInfo &version_info,
                                              std::set<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      // Check to see if we have a deferred applied event yet
      if (!deferred_applied_event.exists())
      {
        deferred_applied_event = Runtime::create_rt_user_event();
        applied_events.insert(deferred_applied_event);
      }
      const DeferPerformTraversalArgs args(this, version_info);
      runtime->issue_runtime_meta_task(args,
          LG_THROUGHPUT_DEFERRED_PRIORITY, precondition);
      return args.done_event;
    }

    //--------------------------------------------------------------------------
    void PhysicalAnalysis::defer_analysis(RtEvent precondition,
                                          EquivalenceSet *set,
                                          const FieldMask &mask,
                                          std::set<RtEvent> &deferral_events,
                                          std::set<RtEvent> &applied_events,
                                          RtUserEvent deferral_event,
                                          const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // Check to see if we have a deferred applied event yet
      if (!deferred_applied_event.exists())
      {
        deferred_applied_event = Runtime::create_rt_user_event();
        applied_events.insert(deferred_applied_event);
      }
      const DeferPerformAnalysisArgs args(this, set, mask, 
                                          deferral_event, already_deferred);
      runtime->issue_runtime_meta_task(args, 
          LG_THROUGHPUT_DEFERRED_PRIORITY, precondition);
      deferral_events.insert(args.done_event);
    }

    //--------------------------------------------------------------------------
    RtEvent PhysicalAnalysis::defer_remote(RtEvent precondition,
                                           std::set<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      // Check to see if we have a deferred applied event yet
      if (!deferred_applied_event.exists())
      {
        deferred_applied_event = Runtime::create_rt_user_event();
        applied_events.insert(deferred_applied_event);
      }
      const DeferPerformRemoteArgs args(this);
      runtime->issue_runtime_meta_task(args,
          LG_THROUGHPUT_DEFERRED_PRIORITY, precondition);
      return args.done_event;
    }

    //--------------------------------------------------------------------------
    RtEvent PhysicalAnalysis::defer_updates(RtEvent precondition,
                                            std::set<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      // Check to see if we have a deferred applied event yet
      if (!deferred_applied_event.exists())
      {
        deferred_applied_event = Runtime::create_rt_user_event();
        applied_events.insert(deferred_applied_event);
      }
      const DeferPerformUpdateArgs args(this);
      runtime->issue_runtime_meta_task(args,
          LG_THROUGHPUT_DEFERRED_PRIORITY, precondition);
      return args.done_event;
    }

    //--------------------------------------------------------------------------
    RtEvent PhysicalAnalysis::defer_registration(RtEvent precondition,
                                const RegionUsage &usage,
                                std::set<RtEvent> &applied_events,
                                const PhysicalTraceInfo &trace_info,
                                ApEvent init_precondition, ApEvent termination,
                                ApEvent &instances_ready, bool symbolic)
    //--------------------------------------------------------------------------
    {
      // Check to see if we have a deferred applied event yet
      if (!deferred_applied_event.exists())
      {
        deferred_applied_event = Runtime::create_rt_user_event();
        applied_events.insert(deferred_applied_event);
      }
      const DeferPerformRegistrationArgs args(this, usage, trace_info,
                            init_precondition, termination, symbolic);
      runtime->issue_runtime_meta_task(args,
          LG_THROUGHPUT_DEFERRED_PRIORITY, precondition);
      instances_ready = args.instances_ready;
      return args.done_event;
    }

    //--------------------------------------------------------------------------
    ApEvent PhysicalAnalysis::defer_output(RtEvent precondition,
                                           const PhysicalTraceInfo &trace_info,
                                           bool track_effects,
                                           std::set<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      // Check to see if we have a deferred applied event yet
      if (!deferred_applied_event.exists())
      {
        deferred_applied_event = Runtime::create_rt_user_event();
        applied_events.insert(deferred_applied_event);
      }
      const DeferPerformOutputArgs args(this, track_effects, trace_info);
      runtime->issue_runtime_meta_task(args,
          LG_THROUGHPUT_DEFERRED_PRIORITY, precondition);
      return args.effects_event;
    }

    //--------------------------------------------------------------------------
    void PhysicalAnalysis::record_deferred_applied_events(
                                                     std::set<RtEvent> &applied)
    //--------------------------------------------------------------------------
    {
      AutoLock p_lock(*this);
      if (!deferred_applied_events.empty())
        deferred_applied_events.insert(applied.begin(), applied.end());
      else
        deferred_applied_events.swap(applied);
    }

    //--------------------------------------------------------------------------
    RtEvent PhysicalAnalysis::perform_traversal(RtEvent precondition,
                     const VersionInfo &info, std::set<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      if (precondition.exists() && !precondition.has_triggered())
        return defer_traversal(precondition, info, applied_events);
      std::set<RtEvent> deferral_events;
      const FieldMaskSet<EquivalenceSet> &eq_sets = info.get_equivalence_sets();
      for (FieldMaskSet<EquivalenceSet>::const_iterator it =
            eq_sets.begin(); it != eq_sets.end(); it++)
        analyze(it->first, it->second, deferral_events, applied_events);
      if (!deferral_events.empty())
        return Runtime::merge_events(deferral_events);
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    RtEvent PhysicalAnalysis::perform_remote(RtEvent precondition,
                                             std::set<RtEvent> &applied_events,
                                             const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // only called by derived classes
      assert(false);
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    RtEvent PhysicalAnalysis::perform_updates(RtEvent precondition,
                                              std::set<RtEvent> &applied_events,
                                              const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // only called by derived classes
      assert(false);
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    RtEvent PhysicalAnalysis::perform_registration(RtEvent precondition,
                                const RegionUsage &usage,
                                std::set<RtEvent> &applied_events,
                                ApEvent init_precondition, ApEvent termination,
                                ApEvent &instances_ready, bool symbolic)
    //--------------------------------------------------------------------------
    {
      // only called by derived classes
      assert(false);
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    ApEvent PhysicalAnalysis::perform_output(RtEvent precondition,
                                             std::set<RtEvent> &applied_events,
                                             const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // only called by derived classes
      assert(false);
      return ApEvent::NO_AP_EVENT;
    }

    //--------------------------------------------------------------------------
    void PhysicalAnalysis::process_remote_instances(Deserializer &derez,
                                                std::set<RtEvent> &ready_events)
    //--------------------------------------------------------------------------
    {
      size_t num_views;
      derez.deserialize(num_views);
      AutoLock a_lock(*this);
      if (recorded_instances == NULL)
        recorded_instances = new FieldMaskSet<LogicalView>();
      for (unsigned idx = 0; idx < num_views; idx++)
      {
        DistributedID view_did;
        derez.deserialize(view_did);
        RtEvent ready;  
        LogicalView *view = 
          runtime->find_or_request_logical_view(view_did, ready);
        if (ready.exists())
          ready_events.insert(ready);
        FieldMask mask;
        derez.deserialize(mask);
        recorded_instances->insert(view, mask);
      }
      bool remote_restrict;
      derez.deserialize(remote_restrict);
      if (remote_restrict)
        restricted = true;
    }

    //--------------------------------------------------------------------------
    void PhysicalAnalysis::process_local_instances(
            const FieldMaskSet<LogicalView> &views, const bool local_restricted)
    //--------------------------------------------------------------------------
    {
      AutoLock a_lock(*this);
      if (recorded_instances == NULL)
        recorded_instances = new FieldMaskSet<LogicalView>();
      for (FieldMaskSet<LogicalView>::const_iterator it = 
            views.begin(); it != views.end(); it++)
        if (it->first->is_instance_view())
          recorded_instances->insert(it->first, it->second);
      if (local_restricted)
        restricted = true;
    }

    //--------------------------------------------------------------------------
    void PhysicalAnalysis::filter_remote_expressions(
                                      FieldMaskSet<IndexSpaceExpression> &exprs)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!remote_sets.empty());
#endif
      FieldMaskSet<IndexSpaceExpression> remote_exprs; 
      for (LegionMap<AddressSpaceID,FieldMaskSet<EquivalenceSet> >::
            const_iterator rit = remote_sets.begin(); 
            rit != remote_sets.end(); rit++)
        for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
              rit->second.begin(); it != rit->second.end(); it++)
          remote_exprs.insert(it->first->set_expr, it->second);
      FieldMaskSet<IndexSpaceExpression> to_add;
      std::vector<IndexSpaceExpression*> to_remove;
      if (remote_exprs.size() > 1)
      {
        LegionList<FieldSet<IndexSpaceExpression*> > field_sets;
        remote_exprs.compute_field_sets(FieldMask(), field_sets);
        for (LegionList<FieldSet<IndexSpaceExpression*> >::const_iterator
              fit = field_sets.begin(); fit != field_sets.end(); fit++)
        {
          IndexSpaceExpression *remote_expr = (fit->elements.size() == 1) ?
            *(fit->elements.begin()) : 
            runtime->forest->union_index_spaces(fit->elements);
          for (FieldMaskSet<IndexSpaceExpression>::iterator it = 
                exprs.begin(); it != exprs.end(); it++)
          {
            const FieldMask overlap = it->second & fit->set_mask;
            if (!overlap)
              continue;
            IndexSpaceExpression *diff = 
              runtime->forest->subtract_index_spaces(it->first, remote_expr);
            if (!diff->is_empty())
              to_add.insert(diff, overlap);
            it.filter(overlap);
            if (!it->second)
              to_remove.push_back(it->first);
          }
        }
      }
      else
      {
        FieldMaskSet<IndexSpaceExpression>::const_iterator first = 
          remote_exprs.begin();
        
        for (FieldMaskSet<IndexSpaceExpression>::iterator it = 
              exprs.begin(); it != exprs.end(); it++)
        {
          const FieldMask overlap = it->second & first->second;
          if (!overlap)
            continue;
          IndexSpaceExpression *diff = 
            runtime->forest->subtract_index_spaces(it->first, first->first);
          if (!diff->is_empty())
            to_add.insert(diff, overlap);
          it.filter(overlap);
          if (!it->second)
            to_remove.push_back(it->first);
        }
      }
      if (!to_remove.empty())
      {
        for (std::vector<IndexSpaceExpression*>::const_iterator it = 
              to_remove.begin(); it != to_remove.end(); it++)
          exprs.erase(*it);
      }
      if (!to_add.empty())
      {
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it = 
              to_add.begin(); it != to_add.end(); it++)
          exprs.insert(it->first, it->second);
      }
    }

    //--------------------------------------------------------------------------
    bool PhysicalAnalysis::report_instances(FieldMaskSet<LogicalView> &insts)
    //--------------------------------------------------------------------------
    {
      // No need for the lock since we shouldn't be mutating anything at 
      // this point anyway
      if (recorded_instances != NULL)
        recorded_instances->swap(insts);
      return restricted;
    }

    //--------------------------------------------------------------------------
    void PhysicalAnalysis::record_remote(EquivalenceSet *set, 
                                         const FieldMask &mask,
                                         const AddressSpaceID owner)
    //--------------------------------------------------------------------------
    {
      if (parallel_traversals)
      {
        AutoLock a_lock(*this);
        remote_sets[owner].insert(set, mask);
      }
      else
        // No lock needed if we're the only one
        remote_sets[owner].insert(set, mask);
    }

    //--------------------------------------------------------------------------
    void PhysicalAnalysis::record_instance(LogicalView *view, 
                                           const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      // Lock held from caller
      if (recorded_instances == NULL)
        recorded_instances = new FieldMaskSet<LogicalView>();
      recorded_instances->insert(view, mask);
    }

    //--------------------------------------------------------------------------
    /*static*/ void PhysicalAnalysis::handle_remote_instances(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      PhysicalAnalysis *target;
      derez.deserialize(target);
      RtUserEvent done_event;
      derez.deserialize(done_event);
      std::set<RtEvent> ready_events;
      target->process_remote_instances(derez, ready_events);
      if (!ready_events.empty())
        Runtime::trigger_event(done_event, Runtime::merge_events(ready_events));
      else
        Runtime::trigger_event(done_event); 
    }

    //--------------------------------------------------------------------------
    PhysicalAnalysis::DeferPerformTraversalArgs::DeferPerformTraversalArgs(
        PhysicalAnalysis *ana, const VersionInfo &info)
      : LgTaskArgs<DeferPerformTraversalArgs>(ana->op->get_unique_op_id()),
        analysis(ana), version_info(&info), 
        done_event(Runtime::create_rt_user_event())
    //--------------------------------------------------------------------------
    {
      if (analysis->on_heap)
        analysis->add_reference();
    }

    //--------------------------------------------------------------------------
    /*static*/void PhysicalAnalysis::handle_deferred_traversal(const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferPerformTraversalArgs *dargs =
        (const DeferPerformTraversalArgs*)args;
      std::set<RtEvent> applied_events;
      Runtime::trigger_event(dargs->done_event,
          dargs->analysis->perform_traversal(RtEvent::NO_RT_EVENT,
            *(dargs->version_info), applied_events));
      if (!applied_events.empty())
        dargs->analysis->record_deferred_applied_events(applied_events);
      if (dargs->analysis->on_heap && dargs->analysis->remove_reference())
        delete dargs->analysis;
    }

    //--------------------------------------------------------------------------
    PhysicalAnalysis::DeferPerformAnalysisArgs::DeferPerformAnalysisArgs(
        PhysicalAnalysis *ana, EquivalenceSet *s, const FieldMask &m, 
        RtUserEvent done, bool def)
      : LgTaskArgs<DeferPerformAnalysisArgs>(ana->op->get_unique_op_id()),
        analysis(ana), set(s), mask(new FieldMask(m)), 
        done_event(done.exists() ? done : Runtime::create_rt_user_event()), 
        already_deferred(def)
    //--------------------------------------------------------------------------
    {
      analysis->record_parallel_traversals();
      if (analysis->on_heap)
        analysis->add_reference();
    }

    //--------------------------------------------------------------------------
    /*static*/ void PhysicalAnalysis::handle_deferred_analysis(const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferPerformAnalysisArgs *dargs = 
        (const DeferPerformAnalysisArgs*)args;
      // Get this before doing anything
      const bool on_heap = dargs->analysis->on_heap;
      std::set<RtEvent> deferral_events, applied_events;
      dargs->analysis->analyze(dargs->set, *(dargs->mask), 
          deferral_events, applied_events, RtEvent::NO_RT_EVENT, 
          dargs->already_deferred);
      if (!deferral_events.empty())
        Runtime::trigger_event(dargs->done_event,
            Runtime::merge_events(deferral_events));
      else
        Runtime::trigger_event(dargs->done_event);
      if (!applied_events.empty())
        dargs->analysis->record_deferred_applied_events(applied_events);
      if (on_heap && dargs->analysis->remove_reference())
        delete dargs->analysis;
      delete dargs->mask;
    }

    //--------------------------------------------------------------------------
    PhysicalAnalysis::DeferPerformRemoteArgs::DeferPerformRemoteArgs(
                                                          PhysicalAnalysis *ana)
      : LgTaskArgs<DeferPerformRemoteArgs>(ana->op->get_unique_op_id()), 
        analysis(ana), done_event(Runtime::create_rt_user_event())
    //--------------------------------------------------------------------------
    {
      if (analysis->on_heap)
        analysis->add_reference();
    }

    //--------------------------------------------------------------------------
    /*static*/ void PhysicalAnalysis::handle_deferred_remote(const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferPerformRemoteArgs *dargs = (const DeferPerformRemoteArgs*)args;
      std::set<RtEvent> applied_events;
      // Get this before doing anything
      const bool on_heap = dargs->analysis->on_heap;
      const RtEvent done = dargs->analysis->perform_remote(RtEvent::NO_RT_EVENT,
                                      applied_events, true/*already deferred*/);
      Runtime::trigger_event(dargs->done_event, done);
      if (!applied_events.empty())
        dargs->analysis->record_deferred_applied_events(applied_events);
      if (on_heap && dargs->analysis->remove_reference())
        delete dargs->analysis;
    }

    //--------------------------------------------------------------------------
    PhysicalAnalysis::DeferPerformUpdateArgs::DeferPerformUpdateArgs(
                                                          PhysicalAnalysis *ana)
      : LgTaskArgs<DeferPerformUpdateArgs>(ana->op->get_unique_op_id()), 
        analysis(ana), done_event(Runtime::create_rt_user_event())
    //--------------------------------------------------------------------------
    {
      if (analysis->on_heap)
        analysis->add_reference();
    }

    //--------------------------------------------------------------------------
    /*static*/ void PhysicalAnalysis::handle_deferred_update(const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferPerformUpdateArgs *dargs = (const DeferPerformUpdateArgs*)args;
      std::set<RtEvent> applied_events;
      // Get this before doing anything
      const bool on_heap = dargs->analysis->on_heap;
      const RtEvent done =dargs->analysis->perform_updates(RtEvent::NO_RT_EVENT,
                                      applied_events, true/*already deferred*/); 
      Runtime::trigger_event(dargs->done_event, done);
      if (!applied_events.empty())
        dargs->analysis->record_deferred_applied_events(applied_events);
      if (on_heap && dargs->analysis->remove_reference())
        delete dargs->analysis;
    }

    //--------------------------------------------------------------------------
    PhysicalAnalysis::DeferPerformRegistrationArgs::
      DeferPerformRegistrationArgs(PhysicalAnalysis *ana, 
          const RegionUsage &use, const PhysicalTraceInfo &info,
          ApEvent pre, ApEvent term, bool symb)
      : LgTaskArgs<DeferPerformRegistrationArgs>(ana->op->get_unique_op_id()),
        analysis(ana), usage(use), trace_info(&info), precondition(pre),
        termination(term),instances_ready(Runtime::create_ap_user_event(&info)),
        done_event(Runtime::create_rt_user_event()), symbolic(symb)
    //--------------------------------------------------------------------------
    {
      if (analysis->on_heap)
        analysis->add_reference();
    }

    //--------------------------------------------------------------------------
    /*static*/ void PhysicalAnalysis::handle_deferred_registration(
                                                               const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferPerformRegistrationArgs *dargs =
        (const DeferPerformRegistrationArgs*)args;
      ApEvent insts_ready;
      std::set<RtEvent> applied_events;
      const RtEvent done = dargs->analysis->perform_registration(
        RtEvent::NO_RT_EVENT, dargs->usage, applied_events,
        dargs->precondition, dargs->termination, insts_ready, dargs->symbolic);
      Runtime::trigger_event(dargs->trace_info,
          dargs->instances_ready, insts_ready);
      Runtime::trigger_event(dargs->done_event, done);
      if (!applied_events.empty())
        dargs->analysis->record_deferred_applied_events(applied_events);
      if (dargs->analysis->on_heap && dargs->analysis->remove_reference())
        delete dargs->analysis;
    }

    //--------------------------------------------------------------------------
    PhysicalAnalysis::DeferPerformOutputArgs::DeferPerformOutputArgs(
               PhysicalAnalysis *ana, bool track, const PhysicalTraceInfo &info)
      : LgTaskArgs<DeferPerformOutputArgs>(ana->op->get_unique_op_id()), 
        analysis(ana), trace_info(&info), effects_event(track ?
          Runtime::create_ap_user_event(&info) : ApUserEvent::NO_AP_USER_EVENT)
    //--------------------------------------------------------------------------
    {
      if (analysis->on_heap)
        analysis->add_reference();
    }

    //--------------------------------------------------------------------------
    /*static*/ void PhysicalAnalysis::handle_deferred_output(const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferPerformOutputArgs *dargs = (const DeferPerformOutputArgs*)args;
      std::set<RtEvent> applied_events;
      const bool on_heap = dargs->analysis->on_heap;
      const ApEvent result =
        dargs->analysis->perform_output(
            RtEvent::NO_RT_EVENT, applied_events, true/*already deferred*/);
      if (dargs->effects_event.exists())
        Runtime::trigger_event(dargs->trace_info, dargs->effects_event, result);
      if (!applied_events.empty())
        dargs->analysis->record_deferred_applied_events(applied_events);
      if (on_heap && dargs->analysis->remove_reference())
        delete dargs->analysis;
    }

    /////////////////////////////////////////////////////////////
    // RegistrationAnalysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RegistrationAnalysis::RegistrationAnalysis(
                               Runtime *rt, Operation *op, unsigned index,
                               RegionNode *node, bool on_heap,
                               const PhysicalTraceInfo &t_info, bool exclusive)
      : PhysicalAnalysis(rt, op, index, node->row_source, on_heap,
                         false/*immutable*/, exclusive), 
        region(node), context_index(op->get_ctx_index()), trace_info(t_info)
    //--------------------------------------------------------------------------
    {
      region->add_base_resource_ref(PHYSICAL_ANALYSIS_REF);
    }

    //--------------------------------------------------------------------------
    RegistrationAnalysis::RegistrationAnalysis(Runtime *rt,
                                AddressSpaceID src, AddressSpaceID prev,
                                Operation *op, unsigned index,
                                RegionNode *node, bool on_heap, 
                                std::vector<PhysicalManager*> &&target_insts,
                                LegionVector<
                                   FieldMaskSet<InstanceView> > &&target_vws,
                                std::vector<IndividualView*> &&source_vws,
                                const PhysicalTraceInfo &t_info,
                                CollectiveMapping *mapping, bool first_local,
                                bool exclusive)
      : PhysicalAnalysis(rt, src, prev, op, index, node->row_source, on_heap, 
                         false/*immutable*/, mapping, exclusive, first_local),
        region(node), context_index(op->get_ctx_index()), trace_info(t_info),
        target_instances(target_insts), target_views(target_vws), 
        source_views(source_vws)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(on_heap);
      assert(target_instances.size() == target_views.size());
#endif
      region->add_base_resource_ref(PHYSICAL_ANALYSIS_REF);
    }

    //--------------------------------------------------------------------------
    RegistrationAnalysis::RegistrationAnalysis(Runtime *rt,
                                AddressSpaceID src, AddressSpaceID prev,
                                Operation *op, unsigned index,
                                RegionNode *node, bool on_heap, 
                                const PhysicalTraceInfo &t_info,
                                CollectiveMapping *mapping, bool first_local,
                                bool exclusive)
      : PhysicalAnalysis(rt, src, prev, op, index, node->row_source, on_heap, 
                         false/*immutable*/, mapping, exclusive, first_local),
        region(node), context_index(op->get_ctx_index()), trace_info(t_info)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(on_heap);
#endif
      region->add_base_resource_ref(PHYSICAL_ANALYSIS_REF);
    }

    //--------------------------------------------------------------------------
    RegistrationAnalysis::~RegistrationAnalysis(void)
    //--------------------------------------------------------------------------
    {
      if (region->remove_base_resource_ref(PHYSICAL_ANALYSIS_REF))
        delete region;
    }

    //--------------------------------------------------------------------------
    RtEvent RegistrationAnalysis::convert_views(LogicalRegion region,
                        const InstanceSet &targets,
                        const std::vector<PhysicalManager*> *sources,
                        const RegionUsage *usage,
                        bool collective_rendezvous, unsigned analysis_index)
    //--------------------------------------------------------------------------
    {
      InnerContext *context = op->find_physical_context(index);
      if ((sources != NULL) && !sources->empty())
        context->convert_individual_views(*sources, source_views);
      target_instances.resize(targets.size());
      for (unsigned idx = 0; idx < targets.size(); idx++)
        target_instances[idx] = targets[idx].get_physical_manager();
      // Find any atomic locks we need to take for these instances
      // Note that for now we also treat exclusive-reductions as
      // needing to be atomic since we don't have a semantics for
      // what exclusive reductions mean today
      // Note this needs to be done eagerly and cannot be deferred!
      // All reductions need to get an atomic lock since they can race
      // with copy reductions to the same instance as the task
      if ((usage != NULL) && (IS_ATOMIC(*usage) || IS_REDUCE(*usage)))
      {
        std::vector<IndividualView*> individual_views;
        context->convert_individual_views(target_instances, individual_views);
        // If we're doing a reduction, we need exclusive coherence for any
        // exclusive or atomic coherence, otherwise non-exclusive is fine 
        // since that will still prevent races with reduction copies
        const bool exclusive = IS_REDUCE(*usage) ?
          (IS_EXCLUSIVE(*usage) || IS_ATOMIC(*usage)) : HAS_WRITE(*usage);
        for (unsigned idx = 0; idx < individual_views.size(); idx++)
          individual_views[idx]->find_atomic_reservations(
              targets[idx].get_valid_fields(), op, index, exclusive);
      }
      if (collective_rendezvous)
        return op->convert_collective_views(index, analysis_index,
                                  region, targets, context, collective_mapping,
                                  collective_first_local, target_views,
                                  collective_arrivals);
      else if (op->perform_collective_analysis(collective_mapping,
                                               collective_first_local))
      {
        if (collective_mapping != NULL)
        {
          std::vector<IndividualView*> views(targets.size());
          context->convert_individual_views(targets, views, collective_mapping);
          target_views.resize(views.size());
          for (unsigned idx = 0; idx < views.size(); idx++)
            target_views[idx].insert(views[idx],
                targets[idx].get_valid_fields());
        }
        else
          return op->convert_collective_views(index, analysis_index,
                                  region, targets, context, collective_mapping,
                                  collective_first_local, target_views,
                                  collective_arrivals);
      }
      else
        context->convert_analysis_views(targets, target_views);
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    RtEvent RegistrationAnalysis::perform_registration(RtEvent precondition,
                                const RegionUsage &usage,
                                std::set<RtEvent> &applied_events,
                                ApEvent init_precondition, ApEvent termination,
                                ApEvent &instances_ready, bool symbolic)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(region != NULL);
      assert(termination.exists());
#endif
      if (precondition.exists() && !precondition.has_triggered())
        return defer_registration(precondition, usage, applied_events,
         trace_info, init_precondition, termination, instances_ready, symbolic);
      // Perform the registration
      const UniqueID op_id = op->get_unique_op_id();
      const size_t op_ctx_index = op->get_ctx_index();
      const AddressSpaceID local_space = runtime->address_space;
      IndexSpaceNode *expr_node = region->row_source;
#ifdef DEBUG_LEGION
      assert(expr_node == analysis_expr);
#endif
      std::vector<RtEvent> registered_events;
      std::vector<ApEvent> inst_ready_events;
      const IndexSpaceID match_space = get_collective_match_space();
      for (unsigned idx = 0; idx < target_views.size(); idx++)
      {
        for (FieldMaskSet<InstanceView>::const_iterator it =
              target_views[idx].begin(); it != target_views[idx].end(); it++)
        {
          size_t view_collective_arrivals = 0;
          if (!collective_arrivals.empty())
          {
            std::map<InstanceView*,size_t>::const_iterator finder =
              collective_arrivals.find(it->first);
#ifdef DEBUG_LEGION
            assert(finder != collective_arrivals.end()); 
#endif
            view_collective_arrivals = finder->second;
          }
          const ApEvent ready = it->first->register_user(usage, it->second,
              expr_node, op_id, op_ctx_index, index, match_space, termination,
              target_instances[idx], collective_mapping,
              view_collective_arrivals, registered_events, applied_events,
              trace_info, local_space, symbolic);
          if (ready.exists())
            inst_ready_events.push_back(ready);
        }
      }
      if (!inst_ready_events.empty())
      {
        if (init_precondition.exists())
          inst_ready_events.push_back(init_precondition);
        instances_ready = 
          Runtime::merge_events(&trace_info, inst_ready_events);
      }
      else
        instances_ready = init_precondition;
      if (trace_info.recording)
      {
        InnerContext *context = op->find_physical_context(index);
        std::vector<IndividualView*> individual_views;
        context->convert_individual_views(target_instances, individual_views);
        for (unsigned idx = 0; idx < individual_views.size(); idx++)
        {
          const UniqueInst unique_inst(individual_views[idx]);
          trace_info.record_op_inst(usage, target_views[idx].get_valid_mask(),
                                    unique_inst, region, op, applied_events);
        }
      }
      if (!registered_events.empty())
        return Runtime::merge_events(registered_events);
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    IndexSpaceID RegistrationAnalysis::get_collective_match_space(void) const
    //--------------------------------------------------------------------------
    {
      return region->row_source->handle.get_id();
    }

    /////////////////////////////////////////////////////////////
    // RemoteCollectiveAnalysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    void CollectiveAnalysis::pack_collective_analysis(
       Serializer &rez, AddressSpaceID target, std::set<RtEvent> &applied) const
    //--------------------------------------------------------------------------
    {
      rez.serialize(get_context_index());
      rez.serialize(get_requirement_index());
      rez.serialize(get_match_space());
      Operation *op = get_operation();
      op->pack_remote_operation(rez, target, applied);
      const PhysicalTraceInfo &trace_info = get_trace_info();
      trace_info.pack_trace_info(rez, applied);
    }

    //--------------------------------------------------------------------------
    RemoteCollectiveAnalysis::RemoteCollectiveAnalysis(size_t ctx_index,
                          unsigned req_index, IndexSpaceID match, RemoteOp *op,
                          Deserializer &derez, Runtime *runtime)
      : context_index(ctx_index), requirement_index(req_index),
        match_space(match), operation(op),
        trace_info(PhysicalTraceInfo::unpack_trace_info(derez, runtime))
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RemoteCollectiveAnalysis::~RemoteCollectiveAnalysis(void)
    //--------------------------------------------------------------------------
    {
      delete operation;
    }

    //--------------------------------------------------------------------------
    Operation* RemoteCollectiveAnalysis::get_operation(void) const
    //--------------------------------------------------------------------------
    {
      return operation;
    }

    //--------------------------------------------------------------------------
    /*static*/ RemoteCollectiveAnalysis* RemoteCollectiveAnalysis::unpack(
         Deserializer &derez, Runtime *runtime, std::set<RtEvent> &ready_events)
    //--------------------------------------------------------------------------
    {
      size_t context_index;
      derez.deserialize(context_index);
      unsigned requirement_index;
      derez.deserialize(requirement_index);
      IndexSpaceID match_space;
      derez.deserialize(match_space);
      RemoteOp *op =
        RemoteOp::unpack_remote_operation(derez, runtime, ready_events);
      return new RemoteCollectiveAnalysis(context_index, requirement_index,
                                          match_space, op, derez, runtime);
    }

    /////////////////////////////////////////////////////////////
    // CollectiveCopyFillAnalysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    CollectiveCopyFillAnalysis::CollectiveCopyFillAnalysis(
                               Runtime *rt, Operation *op, unsigned index,
                               RegionNode *node, bool on_heap,
                               const PhysicalTraceInfo &t_info, bool exclusive)
      : RegistrationAnalysis(rt, op, index, node, on_heap, t_info, exclusive)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    CollectiveCopyFillAnalysis::CollectiveCopyFillAnalysis(Runtime *rt,
                                AddressSpaceID src, AddressSpaceID prev,
                                Operation *op, unsigned index,
                                RegionNode *node, bool on_heap, 
                                std::vector<PhysicalManager*> &&target_insts,
                                LegionVector<
                                   FieldMaskSet<InstanceView> > &&target_vws,
                                std::vector<IndividualView*> &&source_vws,
                                const PhysicalTraceInfo &t_info,
                                CollectiveMapping *mapping, bool first_local,
                                bool exclusive)
      : RegistrationAnalysis(rt, src, prev, op, index, node, on_heap,
          std::move(target_insts), std::move(target_vws), std::move(source_vws),
          t_info, mapping, first_local, exclusive)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(on_heap);
      assert(target_instances.size() == target_views.size());
#endif
      // Remote case so no registration to perform
    }  

    //--------------------------------------------------------------------------
    RtEvent CollectiveCopyFillAnalysis::perform_traversal(
                          RtEvent precondition, const VersionInfo &version_info,
                          std::set<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      if (precondition.exists() && !precondition.has_triggered())
        return defer_traversal(precondition, version_info, applied_events);
      // Record ourselves with any collective target views before doing
      // the traversal so that we will be available for collective copies
      for (unsigned idx = 0; idx < target_views.size(); idx++)
      {
        PhysicalManager *manager = target_instances[idx];
        for (FieldMaskSet<InstanceView>::const_iterator it =
              target_views[idx].begin(); it != target_views[idx].end(); it++)
        {
          if (!it->first->is_collective_view())
            continue;
          CollectiveView *collective = it->first->as_collective_view();
          collective->register_collective_analysis(manager, this,
                                                   applied_events);
        }
      }
      return RegistrationAnalysis::perform_traversal(precondition,
                                    version_info, applied_events);
    }

    /////////////////////////////////////////////////////////////
    // Valid Inst Analysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ValidInstAnalysis::ValidInstAnalysis(Runtime *rt, Operation *o,unsigned idx, 
                                  IndexSpaceExpression *expr, ReductionOpID red)
      : PhysicalAnalysis(rt, o, idx, expr, false/*on heap*/, true/*immutable*/),
        redop(red), target_analysis(this)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    ValidInstAnalysis::ValidInstAnalysis(Runtime *rt, AddressSpaceID src, 
            AddressSpaceID prev, Operation *o, unsigned idx,
            IndexSpaceExpression *expr, ValidInstAnalysis *t, ReductionOpID red)
      : PhysicalAnalysis(rt, src, prev, o, idx, expr, true/*on heap*/),
        redop(red), target_analysis(t)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    ValidInstAnalysis::~ValidInstAnalysis(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    bool ValidInstAnalysis::perform_analysis(EquivalenceSet *set,
                                             IndexSpaceExpression *expr,
                                             const bool expr_covers,
                                             const FieldMask &mask,
                                             std::set<RtEvent> &deferral_events,
                                             std::set<RtEvent> &applied_events,
                                             const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      set->find_valid_instances(*this, expr, expr_covers, mask, deferral_events, 
                                applied_events, already_deferred);
      // No migration check for reading
      return false;
    }

    //--------------------------------------------------------------------------
    RtEvent ValidInstAnalysis::perform_remote(RtEvent perform_precondition,
                                              std::set<RtEvent> &applied_events,
                                              const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_remote(perform_precondition, applied_events);
      // Easy out if we don't have remote sets
      if (remote_sets.empty())
        return RtEvent::NO_RT_EVENT;
      std::set<RtEvent> ready_events;
      for (std::map<AddressSpaceID,
                    FieldMaskSet<EquivalenceSet> >::const_iterator
            rit = remote_sets.begin(); rit != remote_sets.end(); rit++)
      {
#ifdef DEBUG_LEGION
        assert(!rit->second.empty());
#endif
        const AddressSpaceID target = rit->first;
        const RtUserEvent ready = Runtime::create_rt_user_event();
        const RtUserEvent applied = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(original_source);
          rez.serialize<size_t>(rit->second.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                rit->second.begin(); it != rit->second.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          analysis_expr->pack_expression(rez, target);
          op->pack_remote_operation(rez, target, applied_events);
          rez.serialize(index);
          rez.serialize(redop);
          rez.serialize(target_analysis);
          rez.serialize(ready);
          rez.serialize(applied);
        }
        runtime->send_equivalence_set_remote_request_instances(target, rez);
        ready_events.insert(ready);
        applied_events.insert(applied);
      }
      return Runtime::merge_events(ready_events);
    }

    //--------------------------------------------------------------------------
    RtEvent ValidInstAnalysis::perform_updates(RtEvent perform_precondition,
                                              std::set<RtEvent> &applied_events,
                                              const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_updates(perform_precondition, applied_events);
      if (recorded_instances != NULL)
      {
        if (original_source != runtime->address_space)
        {
          const RtUserEvent response_event = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(target_analysis);
            rez.serialize(response_event);
            rez.serialize<size_t>(recorded_instances->size());
            for (FieldMaskSet<LogicalView>::const_iterator it = 
                  recorded_instances->begin(); it != 
                  recorded_instances->end(); it++)
            {
              rez.serialize(it->first->did);
              rez.serialize(it->second);
            }
            rez.serialize<bool>(restricted);
          }
          runtime->send_equivalence_set_remote_instances(original_source, rez);
          return response_event;
        }
        else
          target_analysis->process_local_instances(*recorded_instances,
                                                   restricted);
      }
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    /*static*/ void ValidInstAnalysis::handle_remote_request_instances(
                 Deserializer &derez, Runtime *runtime, AddressSpaceID previous)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      AddressSpaceID original_source;
      derez.deserialize(original_source);
      size_t num_eq_sets;
      derez.deserialize(num_eq_sets);
      std::set<RtEvent> ready_events;

      std::vector<EquivalenceSet*> eq_sets(num_eq_sets);
      LegionVector<FieldMask> eq_masks(num_eq_sets);
      for (unsigned idx = 0; idx < num_eq_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        eq_sets[idx] = runtime->find_or_request_equivalence_set(did, ready);
        if (ready.exists())
          ready_events.insert(ready);
        derez.deserialize(eq_masks[idx]);
      }
      IndexSpaceExpression *expr = 
        IndexSpaceExpression::unpack_expression(derez,runtime->forest,previous);
      RemoteOp *op =
        RemoteOp::unpack_remote_operation(derez, runtime, ready_events);
      unsigned index;
      derez.deserialize(index);
      ReductionOpID redop;
      derez.deserialize(redop);
      ValidInstAnalysis *target;
      derez.deserialize(target);
      RtUserEvent ready;
      derez.deserialize(ready);
      RtUserEvent applied;
      derez.deserialize(applied);

      ValidInstAnalysis *analysis = new ValidInstAnalysis(runtime, 
          original_source, previous, op, index, expr, target, redop);
      analysis->add_reference();
      std::set<RtEvent> deferral_events, applied_events;
      // Wait for the equivalence sets to be ready if necessary
      RtEvent ready_event;
      if (!ready_events.empty())
        ready_event = Runtime::merge_events(ready_events);
      for (unsigned idx = 0; idx < eq_sets.size(); idx++)
        analysis->analyze(eq_sets[idx], eq_masks[idx],
            deferral_events, applied_events, ready_event);
      const RtEvent traversal_done = deferral_events.empty() ?
        RtEvent::NO_RT_EVENT : Runtime::merge_events(deferral_events);
      if (traversal_done.exists() || analysis->has_remote_sets())
      {
        const RtEvent remote_ready = 
          analysis->perform_remote(traversal_done, applied_events);
        if (remote_ready.exists())
          ready_events.insert(remote_ready);
      }
      // Defer sending the updates until we're ready
      const RtEvent local_ready = 
        analysis->perform_updates(traversal_done, applied_events);
      if (local_ready.exists())
        ready_events.insert(local_ready);
      if (!ready_events.empty())
        Runtime::trigger_event(ready, Runtime::merge_events(ready_events));
      else
        Runtime::trigger_event(ready);
      if (!applied_events.empty())
        Runtime::trigger_event(applied, Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(applied);
      if (analysis->remove_reference())
        delete analysis;
    }

    /////////////////////////////////////////////////////////////
    // Invalid Inst Analysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    InvalidInstAnalysis::InvalidInstAnalysis(Runtime *rt, Operation *o, 
                                unsigned idx, IndexSpaceExpression *expr, 
                                const FieldMaskSet<LogicalView> &valid_insts)
      : PhysicalAnalysis(rt, o, idx, expr, true/*on heap*/, true/*immutable*/),
        valid_instances(valid_insts), target_analysis(this)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    InvalidInstAnalysis::InvalidInstAnalysis(Runtime *rt, AddressSpaceID src, 
                             AddressSpaceID prev, Operation *o, unsigned idx,
                             IndexSpaceExpression *expr, InvalidInstAnalysis *t,
                             const FieldMaskSet<LogicalView> &valid_insts)
      : PhysicalAnalysis(rt, src, prev, o, idx, expr, true/*on heap*/),
        valid_instances(valid_insts), target_analysis(t)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    InvalidInstAnalysis::~InvalidInstAnalysis(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    bool InvalidInstAnalysis::perform_analysis(EquivalenceSet *set,
                                             IndexSpaceExpression *expr,
                                             const bool expr_covers,
                                             const FieldMask &mask,
                                             std::set<RtEvent> &deferral_events,
                                             std::set<RtEvent> &applied_events,
                                             const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      set->find_invalid_instances(*this, expr, expr_covers,mask,deferral_events,
                                  applied_events, already_deferred);
      // No migration check for reading
      return false;
    }

    //--------------------------------------------------------------------------
    RtEvent InvalidInstAnalysis::perform_remote(RtEvent perform_precondition,
                                              std::set<RtEvent> &applied_events,
                                              const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_remote(perform_precondition, applied_events);
      // Easy out if we don't have remote sets
      if (remote_sets.empty())
        return RtEvent::NO_RT_EVENT;
      std::set<RtEvent> ready_events;
      for (LegionMap<AddressSpaceID,
                     FieldMaskSet<EquivalenceSet> >::const_iterator
            rit = remote_sets.begin(); rit != remote_sets.end(); rit++)
      {
#ifdef DEBUG_LEGION
        assert(!rit->second.empty());
#endif
        const AddressSpaceID target = rit->first;
        const RtUserEvent ready = Runtime::create_rt_user_event();
        const RtUserEvent applied = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(original_source);
          rez.serialize<size_t>(rit->second.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                rit->second.begin(); it != rit->second.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          analysis_expr->pack_expression(rez, target);
          op->pack_remote_operation(rez, target, applied_events);
          rez.serialize(index);
          rez.serialize<size_t>(valid_instances.size());
          for (FieldMaskSet<LogicalView>::const_iterator it = 
                valid_instances.begin(); it != valid_instances.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          rez.serialize(target_analysis);
          rez.serialize(ready);
          rez.serialize(applied);
        }
        runtime->send_equivalence_set_remote_request_invalid(target, rez);
        ready_events.insert(ready);
        applied_events.insert(applied);
      }
      return Runtime::merge_events(ready_events);
    }

    //--------------------------------------------------------------------------
    RtEvent InvalidInstAnalysis::perform_updates(RtEvent perform_precondition,
                                              std::set<RtEvent> &applied_events,
                                              const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_updates(perform_precondition, applied_events);
      if (recorded_instances != NULL)
      {
        if (original_source != runtime->address_space)
        {
          const RtUserEvent response_event = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(target_analysis);
            rez.serialize(response_event);
            rez.serialize<size_t>(recorded_instances->size());
            for (FieldMaskSet<LogicalView>::const_iterator it = 
                  recorded_instances->begin(); it != 
                  recorded_instances->end(); it++)
            {
              rez.serialize(it->first->did);
              rez.serialize(it->second);
            }
            rez.serialize<bool>(restricted);
          }
          runtime->send_equivalence_set_remote_instances(original_source, rez);
          return response_event;
        }
        else
          target_analysis->process_local_instances(*recorded_instances, 
                                                   restricted);
      }
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InvalidInstAnalysis::handle_remote_request_invalid(
                 Deserializer &derez, Runtime *runtime, AddressSpaceID previous)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      AddressSpaceID original_source;
      derez.deserialize(original_source);
      size_t num_eq_sets;
      derez.deserialize(num_eq_sets);
      std::set<RtEvent> ready_events;
      std::vector<EquivalenceSet*> eq_sets(num_eq_sets, NULL);
      LegionVector<FieldMask> eq_masks(num_eq_sets);
      for (unsigned idx = 0; idx < num_eq_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        eq_sets[idx] = runtime->find_or_request_equivalence_set(did, ready);
        if (ready.exists())
          ready_events.insert(ready);
        derez.deserialize(eq_masks[idx]);
      }
      IndexSpaceExpression *expr = 
        IndexSpaceExpression::unpack_expression(derez,runtime->forest,previous);
      RemoteOp *op = 
        RemoteOp::unpack_remote_operation(derez, runtime, ready_events);
      unsigned index;
      derez.deserialize(index);
      FieldMaskSet<LogicalView> valid_instances;
      size_t num_valid_instances;
      derez.deserialize<size_t>(num_valid_instances);
      for (unsigned idx = 0; idx < num_valid_instances; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        LogicalView *view = runtime->find_or_request_logical_view(did, ready);
        if (ready.exists())
          ready_events.insert(ready);
        FieldMask view_mask;
        derez.deserialize(view_mask);
        valid_instances.insert(view, view_mask);
      }
      InvalidInstAnalysis *target;
      derez.deserialize(target);
      RtUserEvent ready;
      derez.deserialize(ready);
      RtUserEvent applied;
      derez.deserialize(applied);

      InvalidInstAnalysis *analysis = new InvalidInstAnalysis(runtime, 
          original_source, previous, op, index, expr, target, valid_instances);
      analysis->add_reference();
      std::set<RtEvent> deferral_events, applied_events;
      // Wait for the equivalence sets to be ready if necessary
      RtEvent ready_event;
      if (!ready_events.empty())
        ready_event = Runtime::merge_events(ready_events);
      for (unsigned idx = 0; idx < eq_sets.size(); idx++)
        analysis->analyze(eq_sets[idx], eq_masks[idx], deferral_events, 
                          applied_events, ready_event);
      const RtEvent traversal_done = deferral_events.empty() ?
        RtEvent::NO_RT_EVENT : Runtime::merge_events(deferral_events);
      if (traversal_done.exists() || analysis->has_remote_sets())
      {
        const RtEvent remote_ready = 
          analysis->perform_remote(traversal_done, applied_events);
        if (remote_ready.exists())
          ready_events.insert(remote_ready);
      }
      // Defer sending the updates until we're ready
      const RtEvent local_ready = 
        analysis->perform_updates(traversal_done, applied_events);
      if (local_ready.exists())
        ready_events.insert(local_ready);
      if (!ready_events.empty())
        Runtime::trigger_event(ready, Runtime::merge_events(ready_events));
      else
        Runtime::trigger_event(ready);
      if (!applied_events.empty())
        Runtime::trigger_event(applied, Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(applied);
      if (analysis->remove_reference())
        delete analysis;
    }

    /////////////////////////////////////////////////////////////
    // Antivalid Inst Analysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    AntivalidInstAnalysis::AntivalidInstAnalysis(Runtime *rt, Operation *o,
                                 unsigned idx, IndexSpaceExpression *expr,
                                 const FieldMaskSet<LogicalView> &anti_insts)
      : PhysicalAnalysis(rt, o, idx, expr, true/*on heap*/, true/*immutable*/),
        antivalid_instances(anti_insts), target_analysis(this)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    AntivalidInstAnalysis::AntivalidInstAnalysis(Runtime *rt,AddressSpaceID src, 
                           AddressSpaceID prev, Operation *o, unsigned idx,
                           IndexSpaceExpression *expr, AntivalidInstAnalysis *a,
                           const FieldMaskSet<LogicalView> &anti_insts)
      : PhysicalAnalysis(rt, src, prev, o, idx, expr, true/*on heap*/),
        antivalid_instances(anti_insts), target_analysis(a)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    AntivalidInstAnalysis::~AntivalidInstAnalysis(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    bool AntivalidInstAnalysis::perform_analysis(EquivalenceSet *set,
                                             IndexSpaceExpression *expr,
                                             const bool expr_covers,
                                             const FieldMask &mask,
                                             std::set<RtEvent> &deferral_events,
                                             std::set<RtEvent> &applied_events,
                                             const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      set->find_antivalid_instances(*this,expr,expr_covers,mask,deferral_events,
                                    applied_events, already_deferred);
      // No migration check for reading
      return false;
    }

    //--------------------------------------------------------------------------
    RtEvent AntivalidInstAnalysis::perform_remote(RtEvent perform_precondition,
                                              std::set<RtEvent> &applied_events,
                                              const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_remote(perform_precondition, applied_events);
      // Easy out if we don't have remote sets
      if (remote_sets.empty())
        return RtEvent::NO_RT_EVENT;
      std::set<RtEvent> ready_events;
      for (LegionMap<AddressSpaceID,
                     FieldMaskSet<EquivalenceSet> >::const_iterator 
            rit = remote_sets.begin(); rit != remote_sets.end(); rit++)
      {
#ifdef DEBUG_LEGION
        assert(!rit->second.empty());
#endif
        const AddressSpaceID target = rit->first;
        const RtUserEvent ready = Runtime::create_rt_user_event();
        const RtUserEvent applied = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(original_source);
          rez.serialize<size_t>(rit->second.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                rit->second.begin(); it != rit->second.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          analysis_expr->pack_expression(rez, target);
          op->pack_remote_operation(rez, target, applied_events);
          rez.serialize(index);
          rez.serialize<size_t>(antivalid_instances.size());
          for (FieldMaskSet<LogicalView>::const_iterator it = 
                antivalid_instances.begin(); it != 
                antivalid_instances.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          rez.serialize(target_analysis);
          rez.serialize(ready);
          rez.serialize(applied);
        }
        runtime->send_equivalence_set_remote_request_antivalid(target, rez);
        ready_events.insert(ready);
        applied_events.insert(applied);
      }
      return Runtime::merge_events(ready_events);
    }

    //--------------------------------------------------------------------------
    RtEvent AntivalidInstAnalysis::perform_updates(RtEvent perform_precondition,
                                              std::set<RtEvent> &applied_events,
                                              const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_updates(perform_precondition, applied_events);
      if (recorded_instances != NULL)
      {
        if (original_source != runtime->address_space)
        {
          const RtUserEvent response_event = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(target_analysis);
            rez.serialize(response_event);
            rez.serialize<size_t>(recorded_instances->size());
            for (FieldMaskSet<LogicalView>::const_iterator it = 
                  recorded_instances->begin(); it != 
                  recorded_instances->end(); it++)
            {
              rez.serialize(it->first->did);
              rez.serialize(it->second);
            }
            rez.serialize<bool>(restricted);
          }
          runtime->send_equivalence_set_remote_instances(original_source, rez);
          return response_event;
        }
        else
          target_analysis->process_local_instances(*recorded_instances, 
                                                   restricted);
      }
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    /*static*/ void AntivalidInstAnalysis::handle_remote_request_antivalid(
                 Deserializer &derez, Runtime *runtime, AddressSpaceID previous)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      AddressSpaceID original_source;
      derez.deserialize(original_source);
      size_t num_eq_sets;
      derez.deserialize(num_eq_sets);
      std::set<RtEvent> ready_events;
      std::vector<EquivalenceSet*> eq_sets(num_eq_sets, NULL);
      LegionVector<FieldMask> eq_masks(num_eq_sets);
      for (unsigned idx = 0; idx < num_eq_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        eq_sets[idx] = runtime->find_or_request_equivalence_set(did, ready);
        if (ready.exists())
          ready_events.insert(ready);
        derez.deserialize(eq_masks[idx]);
      }
      IndexSpaceExpression *expr = 
        IndexSpaceExpression::unpack_expression(derez,runtime->forest,previous);
      RemoteOp *op = 
        RemoteOp::unpack_remote_operation(derez, runtime, ready_events);
      unsigned index;
      derez.deserialize(index);
      FieldMaskSet<LogicalView> antivalid_instances;
      size_t num_antivalid_instances;
      derez.deserialize<size_t>(num_antivalid_instances);
      for (unsigned idx = 0; idx < num_antivalid_instances; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        LogicalView *view = runtime->find_or_request_logical_view(did, ready);
        if (ready.exists())
          ready_events.insert(ready);
        FieldMask view_mask;
        derez.deserialize(view_mask);
        antivalid_instances.insert(view, view_mask);
      }
      AntivalidInstAnalysis *target;
      derez.deserialize(target);
      RtUserEvent ready;
      derez.deserialize(ready);
      RtUserEvent applied;
      derez.deserialize(applied);

      AntivalidInstAnalysis *analysis = new AntivalidInstAnalysis(runtime, 
       original_source, previous, op, index, expr, target, antivalid_instances);
      analysis->add_reference();
      std::set<RtEvent> deferral_events, applied_events;
      // Wait for the equivalence sets to be ready if necessary
      RtEvent ready_event;
      if (!ready_events.empty())
        ready_event = Runtime::merge_events(ready_events);
      for (unsigned idx = 0; idx < eq_sets.size(); idx++)
        analysis->analyze(eq_sets[idx], eq_masks[idx], deferral_events, 
                          applied_events, ready_event);
      const RtEvent traversal_done = deferral_events.empty() ?
        RtEvent::NO_RT_EVENT : Runtime::merge_events(deferral_events);
      if (traversal_done.exists() || analysis->has_remote_sets())
      {
        const RtEvent remote_ready = 
          analysis->perform_remote(traversal_done, applied_events);
        if (remote_ready.exists())
          ready_events.insert(remote_ready);
      }
      // Defer sending the updates until we're ready
      const RtEvent local_ready = 
        analysis->perform_updates(traversal_done, applied_events);
      if (local_ready.exists())
        ready_events.insert(local_ready);
      if (!ready_events.empty())
        Runtime::trigger_event(ready, Runtime::merge_events(ready_events));
      else
        Runtime::trigger_event(ready);
      if (!applied_events.empty())
        Runtime::trigger_event(applied, Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(applied);
      if (analysis->remove_reference())
        delete analysis;
    }

    /////////////////////////////////////////////////////////////
    // Update Analysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    UpdateAnalysis::UpdateAnalysis(Runtime *rt, Operation *o, unsigned idx,
                    const RegionRequirement &req, RegionNode *rn,
                    const PhysicalTraceInfo &t_info, const ApEvent pre,
                    const ApEvent term, const bool check, const bool record)
      : CollectiveCopyFillAnalysis(rt, o, idx, rn, true/*on heap*/,
                                   t_info, IS_WRITE(req)),
        usage(req), precondition(pre), term_event(term),
        check_initialized(check && !IS_DISCARD(usage) && !IS_SIMULT(usage)), 
        record_valid(record), output_aggregator(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    UpdateAnalysis::UpdateAnalysis(Runtime *rt, AddressSpaceID src, 
                     AddressSpaceID prev, Operation *o, unsigned idx, 
                     const RegionUsage &use, RegionNode *rn,
                     std::vector<PhysicalManager*> &&target_insts,
                     LegionVector<FieldMaskSet<InstanceView> > &&target_vws,
                     std::vector<IndividualView*> &&source_vws,
                     const PhysicalTraceInfo &info, CollectiveMapping *mapping,
                     const RtEvent user_reg, const ApEvent pre, 
                     const ApEvent term, const bool check, 
                     const bool record, const bool first_local)
      : CollectiveCopyFillAnalysis(rt, src, prev, o, idx, rn,
          true/*on heap*/, std::move(target_insts), std::move(target_vws), 
          std::move(source_vws), info, mapping, first_local, IS_WRITE(use)),
        usage(use), precondition(pre), term_event(term),
        check_initialized(check), record_valid(record),
        output_aggregator(NULL), remote_user_registered(user_reg)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    UpdateAnalysis::~UpdateAnalysis(void)
    //--------------------------------------------------------------------------
    { 
      // If we didn't perform a registration and someone wanted to know that
      // the registration was done then we need to trigger that
      if (user_registered.exists())
        Runtime::trigger_event(user_registered);
    }

    //--------------------------------------------------------------------------
    void UpdateAnalysis::record_uninitialized(const FieldMask &uninit,
                                              std::set<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      if (!uninitialized)
      {
#ifdef DEBUG_LEGION
        assert(!uninitialized_reported.exists());
#endif
        uninitialized_reported = Runtime::create_rt_user_event();
        applied_events.insert(uninitialized_reported);
      }
      uninitialized |= uninit;
    }

    //--------------------------------------------------------------------------
    bool UpdateAnalysis::perform_analysis(EquivalenceSet *set,
                                          IndexSpaceExpression *expr,
                                          const bool expr_covers,
                                          const FieldMask &mask,
                                          std::set<RtEvent> &deferral_events,
                                          std::set<RtEvent> &applied_events,
                                          const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      set->update_set(*this, expr, expr_covers, mask, deferral_events, 
                      applied_events, already_deferred);
      // Perform a check for migration
      return true;
    }

    //--------------------------------------------------------------------------
    RtEvent UpdateAnalysis::perform_remote(RtEvent perform_precondition,
                                           std::set<RtEvent> &applied_events,
                                           const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_remote(perform_precondition, applied_events);
      // Easy out if we don't have any remote sets
      if (remote_sets.empty())
        return RtEvent::NO_RT_EVENT;
#ifdef DEBUG_LEGION
      assert(!target_instances.empty());
      assert(target_instances.size() == target_views.size());
#endif
      if (!remote_user_registered.exists())
      {
#ifdef DEBUG_LEGION
        assert(original_source == runtime->address_space);
        assert(!user_registered.exists());
#endif
        user_registered = Runtime::create_rt_user_event(); 
        remote_user_registered = user_registered;
      }
      std::set<RtEvent> remote_events;
      for (LegionMap<AddressSpaceID,
                     FieldMaskSet<EquivalenceSet> >::const_iterator 
            rit = remote_sets.begin(); rit != remote_sets.end(); rit++)
      {
#ifdef DEBUG_LEGION
        assert(!rit->second.empty());
#endif
        const AddressSpaceID target = rit->first;
        const RtUserEvent updated = Runtime::create_rt_user_event();
        const RtUserEvent applied = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(original_source);
          rez.serialize<size_t>(rit->second.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                rit->second.begin(); it != rit->second.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          op->pack_remote_operation(rez, target, applied_events);
          rez.serialize(index);
          rez.serialize(region->handle);
          rez.serialize(usage);
          rez.serialize<size_t>(target_instances.size());
          for (unsigned idx = 0; idx < target_instances.size(); idx++)
          {
            rez.serialize(target_instances[idx]->did);
            rez.serialize<size_t>(target_views[idx].size());
            for (FieldMaskSet<InstanceView>::const_iterator it =
                 target_views[idx].begin(); it != target_views[idx].end(); it++)
            {
              rez.serialize(it->first->did);
              rez.serialize(it->second);
            }
          }
          rez.serialize<size_t>(source_views.size());
          for (unsigned idx = 0; idx < source_views.size(); idx++)
            rez.serialize(source_views[idx]->did);
          trace_info.pack_trace_info(rez, applied_events);
          // We only need to pack the collective mapping once when going
          // from the origin space to the next space
          CollectiveMapping *mapping = get_replicated_mapping();
          if ((mapping != NULL) && (original_source == runtime->address_space))
          {
            mapping->pack(rez);
            rez.serialize<bool>(is_collective_first_local());
          }
          else
            rez.serialize<size_t>(0);
          rez.serialize(precondition);
          rez.serialize(term_event);
          rez.serialize(updated);
          rez.serialize(remote_user_registered);
          rez.serialize(applied);
          rez.serialize<bool>(check_initialized);
          rez.serialize<bool>(record_valid);
        }
        runtime->send_equivalence_set_remote_updates(target, rez);
        remote_events.insert(updated);
        applied_events.insert(applied);
      }
      return Runtime::merge_events(remote_events);
    }

    //--------------------------------------------------------------------------
    RtEvent UpdateAnalysis::perform_updates(RtEvent perform_precondition,
                                            std::set<RtEvent> &applied_events,
                                            const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_updates(perform_precondition, applied_events);
      // Report any uninitialized data now that we know the traversal is done
      if (!!uninitialized)
      {
#ifdef DEBUG_LEGION
        assert(check_initialized);
        assert(uninitialized_reported.exists());
#endif
        region->report_uninitialized_usage(op, index, usage, uninitialized,
                                           uninitialized_reported);
      }
      if (!input_aggregators.empty())
      {
#ifndef NON_AGGRESSIVE_AGGREGATORS
        const bool is_local = (original_source == runtime->address_space);
#endif
        for (std::map<RtEvent,CopyFillAggregator*>::const_iterator it = 
              input_aggregators.begin(); it != input_aggregators.end(); it++)
        {
          it->second->issue_updates(trace_info, precondition);
#ifdef NON_AGGRESSIVE_AGGREGATORS
          if (!it->second->effects_applied.has_triggered())
            guard_events.insert(it->second->effects_applied);
#else
          if (!it->second->effects_applied.has_triggered())
          {
            if (is_local)
            {
              if (!it->second->guard_postcondition.has_triggered())
                guard_events.insert(it->second->guard_postcondition);
              applied_events.insert(it->second->effects_applied);
            }
            else
              guard_events.insert(it->second->effects_applied);
          }
#endif
          if (it->second->release_guards(op->runtime, applied_events))
            delete it->second;
        }
      }
      if (!guard_events.empty())
        return Runtime::merge_events(guard_events);
      else
        return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    RtEvent UpdateAnalysis::perform_registration(RtEvent precondition,
                                const RegionUsage &usage,
                                std::set<RtEvent> &applied_events,
                                ApEvent init_precondition, ApEvent termination,
                                ApEvent &instances_ready, bool symbolic)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(usage == this->usage);
      // Should always be local
      assert(original_source == runtime->address_space);
#endif
      if (precondition.exists() && ! precondition.has_triggered())
        return defer_registration(precondition, usage, applied_events,
         trace_info, init_precondition, termination, instances_ready, symbolic);
      // Invoke the base implementation to see if we actually do it now
      const RtEvent registered = 
        CollectiveCopyFillAnalysis::perform_registration(precondition,
                              usage, applied_events, init_precondition,
                              termination, instances_ready, symbolic);
      if (user_registered.exists())
      {
        Runtime::trigger_event(user_registered, registered);
        user_registered = RtUserEvent::NO_RT_USER_EVENT;
      }
      return registered;
    }

    //--------------------------------------------------------------------------
    ApEvent UpdateAnalysis::perform_output(RtEvent perform_precondition,
                                           std::set<RtEvent> &applied_events,
                                           const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_output(perform_precondition, trace_info,
            false/*track*/, applied_events);
      if (output_aggregator != NULL)
      {
#ifdef DEBUG_LEGION
        assert(output_aggregator->track_events);
#endif
        const ApEvent effect =
          output_aggregator->issue_updates(trace_info, term_event,
                                           true/*restricted output*/);
        if (effect.exists())
          op->record_completion_effect(effect, applied_events);
        if (output_aggregator->effects_applied.has_triggered())
          applied_events.insert(output_aggregator->effects_applied);
        if (output_aggregator->release_guards(op->runtime, applied_events))
          delete output_aggregator;
      }
      return ApEvent::NO_AP_EVENT;
    }

    //--------------------------------------------------------------------------
    /*static*/ void UpdateAnalysis::handle_remote_updates(Deserializer &derez, 
                                      Runtime *runtime, AddressSpaceID previous)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      AddressSpaceID original_source;
      derez.deserialize(original_source);
      size_t num_eq_sets;
      derez.deserialize(num_eq_sets);
      std::set<RtEvent> ready_events;
      std::vector<EquivalenceSet*> eq_sets(num_eq_sets, NULL);
      LegionVector<FieldMask> eq_masks(num_eq_sets);
      FieldMask user_mask;
      for (unsigned idx = 0; idx < num_eq_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        eq_sets[idx] = runtime->find_or_request_equivalence_set(did, ready); 
        if (ready.exists())
          ready_events.insert(ready);
        derez.deserialize(eq_masks[idx]);
        user_mask |= eq_masks[idx];
      }
      RemoteOp *op = 
        RemoteOp::unpack_remote_operation(derez, runtime, ready_events);
      unsigned index;
      derez.deserialize(index);
      LogicalRegion handle;
      derez.deserialize(handle);
      RegionUsage usage;
      derez.deserialize(usage);
      size_t num_targets;
      derez.deserialize(num_targets);
      std::vector<PhysicalManager*> targets(num_targets);
      LegionVector<FieldMaskSet<InstanceView> > target_views(num_targets);
      for (unsigned idx1 = 0; idx1 < num_targets; idx1++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        targets[idx1] = runtime->find_or_request_instance_manager(did, ready);
        if (ready.exists())
          ready_events.insert(ready);
        size_t num_views;
        derez.deserialize(num_views);
        for (unsigned idx2 = 0; idx2 < num_views; idx2++)
        {
          derez.deserialize(did);
          LogicalView *view = runtime->find_or_request_logical_view(did, ready);
          if (ready.exists())
            ready_events.insert(ready);
          FieldMask mask;
          derez.deserialize(mask);
          target_views[idx1].insert(static_cast<InstanceView*>(view), mask);
        }
      }
      size_t num_sources;
      derez.deserialize(num_sources);
      std::vector<IndividualView*> source_views(num_sources);
      for (unsigned idx = 0; idx < num_sources; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        LogicalView *view = runtime->find_or_request_logical_view(did, ready);
        source_views[idx] = static_cast<IndividualView*>(view);
        if (ready.exists())
          ready_events.insert(ready);
      }
      PhysicalTraceInfo trace_info = 
        PhysicalTraceInfo::unpack_trace_info(derez, runtime);
      bool first_local = true;
      size_t collective_mapping_size;
      derez.deserialize(collective_mapping_size);
      CollectiveMapping *collective_mapping = ((collective_mapping_size) > 0) ?
        new CollectiveMapping(derez, collective_mapping_size) : NULL;
      if (collective_mapping != NULL)
        derez.deserialize<bool>(first_local);
      ApEvent precondition;
      derez.deserialize(precondition);
      ApEvent term_event;
      derez.deserialize(term_event);
      RtUserEvent updated;
      derez.deserialize(updated);
      RtEvent remote_user_registered;
      derez.deserialize(remote_user_registered);
      RtUserEvent applied;
      derez.deserialize(applied);
      bool check_initialized;
      derez.deserialize(check_initialized);
      bool record_valid;
      derez.deserialize(record_valid);

      RegionNode *node = runtime->forest->get_node(handle);
      // This takes ownership of the remote operation
      UpdateAnalysis *analysis = new UpdateAnalysis(runtime, original_source,
        previous, op, index, usage, node, std::move(targets), 
        std::move(target_views), std::move(source_views), trace_info,
        collective_mapping, remote_user_registered, precondition,
        term_event, check_initialized, record_valid, first_local);
      analysis->add_reference();
      std::set<RtEvent> deferral_events, applied_events; 
      // Make sure that all our pointers are ready
      RtEvent ready_event;
      if (!ready_events.empty())
        ready_event = Runtime::merge_events(ready_events);
      for (unsigned idx = 0; idx < eq_sets.size(); idx++)
        analysis->analyze(eq_sets[idx], eq_masks[idx], deferral_events, 
                          applied_events, ready_event);
      const RtEvent traversal_done = deferral_events.empty() ?
        RtEvent::NO_RT_EVENT : Runtime::merge_events(deferral_events);
      std::set<RtEvent> update_events;
      // If we have remote messages to send do that now
      if (traversal_done.exists() || analysis->has_remote_sets())
      {
        const RtEvent remote_ready = 
          analysis->perform_remote(traversal_done, applied_events);
        if (remote_ready.exists())
          update_events.insert(remote_ready);
      }
      // Then perform the updates
      // Note that we need to capture all the effects of these updates
      // before we can consider them applied, so we can't use the 
      // applied_events data structure here
      const RtEvent updates_ready = 
        analysis->perform_updates(traversal_done, update_events);
      if (updates_ready.exists())
        update_events.insert(updates_ready);
      // We can trigger our updated event done when all the guards are done 
      if (!update_events.empty())
        Runtime::trigger_event(updated, Runtime::merge_events(update_events));
      else
        Runtime::trigger_event(updated);
      // If we have outputs we need for the user to be registered
      // before we can apply the output copies
      analysis->perform_output(remote_user_registered, applied_events);
      // Do the rest of the triggers
      if (!applied_events.empty())
        Runtime::trigger_event(applied, Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(applied);
      if (analysis->remove_reference())
        delete analysis;
    }

    /////////////////////////////////////////////////////////////
    // Acquire Analysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    AcquireAnalysis::AcquireAnalysis(Runtime *rt, Operation *o, unsigned idx,
                                     RegionNode *node,
                                     const PhysicalTraceInfo &t_info)
      : RegistrationAnalysis(rt, o, idx, node, true/*on heap*/, t_info, 
                             true/*exclusive*/), target_analysis(this)
    //--------------------------------------------------------------------------
    {
    }
    
    //--------------------------------------------------------------------------
    AcquireAnalysis::AcquireAnalysis(Runtime *rt, AddressSpaceID src, 
                      AddressSpaceID prev, Operation *o, unsigned idx, 
                      RegionNode *node, AcquireAnalysis *t,
                      const PhysicalTraceInfo &t_info,
                      CollectiveMapping *mapping, bool first_local)
      : RegistrationAnalysis(rt, src, prev, o, idx, node, true/*on heap*/,
          t_info, mapping, first_local, true/*exclusive*/), target_analysis(t)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    AcquireAnalysis::~AcquireAnalysis(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    bool AcquireAnalysis::perform_analysis(EquivalenceSet *set,
                                           IndexSpaceExpression *expr,
                                           const bool expr_covers,
                                           const FieldMask &mask,
                                           std::set<RtEvent> &deferral_events,
                                           std::set<RtEvent> &applied_events,
                                           const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      set->acquire_restrictions(*this, expr, expr_covers, mask, deferral_events,
                                applied_events, already_deferred);
      // Perform a check for migration after this
      return true;
    }

    //--------------------------------------------------------------------------
    RtEvent AcquireAnalysis::perform_remote(RtEvent perform_precondition, 
                                            std::set<RtEvent> &applied_events,
                                            const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_remote(perform_precondition, applied_events);
      // Easy out if there is nothing to do
      if (remote_sets.empty())
        return RtEvent::NO_RT_EVENT;
      std::set<RtEvent> remote_events;
      for (LegionMap<AddressSpaceID,
                     FieldMaskSet<EquivalenceSet> >::const_iterator 
            rit = remote_sets.begin(); rit != remote_sets.end(); rit++)
      {
#ifdef DEBUG_LEGION
        assert(!rit->second.empty());
#endif
        const AddressSpaceID target = rit->first;
        const RtUserEvent returned = Runtime::create_rt_user_event();
        const RtUserEvent applied = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(original_source);
          rez.serialize<size_t>(rit->second.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                rit->second.begin(); it != rit->second.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          rez.serialize(region->handle);
          op->pack_remote_operation(rez, target, applied_events);
          rez.serialize(index);
          rez.serialize(returned);
          rez.serialize(applied);
          rez.serialize(target_analysis);
          trace_info.pack_trace_info(rez, applied_events);
          // We only need to pack the collective mapping once when going
          // from the origin space to the next space
          CollectiveMapping *mapping = get_replicated_mapping();
          if ((mapping != NULL) && (original_source == runtime->address_space))
          {
            mapping->pack(rez);
            rez.serialize<bool>(is_collective_first_local());
          }
          else
            rez.serialize<size_t>(0);
        }
        runtime->send_equivalence_set_remote_acquires(target, rez);
        applied_events.insert(applied);
        remote_events.insert(returned);
      }
      return Runtime::merge_events(remote_events);
    }

    //--------------------------------------------------------------------------
    RtEvent AcquireAnalysis::perform_updates(RtEvent perform_precondition,
                                             std::set<RtEvent> &applied_events,
                                             const bool already_deferred)
    //-------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_updates(perform_precondition, applied_events);
      if (recorded_instances != NULL)
      {
        if (original_source != runtime->address_space)
        {
          const RtUserEvent response_event = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(target_analysis);
            rez.serialize(response_event);
            rez.serialize<size_t>(recorded_instances->size());
            for (FieldMaskSet<LogicalView>::const_iterator it = 
                  recorded_instances->begin(); it != 
                  recorded_instances->end(); it++)
            {
              rez.serialize(it->first->did);
              rez.serialize(it->second);
            }
            rez.serialize<bool>(restricted);
          }
          runtime->send_equivalence_set_remote_instances(original_source, rez);
          return response_event;
        }
        else
          target_analysis->process_local_instances(*recorded_instances, 
                                                   restricted);
      }
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    /*static*/ void AcquireAnalysis::handle_remote_acquires(Deserializer &derez,
                                      Runtime *runtime, AddressSpaceID previous)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      AddressSpaceID original_source;
      derez.deserialize(original_source);
      size_t num_eq_sets;
      derez.deserialize(num_eq_sets);
      std::set<RtEvent> ready_events;
      std::vector<EquivalenceSet*> eq_sets(num_eq_sets, NULL);
      LegionVector<FieldMask> eq_masks(num_eq_sets);
      for (unsigned idx = 0; idx < num_eq_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        eq_sets[idx] = runtime->find_or_request_equivalence_set(did, ready); 
        if (ready.exists())
          ready_events.insert(ready);
        derez.deserialize(eq_masks[idx]);
      }
      LogicalRegion handle;
      derez.deserialize(handle);
      RegionNode *region = runtime->forest->get_node(handle);
      RemoteOp *op = 
        RemoteOp::unpack_remote_operation(derez, runtime, ready_events);
      unsigned index;
      derez.deserialize(index);
      RtUserEvent returned;
      derez.deserialize(returned);
      RtUserEvent applied;
      derez.deserialize(applied);
      AcquireAnalysis *target;
      derez.deserialize(target);
      const PhysicalTraceInfo trace_info = 
        PhysicalTraceInfo::unpack_trace_info(derez, runtime);
      size_t collective_mapping_size;
      derez.deserialize(collective_mapping_size);
      CollectiveMapping *mapping = ((collective_mapping_size) > 0) ?
        new CollectiveMapping(derez, collective_mapping_size) : NULL;
      bool first_local = true;
      if (mapping != NULL)
        derez.deserialize<bool>(first_local);

      // This takes ownership of the operation
      AcquireAnalysis *analysis = new AcquireAnalysis(runtime, original_source,
         previous, op, index, region, target, trace_info, mapping, first_local);
      analysis->add_reference();
      std::set<RtEvent> deferral_events, applied_events;
      // Make sure that all our pointers are ready
      RtEvent ready_event;
      if (!ready_events.empty())
        ready_event = Runtime::merge_events(ready_events);
      for (unsigned idx = 0; idx < eq_sets.size(); idx++)
        analysis->analyze(eq_sets[idx], eq_masks[idx], deferral_events, 
                          applied_events, ready_event);
      const RtEvent traversal_done = deferral_events.empty() ?
        RtEvent::NO_RT_EVENT : Runtime::merge_events(deferral_events);
      if (traversal_done.exists() || analysis->has_remote_sets())
      {
        const RtEvent remote_ready = 
          analysis->perform_remote(traversal_done, applied_events);
        if (remote_ready.exists())
          ready_events.insert(remote_ready);
      }
      // Defer sending the updates until we're ready
      const RtEvent local_ready = 
        analysis->perform_updates(traversal_done, applied_events);
      if (local_ready.exists())
        ready_events.insert(local_ready);
      if (!ready_events.empty())
        Runtime::trigger_event(returned, Runtime::merge_events(ready_events));
      else
        Runtime::trigger_event(returned);
      if (!applied_events.empty())
        Runtime::trigger_event(applied, Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(applied);
      if (analysis->remove_reference())
        delete analysis;
    }

    /////////////////////////////////////////////////////////////
    // Release Analysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ReleaseAnalysis::ReleaseAnalysis(Runtime *rt, Operation *o, unsigned idx, 
                               ApEvent pre, RegionNode *node,
                               const PhysicalTraceInfo &t_info)
      : CollectiveCopyFillAnalysis(rt, o, idx, node, true/*on heap*/,
                                   t_info, true/*exclusive*/),
        precondition(pre), target_analysis(this), release_aggregator(NULL)
    //--------------------------------------------------------------------------
    {
    }
    
    //--------------------------------------------------------------------------
    ReleaseAnalysis::ReleaseAnalysis(Runtime *rt, AddressSpaceID src, 
          AddressSpaceID prev, Operation *o, unsigned idx, 
          RegionNode *node, ApEvent pre, ReleaseAnalysis *t, 
          std::vector<PhysicalManager*> &&target_insts,
          LegionVector<FieldMaskSet<InstanceView> > &&target_vws,
          std::vector<IndividualView*> &&source_vws,
          const PhysicalTraceInfo &info,
          CollectiveMapping *mapping, const bool first)
      : CollectiveCopyFillAnalysis(rt, src, prev, o, idx, node, true/*on heap*/,
                                   std::move(target_insts),
                                   std::move(target_vws), std::move(source_vws),
                                   info, mapping, first, true/*exclusive*/),
        precondition(pre), target_analysis(t), release_aggregator(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    ReleaseAnalysis::~ReleaseAnalysis(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    bool ReleaseAnalysis::perform_analysis(EquivalenceSet *set,
                                           IndexSpaceExpression *expr,
                                           const bool expr_covers,
                                           const FieldMask &mask,
                                           std::set<RtEvent> &deferral_events,
                                           std::set<RtEvent> &applied_events,
                                           const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      set->release_restrictions(*this, expr, expr_covers, mask, deferral_events,
                                applied_events, already_deferred);
      // Perform a check for migration after this
      return true;
    }

    //--------------------------------------------------------------------------
    RtEvent ReleaseAnalysis::perform_remote(RtEvent perform_precondition, 
                                            std::set<RtEvent> &applied_events,
                                            const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_remote(perform_precondition, applied_events);
      // Easy out if there is nothing to do
      if (remote_sets.empty())
        return RtEvent::NO_RT_EVENT;
      std::set<RtEvent> remote_events;
      for (LegionMap<AddressSpaceID,
                     FieldMaskSet<EquivalenceSet> >::const_iterator 
            rit = remote_sets.begin(); rit != remote_sets.end(); rit++)
      {
#ifdef DEBUG_LEGION
        assert(!rit->second.empty());
#endif
        const AddressSpaceID target = rit->first;
        const RtUserEvent returned = Runtime::create_rt_user_event();
        const RtUserEvent applied = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(original_source);
          rez.serialize<size_t>(rit->second.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                rit->second.begin(); it != rit->second.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          rez.serialize<size_t>(target_instances.size());
          for (unsigned idx = 0; idx < target_instances.size(); idx++)
          {
            rez.serialize(target_instances[idx]->did);
            rez.serialize<size_t>(target_views[idx].size());
            for (FieldMaskSet<InstanceView>::const_iterator it =
                 target_views[idx].begin(); it != target_views[idx].end(); it++)
            {
              rez.serialize(it->first->did);
              rez.serialize(it->second);
            }
          }
          rez.serialize<size_t>(source_views.size());
          for (std::vector<IndividualView*>::const_iterator it =
                source_views.begin(); it != source_views.end(); it++)
            rez.serialize((*it)->did);
          rez.serialize(region->handle);
          op->pack_remote_operation(rez, target, applied_events);
          rez.serialize(index);
          rez.serialize(precondition);
          rez.serialize(returned);
          rez.serialize(applied);
          rez.serialize(target_analysis);
          trace_info.pack_trace_info(rez, applied_events);
          // We only need to pack the collective mapping once when going
          // from the origin space to the next space
          CollectiveMapping *mapping = get_replicated_mapping();
          if ((mapping != NULL) && (original_source == runtime->address_space))
          {
            mapping->pack(rez);
            rez.serialize<bool>(is_collective_first_local());
          }
          else
            rez.serialize<size_t>(0);
        }
        runtime->send_equivalence_set_remote_releases(target, rez);
        applied_events.insert(applied);
        remote_events.insert(returned);
      }
      return Runtime::merge_events(remote_events);
    }

    //--------------------------------------------------------------------------
    RtEvent ReleaseAnalysis::perform_updates(RtEvent perform_precondition, 
                                            std::set<RtEvent> &applied_events,
                                            const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // Defer this if necessary
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_remote(perform_precondition, applied_events);
      // See if we have any instance names to send back
      if ((target_analysis != this) && (recorded_instances != NULL))
      {
        if (original_source != runtime->address_space)
        {
          const RtUserEvent response_event = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(target_analysis);
            rez.serialize(response_event);
            rez.serialize<size_t>(recorded_instances->size());
            for (FieldMaskSet<LogicalView>::const_iterator it = 
                  recorded_instances->begin(); it != 
                  recorded_instances->end(); it++)
            {
              rez.serialize(it->first->did);
              rez.serialize(it->second);
            }
            rez.serialize<bool>(restricted);
          }
          runtime->send_equivalence_set_remote_instances(original_source, rez);
          applied_events.insert(response_event);
        }
        else
          target_analysis->process_local_instances(*recorded_instances, 
                                                   restricted);
      }
      if (release_aggregator != NULL)
      {
        std::set<RtEvent> guard_events;
        release_aggregator->issue_updates(trace_info, precondition);
#ifdef NON_AGGRESSIVE_AGGREGATORS
        if (release_aggregator->effects_applied.has_triggered())
          guard_events.insert(release_aggregator->effects_applied);
#else
        if (release_aggregator->effects_applied.has_triggered())
        {
          if (original_source == runtime->address_space)
          {
            if (!release_aggregator->guard_postcondition.has_triggered())
              guard_events.insert(release_aggregator->guard_postcondition);
            applied_events.insert(release_aggregator->effects_applied);
          }
          else
            guard_events.insert(release_aggregator->effects_applied);
        }
#endif
        if (release_aggregator->release_guards(op->runtime, applied_events))
          delete release_aggregator;
        if (!guard_events.empty())
          return Runtime::merge_events(guard_events);
      }
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReleaseAnalysis::handle_remote_releases(Deserializer &derez,
                                      Runtime *runtime, AddressSpaceID previous)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      AddressSpaceID original_source;
      derez.deserialize(original_source);
      size_t num_eq_sets;
      derez.deserialize(num_eq_sets);
      std::set<RtEvent> ready_events;
      std::vector<EquivalenceSet*> eq_sets(num_eq_sets, NULL);
      LegionVector<FieldMask> eq_masks(num_eq_sets);
      for (unsigned idx = 0; idx < num_eq_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        eq_sets[idx] = runtime->find_or_request_equivalence_set(did, ready); 
        if (ready.exists())
          ready_events.insert(ready);
        derez.deserialize(eq_masks[idx]);
      }
      size_t num_targets;
      derez.deserialize(num_targets);
      std::vector<PhysicalManager*> targets(num_targets);
      LegionVector<FieldMaskSet<InstanceView> > target_views(num_targets);
      for (unsigned idx1 = 0; idx1 < num_targets; idx1++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        targets[idx1] = runtime->find_or_request_instance_manager(did, ready);
        if (ready.exists())
          ready_events.insert(ready);
        size_t num_views;
        derez.deserialize(num_views);
        for (unsigned idx2 = 0; idx2 < num_views; idx2++)
        {
          derez.deserialize(did);
          LogicalView *view = runtime->find_or_request_logical_view(did, ready);
          if (ready.exists())
            ready_events.insert(ready);
          FieldMask mask;
          derez.deserialize(mask);
          target_views[idx1].insert(static_cast<InstanceView*>(view), mask);
        }
      }
      size_t num_sources;
      derez.deserialize(num_sources);
      std::vector<IndividualView*> source_views(num_sources);
      for (unsigned idx = 0; idx < num_sources; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        source_views[idx] = static_cast<IndividualView*>(
            runtime->find_or_request_logical_view(did, ready));
        if (ready.exists())
          ready_events.insert(ready);
      }
      LogicalRegion handle;
      derez.deserialize(handle);
      RegionNode *region = runtime->forest->get_node(handle);
      RemoteOp *op = 
        RemoteOp::unpack_remote_operation(derez, runtime, ready_events);
      unsigned index;
      derez.deserialize(index);
      ApEvent precondition;
      derez.deserialize(precondition);
      RtUserEvent returned;
      derez.deserialize(returned);
      RtUserEvent applied;
      derez.deserialize(applied);
      ReleaseAnalysis *target;
      derez.deserialize(target);
      const PhysicalTraceInfo trace_info = 
        PhysicalTraceInfo::unpack_trace_info(derez, runtime);
      size_t collective_mapping_size;
      derez.deserialize(collective_mapping_size);
      CollectiveMapping *mapping = ((collective_mapping_size) > 0) ?
        new CollectiveMapping(derez, collective_mapping_size) : NULL;
      bool first_local = true;
      if (mapping != NULL)
        derez.deserialize<bool>(first_local);

      ReleaseAnalysis *analysis = new ReleaseAnalysis(runtime, original_source,
          previous, op, index, region, precondition, target, std::move(targets),
          std::move(target_views), std::move(source_views), trace_info, 
          mapping, first_local);
      analysis->add_reference();
      std::set<RtEvent> deferral_events, applied_events;
      RtEvent ready_event;
      // Make sure that all our pointers are ready
      if (!ready_events.empty())
        ready_event = Runtime::merge_events(ready_events);
      for (unsigned idx = 0; idx < eq_sets.size(); idx++)
        analysis->analyze(eq_sets[idx], eq_masks[idx], deferral_events, 
                          applied_events, ready_event);
      const RtEvent traversal_done = deferral_events.empty() ?
        RtEvent::NO_RT_EVENT : Runtime::merge_events(deferral_events);
      if (traversal_done.exists() || analysis->has_remote_sets())
      {
        const RtEvent remote_ready = 
          analysis->perform_remote(traversal_done, applied_events);
        if (remote_ready.exists())
          ready_events.insert(remote_ready);
      }
      // Note that we use the ready events here for applied so that
      // we can know that all our updates are done before we tell
      // the original source node that we've returned
      const RtEvent local_ready = 
        analysis->perform_updates(traversal_done, 
            (original_source == runtime->address_space) ?
              applied_events : ready_events);
      if (local_ready.exists())
        ready_events.insert(local_ready);
      if (!ready_events.empty())
        Runtime::trigger_event(returned, Runtime::merge_events(ready_events));
      else
        Runtime::trigger_event(returned);
      if (!applied_events.empty())
        Runtime::trigger_event(applied, Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(applied);
      if (analysis->remove_reference())
        delete analysis;
    }

    /////////////////////////////////////////////////////////////
    // Copy Across Analysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    CopyAcrossAnalysis::CopyAcrossAnalysis(Runtime *rt, Operation *o, 
        unsigned src_idx, unsigned dst_idx, const RegionRequirement &src_req,
        const RegionRequirement &dst_req, const InstanceSet &target_insts,
        const LegionVector<FieldMaskSet<InstanceView> > &target_vws,
        const std::vector<IndividualView*> &source_vws,
        const ApEvent pre, const ApEvent dst_ready,
        const PredEvent pred, const ReductionOpID red,
        const std::vector<unsigned> &src_idxes,
        const std::vector<unsigned> &dst_idxes, 
        const PhysicalTraceInfo &t_info, const bool perf)
      : PhysicalAnalysis(rt, o, dst_idx, 
          rt->forest->get_node(dst_req.region)->row_source,
          true/*on heap*/, false/*immutable*/),
        src_mask(perf ? FieldMask() : initialize_mask(src_idxes)), 
        dst_mask(perf ? FieldMask() : initialize_mask(dst_idxes)),
        src_index(src_idx), dst_index(dst_idx), src_usage(src_req), 
        dst_usage(dst_req), src_region(src_req.region), 
        dst_region(dst_req.region), targets_ready(dst_ready),
        target_instances(convert_instances(target_insts)),
        target_views(target_vws), source_views(source_vws), precondition(pre),
        pred_guard(pred), redop(red), src_indexes(src_idxes), 
        dst_indexes(dst_idxes), across_helpers(perf ? 
              std::vector<CopyAcrossHelper*>() :
              create_across_helpers(src_mask, dst_mask, target_instances,
                                    src_indexes, dst_indexes)),
        trace_info(t_info), perfect(perf), across_aggregator(NULL)
    //--------------------------------------------------------------------------
    {
    } 

    //--------------------------------------------------------------------------
    CopyAcrossAnalysis::CopyAcrossAnalysis(Runtime *rt, AddressSpaceID src, 
        AddressSpaceID prev, Operation *o, unsigned src_idx, unsigned dst_idx,
        const RegionUsage &src_use, const RegionUsage &dst_use, 
        const LogicalRegion src_reg, const LogicalRegion dst_reg, 
        const ApEvent dst_ready, std::vector<PhysicalManager*> &&target_insts,
        LegionVector<FieldMaskSet<InstanceView> > &&target_vws,
        std::vector<IndividualView*> &&source_vws,
        const ApEvent pre, const PredEvent pred, const ReductionOpID red,
        const std::vector<unsigned> &src_idxes,
        const std::vector<unsigned> &dst_idxes, 
        const PhysicalTraceInfo &t_info, const bool perf)
      : PhysicalAnalysis(rt, src, prev, o, dst_idx,
          rt->forest->get_node(dst_reg)->row_source, true/*on heap*/),
        src_mask(perf ? FieldMask() : initialize_mask(src_idxes)), 
        dst_mask(perf ? FieldMask() : initialize_mask(dst_idxes)),
        src_index(src_idx), dst_index(dst_idx), src_usage(src_use),
        dst_usage(dst_use), src_region(src_reg), dst_region(dst_reg),
        targets_ready(dst_ready), target_instances(target_insts),
        target_views(target_vws), source_views(source_vws), precondition(pre),
        pred_guard(pred), redop(red), src_indexes(src_idxes), 
        dst_indexes(dst_idxes), across_helpers(perf ? 
              std::vector<CopyAcrossHelper*>() :
              create_across_helpers(src_mask, dst_mask, target_instances,
                                    src_indexes, dst_indexes)),
        trace_info(t_info), perfect(perf), across_aggregator(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    CopyAcrossAnalysis::~CopyAcrossAnalysis(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!aggregator_guard.exists() || aggregator_guard.has_triggered());
#endif 
      for (std::vector<CopyAcrossHelper*>::const_iterator it = 
            across_helpers.begin(); it != across_helpers.end(); it++)
        delete (*it);
    }

    //--------------------------------------------------------------------------
    void CopyAcrossAnalysis::record_uninitialized(const FieldMask &uninit,
                                              std::set<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      if (!uninitialized)
      {
#ifdef DEBUG_LEGION
        assert(!uninitialized_reported.exists());
#endif
        uninitialized_reported = Runtime::create_rt_user_event();
        applied_events.insert(uninitialized_reported);
      }
      uninitialized |= uninit;
    }

    //--------------------------------------------------------------------------
    CopyFillAggregator* CopyAcrossAnalysis::get_across_aggregator(void)
    //--------------------------------------------------------------------------
    {
      if (across_aggregator == NULL)
      {
#ifdef DEBUG_LEGION
        assert(!aggregator_guard.exists());
#endif
        aggregator_guard = Runtime::create_rt_user_event();
        across_aggregator = new CopyFillAggregator(runtime->forest, this,
            src_index, dst_index, NULL/*no previous guard*/, true/*track*/,
            pred_guard, aggregator_guard);
      }
      return across_aggregator;
    }
    
    //--------------------------------------------------------------------------
    bool CopyAcrossAnalysis::perform_analysis(EquivalenceSet *set,
                                             IndexSpaceExpression *expr,
                                             const bool expr_covers,
                                             const FieldMask &mask,
                                             std::set<RtEvent> &deferral_events,
                                             std::set<RtEvent> &applied_events,
                                             const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      set->issue_across_copies(*this, mask, expr, expr_covers, deferral_events,
                               applied_events, already_deferred);
      // Perform a check for migration after this
      return true;
    }

    //--------------------------------------------------------------------------
    RtEvent CopyAcrossAnalysis::perform_remote(RtEvent perform_precondition,
                                              std::set<RtEvent> &applied_events,
                                              const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_remote(perform_precondition, applied_events);
      if (remote_sets.empty())
        return RtEvent::NO_RT_EVENT;
#ifdef DEBUG_LEGION
      assert(target_instances.size() == target_views.size());
      assert(src_indexes.size() == dst_indexes.size());
#endif
      for (LegionMap<AddressSpaceID,
                     FieldMaskSet<EquivalenceSet> >::const_iterator 
            rit = remote_sets.begin(); rit != remote_sets.end(); rit++)
      {
#ifdef DEBUG_LEGION
        assert(!rit->second.empty());
#endif
        const AddressSpaceID target = rit->first;
        const ApUserEvent copy = Runtime::create_ap_user_event(&trace_info);
        const RtUserEvent applied = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(original_source);
          rez.serialize<size_t>(rit->second.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                rit->second.begin(); it != rit->second.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          op->pack_remote_operation(rez, target, applied_events);
          rez.serialize(src_index);
          rez.serialize(dst_index);
          rez.serialize(src_usage);
          rez.serialize(dst_usage);
          rez.serialize(targets_ready);
          rez.serialize<size_t>(target_instances.size());
          for (unsigned idx = 0; idx < target_instances.size(); idx++)
          {
            rez.serialize(target_instances[idx]->did);
            rez.serialize<size_t>(target_views[idx].size());
            for (FieldMaskSet<InstanceView>::const_iterator it =
                  target_views[idx].begin(); it !=
                  target_views[idx].end(); it++)
            {
              rez.serialize(it->first->did);
              rez.serialize(it->second);
            }
          }
          rez.serialize<size_t>(source_views.size());
          for (unsigned idx = 0; idx < source_views.size(); idx++)
            rez.serialize(source_views[idx]->did);
          rez.serialize(src_region);
          rez.serialize(dst_region);
          rez.serialize(pred_guard);
          rez.serialize(precondition);
          rez.serialize(redop);
          rez.serialize<bool>(perfect);
          if (!perfect)
          {
            rez.serialize<size_t>(src_indexes.size());
            for (unsigned idx = 0; idx < src_indexes.size(); idx++)
            {
              rez.serialize(src_indexes[idx]);
              rez.serialize(dst_indexes[idx]);
            }
          }
          rez.serialize(applied);
          rez.serialize(copy);
          trace_info.pack_trace_info(rez, applied_events);
        }
        runtime->send_equivalence_set_remote_copies_across(target, rez);
        applied_events.insert(applied);
        copy_events.push_back(copy);
      }
      // Filter all the remote expressions from the local ones here
      filter_remote_expressions(local_exprs);
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    RtEvent CopyAcrossAnalysis::perform_updates(RtEvent perform_precondition,
                                              std::set<RtEvent> &applied_events,
                                              const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_updates(perform_precondition, applied_events);
      // Report any uninitialized data now that we know the traversal is done
      if (!!uninitialized)
      {
#ifdef DEBUG_LEGION
        assert(uninitialized_reported.exists());
#endif
        RegionNode *src_node = runtime->forest->get_node(src_region);
        src_node->report_uninitialized_usage(op, src_index, src_usage, 
                                uninitialized, uninitialized_reported);
      }
      if (across_aggregator != NULL)
      {
#ifdef DEBUG_LEGION
        assert(aggregator_guard.exists());
        assert(across_aggregator->track_events);
#endif
        // Trigger the guard event for the aggregator once all the 
        // actual guard events are done. Note that this is safe for
        // copy across aggregators because unlike other aggregators
        // they are moving data from one field to another so it is
        // safe to create entanglements between fields since they are
        // all going to be subsumed by the same completion event for
        // the copy-across operation anyway
        if (!guard_events.empty())
          Runtime::trigger_event(aggregator_guard, 
              Runtime::merge_events(guard_events));
        else
          Runtime::trigger_event(aggregator_guard);
        // Record the event field preconditions for each view
        std::map<InstanceView*,std::vector<ApEvent> > dst_events;
        for (unsigned idx = 0; idx < target_views.size(); idx++)
        {
          for (FieldMaskSet<InstanceView>::const_iterator it =
                target_views[idx].begin(); it != target_views[idx].end(); it++)
          {
            // Always instantiate the entry in the map
            dst_events[it->first].push_back(targets_ready);
          }
        }
        // This is a copy-across aggregator so the destination events
        // are being handled by the copy operation that mapped the
        // target instance for us
        const ApEvent effect = 
          across_aggregator->issue_updates(trace_info, precondition,
              false/*restricted*/, false/*manage dst events*/, &dst_events);
        if (effect.exists())
          copy_events.push_back(effect);
        if (across_aggregator->effects_applied.has_triggered())
          applied_events.insert(across_aggregator->effects_applied);
        if (across_aggregator->release_guards(op->runtime, applied_events))
          delete across_aggregator;
      }
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    ApEvent CopyAcrossAnalysis::perform_output(RtEvent perform_precondition,
                                              std::set<RtEvent> &applied_events,
                                              const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_output(perform_precondition, trace_info,
                            true/*track*/, applied_events);
      if (!copy_events.empty())
        return Runtime::merge_events(&trace_info, copy_events);
      else
        return ApEvent::NO_AP_EVENT;
    }

    //--------------------------------------------------------------------------
    /*static*/ void CopyAcrossAnalysis::handle_remote_copies_across(
                 Deserializer &derez, Runtime *runtime, AddressSpaceID previous)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      AddressSpaceID original_source;
      derez.deserialize(original_source);
      size_t num_eq_sets;
      derez.deserialize(num_eq_sets);
      std::set<RtEvent> ready_events;
      std::vector<EquivalenceSet*> eq_sets(num_eq_sets, NULL);
      LegionVector<FieldMask> eq_masks(num_eq_sets);
      FieldMask src_mask;
      for (unsigned idx = 0; idx < num_eq_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        eq_sets[idx] = runtime->find_or_request_equivalence_set(did, ready); 
        if (ready.exists())
          ready_events.insert(ready);
        derez.deserialize(eq_masks[idx]);
        src_mask |= eq_masks[idx];
      }
      RemoteOp *op = 
        RemoteOp::unpack_remote_operation(derez, runtime, ready_events);
      unsigned src_index, dst_index;
      derez.deserialize(src_index);
      derez.deserialize(dst_index);
      RegionUsage src_usage, dst_usage;
      derez.deserialize(src_usage);
      derez.deserialize(dst_usage);
      ApEvent dst_ready;
      derez.deserialize(dst_ready);
      size_t num_dsts;
      derez.deserialize(num_dsts);
      std::vector<PhysicalManager*> dst_instances(num_dsts);
      LegionVector<FieldMaskSet<InstanceView> > dst_views(num_dsts);
      for (unsigned idx1 = 0; idx1 < num_dsts; idx1++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        dst_instances[idx1] = 
          runtime->find_or_request_instance_manager(did, ready);
        if (ready.exists())
          ready_events.insert(ready);
        size_t num_views;
        derez.deserialize(num_views);
        for (unsigned idx2 = 0; idx2 < num_views; idx2++)
        {
          derez.deserialize(did);
          LogicalView *view = runtime->find_or_request_logical_view(did, ready);
          if (ready.exists())
            ready_events.insert(ready);
          FieldMask mask;
          derez.deserialize(mask);
          dst_views[idx1].insert(static_cast<InstanceView*>(view), mask);
        }
      }
      size_t num_srcs;
      derez.deserialize(num_srcs);
      std::vector<IndividualView*> src_views(num_srcs, NULL);
      for (unsigned idx = 0; idx < num_srcs; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        LogicalView *view = runtime->find_or_request_logical_view(did, ready);
        src_views[idx] = static_cast<IndividualView*>(view);
        if (ready.exists())
          ready_events.insert(ready);
      }
      LogicalRegion src_handle, dst_handle;
      derez.deserialize(src_handle);
      derez.deserialize(dst_handle);
      PredEvent pred_guard;
      derez.deserialize(pred_guard);
      ApEvent precondition;
      derez.deserialize(precondition);
      ReductionOpID redop;
      derez.deserialize(redop);
      bool perfect;
      derez.deserialize(perfect);
      std::vector<unsigned> src_indexes, dst_indexes;
      if (!perfect)
      {
        size_t num_indexes;
        derez.deserialize(num_indexes);
        src_indexes.resize(num_indexes);
        dst_indexes.resize(num_indexes);
        for (unsigned idx = 0; idx < num_indexes; idx++)
        {
          derez.deserialize(src_indexes[idx]);
          derez.deserialize(dst_indexes[idx]);
        }
      }
      RtUserEvent applied;
      derez.deserialize(applied);
      ApUserEvent copy;
      derez.deserialize(copy);
      const PhysicalTraceInfo trace_info =
        PhysicalTraceInfo::unpack_trace_info(derez, runtime);

      std::vector<CopyAcrossHelper*> across_helpers;
      std::set<RtEvent> deferral_events, applied_events;
      RegionNode *dst_node = runtime->forest->get_node(dst_handle);
      IndexSpaceExpression *dst_expr = dst_node->row_source;
      // Make sure that all our pointers are ready
      RtEvent ready_event;
      if (!ready_events.empty())
        ready_event = Runtime::merge_events(ready_events);
      // If we're not perfect we need to wait on the ready event here
      // because we need the dst_instances to be valid to construct
      // the copy-across helpers
      if (!perfect && ready_event.exists() && !ready_event.has_triggered())
        ready_event.wait();
      // This takes ownership of the op and the across helpers
      CopyAcrossAnalysis *analysis = new CopyAcrossAnalysis(runtime, 
          original_source, previous, op, src_index, dst_index,
          src_usage, dst_usage, src_handle, dst_handle, dst_ready,
          std::move(dst_instances), std::move(dst_views),
          std::move(src_views), precondition, pred_guard,
          redop, src_indexes, dst_indexes, trace_info, perfect);
      analysis->add_reference();
      for (unsigned idx = 0; idx < eq_sets.size(); idx++)
        analysis->analyze(eq_sets[idx], eq_masks[idx], deferral_events,
                          applied_events, ready_event);
      const RtEvent traversal_done = deferral_events.empty() ?
        RtEvent::NO_RT_EVENT : Runtime::merge_events(deferral_events);
      // Start with the source mask here in case we need to filter which
      // is all done on the source fields
      analysis->local_exprs.insert(dst_expr, src_mask);
      RtEvent remote_ready;
      if (traversal_done.exists() || analysis->has_remote_sets())
        remote_ready = analysis->perform_remote(traversal_done, applied_events);
      RtEvent updates_ready;
      // Chain these so we get the local_exprs set correct
      if (remote_ready.exists() || analysis->has_across_updates())
        updates_ready = 
          analysis->perform_updates(remote_ready, applied_events); 
      const ApEvent result = 
        analysis->perform_output(updates_ready, applied_events);
      Runtime::trigger_event(&trace_info, copy, result);
      // Now we can trigger our applied event
      if (!applied_events.empty())
        Runtime::trigger_event(applied, Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(applied);
      // Clean up our analysis
      if (analysis->remove_reference())
        delete analysis;
    }

    //--------------------------------------------------------------------------
    /*static*/ std::vector<CopyAcrossHelper*> 
                          CopyAcrossAnalysis::create_across_helpers(
                                       const FieldMask &src_mask,
                                       const FieldMask &dst_mask,
                             const std::vector<PhysicalManager*> &dst_instances,
                                       const std::vector<unsigned> &src_indexes,
                                       const std::vector<unsigned> &dst_indexes)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!dst_instances.empty());
#endif
      std::vector<CopyAcrossHelper*> result(dst_instances.size());
      for (unsigned idx = 0; idx < dst_instances.size(); idx++)
      {
        result[idx] = new CopyAcrossHelper(src_mask, src_indexes, dst_indexes);
        dst_instances[idx]->initialize_across_helper(result[idx],
                              dst_mask, src_indexes, dst_indexes);
      }
      return result;
    }

    //--------------------------------------------------------------------------
    /*static*/ std::vector<PhysicalManager*>
             CopyAcrossAnalysis::convert_instances(const InstanceSet &instances)
    //--------------------------------------------------------------------------
    {
      std::vector<PhysicalManager*> result(instances.size());
      for (unsigned idx = 0; idx < instances.size(); idx++)
        result[idx] = instances[idx].get_physical_manager();
      return result;
    }

    /////////////////////////////////////////////////////////////
    // Overwrite Analysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    OverwriteAnalysis::OverwriteAnalysis(Runtime *rt, Operation *o, 
                        unsigned idx, const RegionUsage &use,
                        IndexSpaceExpression *expr, LogicalView *view, 
                        const FieldMask &mask, const PhysicalTraceInfo &t_info,
                        CollectiveMapping *mapping, const ApEvent pre,
                        const PredEvent true_g, const PredEvent false_g,
                        const bool restriction, const bool first_local)
      : PhysicalAnalysis(rt, o, idx, expr, true/*on heap*/, false/*immutable*/,
                         true/*exclusive*/, mapping, first_local),
        usage(use), trace_info(t_info), precondition(pre), true_guard(true_g),
        false_guard(false_g), add_restriction(restriction),
        output_aggregator(NULL)
    //--------------------------------------------------------------------------
    {
      if (view != NULL)
      {
        if (view->is_reduction_kind())
          reduction_views.insert(view->as_instance_view(), mask);
        else
          views.insert(view, mask);
      }
    }

    //--------------------------------------------------------------------------
    OverwriteAnalysis::OverwriteAnalysis(Runtime *rt, Operation *o, 
                        unsigned idx, const RegionUsage &use,
                        IndexSpaceExpression *expr,
                        const PhysicalTraceInfo &t_info,
                        const ApEvent pre, const bool restriction)
      : PhysicalAnalysis(rt, o, idx, expr, true/*on heap*/, false/*immutable*/,
                         true/*exclusive*/), usage(use), trace_info(t_info),
        precondition(pre), add_restriction(restriction),
        output_aggregator(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    OverwriteAnalysis::OverwriteAnalysis(Runtime *rt, Operation *o, 
                        unsigned idx, const RegionUsage &use, 
                        IndexSpaceExpression *expr,
                        const FieldMaskSet<LogicalView> &overwrite_views,
                        const PhysicalTraceInfo &t_info,
                        const ApEvent pre, const bool restriction)
      : PhysicalAnalysis(rt, o, idx, expr, true/*on heap*/, false/*immutable*/,
          true/*exclusive*/), usage(use), trace_info(t_info), precondition(pre),
        add_restriction(restriction), output_aggregator(NULL)
    //--------------------------------------------------------------------------
    {
      for (FieldMaskSet<LogicalView>::const_iterator it =
            overwrite_views.begin(); it != overwrite_views.end(); it++)
      {
        if (it->first->is_reduction_kind())
          reduction_views.insert(it->first->as_instance_view(), it->second);
        else
          views.insert(it->first, it->second);
      }
    }

    //--------------------------------------------------------------------------
    OverwriteAnalysis::OverwriteAnalysis(Runtime *rt, AddressSpaceID src, 
                        AddressSpaceID prev, Operation *o, unsigned idx, 
                        IndexSpaceExpression *expr, const RegionUsage &use,
                        FieldMaskSet<LogicalView> &vws,
                        FieldMaskSet<InstanceView> &reductions,
                        const PhysicalTraceInfo &t_info,
                        const ApEvent pre, const PredEvent true_g,
                        const PredEvent false_g, CollectiveMapping *mapping,
                        const bool first_local, const bool restriction)
      : PhysicalAnalysis(rt, src, prev, o, idx, expr, true/*on heap*/,
          false/*immutable*/, mapping, true/*exclusive*/, first_local),
        usage(use), trace_info(t_info), views(vws, true/*copy*/),
        reduction_views(reductions, true/*copy*/),
        precondition(pre), true_guard(true_g), false_guard(false_g),
        add_restriction(restriction), output_aggregator(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    OverwriteAnalysis::~OverwriteAnalysis(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RtEvent OverwriteAnalysis::convert_views(LogicalRegion region,
                            const InstanceSet &targets, unsigned analysis_index)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(targets.size() == 1);
#endif
      target_instances.resize(targets.size());
      for (unsigned idx = 0; idx < targets.size(); idx++)
        target_instances[idx] = targets[idx].get_physical_manager();
      InnerContext *context = op->find_physical_context(index);
      if (op->perform_collective_analysis(collective_mapping,
                                          collective_first_local))
      {
        if (collective_mapping != NULL)
        {
          std::vector<IndividualView*> indiv(targets.size());
          context->convert_individual_views(targets, indiv, collective_mapping);
          target_views.resize(indiv.size());
          for (unsigned idx = 0; idx < indiv.size(); idx++)
            target_views[idx].insert(indiv[idx],
                targets[idx].get_valid_fields());
        }
        else
          return op->convert_collective_views(index, analysis_index,
                    region, targets, context, collective_mapping,
                    collective_first_local, target_views, collective_arrivals);
      }
      else
        context->convert_analysis_views(targets, target_views);
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    RtEvent OverwriteAnalysis::perform_traversal(RtEvent precondition,
            const VersionInfo &version_info, std::set<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      if (precondition.exists() && !precondition.has_triggered())
        return defer_traversal(precondition, version_info, applied_events);
      if (!target_views.empty())
      {
        for (FieldMaskSet<InstanceView>::const_iterator it =
              target_views[0].begin(); it != target_views[0].end(); it++)
        {
          if (it->first->is_reduction_view())
            reduction_views.insert(it->first->as_reduction_view(), it->second);
          else
            views.insert(it->first, it->second);
        }
      }
      return PhysicalAnalysis::perform_traversal(precondition,
                                version_info, applied_events);
    }

    //--------------------------------------------------------------------------
    bool OverwriteAnalysis::perform_analysis(EquivalenceSet *set,
                                             IndexSpaceExpression *expr,
                                             const bool expr_covers,
                                             const FieldMask &mask,
                                             std::set<RtEvent> &deferral_events,
                                             std::set<RtEvent> &applied_events,
                                             const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      set->overwrite_set(*this, expr, expr_covers, mask, deferral_events, 
                         applied_events, already_deferred);
      // Perform a check for migration after this
      return true;
    }

    //--------------------------------------------------------------------------
    RtEvent OverwriteAnalysis::perform_remote(RtEvent perform_precondition,
                                              std::set<RtEvent> &applied_events,
                                              const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_remote(perform_precondition, applied_events);
      // If there are no sets we're done
      if (remote_sets.empty())
        return RtEvent::NO_RT_EVENT;
      for (LegionMap<AddressSpaceID,
                     FieldMaskSet<EquivalenceSet> >::const_iterator 
            rit = remote_sets.begin(); rit != remote_sets.end(); rit++)
      {
#ifdef DEBUG_LEGION
        assert(!rit->second.empty());
#endif
        const AddressSpace target = rit->first;
        const RtUserEvent applied = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(original_source);
          rez.serialize<size_t>(rit->second.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                rit->second.begin(); it != rit->second.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          analysis_expr->pack_expression(rez, target);
          op->pack_remote_operation(rez, target, applied_events);
          rez.serialize(index);
          rez.serialize(usage);
          rez.serialize<size_t>(views.size());
          if (!views.empty())
          {
            for (FieldMaskSet<LogicalView>::const_iterator it =
                  views.begin(); it != views.end(); it++)
            {
              rez.serialize(it->first->did);  
              rez.serialize(it->second);
            }
          }
          rez.serialize<size_t>(reduction_views.size());
          if (!reduction_views.empty())
          {
            for (FieldMaskSet<InstanceView>::const_iterator it =
                  reduction_views.begin(); it != reduction_views.end(); it++)
            {
              rez.serialize(it->first->did);  
              rez.serialize(it->second);
            }
          }
          trace_info.pack_trace_info(rez, applied_events);
          rez.serialize(true_guard);
          rez.serialize(false_guard);
          rez.serialize(precondition);
          rez.serialize<bool>(add_restriction);
          // We only need to pack the collective mapping once when going
          // from the origin space to the next space
          CollectiveMapping *mapping = get_replicated_mapping();
          if ((mapping != NULL) && (original_source == runtime->address_space))
          {
            mapping->pack(rez);
            rez.serialize<bool>(is_collective_first_local());
          }
          else
            rez.serialize<size_t>(0);
          rez.serialize(applied);
        }
        runtime->send_equivalence_set_remote_overwrites(target, rez);
        applied_events.insert(applied);
      }
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    RtEvent OverwriteAnalysis::perform_registration(RtEvent precondition,
                                           const RegionUsage &usage,
                                           std::set<RtEvent> &applied_events,
                                           ApEvent init_precondition,
                                           ApEvent termination,
                                           ApEvent &instances_ready,
                                           bool symbolic)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(termination.exists());
      assert(!trace_info.recording);
#endif
      if (precondition.exists() && !precondition.has_triggered())
        return defer_registration(precondition, usage, applied_events,
         trace_info, init_precondition, termination, instances_ready, symbolic);
      const UniqueID op_id = op->get_unique_op_id();
      const size_t op_ctx_index = op->get_ctx_index();
      const AddressSpaceID local_space = runtime->address_space;
#ifdef DEBUG_LEGION
      // In this case we know the expression should be a region
      IndexSpaceNode *expr_node = dynamic_cast<IndexSpaceNode*>(analysis_expr);
      assert(expr_node != NULL);
#else
      IndexSpaceNode *expr_node = static_cast<IndexSpaceNode*>(analysis_expr);
#endif
      const IndexSpaceID match_space = expr_node->handle.get_id();
      std::vector<RtEvent> registered_events;
      std::vector<ApEvent> inst_ready_events;
      for (unsigned idx = 0; idx < target_views.size(); idx++)
      {
        for (FieldMaskSet<InstanceView>::const_iterator it =
              target_views[idx].begin(); it != target_views[idx].end(); it++)
        {
          size_t view_collective_arrivals = 0;
          if (!collective_arrivals.empty())
          {
            std::map<InstanceView*,size_t>::const_iterator finder =
              collective_arrivals.find(it->first);
#ifdef DEBUG_LEGION
            assert(finder != collective_arrivals.end()); 
#endif
            view_collective_arrivals = finder->second;
          }
          const ApEvent ready = it->first->register_user(usage, it->second,
              expr_node, op_id, op_ctx_index, index, match_space, termination,
              target_instances[idx], collective_mapping,
              view_collective_arrivals, registered_events, applied_events,
              trace_info, local_space, symbolic);
          if (ready.exists())
            inst_ready_events.push_back(ready);
        }
      }
      if (!inst_ready_events.empty())
      {
        if ((inst_ready_events.size() > 1) || init_precondition.exists())
        {
          if (init_precondition.exists())
            inst_ready_events.push_back(init_precondition);
          instances_ready = 
            Runtime::merge_events(&trace_info, inst_ready_events);
        }
        else
          instances_ready = inst_ready_events.back();
      }
      else
        instances_ready = init_precondition;
      if (!registered_events.empty())
        return Runtime::merge_events(registered_events);
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    ApEvent OverwriteAnalysis::perform_output(RtEvent perform_precondition,
                                              std::set<RtEvent> &applied_events,
                                              const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_output(perform_precondition, trace_info,
                            false/*track*/, applied_events);
      if (output_aggregator != NULL)
      {
#ifdef DEBUG_LEGION
        assert(output_aggregator->track_events);
#endif
        const ApEvent effect =
          output_aggregator->issue_updates(trace_info, precondition,
                                           true/*restricted output*/);
        if (effect.exists())
          op->record_completion_effect(effect, applied_events);
        if (output_aggregator->effects_applied.has_triggered())
          applied_events.insert(output_aggregator->effects_applied);
        if (output_aggregator->release_guards(op->runtime, applied_events))
          delete output_aggregator;
      }
      return ApEvent::NO_AP_EVENT;
    }

    //--------------------------------------------------------------------------
    /*static*/ void OverwriteAnalysis::handle_remote_overwrites(
                 Deserializer &derez, Runtime *runtime, AddressSpaceID previous)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      AddressSpaceID original_source;
      derez.deserialize(original_source);
      size_t num_eq_sets;
      derez.deserialize(num_eq_sets);
      std::set<RtEvent> ready_events;
      std::vector<EquivalenceSet*> eq_sets(num_eq_sets, NULL);
      LegionVector<FieldMask> eq_masks(num_eq_sets);
      for (unsigned idx = 0; idx < num_eq_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        eq_sets[idx] = runtime->find_or_request_equivalence_set(did, ready); 
        if (ready.exists())
          ready_events.insert(ready);
        derez.deserialize(eq_masks[idx]);
      }
      IndexSpaceExpression *expr =
        IndexSpaceExpression::unpack_expression(derez,runtime->forest,previous);
      RemoteOp *op = 
        RemoteOp::unpack_remote_operation(derez, runtime, ready_events);
      unsigned index;
      derez.deserialize(index);
      RegionUsage usage;
      derez.deserialize(usage);
      size_t num_views;
      derez.deserialize(num_views);
      FieldMaskSet<LogicalView> views;
      for (unsigned idx = 0; idx < num_views; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        LogicalView *view = runtime->find_or_request_logical_view(did, ready);
        if (ready.exists())
          ready_events.insert(ready);
        FieldMask mask;
        derez.deserialize(mask);
        views.insert(view, mask);
      }
      size_t num_reductions;
      derez.deserialize(num_reductions);
      FieldMaskSet<InstanceView> reductions;
      for (unsigned idx = 0; idx < num_reductions; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        LogicalView *view = runtime->find_or_request_logical_view(did, ready);
        if (ready.exists())
          ready_events.insert(ready);
        FieldMask mask;
        derez.deserialize(mask);
        views.insert(static_cast<InstanceView*>(view), mask);
      }
      const PhysicalTraceInfo trace_info = 
        PhysicalTraceInfo::unpack_trace_info(derez, runtime);
      PredEvent true_guard, false_guard;
      derez.deserialize(true_guard);
      derez.deserialize(false_guard);
      ApEvent precondition;
      derez.deserialize(precondition);
      bool add_restriction;
      derez.deserialize(add_restriction);
      bool first_local = true;
      size_t collective_mapping_size;
      derez.deserialize(collective_mapping_size);
      CollectiveMapping *collective_mapping = ((collective_mapping_size) > 0) ?
        new CollectiveMapping(derez, collective_mapping_size) : NULL;
      if (collective_mapping != NULL)
        derez.deserialize<bool>(first_local);
      RtUserEvent applied;
      derez.deserialize(applied);

      // This takes ownership of the operation
      OverwriteAnalysis *analysis = new OverwriteAnalysis(runtime,
          original_source, previous, op, index, expr, usage, views, 
          reductions, trace_info, precondition, true_guard, false_guard,
          collective_mapping, first_local, add_restriction);
      analysis->add_reference();
      std::set<RtEvent> deferral_events, applied_events;
      // Make sure that all our pointers are ready
      RtEvent ready_event;
      if (!ready_events.empty())
        ready_event = Runtime::merge_events(ready_events);
      for (unsigned idx = 0; idx < eq_sets.size(); idx++)
        analysis->analyze(eq_sets[idx], eq_masks[idx], deferral_events, 
                          applied_events, ready_event);
      const RtEvent traversal_done = deferral_events.empty() ?
        RtEvent::NO_RT_EVENT : Runtime::merge_events(deferral_events);
      if (traversal_done.exists() || analysis->has_remote_sets())
        analysis->perform_remote(traversal_done, applied_events);
      if (traversal_done.exists() || analysis->has_output_updates())
        analysis->perform_output(traversal_done, applied_events);
      // Now we can trigger our applied event
      if (!applied_events.empty())
        Runtime::trigger_event(applied, Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(applied);
      if (analysis->remove_reference())
        delete analysis;
    }

    /////////////////////////////////////////////////////////////
    // Filter Analysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    FilterAnalysis::FilterAnalysis(Runtime *rt, Operation *o, unsigned idx,
                              RegionNode *node, const PhysicalTraceInfo &t_info,
                              const bool remove_restrict)
      : RegistrationAnalysis(rt, o, idx, node, true/*on heap*/,
                             t_info, true/*exclusive*/),
        remove_restriction(remove_restrict)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    FilterAnalysis::FilterAnalysis(Runtime *rt, AddressSpaceID src, 
                              AddressSpaceID prev, Operation *o, unsigned idx, 
                              RegionNode *node, const PhysicalTraceInfo &t_info,
                              const FieldMaskSet<InstanceView> &views,
                              CollectiveMapping *mapping, const bool first,
                              const bool remove_restrict)
      : RegistrationAnalysis(rt, src, prev, o, idx, node, true/*on heap*/,
          t_info, mapping, first, true/*exclusive*/),
        filter_views(views), remove_restriction(remove_restrict)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    FilterAnalysis::~FilterAnalysis(void)
    //--------------------------------------------------------------------------
    {
      // If we're "remote" then unpack the references we sent
      if ((runtime->address_space != original_source) ||
          (previous != original_source))
      {
        for (FieldMaskSet<InstanceView>::const_iterator it =
              filter_views.begin(); it != filter_views.end(); it++)
          it->first->unpack_global_ref();
      }
    }

    //--------------------------------------------------------------------------
    RtEvent FilterAnalysis::perform_traversal(RtEvent precondition,
                     const VersionInfo &info, std::set<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      if (precondition.exists() && !precondition.has_triggered())
        return defer_traversal(precondition, info, applied_events);
      if (!target_views.empty())
      {
#ifdef DEBUG_LEGION
        assert(target_views.size() == 1);
#endif
        filter_views = target_views.back();
      }
      return RegistrationAnalysis::perform_traversal(precondition,
                                                     info, applied_events);
    }

    //--------------------------------------------------------------------------
    bool FilterAnalysis::perform_analysis(EquivalenceSet *set,
                                          IndexSpaceExpression *expr,
                                          const bool expr_covers,
                                          const FieldMask &mask,
                                          std::set<RtEvent> &deferral_events,
                                          std::set<RtEvent> &applied_events,
                                          const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      set->filter_set(*this, expr, expr_covers, mask, deferral_events, 
                      applied_events, already_deferred);
      // Perform a check for migration after this
      return true;
    }

    //--------------------------------------------------------------------------
    RtEvent FilterAnalysis::perform_remote(RtEvent perform_precondition,
                                           std::set<RtEvent> &applied_events,
                                           const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_remote(perform_precondition, applied_events);
      if (remote_sets.empty())
        return RtEvent::NO_RT_EVENT;
      for (LegionMap<AddressSpaceID,
                     FieldMaskSet<EquivalenceSet> >::const_iterator 
            rit = remote_sets.begin(); rit != remote_sets.end(); rit++)
      {
#ifdef DEBUG_LEGION
        assert(!rit->second.empty());
#endif
        const AddressSpaceID target = rit->first;
        const RtUserEvent applied = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(original_source);
          rez.serialize<size_t>(rit->second.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                rit->second.begin(); it != rit->second.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          rez.serialize(region->handle);
          op->pack_remote_operation(rez, target, applied_events);
          rez.serialize(index);
          rez.serialize<size_t>(filter_views.size());
          for (FieldMaskSet<InstanceView>::const_iterator it =
                filter_views.begin(); it != filter_views.end(); it++)
          {
            it->first->pack_global_ref();
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          rez.serialize(remove_restriction);
          trace_info.pack_trace_info(rez, applied_events);
          // We only need to pack the collective mapping once when going
          // from the origin space to the next space
          CollectiveMapping *mapping = get_replicated_mapping();
          if ((mapping != NULL) && (original_source == runtime->address_space))
          {
            mapping->pack(rez);
            rez.serialize<bool>(is_collective_first_local());
          }
          else
            rez.serialize<size_t>(0);
          rez.serialize(applied);
        }
        runtime->send_equivalence_set_remote_filters(target, rez);
        applied_events.insert(applied);
      }
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    /*static*/ void FilterAnalysis::handle_remote_filters(Deserializer &derez,
                                      Runtime *runtime, AddressSpaceID previous)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      AddressSpaceID original_source;
      derez.deserialize(original_source);
      size_t num_eq_sets;
      derez.deserialize(num_eq_sets);
      std::set<RtEvent> ready_events;
      std::vector<EquivalenceSet*> eq_sets(num_eq_sets, NULL);
      LegionVector<FieldMask> eq_masks(num_eq_sets);
      for (unsigned idx = 0; idx < num_eq_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        eq_sets[idx] = runtime->find_or_request_equivalence_set(did, ready); 
        if (ready.exists())
          ready_events.insert(ready);
        derez.deserialize(eq_masks[idx]);
      }
      LogicalRegion handle;
      derez.deserialize(handle);
      RegionNode *region = runtime->forest->get_node(handle);
      RemoteOp *op = 
        RemoteOp::unpack_remote_operation(derez, runtime, ready_events);
      unsigned index;
      derez.deserialize(index);
      FieldMaskSet<InstanceView> filter_views;
      size_t num_views;
      derez.deserialize(num_views);
      for (unsigned idx = 0; idx < num_views; idx++)
      {
        DistributedID view_did;
        derez.deserialize(view_did);
        RtEvent view_ready;
        InstanceView *inst_view = static_cast<InstanceView*>(
          runtime->find_or_request_logical_view(view_did, view_ready));
        FieldMask mask;
        derez.deserialize(mask);
        filter_views.insert(inst_view, mask);
        if (view_ready.exists())
          ready_events.insert(view_ready);
      }
      bool remove_restriction;
      derez.deserialize(remove_restriction);
      const PhysicalTraceInfo trace_info = 
        PhysicalTraceInfo::unpack_trace_info(derez, runtime);
      bool first_local = true;
      size_t collective_mapping_size;
      derez.deserialize(collective_mapping_size);
      CollectiveMapping *collective_mapping = ((collective_mapping_size) > 0) ?
        new CollectiveMapping(derez, collective_mapping_size) : NULL;
      if (collective_mapping != NULL)
        derez.deserialize<bool>(first_local);
      RtUserEvent applied;
      derez.deserialize(applied);

      // This takes ownership of the remote operation
      FilterAnalysis *analysis = new FilterAnalysis(runtime, original_source,
          previous, op, index, region, trace_info, filter_views, 
          collective_mapping, first_local, remove_restriction);
      analysis->add_reference();
      std::set<RtEvent> deferral_events, applied_events;
      // Make sure that all our pointers are ready
      RtEvent ready_event;
      if (!ready_events.empty())
        ready_event = Runtime::merge_events(ready_events);
      for (unsigned idx = 0; idx < eq_sets.size(); idx++)
        analysis->analyze(eq_sets[idx], eq_masks[idx], deferral_events, 
                          applied_events, ready_event);
      const RtEvent traversal_done = deferral_events.empty() ?
        RtEvent::NO_RT_EVENT : Runtime::merge_events(deferral_events);
      RtEvent remote_ready;
      if (traversal_done.exists() || analysis->has_remote_sets())     
        analysis->perform_remote(traversal_done, applied_events);
      // Now we can trigger our applied event
      if (!applied_events.empty())
        Runtime::trigger_event(applied, Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(applied);
      if (analysis->remove_reference())
        delete analysis;
    }

    /////////////////////////////////////////////////////////////
    // Clone Analysis
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    CloneAnalysis::CloneAnalysis(Runtime *rt, IndexSpaceExpression *expr,
               Operation *op, unsigned idx, FieldMaskSet<EquivalenceSet> &&srcs)
      : PhysicalAnalysis(rt, op, idx, expr, true/*on heap*/, false/*immutable*/,
                         true/*exclusive*/), sources(std::move(srcs))
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    CloneAnalysis::CloneAnalysis(Runtime *rt, AddressSpaceID src,
                                 AddressSpaceID prev,IndexSpaceExpression *expr,
                                 Operation *op, unsigned idx,
                                 FieldMaskSet<EquivalenceSet> &&srcs)
      : PhysicalAnalysis(rt, src, prev, op, idx, expr, true/*on heap*/),
        sources(std::move(srcs))
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    CloneAnalysis::~CloneAnalysis(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    bool CloneAnalysis::perform_analysis(EquivalenceSet *set,
                                         IndexSpaceExpression *expr,
                                         const bool expr_covers,
                                         const FieldMask &mask,
                                         std::set<RtEvent> &deferral_events,
                                         std::set<RtEvent> &applied_events,
                                         const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      std::vector<RtEvent> applied;
      set->clone_set(*this, expr, expr_covers, mask, deferral_events,
                     applied, already_deferred);
      if (!applied.empty())
        applied_events.insert(applied.begin(), applied.end());
      // No check for migration after this
      return false;
    }

    //--------------------------------------------------------------------------
    RtEvent CloneAnalysis::perform_remote(RtEvent perform_precondition,
                                          std::set<RtEvent> &applied_events,
                                          const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      if (perform_precondition.exists() && 
          !perform_precondition.has_triggered())
        return defer_remote(perform_precondition, applied_events);
      // If there are no sets we're done
      if (remote_sets.empty())
        return RtEvent::NO_RT_EVENT;
      for (LegionMap<AddressSpaceID,
                     FieldMaskSet<EquivalenceSet> >::const_iterator 
            rit = remote_sets.begin(); rit != remote_sets.end(); rit++)
      {
#ifdef DEBUG_LEGION
        assert(!rit->second.empty());
#endif
        const AddressSpace target = rit->first;
        const RtUserEvent applied = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(original_source);
          rez.serialize<size_t>(rit->second.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                rit->second.begin(); it != rit->second.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          analysis_expr->pack_expression(rez, target);
          op->pack_remote_operation(rez, target, applied_events);
          rez.serialize(index);
          rez.serialize<size_t>(sources.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                sources.begin(); it != sources.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          rez.serialize(applied);
        }
        runtime->send_equivalence_set_remote_clones(target, rez);
        applied_events.insert(applied);
      }
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    /*static*/ void CloneAnalysis::handle_remote_clones(Deserializer &derez,
                                      Runtime *runtime, AddressSpaceID previous)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      AddressSpaceID original_source;
      derez.deserialize(original_source);
      size_t num_eq_sets;
      derez.deserialize(num_eq_sets);
      std::set<RtEvent> ready_events;
      std::vector<EquivalenceSet*> eq_sets(num_eq_sets, NULL);
      LegionVector<FieldMask> eq_masks(num_eq_sets);
      for (unsigned idx = 0; idx < num_eq_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        eq_sets[idx] = runtime->find_or_request_equivalence_set(did, ready); 
        if (ready.exists())
          ready_events.insert(ready);
        derez.deserialize(eq_masks[idx]);
      }
      IndexSpaceExpression *expr =
        IndexSpaceExpression::unpack_expression(derez,runtime->forest,previous);
      RemoteOp *op = 
        RemoteOp::unpack_remote_operation(derez, runtime, ready_events);
      unsigned index;
      derez.deserialize(index);
      size_t num_sources;
      derez.deserialize(num_sources);
      FieldMaskSet<EquivalenceSet> sources;
      for (unsigned idx = 0; idx < num_sources; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        EquivalenceSet *set = 
          runtime->find_or_request_equivalence_set(did, ready); 
        if (ready.exists())
          ready_events.insert(ready);
        FieldMask mask;
        derez.deserialize(mask);
        sources.insert(set, mask);
      }
      RtUserEvent applied;
      derez.deserialize(applied);

      // This takes ownership of the operation
      CloneAnalysis *analysis = new CloneAnalysis(runtime, original_source,
                            previous, expr, op, index, std::move(sources));
      analysis->add_reference();
      std::set<RtEvent> deferral_events, applied_events;
      // Make sure that all our pointers are ready
      RtEvent ready_event;
      if (!ready_events.empty())
        ready_event = Runtime::merge_events(ready_events);
      for (unsigned idx = 0; idx < eq_sets.size(); idx++)
        analysis->analyze(eq_sets[idx], eq_masks[idx], deferral_events, 
                          applied_events, ready_event);
      const RtEvent traversal_done = deferral_events.empty() ?
        RtEvent::NO_RT_EVENT : Runtime::merge_events(deferral_events);
      if (traversal_done.exists() || analysis->has_remote_sets())
        analysis->perform_remote(traversal_done, applied_events);
      // Now we can trigger our applied event
      if (!applied_events.empty())
        Runtime::trigger_event(applied, Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(applied);
      if (analysis->remove_reference())
        delete analysis;
    }

    /////////////////////////////////////////////////////////////
    // CollectiveRefinementTree
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    template<typename T>
    CollectiveRefinementTree<T>::CollectiveRefinementTree(CollectiveView *c)
      : collective(c)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    template<typename T>
    CollectiveRefinementTree<T>::CollectiveRefinementTree(
                                              std::vector<DistributedID> &&dids)
      : collective(NULL), inst_dids(dids)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    template<typename T>
    CollectiveRefinementTree<T>::~CollectiveRefinementTree(void)
    //--------------------------------------------------------------------------
    {
      for (typename FieldMaskSet<T>::const_iterator it =
            refinements.begin(); it != refinements.end(); it++)
        delete it->first;
    }

    //--------------------------------------------------------------------------
    template<typename T>
    const std::vector<DistributedID>& 
                           CollectiveRefinementTree<T>::get_instances(void) const
    //--------------------------------------------------------------------------
    {
      if (collective != NULL)
        return collective->instances;
      else
        return inst_dids;
    }

    //--------------------------------------------------------------------------
    template<typename T> template<typename... Args>
    void CollectiveRefinementTree<T>::traverse(InstanceView *view, 
                                          const FieldMask &mask, Args&&... args)
    //--------------------------------------------------------------------------
    {
      // Check first for any instance overlap first
      const std::vector<DistributedID> &local_dids = get_instances();
      if (view->is_individual_view())
      {
        const DistributedID inst_did = 
          view->as_individual_view()->get_manager()->did; 
        // No instance over means there's nothing to do
        if (!std::binary_search(local_dids.begin(), local_dids.end(), inst_did))
          return;
        // See if we need to be refined
        if (local_dids.size() > 1)
        {
          // Make the refinements for all the fields that aren't already refined
          const FieldMask refinement_mask = mask - get_refinement_mask();
          if (!!refinement_mask)
          {
            std::vector<DistributedID> single_did(1, inst_did);
            refinements.insert(
                static_cast<T*>(this)->clone(view, refinement_mask,
                  std::move(single_did)), refinement_mask);
            std::vector<DistributedID> remainder_dids;
            remainder_dids.reserve(local_dids.size() - 1);
            for (std::vector<DistributedID>::const_iterator it =
                  local_dids.begin(); it != local_dids.end(); it++)
              if (*it != inst_did)
                remainder_dids.push_back(*it);
            refinements.insert(
                static_cast<T*>(this)->clone((InstanceView*)NULL/*no view*/, 
                  refinement_mask, std::move(remainder_dids)), refinement_mask);
          }
        }
      }
      else
      {
        // Compute the intersection first
        CollectiveView *collective_view = view->as_collective_view();
        std::vector<DistributedID> overlap;
        std::vector<DistributedID>::const_iterator first = 
          collective_view->instances.begin();
        std::vector<DistributedID>::const_iterator second = local_dids.begin();
        while ((first != collective_view->instances.end()) && 
               (second != local_dids.end()))
        {
          if (*first < *second) 
            first++;
          else if (*second < *first)
            second++;
          else 
          {
            overlap.push_back(*first);
            first++;
            second++;
          }
        }
        // No instance overlap means we don't even need to be here
        if (overlap.empty())
          return;
        if (overlap.size() != local_dids.size())
        {
          // Make the refinements for all the fields that aren't already refined
          const FieldMask refinement_mask = mask - get_refinement_mask();
          if (!!refinement_mask)
          {
            // compute the difference and make the refinements
            std::vector<DistributedID> remainder; 
            first = local_dids.begin();
            second = collective_view->instances.begin();
            while (first != local_dids.end())
            {
              if ((second == collective_view->instances.end()) || 
                  (*first < *second))
                remainder.push_back(*first++);
              else if (*second < *first)
                second++;
              else
              {
                first++;
                second++;
              }
            }
            refinements.insert(
                static_cast<T*>(this)->clone(
                  (overlap.size() == collective_view->instances.size()) ?
                  collective_view : (InstanceView*)NULL, refinement_mask, 
                  std::move(overlap)), refinement_mask);
            refinements.insert(
                static_cast<T*>(this)->clone((InstanceView*)NULL/*view*/, 
                  refinement_mask, std::move(remainder)), refinement_mask);
          }
        }
      }
      for (typename FieldMaskSet<T>::const_iterator it = 
            refinements.begin(); it != refinements.end(); it++)
      {
        const FieldMask &overlap = mask & it->second;
        if (!overlap)
          continue;
        it->first->traverse(view, overlap, args...);
      }
      const FieldMask local_mask = mask - refinements.get_valid_mask();
      if (!!local_mask)
        static_cast<T*>(this)->analyze(view, local_mask, args...);
    }

    //--------------------------------------------------------------------------
    template<typename T>
    InstanceView* CollectiveRefinementTree<T>::get_instance_view(
                                  InnerContext *context, RegionTreeID tid) const
    //--------------------------------------------------------------------------
    {
      if (collective != NULL)
        return collective;
#ifdef DEBUG_LEGION
      assert(!inst_dids.empty());
#endif
      Runtime *runtime = context->runtime;
      if (inst_dids.size() > 1)
      {
        RtEvent ready;
        InnerContext::CollectiveResult *result =
          context->find_or_create_collective_view(tid, inst_dids, ready);
        // At some point in the future we might want to stop doing
        // all this waiting here and turn these into continuations
        // Wait for the collective did to be ready
        if (ready.exists() && !ready.has_triggered())
          ready.wait();
        // Then wait for the collective view to be registered
        if (result->ready_event.exists() && 
            !result->ready_event.has_triggered())
          result->ready_event.wait();
        InstanceView *view = static_cast<InstanceView*>(
          runtime->find_or_request_logical_view(result->collective_did, ready));
        if (result->remove_reference())
          delete result;
        if (ready.exists() && !ready.has_triggered())
          ready.wait();
        return view;
      }
      else
      {
        RtEvent ready;
        PhysicalManager *manager =
          runtime->find_or_request_instance_manager(inst_dids.back(), ready);
        if (ready.exists() && !ready.has_triggered())
          ready.wait();
        return context->create_instance_top_view(manager,
                                  runtime->address_space);
      }
    }

    //--------------------------------------------------------------------------
    template<typename T>
    void CollectiveRefinementTree<T>::traverse_total(const FieldMask &mask,
                             IndexSpaceExpression *set_expr,
                             const FieldMaskSet<LogicalView> &total_valid_views)
    //--------------------------------------------------------------------------
    {
      if (mask * total_valid_views.get_valid_mask())
        return;
      for (FieldMaskSet<LogicalView>::const_iterator it =
            total_valid_views.begin(); it != total_valid_views.end(); it++)
      {
        if (!it->first->is_instance_view())
          continue;
        const FieldMask overlap = mask & it->second;
        if (!overlap)
          continue;
        InstanceView *inst_view = it->first->as_instance_view(); 
        if (inst_view->aliases(collective))
          traverse(inst_view, overlap, set_expr);
      }
    }

    //--------------------------------------------------------------------------
    template<typename T>
    void CollectiveRefinementTree<T>::traverse_partial(const FieldMask &mask,
        const LegionMap<LogicalView*, 
                     FieldMaskSet<IndexSpaceExpression> > &partial_valid_views)
    //--------------------------------------------------------------------------
    {
      for (LegionMap<LogicalView*,FieldMaskSet<IndexSpaceExpression> >::
            const_iterator it = partial_valid_views.begin(); it != 
            partial_valid_views.end(); it++)
      {
        if (!it->first->is_instance_view())
          continue;
        const FieldMask overlap = mask & it->second.get_valid_mask();
        if (!overlap)
          continue;
        InstanceView *inst_view = it->first->as_instance_view();
        if (inst_view->aliases(collective))
          traverse(inst_view, overlap, it->second);
      }
    }

    /////////////////////////////////////////////////////////////
    // MakeCollectiveValid
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    MakeCollectiveValid::MakeCollectiveValid(CollectiveView *v,
                                const FieldMaskSet<IndexSpaceExpression> &exprs)
      : CollectiveRefinementTree<MakeCollectiveValid>(v), view(v),
        needed_exprs(exprs)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    MakeCollectiveValid::MakeCollectiveValid(
                                std::vector<DistributedID> &&insts,
                                const FieldMaskSet<IndexSpaceExpression> &exprs,
                                const FieldMaskSet<IndexSpaceExpression> &valid,
                                const FieldMask &mask, InstanceView *v)
      : CollectiveRefinementTree<MakeCollectiveValid>(std::move(insts)), 
        view(v), needed_exprs(exprs)
    //--------------------------------------------------------------------------
    {
      analyze(view, mask, valid);
    }

    //--------------------------------------------------------------------------
    MakeCollectiveValid* MakeCollectiveValid::clone(InstanceView *view,
                const FieldMask &mask, std::vector<DistributedID> &&insts) const
    //--------------------------------------------------------------------------
    {
      return new MakeCollectiveValid(std::move(insts), needed_exprs,
                                       valid_exprs, mask, view);
    }

    //--------------------------------------------------------------------------
    void MakeCollectiveValid::analyze(InstanceView *view, 
                              const FieldMask &mask, IndexSpaceExpression *expr)
    //--------------------------------------------------------------------------
    {
      valid_exprs.insert(expr, mask);
    }

    //--------------------------------------------------------------------------
    void MakeCollectiveValid::analyze(InstanceView *view, 
         const FieldMask &mask, const FieldMaskSet<IndexSpaceExpression> &exprs)
    //--------------------------------------------------------------------------
    {
      if (!(exprs.get_valid_mask() - mask))
      {
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
              exprs.begin(); it != exprs.end(); it++)
          valid_exprs.insert(it->first, it->second);
      }
      else
      {
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
              exprs.begin(); it != exprs.end(); it++)
        {
          const FieldMask overlap = mask & it->second;
          if (!overlap)
            continue;
          valid_exprs.insert(it->first, overlap);
        }
      }
    }

    //--------------------------------------------------------------------------
    void MakeCollectiveValid::visit_leaf(const FieldMask &mask,
          InnerContext *context, RegionTreeForest *forest, RegionTreeID tid,
          LegionMap<InstanceView*,FieldMaskSet<IndexSpaceExpression> > &updates)
    //--------------------------------------------------------------------------
    {
      LegionList<FieldSet<IndexSpaceExpression*> > field_sets;
      valid_exprs.compute_field_sets(mask, field_sets);
      InstanceView *local_view = view;
      for (LegionList<FieldSet<IndexSpaceExpression*> >::const_iterator fit =
            field_sets.begin(); fit != field_sets.end(); fit++)
      {
        IndexSpaceExpression *valid_expr = fit->elements.empty() ? NULL :
            (fit->elements.size() == 1) ? *(fit->elements.begin()) :
            forest->union_index_spaces(fit->elements);
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
              needed_exprs.begin(); it != needed_exprs.end(); it++)
        {
          const FieldMask overlap = fit->set_mask & it->second;
          if (!overlap)
            continue;
          IndexSpaceExpression *needed_expr = it->first;
          if (valid_expr != NULL)
            needed_expr = forest->subtract_index_spaces(needed_expr,valid_expr);
          if (!needed_expr->is_empty())
          {
            if (local_view == NULL)
              local_view = get_instance_view(context, tid);
            updates[local_view].insert(needed_expr, overlap);
          }
        }
      }
    }

    // Explicit instantiation for CollectiveRefinementTree
    template class CollectiveRefinementTree<MakeCollectiveValid>;

    /////////////////////////////////////////////////////////////
    // CollectiveAntiAlias
    /////////////////////////////////////////////////////////////
    
    //--------------------------------------------------------------------------
    CollectiveAntiAlias::CollectiveAntiAlias(CollectiveView *v)
      : CollectiveRefinementTree<CollectiveAntiAlias>(v)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    CollectiveAntiAlias::CollectiveAntiAlias(
                                std::vector<DistributedID> &&insts,
                                const FieldMaskSet<IndexSpaceExpression> &valid,
                                const FieldMask &mask)
      : CollectiveRefinementTree<CollectiveAntiAlias>(std::move(insts))
    //--------------------------------------------------------------------------
    {
      analyze((InstanceView*)NULL, mask, valid);
    }

    //--------------------------------------------------------------------------
    CollectiveAntiAlias* CollectiveAntiAlias::clone(InstanceView *view,
                const FieldMask &mask, std::vector<DistributedID> &&insts) const
    //--------------------------------------------------------------------------
    {
      return new CollectiveAntiAlias(std::move(insts), valid_exprs, mask);
    }

    //--------------------------------------------------------------------------
    void CollectiveAntiAlias::analyze(InstanceView *view, 
                              const FieldMask &mask, IndexSpaceExpression *expr)
    //--------------------------------------------------------------------------
    {
      valid_exprs.insert(expr, mask);
    }

    //--------------------------------------------------------------------------
    void CollectiveAntiAlias::analyze(InstanceView *view, 
         const FieldMask &mask, const FieldMaskSet<IndexSpaceExpression> &exprs)
    //--------------------------------------------------------------------------
    {
      if (!(exprs.get_valid_mask() - mask))
      {
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
              exprs.begin(); it != exprs.end(); it++)
          valid_exprs.insert(it->first, it->second);
      }
      else
      {
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
              exprs.begin(); it != exprs.end(); it++)
        {
          const FieldMask overlap = mask & it->second;
          if (!overlap)
            continue;
          valid_exprs.insert(it->first, overlap);
        }
      }
    }

    //--------------------------------------------------------------------------
    void CollectiveAntiAlias::visit_leaf(const FieldMask &mask,
                  FieldMask &allvalid_mask,
                  IndexSpaceExpression *expr, RegionTreeForest *forest,
                  const FieldMaskSet<IndexSpaceExpression> &partial_valid_exprs)
    //--------------------------------------------------------------------------
    {
      if (!allvalid_mask)
        return;
      if (!partial_valid_exprs.empty())
      {
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
             partial_valid_exprs.begin(); it != partial_valid_exprs.end(); it++)
          valid_exprs.insert(it->first, it->second);
      }
      if (!valid_exprs.empty())
      {
        // Sort into field sets, union, and then compare to the expression
        LegionList<FieldSet<IndexSpaceExpression*> > field_sets;
        valid_exprs.compute_field_sets(allvalid_mask, field_sets);
        for (LegionList<FieldSet<IndexSpaceExpression*> >::const_iterator it =
              field_sets.begin(); it != field_sets.end(); it++)
        {
          if (!it->elements.empty())
          {
            IndexSpaceExpression *valid_expr =
                (it->elements.size() == 1) ? *(it->elements.begin()) :
                forest->union_index_spaces(it->elements);
            IndexSpaceExpression *overlap_expr =
                forest->intersect_index_spaces(expr, valid_expr);
            if (overlap_expr->get_volume() < expr->get_volume())
              allvalid_mask -= it->set_mask;
          }
          else
            allvalid_mask -= it->set_mask;
        }
      }
      else
        allvalid_mask.clear();
    }

    //--------------------------------------------------------------------------
    void CollectiveAntiAlias::visit_leaf(const FieldMask &mask, 
                                           FieldMask &allvalid_mask,
                           FieldMaskSet<IndexSpaceExpression> &non_dominated,
                           IndexSpaceExpression *expr, RegionTreeForest *forest)
    //--------------------------------------------------------------------------
    {
      if (!valid_exprs.empty())
      {
        // Sort into field sets, union, and then compare to the expression
        LegionList<FieldSet<IndexSpaceExpression*> > field_sets;
        valid_exprs.compute_field_sets(allvalid_mask, field_sets);
        for (LegionList<FieldSet<IndexSpaceExpression*> >::const_iterator it =
              field_sets.begin(); it != field_sets.end(); it++)
        {
          if (!it->elements.empty())
          {
            IndexSpaceExpression *valid_expr =
                (it->elements.size() == 1) ? *(it->elements.begin()) :
                forest->union_index_spaces(it->elements);
            IndexSpaceExpression *diff_expr =
                forest->subtract_index_spaces(expr, valid_expr);
            if (!diff_expr->is_empty())
            {
              non_dominated.insert(diff_expr, it->set_mask);
              allvalid_mask -= it->set_mask;
            }
          }
          else
            allvalid_mask -= it->set_mask;
        }
      }
      else
        allvalid_mask.clear();
    }

    //--------------------------------------------------------------------------
    void CollectiveAntiAlias::visit_leaf(const FieldMask &mask, 
                                         FieldMask &allvalid_mask,
                                         TraceViewSet &view_set,
                                         FieldMaskSet<InstanceView> &alt_views)
    //--------------------------------------------------------------------------
    {
      if (valid_exprs.empty())
        return;
#ifdef DEBUG_LEGION
      assert(valid_exprs.size() == 1); // should just have one null entry
#endif
      const FieldMask &local_mask = valid_exprs.get_valid_mask();
      allvalid_mask -= local_mask;
      InstanceView *view = view_set.find_instance_view(inst_dids);
      alt_views.insert(view, local_mask);
    }

    // Explicit instantiation for CollectiveRefinementTree
    template class CollectiveRefinementTree<CollectiveAntiAlias>;

    /////////////////////////////////////////////////////////////
    // InitializeCollectiveReduction
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    InitializeCollectiveReduction::InitializeCollectiveReduction(
              AllreduceView *v, RegionTreeForest *f, IndexSpaceExpression *expr)
      : CollectiveRefinementTree<InitializeCollectiveReduction>(v),
        forest(f), needed_expr(expr), view(v)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    InitializeCollectiveReduction::InitializeCollectiveReduction(
          std::vector<DistributedID> &&insts, RegionTreeForest *f,
          IndexSpaceExpression *expr, InstanceView *v,
          const FieldMaskSet<IndexSpaceExpression> &remainders,
          const FieldMask &covered)
      : CollectiveRefinementTree<InitializeCollectiveReduction>(
          std::move(insts)), forest(f), needed_expr(expr), view(v),
        remainder_exprs(remainders), found_covered(covered)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    InitializeCollectiveReduction* InitializeCollectiveReduction::clone(
                                  InstanceView *new_view, const FieldMask &mask,
                                  std::vector<DistributedID> &&insts) const
    //--------------------------------------------------------------------------
    {
      return new InitializeCollectiveReduction(std::move(insts), forest, 
          needed_expr, new_view, remainder_exprs, found_covered);
    }

    //--------------------------------------------------------------------------
    void InitializeCollectiveReduction::analyze(InstanceView *view, 
                              const FieldMask &mask, IndexSpaceExpression *expr)
    //--------------------------------------------------------------------------
    {
      FieldMask remainder_mask = mask - found_covered;
      if (!!remainder_mask)
      {
        FieldMaskSet<IndexSpaceExpression> to_add;
        std::vector<IndexSpaceExpression*> to_delete;
        for (FieldMaskSet<IndexSpaceExpression>::iterator it =
              remainder_exprs.begin(); it != remainder_exprs.end(); it++)
        {
          const FieldMask overlap = remainder_mask & it->second;
          if (!overlap)
            continue;
          if (expr != it->first)
          {
            IndexSpaceExpression *overlap_expr = 
              forest->intersect_index_spaces(it->first, expr);
            const size_t overlap_volume = overlap_expr->get_volume();
            if (overlap_volume < it->first->get_volume())
            {
              if (overlap_volume > 0)
              {
                IndexSpaceExpression *diff_expr =
                  forest->subtract_index_spaces(it->first, expr);
                to_add.insert(diff_expr, overlap);
                it.filter(overlap);
                if (!it->second)
                  to_delete.push_back(it->first);
              }
            }
            else
            {
              found_covered |= overlap;
              it.filter(overlap);
              if (!it->second)
                to_delete.push_back(it->first);
            }
          }
          else
          {
            found_covered |= overlap;
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
          }
          remainder_mask -= overlap;
          if (!remainder_mask)
            break;
        }
        if (!to_delete.empty())
          for (std::vector<IndexSpaceExpression*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
            remainder_exprs.erase(*it);
        if (!to_add.empty())
        {
          if (!remainder_exprs.empty())
          {
            for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                  to_add.begin(); it != to_add.end(); it++)
              remainder_exprs.insert(it->first, it->second);
          }
          else
            remainder_exprs.swap(to_add);
        }
        if (!!remainder_mask)
        {
          if (expr != needed_expr)
          {
            IndexSpaceExpression *overlap_expr = 
              forest->intersect_index_spaces(needed_expr, expr);
            const size_t overlap_volume = overlap_expr->get_volume();
            if (overlap_volume < needed_expr->get_volume())
            {
              if (overlap_volume > 0)
              {
                IndexSpaceExpression *diff_expr =
                  forest->subtract_index_spaces(needed_expr, expr);
                remainder_exprs.insert(diff_expr, remainder_mask);
              }
            }
            else
              found_covered |= mask;
          }
          else
            found_covered |= mask;
        }
      }
    }

    //--------------------------------------------------------------------------
    void InitializeCollectiveReduction::analyze(InstanceView *view, 
         const FieldMask &mask, const FieldMaskSet<IndexSpaceExpression> &exprs)
    //--------------------------------------------------------------------------
    {
      // This method should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    void InitializeCollectiveReduction::visit_leaf(const FieldMask &mask,
                                      IndexSpaceExpression *expr, bool &failure)
    //--------------------------------------------------------------------------
    {
      if (mask * found_covered)
      {
        if (!(mask * remainder_exprs.get_valid_mask()))
        {
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                remainder_exprs.begin(); it != remainder_exprs.end(); it++)
          {
            const FieldMask overlap = mask & it->second;
            if (!overlap)
              continue;
            if (expr != it->first)
            {
              IndexSpaceExpression *expr_overlap = 
                forest->intersect_index_spaces(expr, it->first);
              // If it's not completely contained within the bounds of 
              // un-filled remainder then that is a failure of the ABA
              if (expr_overlap->get_volume() < expr->get_volume())
                failure = true;
            }
          }
        }
      }
      else
        failure = true;
    }

    //--------------------------------------------------------------------------
    void InitializeCollectiveReduction::visit_leaf(const FieldMask &mask,
      InnerContext *context, UpdateAnalysis &analysis,
      CopyFillAggregator *&fill_aggregator, 
      FillView *fill_view, RegionTreeID tid, EquivalenceSet *eq_set,
      DistributedID did, std::map<unsigned,std::list<
        std::pair<InstanceView*,IndexSpaceExpression*> > > &reduction_instances)
    //--------------------------------------------------------------------------
    {
      FieldMask uncovered = mask - found_covered;
      if (!!uncovered)
      {
        InstanceView *local_view = view;
        if (local_view == NULL)
          local_view = get_instance_view(context, tid);
        if (fill_aggregator == NULL)
        {
          // Fill aggregators never need to wait for any other
          // aggregators since we know they won't depend on each other
          fill_aggregator = new CopyFillAggregator(forest, &analysis, 
              NULL/*no previous guard*/, false/*track events*/);
          analysis.input_aggregators[RtEvent::NO_RT_EVENT] = fill_aggregator;
        }
        // Issue fills for any remainders
        if (!remainder_exprs.empty())
        {
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                remainder_exprs.begin(); it != remainder_exprs.end(); it++)
          {
            fill_aggregator->record_fill(local_view, fill_view,
                it->second, it->first, PredEvent::NO_PRED_EVENT,
                analysis.trace_info.recording ? eq_set : NULL);
            // Record any reduction instances here as well
            int fidx = it->second.find_first_set();
            while (fidx >= 0)
            {
              local_view->add_nested_valid_ref(did);
              it->first->add_nested_expression_reference(did);
              reduction_instances[fidx].push_back(
                  std::make_pair(local_view, it->first));
              fidx = it->second.find_next_set(fidx+1);
            }
          }
          uncovered -= remainder_exprs.get_valid_mask();
        }
        if (!!uncovered)
        {
          // Issue a fill for the full needed_expr for these fields
          fill_aggregator->record_fill(local_view, fill_view,
                uncovered, needed_expr, PredEvent::NO_PRED_EVENT,
                analysis.trace_info.recording ? eq_set : NULL);
          // Record any reduction instances here as well
          int fidx = uncovered.find_first_set();
          while (fidx >= 0)
          {
            local_view->add_nested_valid_ref(did);
            needed_expr->add_nested_expression_reference(did);
            reduction_instances[fidx].push_back(
                std::make_pair(local_view, needed_expr));
            fidx = uncovered.find_next_set(fidx+1);
          }
        }
      }
    }

    // Explicit instantiation for CollectiveRefinementTree
    template class CollectiveRefinementTree<InitializeCollectiveReduction>;

    /////////////////////////////////////////////////////////////
    // Equivalence Set
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    EquivalenceSet::EquivalenceSet(Runtime *rt, DistributedID id,
                                   AddressSpaceID logical,
                                   IndexSpaceExpression *expr,
                                   RegionTreeID tid, InnerContext *ctx,
                                   bool reg_now,
                                   CollectiveMapping *mapping /*= NULL*/)
      : DistributedCollectable(rt,
          LEGION_DISTRIBUTED_HELP_ENCODE(id, EQUIVALENCE_SET_DC),
          reg_now, mapping), context(ctx), set_expr(expr), tree_id(tid),
        tracing_preconditions(NULL), tracing_anticonditions(NULL),
        tracing_postconditions(NULL), logical_owner_space(logical),
        replicated_owner_state(NULL), migration_index(0), sample_count(0)
    //--------------------------------------------------------------------------
    {
      context->add_nested_resource_ref(did);
      context->add_nested_gc_ref(did);
      set_expr->add_nested_expression_reference(did);
      next_deferral_precondition.store(0);
      // If we have a collective mapping then we know that everyone agrees
      // on who the current logical owner is
      if ((mapping != NULL) && mapping->contains(local_space))
      {
        replicated_owner_state = new ReplicatedOwnerState(true/*valid*/);
        mapping->get_children(owner_space, local_space,
                              replicated_owner_state->children);
      }
#ifdef LEGION_GC
      log_garbage.info("GC Equivalence Set %lld %d",
          LEGION_DISTRIBUTED_ID_FILTER(this->did), local_space);
#endif
#ifdef LEGION_SPY_EQUIVALENCE_SETS
      if (is_logical_owner())
        LegionSpy::log_equivalence_set(did, expr->expr_id, tree_id, 
                                       implicit_provenance);
#endif
    }

    //--------------------------------------------------------------------------
    EquivalenceSet::~EquivalenceSet(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(total_valid_instances.empty());
      assert(partial_valid_instances.empty());
      assert(!partial_valid_fields);
      assert(initialized_data.empty());
      assert(partial_invalidations.empty());
      assert(reduction_instances.empty());
      assert(!reduction_fields);
      assert(restricted_instances.empty());
      assert(!restricted_fields);
      assert(released_instances.empty());
      assert(collective_instances.empty());
      assert(tracing_preconditions == NULL);
      assert(tracing_anticonditions == NULL);
      assert(tracing_postconditions == NULL);
#endif 
      if (replicated_owner_state != NULL)
        delete replicated_owner_state;
      if (context->remove_nested_resource_ref(did))
        delete context;
      if (set_expr->remove_nested_expression_reference(did))
        delete set_expr;
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::notify_local(void)
    //--------------------------------------------------------------------------
    {
      if (!total_valid_instances.empty())
      {
        for (FieldMaskSet<LogicalView>::const_iterator it =
              total_valid_instances.begin(); it !=
              total_valid_instances.end(); it++)
          if (it->first->remove_nested_valid_ref(did))
            delete it->first;
        total_valid_instances.clear();
      }
      if (!partial_valid_instances.empty())
      {
        for (ViewExprMaskSets::iterator pit =
              partial_valid_instances.begin(); pit !=
              partial_valid_instances.end(); pit++)
        {
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                pit->second.begin(); it != pit->second.end(); it++)
            if (it->first->remove_nested_expression_reference(did))
              delete it->first;
          if (pit->first->remove_nested_valid_ref(did))
            delete pit->first;
          pit->second.clear();
        }
        partial_valid_instances.clear();
        partial_valid_fields.clear();
      }
      if (!initialized_data.empty())
      {
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
              initialized_data.begin(); it != initialized_data.end(); it++)
          if (it->first->remove_nested_expression_reference(did))
            delete it->first;
        initialized_data.clear();
      }
      if (!partial_invalidations.empty())
      {
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
              partial_invalidations.begin(); it != 
              partial_invalidations.end(); it++)
          if (it->first->remove_nested_expression_reference(did))
            delete it->first;
        partial_invalidations.clear();
      }
      if (!reduction_instances.empty())
      {
        for (std::map<unsigned,std::list<std::pair<InstanceView*,
              IndexSpaceExpression*> > >::iterator it =
              reduction_instances.begin(); it !=
              reduction_instances.end(); it++)
        {
          while (!it->second.empty())
          {
            std::pair<InstanceView*,IndexSpaceExpression*> &back =
              it->second.back();
            if (back.first->remove_nested_valid_ref(did))
              delete back.first;
            if (back.second->remove_nested_expression_reference(did))
              delete back.second;
            it->second.pop_back();
          }
        }
        reduction_instances.clear();
        reduction_fields.clear();
      }
      if (!restricted_instances.empty())
      {
        for (ExprViewMaskSets::iterator rit =
              restricted_instances.begin(); rit !=
              restricted_instances.end(); rit++)
        {
          for (FieldMaskSet<InstanceView>::const_iterator it =
                rit->second.begin(); it != rit->second.end(); it++)
            if (it->first->remove_nested_valid_ref(did))
              delete it->first;
          if (rit->first->remove_nested_expression_reference(did))
            delete rit->first;
          rit->second.clear();
        }
        restricted_instances.clear();
        restricted_fields.clear();
      }
      if (!released_instances.empty())
      {
        for (ExprViewMaskSets::iterator rit = released_instances.begin();
              rit != released_instances.end(); rit++)
        {
          for (FieldMaskSet<InstanceView>::const_iterator it =
                rit->second.begin(); it != rit->second.end(); it++)
            if (it->first->remove_nested_valid_ref(did))
              delete it->first;
          if (rit->first->remove_nested_expression_reference(did))
            delete rit->first;
          rit->second.clear();
        }
        released_instances.clear();
      }
      if (!collective_instances.empty())
      {
        for (FieldMaskSet<CollectiveView>::const_iterator it =
              collective_instances.begin(); it != 
              collective_instances.end(); it++)
          if (it->first->remove_nested_resource_ref(did))
            delete it->first;
        collective_instances.clear();
      }
      if (tracing_preconditions != NULL)
      {
        delete tracing_preconditions;
        tracing_preconditions = NULL;
      }
      if (tracing_anticonditions != NULL)
      {
        delete tracing_anticonditions;
        tracing_anticonditions = NULL;
      }
      if (tracing_postconditions != NULL)
      {
        delete tracing_postconditions;
        tracing_postconditions = NULL;
      }
      // No need to check for deletion since we're still holding a 
      // resource reference to the context as well
      context->remove_nested_gc_ref(did);
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::initialize_collective_references(unsigned local_valid)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(collective_mapping != NULL);
#endif
      if (is_owner())
      {
        if (local_valid > 0)
          add_base_gc_ref(CONTEXT_REF, local_valid);
      }
      else
      {
#ifdef DEBUG_LEGION
        assert(local_valid > 0);
#endif
        add_base_gc_ref(CONTEXT_REF, local_valid);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::initialize_set(const RegionUsage &usage,
                                        const FieldMask &user_mask,
                                        const bool restricted,
                                        const InstanceSet &sources,
                              const std::vector<IndividualView*> &corresponding)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(is_logical_owner() || set_expr->is_empty());
      assert(sources.size() == corresponding.size());
#endif
      AutoLock eq(eq_lock);
      if (IS_REDUCE(usage))
      {
#ifdef DEBUG_LEGION
        // Reduction-only should always be restricted for now
        // Could change if we started issuing reduction close
        // operations at the end of a context
        assert(restricted);
#endif
        // Since these are restricted, we'll make these the actual
        // target logical instances and record them as restricted
        // instead of recording them as reduction instances
        for (unsigned idx = 0; idx < sources.size(); idx++)
        {
          const FieldMask &view_mask = sources[idx].get_valid_fields();
          IndividualView *view = corresponding[idx];
          FieldMaskSet<LogicalView>::iterator finder = 
            total_valid_instances.find(view);
          if (finder == total_valid_instances.end())
          {
            total_valid_instances.insert(view, view_mask);
            view->add_nested_valid_ref(did);
          }
          else
            finder.merge(view_mask);
          // Always restrict reduction-only users since we know the data
          // is going to need to be flushed anyway
          record_restriction(set_expr, true/*covers*/, view_mask, view);
        }
      }
      else
      {
        for (unsigned idx = 0; idx < sources.size(); idx++)
        {
          const FieldMask &view_mask = sources[idx].get_valid_fields();
          IndividualView *view = corresponding[idx];
#ifdef DEBUG_LEGION
          assert(!view->is_reduction_kind());
#endif
          FieldMaskSet<LogicalView>::iterator finder = 
            total_valid_instances.find(view);
          if (finder == total_valid_instances.end())
          {
            total_valid_instances.insert(view, view_mask);
            view->add_nested_valid_ref(did);
            // Check if this is a collective view we need to track
            if (view->is_collective_view() && collective_instances.insert(
                  view->as_collective_view(), view_mask))
              view->add_nested_resource_ref(did);
          }
          else
            finder.merge(view_mask);
          // If this is restricted then record it
          if (restricted)
            record_restriction(set_expr, true/*covers*/, view_mask, view);
        }
      }
      // Record that this data is all valid
      update_initialized_data(set_expr, true/*covers*/, user_mask);
      // Update any restricted fields 
      if (restricted)
      {
#ifdef DEBUG_LEGION
        assert(!restricted_instances.empty());
#endif
        restricted_fields |= user_mask;
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::analyze(PhysicalAnalysis &analysis,
                                 IndexSpaceExpression *expr,
                                 const bool expr_covers,
                                 FieldMask traversal_mask,
                                 std::set<RtEvent> &deferral_events,
                                 std::set<RtEvent> &applied_events,
                                 const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // This block of code is the same across all traversals so we ensure
      // that it is done before traversing the equivalence set for all analyses
      AutoTryLock eq(eq_lock);
      if (!eq.has_lock())
      {
        defer_analysis(eq, analysis, traversal_mask, deferral_events,
                       applied_events, already_deferred);
        return;
      }
      if (is_remote_analysis(analysis, traversal_mask, deferral_events,
                             applied_events, expr_covers))
        return;
#ifdef DEBUG_LEGION
      // Should only be here if we're the owner
      assert(is_logical_owner());
#endif
      bool check_migration = false;
      if (!partial_invalidations.empty()) 
      {
        // Check for any fields which have partial invalid expressions that we
        // also need to remove from from this analysis
        FieldMask invalid_mask = 
          traversal_mask & partial_invalidations.get_valid_mask();
        if (!!invalid_mask)
        {
          traversal_mask -= invalid_mask;
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                partial_invalidations.begin(); it !=
                partial_invalidations.end(); it++)
          {
            const FieldMask overlap = invalid_mask & it->second;
            if (!overlap)
              continue;
#ifdef DEBUG_LEGION
            // Should never be trying to do a traversal on an equivalence
            // set that has been completely invalidated
            assert(it->first != set_expr);
#endif
            IndexSpaceExpression *remainder = 
              runtime->forest->subtract_index_spaces(expr, it->first);
            if (!remainder->is_empty() && analysis.perform_analysis(this, 
                  remainder, false/*expr covers*/, overlap, 
                  deferral_events, applied_events, already_deferred))
                check_migration = true;
            invalid_mask -= overlap;
            if (!invalid_mask)
              break;
          }
        }
      }
      if (!!traversal_mask && analysis.perform_analysis(this, expr, expr_covers,
            traversal_mask, deferral_events, applied_events, already_deferred))
        check_migration = true;
      if (check_migration)
        check_for_migration(analysis, applied_events, expr_covers); 
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::find_valid_instances(ValidInstAnalysis &analysis,
                                             IndexSpaceExpression *expr,
                                             const bool expr_covers, 
                                             const FieldMask &user_mask,
                                             std::set<RtEvent> &deferral_events,
                                             std::set<RtEvent> &applied_events,
                                             const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // Already holding the eq_lock from EquivalenceSet::analyze method
      // Lock the analysis so we can perform updates here
      AutoLock a_lock(analysis);
      if (analysis.redop != 0)
      {
        // Iterate over all the fields
        int fidx = user_mask.find_first_set();
        while (fidx >= 0)
        {
          std::map<unsigned,std::list<
            std::pair<InstanceView*,IndexSpaceExpression*> > >::iterator
              current = reduction_instances.find(fidx);
          if (current != reduction_instances.end())
          {
            FieldMask local_mask;
            local_mask.set_bit(fidx);
            for (std::list<std::pair<InstanceView*,IndexSpaceExpression*> >
                  ::const_reverse_iterator it = current->second.rbegin(); it !=
                  current->second.rend(); it++)
            {
              if (it->first->get_redop() != analysis.redop)
                break;
              if (!expr_covers)
              {
                IndexSpaceExpression *overlap = 
                  runtime->forest->intersect_index_spaces(expr, it->second);
                if (overlap->is_empty())
                  continue;
              }
              analysis.record_instance(it->first, local_mask);
            }
          }
          fidx = user_mask.find_next_set(fidx+1);
        }
      }
      else
      {
        if (!(user_mask * total_valid_instances.get_valid_mask()))
        {
          for (FieldMaskSet<LogicalView>::const_iterator it = 
                total_valid_instances.begin(); it != 
                total_valid_instances.end(); it++)
          {
            if (!it->first->is_instance_view())
              continue;
            const FieldMask overlap = it->second & user_mask;
            if (!overlap)
              continue;
            analysis.record_instance(it->first->as_instance_view(), overlap);
          }
        }
        if (!(user_mask * partial_valid_fields))
        {
          for (ViewExprMaskSets::const_iterator pit = 
                partial_valid_instances.begin(); pit !=
                partial_valid_instances.end(); pit++)
          {
            if (!pit->first->is_instance_view())
              continue;
            if (expr_covers)
            {
              const FieldMask overlap = 
                user_mask & pit->second.get_valid_mask();
              if (!!overlap)
                analysis.record_instance(pit->first->as_instance_view(), 
                                         overlap);
              continue;
            }
            else if (user_mask * pit->second.get_valid_mask())
              continue;
            FieldMask total_overlap;
            for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                  pit->second.begin(); it != pit->second.end(); it++)
            {
              const FieldMask overlap = user_mask & it->second;
              if (!overlap)
                continue;
              IndexSpaceExpression *expr_overlap = 
                  runtime->forest->intersect_index_spaces(expr, it->first);   
              if (expr_overlap->is_empty())
                continue;
              total_overlap |= overlap;
            }
            if (!!total_overlap)
              analysis.record_instance(pit->first->as_instance_view(), 
                                       total_overlap);
          }
        }
      }
      if (!(user_mask * restricted_fields))
      {
        if (!expr_covers)
        {
          // Check for the set expr first which we know overlaps
          ExprViewMaskSets::const_iterator finder =
            restricted_instances.find(set_expr);
          if ((finder == restricted_instances.end()) || 
              (finder->second.get_valid_mask() * user_mask))
          {
            for (ExprViewMaskSets::const_iterator it =
                  restricted_instances.begin(); it != 
                  restricted_instances.end(); it++)
            {
              if (it == finder)
                continue;
              if (it->second.get_valid_mask() * user_mask)
                continue;
              IndexSpaceExpression *overlap = 
                runtime->forest->intersect_index_spaces(it->first, expr);
              if (overlap->is_empty())
                continue;
              analysis.record_restriction();
              break;
            }
          }
          else
            analysis.record_restriction();
        }
        else
          analysis.record_restriction();
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::find_invalid_instances(InvalidInstAnalysis &analysis,
                                    IndexSpaceExpression *expr,
                                    const bool expr_covers, 
                                    const FieldMask &user_mask,
                                    std::set<RtEvent> &deferral_events,
                                    std::set<RtEvent> &applied_events,
                                    const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // Already holding the eq_lock from EquivalenceSet::analyze method
      for (FieldMaskSet<LogicalView>::const_iterator vit = 
            analysis.valid_instances.begin(); vit != 
            analysis.valid_instances.end(); vit++)
      {
        FieldMask invalid_mask = vit->second & user_mask;
        if (!invalid_mask)
          continue;
        if (vit->first->is_reduction_kind())
        {
          // Reduction instance path
          InstanceView *reduction_view = vit->first->as_instance_view();
          if (reduction_view->is_collective_view())
          {
            // Collective reduction view, so need to check that all
            // intances in the collective reduction view are covered
            if (!(invalid_mask - reduction_fields))
            {
              CollectiveAntiAlias alias_analysis(
                  reduction_view->as_collective_view());
              int fidx = invalid_mask.find_first_set();
              while (fidx >= 0)
              {
                std::map<unsigned,std::list<std::pair<InstanceView*,
                  IndexSpaceExpression*> > >::const_iterator finder = 
                    reduction_instances.find(fidx);
                if (finder == reduction_instances.end())
                  break;
                FieldMask reduction_mask;
                reduction_mask.set_bit(fidx);
                for (std::list<std::pair<InstanceView*,
                      IndexSpaceExpression*> >::const_reverse_iterator rit =
                      finder->second.rbegin(); rit != 
                      finder->second.rend(); rit++)
                {
                  // Can't go backwards through any 
                  // reductions of a different type
                  if (rit->first->get_redop() != reduction_view->get_redop())
                    break;
                  if (!rit->first->aliases(reduction_view))
                    continue;
                  alias_analysis.traverse(rit->first, 
                      reduction_mask, rit->second);
                }
                fidx = invalid_mask.find_next_set(fidx+1);
              }
              FieldMask allvalid_mask = invalid_mask;
              FieldMaskSet<IndexSpaceExpression> no_partial_valid_exprs;
              alias_analysis.visit_leaves(invalid_mask, allvalid_mask,
                  expr, runtime->forest, no_partial_valid_exprs);
              if (!!allvalid_mask)
              {
                invalid_mask -= allvalid_mask;
                if (!invalid_mask)
                  continue;
              }
            }
          }
          else
          {
            // Individual instance view so traverse like normal, but be
            // sure to check for aliasing with any collective views
            if (!(invalid_mask - reduction_fields))
            {
              int fidx = invalid_mask.find_first_set();
              while (fidx >= 0)
              {
                std::map<unsigned,std::list<std::pair<InstanceView*,
                  IndexSpaceExpression*> > >::const_iterator finder = 
                    reduction_instances.find(fidx);
                if (finder == reduction_instances.end())
                  break;
                std::set<IndexSpaceExpression*> exprs;
                for (std::list<std::pair<InstanceView*,
                      IndexSpaceExpression*> >::const_reverse_iterator rit =
                      finder->second.rbegin(); rit != 
                      finder->second.rend(); rit++)
                {
                  // Can't go backwards through any 
                  // reductions of a different type
                  if (rit->first->get_redop() != reduction_view->get_redop())
                    break;
                  if (!rit->first->aliases(reduction_view))
                    continue;
                  if ((rit->second == expr) || (rit->second == set_expr))
                  {
                    // covers everything
                    invalid_mask.unset_bit(fidx);
                    exprs.clear();
                    break;
                  }
                  else
                    exprs.insert(rit->second);
                }
                if (!exprs.empty())
                {
                  // See if they cover
                  IndexSpaceExpression *union_expr = 
                    runtime->forest->union_index_spaces(exprs);
                  IndexSpaceExpression *intersection =
                    runtime->forest->intersect_index_spaces(expr, union_expr);
                  if (intersection->get_volume() == expr->get_volume())
                    invalid_mask.unset_bit(fidx);
                  else // no point in checking the rest
                    break;
                }
                // No point in checking the rest if this wasn't covered
                else if (invalid_mask.is_set(fidx))
                  break;
                fidx = invalid_mask.find_next_set(fidx+1);
              }
            }
            if (!invalid_mask)
              continue;
          }
        }
        else
        {
          // Normal instance path
          // Always perform a simple name check since that is easy
          if (!total_valid_instances.empty())
          {
            FieldMaskSet<LogicalView>::const_iterator finder =
              total_valid_instances.find(vit->first);
            if (finder != total_valid_instances.end())
            {
              invalid_mask -= finder->second;
              if (!invalid_mask)
                continue;
            }
          }
          if (vit->first->is_collective_view())
          {
            // Collective view case
            // Next check the partially valid views and see if they
            // cover expression
            FieldMaskSet<IndexSpaceExpression> partial_valid_exprs;
            ViewExprMaskSets::const_iterator finder =
              partial_valid_instances.find(vit->first);
            if ((finder != partial_valid_instances.end()) &&
                !(finder->second.get_valid_mask() * invalid_mask))
            {
              FieldMaskSet<IndexSpaceExpression>::const_iterator expr_finder =
                finder->second.find(expr);
              if (expr_finder != finder->second.end())
              {
                invalid_mask -= expr_finder->second;
                if (!invalid_mask)
                  continue;
              }
              for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                    finder->second.begin(); it != finder->second.end(); it++)
              {
                if (it->first == expr)
                  continue;
                const FieldMask overlap = it->second & invalid_mask;
                if (!overlap)
                  continue;
                IndexSpaceExpression *expr_overlap = 
                  runtime->forest->intersect_index_spaces(expr, it->first);
                const size_t overlap_volume = expr_overlap->get_volume();
                if (overlap_volume == expr->get_volume())
                {
                  invalid_mask -= overlap;
                  if (!invalid_mask)
                    break;
                }
                // Record any partial valid expressions
                else if (overlap_volume > 0)
                  partial_valid_exprs.insert(expr_overlap, overlap);
              }
              if (!invalid_mask)
                continue;
            }
            // If we make it here then still have invalid fields so we
            // need do an alias analysis with all the valid views on 
            // their instances to see if they cover what we're looking for
            CollectiveView *view = vit->first->as_collective_view();
            CollectiveAntiAlias alias_analysis(view);
            alias_analysis.traverse_total(invalid_mask, set_expr, 
                                          total_valid_instances);
            if (!(invalid_mask * partial_valid_fields))
              alias_analysis.traverse_partial(invalid_mask,
                                              partial_valid_instances);
            FieldMask allvalid_mask = invalid_mask;
            alias_analysis.visit_leaves(invalid_mask, allvalid_mask,
                expr, runtime->forest, partial_valid_exprs);
            if (!!allvalid_mask)
            {
              invalid_mask -= allvalid_mask;
              if (!invalid_mask)
                continue;
            }
          }
          else
          {
            // Non-collective view
            // See if there are any collective aliases
            FieldMaskSet<IndexSpaceExpression> partial_valid_exprs;
            if (vit->first->is_individual_view() &&
                !collective_instances.empty() && 
                !(collective_instances.get_valid_mask() * invalid_mask))
            {
              // Check for aliasing with any collective views
              IndividualView *view = vit->first->as_individual_view();
              // Scan through all the collective views and see which ones
              // we alias with and what they are valid for
              for (FieldMaskSet<CollectiveView>::const_iterator cit =
                    collective_instances.begin(); cit != 
                    collective_instances.end(); cit++)
              {
                if (invalid_mask * cit->second)
                  continue;
                if (!view->aliases(cit->first))
                  continue;
                // Alias on fields and instances, check expressions
                FieldMaskSet<LogicalView>::const_iterator total_finder =
                  total_valid_instances.find(cit->first);
                if (!total_finder != total_valid_instances.end())
                {
                  invalid_mask -= total_finder->second;
                  if (!invalid_mask)
                    break;
                }
                // Find all the covering and partial valid expressions
                ViewExprMaskSets::const_iterator finder =
                  partial_valid_instances.find(cit->first);
                if ((finder != partial_valid_instances.end()) &&
                    !(finder->second.get_valid_mask() * invalid_mask))
                {
                  FieldMaskSet<IndexSpaceExpression>::const_iterator 
                    expr_finder = finder->second.find(expr);
                  if (expr_finder != finder->second.end())
                  {
                    invalid_mask -= expr_finder->second;
                    if (!invalid_mask)
                      break;
                  }
                  for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                       finder->second.begin(); it != finder->second.end(); it++)
                  {
                    if (it->first == expr)
                      continue;
                    const FieldMask overlap = it->second & invalid_mask;
                    if (!overlap)
                      continue;
                    IndexSpaceExpression *expr_overlap = 
                      runtime->forest->intersect_index_spaces(expr, it->first);
                    const size_t overlap_volume = expr_overlap->get_volume();
                    if (overlap_volume == expr->get_volume())
                    {
                      invalid_mask -= overlap;
                      if (!invalid_mask)
                        break;
                    }
                    // Need to save partially valid expressions so we can 
                    // union them together later to see if they all cover
                    else if (overlap_volume > 0)
                      partial_valid_exprs.insert(expr_overlap, overlap);
                  }
                  if (!invalid_mask)
                    break;
                }
              }
              if (!invalid_mask)
                continue;
            }
            if (!expr_covers || !partial_valid_exprs.empty())
            {
              // No collective instance aliasing so we can just do a name check
              ViewExprMaskSets::const_iterator finder =
                partial_valid_instances.find(vit->first);
              if ((finder != partial_valid_instances.end()) &&
                  !(finder->second.get_valid_mask() * invalid_mask))
              {
                FieldMaskSet<IndexSpaceExpression>::const_iterator expr_finder =
                  finder->second.find(expr);
                if (expr_finder != finder->second.end())
                {
                  invalid_mask -= expr_finder->second;
                  if (!invalid_mask)
                    continue;
                }
                for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                      finder->second.begin(); it != finder->second.end(); it++)
                {
                  if (it->first == expr)
                    continue;
                  const FieldMask overlap = it->second & invalid_mask;
                  if (!overlap)
                    continue;
                  IndexSpaceExpression *expr_overlap = 
                    runtime->forest->intersect_index_spaces(expr, it->first);
                  const size_t overlap_volume = expr_overlap->get_volume();
                  if (overlap_volume == expr->get_volume())
                  {
                    invalid_mask -= overlap;
                    if (!invalid_mask)
                      break;
                  }
                  // Keep recording partial expressions if we have any
                  // If we didn't already have any then we know this is
                  // all there will be since there's no other way to 
                  // have instance name aliasing
                  else if ((overlap_volume > 0) && !partial_valid_exprs.empty())
                    partial_valid_exprs.insert(expr_overlap, overlap);
                }
                if (!invalid_mask)
                  continue;
              }
              if (!partial_valid_exprs.empty())
              {
                // Last chance, see if the union of all the partial valid 
                // expressions are enough to cover the instance
                LegionList<FieldSet<IndexSpaceExpression*> > expr_field_sets;
                partial_valid_exprs.compute_field_sets(invalid_mask, 
                                                       expr_field_sets);
                for (LegionList<FieldSet<IndexSpaceExpression*> >::
                      const_iterator it = expr_field_sets.begin(); it !=
                      expr_field_sets.end(); it++)
                {
                  if (it->elements.empty())
                    continue;
                  IndexSpaceExpression *union_expr = 
                    (it->elements.size() == 1) ? *(it->elements.begin()) :
                    runtime->forest->union_index_spaces(it->elements);
                  IndexSpaceExpression *expr_overlap =
                    runtime->forest->intersect_index_spaces(expr, union_expr);
                  if (expr_overlap->get_volume() == expr->get_volume())
                  {
                    invalid_mask -= it->set_mask;
                    if (!invalid_mask)
                      break;
                  }
                }
                if (!invalid_mask)
                  continue;
              }
            }
          }
        }
        // If we get here it's because we're not valid for some expression
        // for these fields so record it
#ifdef DEBUG_LEGION
        assert(!!invalid_mask);
#endif
        // Lock the analysis when recording this update
        AutoLock a_lock(analysis);
        analysis.record_instance(vit->first, invalid_mask);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::find_antivalid_instances(
                                            AntivalidInstAnalysis &analysis,
                                            IndexSpaceExpression *expr,
                                            const bool expr_covers, 
                                            const FieldMask &user_mask,
                                            std::set<RtEvent> &deferral_events,
                                            std::set<RtEvent> &applied_events,
                                            const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // Already holding the eq_lock from EquivalenceSet::analyze method
      for (FieldMaskSet<LogicalView>::const_iterator ait =
            analysis.antivalid_instances.begin(); ait !=
            analysis.antivalid_instances.end(); ait++)
      {
        const FieldMask antivalid_mask = ait->second & user_mask;
        if (!antivalid_mask)
          continue;
        if (ait->first->is_reduction_kind())
        {
          // Reduction instance case
          InstanceView *reduction_view = ait->first->as_instance_view();
          int fidx = antivalid_mask.find_first_set();
          while (fidx >= 0)
          {
            std::map<unsigned,std::list<std::pair<InstanceView*,
              IndexSpaceExpression*> > >::const_iterator finder = 
                reduction_instances.find(fidx);
            if (finder != reduction_instances.end())
            {
              for (std::list<std::pair<InstanceView*,
                    IndexSpaceExpression*> >::const_iterator it = 
                    finder->second.begin(); it != finder->second.end(); it++)
              {
                if (!reduction_view->aliases(it->first))
                  continue;
                FieldMask local_mask;
                local_mask.set_bit(fidx);
                if (!expr_covers)
                {
                  if ((it->second != set_expr) && (it->second != expr)) 
                  {
                    IndexSpaceExpression *intersection = 
                      runtime->forest->intersect_index_spaces(expr, it->second);
                    if (!intersection->is_empty())
                    {
                      AutoLock a_lock(analysis); 
                      analysis.record_instance(reduction_view, local_mask);
                    }
                  }
                  else
                  {
                    AutoLock a_lock(analysis);
                    analysis.record_instance(reduction_view, local_mask);
                  }
                }
                else // they intersect so record it
                {
                  AutoLock a_lock(analysis);
                  analysis.record_instance(reduction_view, local_mask);
                }
              }
            }
            fidx = antivalid_mask.find_next_set(fidx+1);
          }
        }
        else
        {
          // Normal instance case
          // Check for it in the total valid instances first
          FieldMaskSet<LogicalView>::const_iterator total_finder = 
            total_valid_instances.find(ait->first);
          if (total_finder != total_valid_instances.end())
          {
            const FieldMask overlap = antivalid_mask & total_finder->second;
            if (!!overlap)
            {
              AutoLock a_lock(analysis);
              analysis.record_instance(ait->first, overlap);
            }
          }
          // Then check for it in the partial valid instances
          ViewExprMaskSets::const_iterator finder = 
            partial_valid_instances.find(ait->first);
          if (finder != partial_valid_instances.end())
          {
            for (FieldMaskSet<IndexSpaceExpression>::const_iterator it = 
                  finder->second.begin(); it != finder->second.end(); it++)
            {
              const FieldMask overlap = it->second & antivalid_mask;
              if (!overlap)
                continue;
              if (!expr_covers)
              {
                if ((it->first != set_expr) && (it->first != expr))
                {
                  IndexSpaceExpression *intersection = 
                    runtime->forest->intersect_index_spaces(expr, it->first);
                  if (!intersection->is_empty())
                  {
                    AutoLock a_lock(analysis);
                    analysis.record_instance(ait->first, overlap);
                  }
                }
                else
                {
                  AutoLock a_lock(analysis);
                  analysis.record_instance(ait->first, overlap);
                }
              }
              else
              {
                AutoLock a_lock(analysis);
                analysis.record_instance(ait->first, overlap);
              }
            }
          }
          // Lastly do the aliasing checks for collective instances
          if (ait->first->is_collective_view())
          {
            CollectiveView *collective = ait->first->as_collective_view(); 
            // Check for aliasing with any of the valid views
            if (!(antivalid_mask * total_valid_instances.get_valid_mask()))
            {
              for (FieldMaskSet<LogicalView>::const_iterator it = 
                    total_valid_instances.begin(); it != 
                    total_valid_instances.end(); it++)
              {
                if (!it->first->is_instance_view())
                  continue;
                const FieldMask overlap = antivalid_mask & it->second;
                if (!overlap)
                  continue;
                if (!collective->aliases(it->first->as_instance_view()))
                  continue;
                AutoLock a_lock(analysis);
                analysis.record_instance(ait->first, overlap);
              }
            }
            if (!(antivalid_mask * partial_valid_fields))
            {
              for (ViewExprMaskSets::const_iterator pit =
                    partial_valid_instances.begin(); pit !=
                    partial_valid_instances.end(); pit++)
              {
                if (!pit->first->is_instance_view())
                  continue;
                if (!(antivalid_mask * pit->second.get_valid_mask()))
                  continue;
                if (!collective->aliases(pit->first->as_instance_view()))
                  continue;
                for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                      pit->second.begin(); it != pit->second.end(); it++)
                {
                  const FieldMask overlap = it->second & antivalid_mask;
                  if (!overlap)
                    continue;
                  if (!expr_covers)
                  {
                    if ((it->first != set_expr) && (it->first != expr))
                    {
                      IndexSpaceExpression *intersection = 
                        runtime->forest->intersect_index_spaces(expr,it->first);
                      if (!intersection->is_empty())
                      {
                        AutoLock a_lock(analysis);
                        analysis.record_instance(ait->first, overlap);
                      }
                    }
                    else
                    {
                      AutoLock a_lock(analysis);
                      analysis.record_instance(ait->first, overlap);
                    }
                  }
                  else
                  {
                    AutoLock a_lock(analysis);
                    analysis.record_instance(ait->first, overlap);
                  }
                }
              }
            }
          }
          else if (ait->first->is_instance_view() &&
              !collective_instances.empty() &&
              !(antivalid_mask * collective_instances.get_valid_mask()))
          {
            IndividualView *view = ait->first->as_individual_view();
            // Check against any collective views to see if we alias
            for (FieldMaskSet<CollectiveView>::const_iterator cit =
                  collective_instances.begin(); cit != 
                  collective_instances.end(); cit++)
            {
              if (antivalid_mask * cit->second)
                continue;
              if (!view->aliases(cit->first))
                continue;
              total_finder = total_valid_instances.find(cit->first);
              if (total_finder != total_valid_instances.end())
              {
                const FieldMask overlap = antivalid_mask & total_finder->second;
                if (!!overlap)
                {
                  AutoLock a_lock(analysis);
                  analysis.record_instance(ait->first, overlap);
                }
              }
              finder = partial_valid_instances.find(cit->first);
              if (finder != partial_valid_instances.end())
              {
                for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                      finder->second.begin(); it != finder->second.end(); it++)
                {
                  const FieldMask overlap = it->second & antivalid_mask;
                  if (!overlap)
                    continue;
                  if (!expr_covers)
                  {
                    if ((it->first != set_expr) && (it->first != expr))
                    {
                      IndexSpaceExpression *intersection = 
                        runtime->forest->intersect_index_spaces(expr,it->first);
                      if (!intersection->is_empty())
                      {
                        AutoLock a_lock(analysis);
                        analysis.record_instance(ait->first, overlap);
                      }
                    }
                    else
                    {
                      AutoLock a_lock(analysis);
                      analysis.record_instance(ait->first, overlap);
                    }
                  }
                  else
                  {
                    AutoLock a_lock(analysis);
                    analysis.record_instance(ait->first, overlap);
                  }
                }
              }
            }
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::update_set(UpdateAnalysis &analysis,
                                    IndexSpaceExpression *expr,
                                    const bool expr_covers,
                                    FieldMask user_mask,
                                    std::set<RtEvent> &deferral_events,
                                    std::set<RtEvent> &applied_events,
                                    const bool already_deferred/*=false*/)
    //--------------------------------------------------------------------------
    {
      // Already holding the eq_lock from EquivalenceSet::analyze method
      // Now that we're ready to perform the analysis 
      // we need to lock the analysis 
      AutoLock a_lock(analysis);
      // Check for any uninitialized data
      // Don't report uninitialized warnings for empty equivalence classes
      if (analysis.check_initialized && !set_expr->is_empty())
        check_for_uninitialized_data(analysis, expr, expr_covers, 
                                     user_mask, applied_events);
      if (analysis.output_aggregator != NULL)
        analysis.output_aggregator->clear_update_fields();
      if (IS_REDUCE(analysis.usage))
      {
        // Reduction-only
        // We only record reductions if the set expression is not empty
        // as we can't guarantee the reductions will ever be read for 
        // empty equivalence sets which can lead to leaked instances
        if (!expr->is_empty())
        {
          record_reductions(analysis, expr, expr_covers, user_mask); 
          // Flush any reductions for restricted fields and expressions
          if (!restricted_instances.empty())
          {
            const FieldMask flush_mask = user_mask & restricted_fields;
            if (!!flush_mask)
            {
              // Find all the restrictions that we overlap with and
              // apply reductions to them to flush any data
              for (ExprViewMaskSets::const_iterator rit =
                    restricted_instances.begin(); rit != 
                    restricted_instances.end(); rit++)
              {
                const FieldMask overlap = 
                  flush_mask & rit->second.get_valid_mask(); 
                if (!overlap)
                  continue;
                if (expr_covers)
                {
                  // Expression covers the full restriction
                  if (rit->first->get_volume() == set_expr->get_volume())
                    apply_restricted_reductions(rit->second, set_expr, 
                        true/*covers*/, overlap, analysis.output_aggregator,
                        NULL/*no guard*/, &analysis,
                        true/*track events*/, analysis.trace_info,
                        NULL/*no applied expr tracking*/);
                  else
                    apply_restricted_reductions(rit->second, rit->first, 
                        false/*covers*/, overlap, analysis.output_aggregator,
                        NULL/*no guard*/, &analysis,
                        true/*track events*/, analysis.trace_info,
                        NULL/*no applied expr tracking*/);
                }
                else if (rit->first == set_expr)
                {
                  // Restriction covers the full expression
                  apply_restricted_reductions(rit->second, expr, expr_covers,
                      overlap, analysis.output_aggregator, NULL/*no guard*/,
                      &analysis, true/*track events*/, analysis.trace_info,
                      NULL/*no applied expr tracking*/);
                }
                else
                {
                  // Check for partial overlap of restrction
                  IndexSpaceExpression *expr_overlap =
                    runtime->forest->intersect_index_spaces(expr, rit->first);
                  if (expr_overlap->is_empty())
                    continue;
                  if (expr_overlap->get_volume() == expr->get_volume())
                    apply_restricted_reductions(rit->second, expr, expr_covers,
                        overlap, analysis.output_aggregator, NULL/*no guard*/,
                        &analysis, true/*track events*/, analysis.trace_info,
                        NULL/*no applied expr tracking*/);
                  else
                    apply_restricted_reductions(rit->second, expr_overlap,
                        false/*covers*/, overlap, analysis.output_aggregator,
                        NULL/*no guard*/, &analysis, true/*track events*/,
                        analysis.trace_info, NULL/*no applied expr tracking*/);
                }
              }
            }
          }
        }
      }
      else if (IS_WRITE(analysis.usage) && IS_DISCARD(analysis.usage))
      {
        // Write-only
        // Update the initialized data before messing with the user mask
        update_initialized_data(expr, expr_covers, user_mask);
        const FieldMask reduce_filter = reduction_fields & user_mask;
        if (!!reduce_filter)
          filter_reduction_instances(expr, expr_covers, reduce_filter);
        FieldMaskSet<InstanceView> new_instances;
        for (unsigned idx = 0; idx < analysis.target_views.size(); idx++)
        {
          const FieldMask overlap = user_mask & 
            analysis.target_views[idx].get_valid_mask();
          if (!overlap)
            continue;
          for (FieldMaskSet<InstanceView>::const_iterator it =
                analysis.target_views[idx].begin(); it !=
                analysis.target_views[idx].end(); it++)
          {
            const FieldMask inst_overlap = user_mask & overlap;
            if (!inst_overlap)
              continue;
            new_instances.insert(it->first, inst_overlap);
          }
        }
        // Filter any normal instances that will be overwritten
        const FieldMask non_restricted = user_mask - restricted_fields; 
        if (!!non_restricted)
        {
          filter_valid_instances(expr, expr_covers, non_restricted);
          // Record any non-restricted instances
          record_instances(expr, expr_covers, non_restricted,
                           new_instances);
        }
        // Issue copy-out copies for any restricted fields
        if (!restricted_instances.empty())
        {
          const FieldMask restricted_mask = user_mask & restricted_fields;
          if (!!restricted_mask)
          {
            filter_unrestricted_instances(expr, expr_covers, 
                                          restricted_mask);
            // Record any of our instances that are unrestricted
            record_unrestricted_instances(expr, expr_covers, restricted_mask,
                                          new_instances);
            copy_out(expr, expr_covers, restricted_mask, new_instances,
                     &analysis, analysis.trace_info,analysis.output_aggregator);
          }
        }
      }
      else if (IS_READ_ONLY(analysis.usage) && !read_only_guards.empty() && 
                !(user_mask * read_only_guards.get_valid_mask()))
      {
        // If we're doing read-only mode, get the set of events that
        // we need to wait for before we can do our registration, this 
        // ensures that we serialize read-only operations correctly
        // In order to avoid deadlock we have to make different copy fill
        // aggregators for each of the different fields of prior updates
        FieldMaskSet<CopyFillAggregator> to_add;
        for (FieldMaskSet<CopyFillGuard>::iterator it = 
              read_only_guards.begin(); it != read_only_guards.end(); it++)
        {
          const FieldMask guard_mask = user_mask & it->second;
          if (!guard_mask)
            continue;
          // No matter what record our dependences on the prior guards
#ifdef NON_AGGRESSIVE_AGGREGATORS
          const RtEvent guard_event = it->first->effects_applied;
          analysis.guard_events.insert(guard_event);
#else
          const RtEvent guard_event = it->first->guard_postcondition;
          if (analysis.original_source == local_space)
            analysis.guard_events.insert(guard_event);
          else
            analysis.guard_events.insert(it->first->effects_applied);
#endif
          CopyFillAggregator *input_aggregator = NULL;
          // See if we have an input aggregator that we can use now
          std::map<RtEvent,CopyFillAggregator*>::const_iterator finder = 
            analysis.input_aggregators.find(guard_event);
          if (finder != analysis.input_aggregators.end())
          {
            input_aggregator = finder->second;
            if (input_aggregator != NULL)
              input_aggregator->clear_update_fields();
          } 
          // Use this to see if any new updates are recorded
          update_set_internal(input_aggregator, it->first, &analysis,
                              analysis.usage, expr, expr_covers, 
                              guard_mask, analysis.target_instances,
                              analysis.target_views, analysis.source_views,
                              analysis.trace_info, analysis.record_valid);
          // If we did any updates record ourselves as the new guard here
          if ((input_aggregator != NULL) && 
              ((finder == analysis.input_aggregators.end()) ||
               input_aggregator->has_update_fields()))
          {
            if (finder == analysis.input_aggregators.end())
              analysis.input_aggregators[guard_event] = input_aggregator;
            const FieldMask &update_mask = 
              input_aggregator->get_update_fields();
            // Record this as a guard for later operations
            to_add.insert(input_aggregator, update_mask);
#ifdef DEBUG_LEGION
            if (!input_aggregator->record_guard_set(this, true/*read only*/))
              assert(false);
#else
            input_aggregator->record_guard_set(this, true/*read only*/);
#endif
            // Remove the current guard since it doesn't matter anymore
            it.filter(update_mask);
          }
          user_mask -= guard_mask;
          if (!user_mask)
            break;
        }
        if (!to_add.empty())
        {
          for (FieldMaskSet<CopyFillAggregator>::const_iterator it =
                to_add.begin(); it != to_add.end(); it++)
            read_only_guards.insert(it->first, it->second);
        }
        // If we have unguarded fields we can easily do those
        if (!!user_mask)
        {
          CopyFillAggregator *input_aggregator = NULL;
          // See if we have an input aggregator that we can use now
          std::map<RtEvent,CopyFillAggregator*>::const_iterator finder = 
            analysis.input_aggregators.find(RtEvent::NO_RT_EVENT);
          if (finder != analysis.input_aggregators.end())
          {
            input_aggregator = finder->second;
            if (input_aggregator != NULL)
              input_aggregator->clear_update_fields();
          }
          update_set_internal(input_aggregator, NULL/*no previous guard*/,
                              &analysis, analysis.usage, expr, expr_covers,
                              user_mask, analysis.target_instances,
                              analysis.target_views, analysis.source_views,
                              analysis.trace_info, analysis.record_valid);
          // If we made the input aggregator then store it
          if ((input_aggregator != NULL) &&
              ((finder == analysis.input_aggregators.end()) ||
               input_aggregator->has_update_fields()))
          {
            analysis.input_aggregators[RtEvent::NO_RT_EVENT] = input_aggregator;
            // Record this as a guard for later operations
            read_only_guards.insert(input_aggregator, 
                input_aggregator->get_update_fields());
#ifdef DEBUG_LEGION
            if (!input_aggregator->record_guard_set(this, true/*read only*/))
              assert(false);
#else
            input_aggregator->record_guard_set(this, true/*read only*/);
#endif
          }
        }
      }
      else
      {
        // Read-write or read-only case
        // Read-only case if there are no guards
        CopyFillAggregator *input_aggregator = NULL;
        // See if we have an input aggregator that we can use now
        std::map<RtEvent,CopyFillAggregator*>::const_iterator finder = 
          analysis.input_aggregators.find(RtEvent::NO_RT_EVENT);
        if (finder != analysis.input_aggregators.end())
        {
          input_aggregator = finder->second;
          if (input_aggregator != NULL)
            input_aggregator->clear_update_fields();
        }
        update_set_internal(input_aggregator, NULL/*no previous guard*/,
                            &analysis, analysis.usage,
                            expr, expr_covers, user_mask, 
                            analysis.target_instances, analysis.target_views,
                            analysis.source_views, analysis.trace_info,
                            analysis.record_valid);
        if (IS_WRITE(analysis.usage))
        {
          update_initialized_data(expr, expr_covers, user_mask);
          // Issue copy-out copies for any restricted fields if we wrote stuff
          if (!restricted_instances.empty())
          {
            const FieldMask restricted_mask = user_mask & restricted_fields;
            if (!!restricted_mask)
              copy_out(expr, expr_covers, restricted_mask,
                       analysis.target_instances, analysis.target_views,
                       &analysis, analysis.trace_info,
                       analysis.output_aggregator);
          }
        }
        // If we made the input aggregator then store it
        if ((input_aggregator != NULL) &&
            ((finder == analysis.input_aggregators.end()) ||
             input_aggregator->has_update_fields()))
        {
          analysis.input_aggregators[RtEvent::NO_RT_EVENT] = input_aggregator;
          // Record this as a guard for later read-only operations
          if (IS_READ_ONLY(analysis.usage))
          {
            read_only_guards.insert(input_aggregator, 
                input_aggregator->get_update_fields());
#ifdef DEBUG_LEGION
            if (!input_aggregator->record_guard_set(this, true/*read only*/))
              assert(false);
#else
            input_aggregator->record_guard_set(this, true/*read only*/);
#endif
          }
        }
      }
      // Update the post conditions for these views if we're recording 
      if (analysis.trace_info.recording)
      {
        if (tracing_postconditions == NULL)
          tracing_postconditions =
            new TraceViewSet(context, did, set_expr, tree_id);
        for (unsigned idx = 0; idx < analysis.target_views.size(); idx++)
        {
          for (FieldMaskSet<InstanceView>::const_iterator it =
                analysis.target_views[idx].begin(); it !=
                analysis.target_views[idx].end(); it++)
            update_tracing_valid_views(it->first, expr, analysis.usage,
                                       it->second, IS_WRITE(analysis.usage));
        }
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::check_for_migration(PhysicalAnalysis &analysis,
                                 std::set<RtEvent> &applied_events, bool covers)
    //--------------------------------------------------------------------------
    {
#ifndef LEGION_DISABLE_EQUIVALENCE_SET_MIGRATION
#ifdef DEBUG_LEGION
      assert(is_logical_owner());
#endif
      const AddressSpaceID eq_source = analysis.original_source;
      // Record our user in the set of previous users
      bool found = false;
      std::vector<std::pair<AddressSpaceID,unsigned> > &current_samples = 
        user_samples[migration_index];
      for (std::vector<std::pair<AddressSpaceID,unsigned> >::iterator it =
            current_samples.begin(); it != current_samples.end(); it++)
      {
        if (it->first != eq_source)
          continue;
        found = true;
        it->second++;
        break;
      }
      if (!found)
        current_samples.push_back(
            std::pair<AddressSpaceID,unsigned>(eq_source,1));
      // Increase the sample count and if we haven't done enough
      // for a test then we can return and keep going
      if (++sample_count < SAMPLES_PER_MIGRATION_TEST)
      {
        // Check to see if the request bounced off a stale owner 
        // and we should send the update message
        if ((eq_source != analysis.previous) && (eq_source != local_space) &&
            (eq_source != logical_owner_space))
        {
          RtUserEvent notification_event = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize(logical_owner_space);
            rez.serialize(notification_event);
          }
          runtime->send_equivalence_set_owner_update(eq_source, rez);
          applied_events.insert(notification_event);
        }
        return;
      }
      // Issue a warning and don't migrate if we hit this case
      if (current_samples.size() == SAMPLES_PER_MIGRATION_TEST)
      {
        REPORT_LEGION_WARNING(LEGION_WARNING_LARGE_EQUIVALENCE_SET_NODE_USAGE,
            "Internal runtime performance warning: equivalence set %llx "
            "has %zd different users which is the same as "
            "the sampling rate of %d. Region requirement %d of operation %s "
            "(UID %lld) triggered this warning. Please report this "
            "application use case to the Legion developers mailing list.",
            did, current_samples.size(),
            SAMPLES_PER_MIGRATION_TEST, analysis.index,
            (analysis.op->get_operation_kind() == Operation::TASK_OP_KIND) ?
              static_cast<TaskOp*>(analysis.op)->get_task_name() :
            analysis.op->get_logging_name(), analysis.op->get_unique_op_id())
        // Reset the data structures for the next run
        current_samples.clear();
        sample_count = 0;
        return;
      }
      // If we're replicated and the analysis is not exclusive and covering
      // then it's not even safe to consider migrating this equivalence set
      CollectiveMapping *collective_mapping = analysis.get_replicated_mapping();
      if ((replicated_owner_state != NULL) && (!analysis.exclusive || !covers ||
          (collective_mapping != NULL)))
        return;
      // Sort the current samples so that they are in order for
      // single epoch cases, for multi-epoch cases they will be
      // sorted by the summary computation below
      if ((MIGRATION_EPOCHS == 1) && (current_samples.size() > 1))
        std::sort(current_samples.begin(), current_samples.end());
      // Increment this for the next pass
      migration_index = (migration_index + 1) % MIGRATION_EPOCHS;
      std::vector<std::pair<AddressSpaceID,unsigned> > &next_samples = 
        user_samples[migration_index];
      if (MIGRATION_EPOCHS > 1)
      {
        // Compute the summary from all the epochs into the epoch
        // that we are about to clear
        std::map<AddressSpaceID,unsigned> summary(
            next_samples.begin(), next_samples.end());
        for (unsigned idx = 1; idx < MIGRATION_EPOCHS; idx++)
        {
          const std::vector<std::pair<AddressSpaceID,unsigned> > &other_samples
            = user_samples[(migration_index + idx) % MIGRATION_EPOCHS];
          for (std::vector<std::pair<AddressSpaceID,unsigned> >::const_iterator
                it = other_samples.begin(); it != other_samples.end(); it++)
          {
            std::map<AddressSpaceID,unsigned>::iterator finder = 
              summary.find(it->first);
            if (finder == summary.end())
              summary.insert(*it);
            else
              finder->second += it->second;
          }
        }
        next_samples.clear();
        next_samples.insert(next_samples.begin(),summary.begin(),summary.end());
      }
      AddressSpaceID new_logical_owner = logical_owner_space;
#ifdef DEBUG_LEGION
      assert(!next_samples.empty());
#endif
      if (next_samples.size() > 1)
      {
        int logical_owner_count = -1;
        // Figure out which node(s) has/have the most uses 
        // Make sure that the current owner node is sticky
        // if it is tied for the most uses
        unsigned max_count = next_samples[0].second; 
        AddressSpaceID max_user = next_samples[0].first;
        for (unsigned idx = 1; idx < next_samples.size(); idx++)
        {
          const AddressSpaceID user = next_samples[idx].first;
          const unsigned user_count = next_samples[idx].second;
          if (user == logical_owner_space)
            logical_owner_count = user_count;
          if (user_count < max_count)
            continue;
          // This is the part where we guarantee stickiness
          if ((user_count == max_count) && (user != logical_owner_space))
            continue;
          max_count = user_count;
          max_user = user;
        }
        if (logical_owner_count > 0)
        {
          if (logical_owner_space != max_user)
          {
            // If the logical owner is one of the current users then
            // we really better have a good reason to move this 
            // equivalence set to a new node. For now the difference 
            // between max_count and the current owner count has to
            // be greater than the number of nodes that we see participating
            // on this equivalence set. This heuristic should avoid 
            // the ping-pong case even when our sampling rate does not
            // naturally align with the number of nodes participating
            if ((max_count - unsigned(logical_owner_count)) >
                next_samples.size()) 
              new_logical_owner = max_user;
          }
        }
        else
          // If we didn't have the current logical owner then
          // just pick the maximum one
          new_logical_owner = max_user;
      }
      else
        // If all the requests came from the same node, send it there
        new_logical_owner = next_samples[0].first;
      // This always get reset here
      sample_count = 0;
      // Reset this for the next iteration
      next_samples.clear();
      // See if we are actually going to do the migration
      if (logical_owner_space == new_logical_owner)
      {
        // No need to do the migration in this case
        // Check to see if the request bounced off a stale owner 
        // and we should send the update message
        if ((eq_source != analysis.previous) && (eq_source != local_space) &&
            (eq_source != logical_owner_space))
        {
          RtUserEvent notification_event = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize(logical_owner_space);
            rez.serialize(notification_event);
          }
          runtime->send_equivalence_set_owner_update(eq_source, rez);
          applied_events.insert(notification_event);
        }
        return;
      }
      // At this point we've decided to do the migration
      log_migration.info("Migrating Equivalence Set %llx from %d to %d",
          did, local_space, new_logical_owner);
      logical_owner_space = new_logical_owner;
      const FieldMask all_ones(LEGION_FIELD_MASK_FIELD_ALL_ONES);
      // Do the migration
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        pack_state(rez, logical_owner_space, did, set_expr, set_expr,
          true/*covers*/, all_ones, true/*pack guards*/, true/*pack invalids*/);
      }
      runtime->send_equivalence_set_migration(logical_owner_space, rez);
      invalidate_state(set_expr, true/*covers*/, all_ones, false/*record*/);
      // If we have any replicated state then we need to invalidate that
      // Note that this is only safe because we know that there is an
      // exclusive user here without any collective mapping which means that
      // they are blocking the mapping of all other potential users so its
      // safe for them to perform the invalidation asynchronously
      if (replicated_owner_state != NULL)
      {
#ifdef DEBUG_LEGION
        assert(analysis.exclusive && covers);
        assert(collective_mapping == NULL); 
#endif
        // Note this is only safe because of the check above stating that
        // the analysis is exclusive and not collective
        // Send out invalidations to all the replicated owner nodes and
        // make sure nothing else can map until they are done
        for (std::vector<AddressSpaceID>::const_iterator it =
              replicated_owner_state->children.begin(); it !=
              replicated_owner_state->children.end(); it++)
        {
          const RtUserEvent done = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize(done);
          }
          runtime->send_equivalence_set_replication_invalidation(*it, rez);
          applied_events.insert(done);
        }
        delete replicated_owner_state;
        replicated_owner_state = NULL;
      }
#endif // LEGION_DISABLE_EQUIVALENCE_SET MIGRATION
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::defer_analysis(AutoTryLock &eq,
                                        PhysicalAnalysis &analysis,
                                        const FieldMask &mask,
                                        std::set<RtEvent> &deferral_events,
                                        std::set<RtEvent> &applied_events,
                                        const bool already_deferred)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!eq.has_lock());
#endif
      // See if we've already deferred this or not
      if (!already_deferred)
      {
        const RtUserEvent deferral_event = Runtime::create_rt_user_event();
        const RtEvent precondition = chain_deferral_events(deferral_event);
        analysis.defer_analysis(precondition, this, mask, deferral_events, 
                                applied_events, deferral_event);
      }
      else
        analysis.defer_analysis(eq.try_next(), this, mask, deferral_events, 
                                applied_events);
    }

    //--------------------------------------------------------------------------
    bool EquivalenceSet::is_remote_analysis(PhysicalAnalysis &analysis,
                      FieldMask &mask, std::set<RtEvent> &deferral_events,
                      std::set<RtEvent> &applied_events, const bool expr_covers)
    //--------------------------------------------------------------------------
    {
      // Check to see if the analysis is replicated or not
      if (analysis.is_replicated())
      {
#ifdef DEBUG_LEGION
        assert(!analysis.immutable);
#endif
        const CollectiveMapping &mapping = *analysis.get_replicated_mapping();
        // Check to see if we have a replicated owner node
        if (!replicate_logical_owner_space(local_space, &mapping,false/*lock*/))
        {
#ifdef DEBUG_LEGION
          assert(replicated_owner_state->ready.exists());
#endif
          analysis.defer_analysis(replicated_owner_state->ready, this, mask,
                                  deferral_events, applied_events);
          return true;
        }
        // Now figure out which analysis is going to perform the traversal
        // If an analysis is already local to an equivalence set then we'll
        // want it to do the traversal to minimimize communication. In the 
        // case where the current logical owner space is not contained in
        // the mapping then we'll simply pick the space in our collective
        // that is closest to the owner in the set of linear space names
        // (assuming some degree of locality).
        if (!mapping.contains(logical_owner_space))
        {
          // If we're on the logical owner then that means we're the analysis
          // that already got migrated to the logical owner
          if (is_logical_owner())
          {
#ifdef DEBUG_LEGION
            assert(analysis.original_source ==
                mapping.find_nearest(local_space));
            assert(analysis.is_collective_first_local());
#endif
            return false;
          }
          // There aren't any analyses that will be local to the
          // logical owner space so we need to pick the closest one
          if ((local_space == mapping.find_nearest(logical_owner_space)) &&
              analysis.is_collective_first_local())
            analysis.record_remote(this, mask, logical_owner_space);
        }
        else 
        {
          // At least one node is local, see if we're it
          if ((logical_owner_space == local_space) &&
              analysis.is_collective_first_local())
            return false;
        }
        return true;
      }
      else
      {
        // See if we are the logical owner or not
        if (!is_logical_owner())
        {
          // Not the logical owner, so just need to send it to the owner
          analysis.record_remote(this, mask, logical_owner_space);
          return true;
        }
        else
          return false;
      }
    }

    //--------------------------------------------------------------------------
    EquivalenceSet::ReplicatedOwnerState::ReplicatedOwnerState(bool val)
      : ready(val ? RtUserEvent::NO_RT_USER_EVENT : 
              Runtime::create_rt_user_event()), valid(val), subscribed(false)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    bool EquivalenceSet::replicate_logical_owner_space(AddressSpaceID source, 
                               const CollectiveMapping *mapping, bool need_lock)
    //--------------------------------------------------------------------------
    {
      if (need_lock)
      {
        AutoLock eq(eq_lock);
        return replicate_logical_owner_space(source,mapping,false/*need lock*/);
      }
      if (replicated_owner_state == NULL)
      {
        replicated_owner_state = new ReplicatedOwnerState(is_logical_owner());
        if (!is_logical_owner())
        {
          // Send the request to the next parent in the map
          if (mapping != NULL)
          {
#ifdef DEBUG_LEGION
            assert(mapping->contains(local_space));
#endif
            const AddressSpaceID origin = mapping->get_origin();
            if (local_space != origin)
            {
              const AddressSpaceID parent =
                mapping->get_parent(origin, local_space);
              // Send the request on to our parent space
              Serializer rez;
              {
                RezCheck z(rez);
                rez.serialize(did);
                mapping->pack(rez);
              }
              runtime->send_equivalence_set_replication_request(parent, rez);
            }
            else
            {
              // Send the request on to whomever we thought was the previous owner
              Serializer rez;
              {
                RezCheck z(rez);
                rez.serialize(did);
                rez.serialize<size_t>(0); // no mapping
              }
              runtime->send_equivalence_set_replication_request(
                                        logical_owner_space, rez);
              replicated_owner_state->subscribed = true;
            }
          }
          else
          {
            // Send the request on to whomever we thought was the previous owner
            Serializer rez;
            {
              RezCheck z(rez);
              rez.serialize(did);
              rez.serialize<size_t>(0); // no mapping
            }
            runtime->send_equivalence_set_replication_request(
                                      logical_owner_space, rez);
            replicated_owner_state->subscribed = true;
          }
        }
      }
      if (source != local_space)
        replicated_owner_state->children.push_back(source);
      if (replicated_owner_state->is_valid())
      {
        // If we're already replicated send back the response now
        if (source != local_space)
        {
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize(logical_owner_space);
          }
          runtime->send_equivalence_set_replication_response(source, rez);
        }
        return true;
      }
      else
      {
        // If we're not already subscribed, then perform the subscription
        if (!replicated_owner_state->is_subscribed() && (mapping == NULL))
        {
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize<size_t>(0); // no mapping
          }
          runtime->send_equivalence_set_replication_request(
                                    logical_owner_space, rez);
          replicated_owner_state->subscribed = true;
        }
        return false;
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::process_replication_response(AddressSpaceID owner)
    //--------------------------------------------------------------------------
    {
      RtUserEvent to_trigger;
      {
        AutoLock eq(eq_lock);
#ifdef DEBUG_LEGION
        assert(replicated_owner_state != NULL);
#endif
        // Handle loops created by collective mappings that then wrap back
        // through theselves tracing the chain of previous logical owner spaces
        if (replicated_owner_state->is_valid())
          return;
#ifdef DEBUG_LEGION
        assert(!is_logical_owner());
#endif
        logical_owner_space = owner;
        // Send out messages to all the other nodes that requested them
        for (std::vector<AddressSpaceID>::const_iterator it =
              replicated_owner_state->children.begin(); it !=
              replicated_owner_state->children.end(); it++)
        {
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize(owner);
          }
          runtime->send_equivalence_set_replication_response(*it, rez);
        }
        to_trigger = replicated_owner_state->ready;
        replicated_owner_state->valid = true;
      }
      Runtime::trigger_event(to_trigger);
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::process_replication_invalidation(
                                           std::vector<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      AutoLock eq(eq_lock);
      // Handle loops created by collective mappings that then wrap back
      // through theselves tracing the chain of previous logical owner spaces
      if (replicated_owner_state == NULL)
        return;
#ifdef DEBUG_LEGION
      assert(replicated_owner_state->is_valid());
#endif
      // Forward on this message to any children
      for (std::vector<AddressSpaceID>::const_iterator it =
            replicated_owner_state->children.begin(); it !=
            replicated_owner_state->children.end(); it++)
      {
        const RtUserEvent done = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(did);
          rez.serialize(done);
        }
        runtime->send_equivalence_set_replication_invalidation(*it, rez);
        applied_events.push_back(done);
      }
      delete replicated_owner_state;
      replicated_owner_state = NULL;
    }

    //--------------------------------------------------------------------------
    template<typename T>
    void EquivalenceSet::check_for_uninitialized_data(T &analysis, 
                      IndexSpaceExpression *expr, const bool expr_covers,
                      FieldMask uninit, std::set<RtEvent> &applied_events) const
    //--------------------------------------------------------------------------
    {
      // Do the easy check for the full cover which will be the common case
      FieldMaskSet<IndexSpaceExpression>::const_iterator finder =
        initialized_data.find(set_expr);
      if (finder != initialized_data.end())
      {
        uninit -= finder->second;
        if (!uninit)
          return;
      }
      if (!expr_covers)
      {
        finder = initialized_data.find(expr);
        if (finder != initialized_data.end())
        {
          uninit -= finder->second;
          if (!uninit)
            return;
        }
      }
      // All the rest of these are partial so only test them if 
      // expr_covers is false because we know they aren't covered otherwise
      if (!expr_covers)
      {
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
              initialized_data.begin(); it != initialized_data.end(); it++)
        {
          if (uninit * it->second)
            continue;
          // Don't actually need to subtract here since we don't care
          // about the difference size, just care about domination
          IndexSpaceExpression *overlap_expr = 
            runtime->forest->intersect_index_spaces(it->first, expr);
          if (overlap_expr->get_volume() != expr->get_volume())
            continue;
          uninit -= it->second;
          if (!uninit)
            return;
        }
      }
      // Record anything that we have left
      analysis.record_uninitialized(uninit, applied_events);
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::update_initialized_data(IndexSpaceExpression *expr,
                                                 const bool expr_covers,
                                                 const FieldMask &user_mask)
    //--------------------------------------------------------------------------
    {
      if (!expr_covers)
      {
        FieldMask subinit = user_mask;
        FieldMaskSet<IndexSpaceExpression>::iterator finder =
          initialized_data.find(set_expr);
        // Check to see if we've already initialized it for the full set_expr
        if (finder != initialized_data.end())
        {
          subinit -= finder->second;
          // Already initialized for full expression so we are done
          if (!subinit)
            return;
        }
        FieldMaskSet<IndexSpaceExpression> to_add;
        std::vector<IndexSpaceExpression*> to_delete;
        for (FieldMaskSet<IndexSpaceExpression>::iterator it = 
              initialized_data.begin(); it != initialized_data.end(); it++)
        {
          if ((it->first == set_expr) || (it->first == expr))
            continue;
          const FieldMask overlap = subinit & it->second;
          if (!overlap)
            continue;
          // Compute the union expression
          IndexSpaceExpression *union_expr = 
            runtime->forest->union_index_spaces(it->first, expr);
          const size_t union_size = union_expr->get_volume();
#ifdef DEBUG_LEGION
          assert(union_size <= set_expr->get_volume());
#endif
          if (union_size == it->first->get_volume())
          {
            // Existing expression already covers expr
            subinit -= overlap;
            if (!subinit)
              break;
          }
          else if (union_size == set_expr->get_volume())
          {
            // Union is the same as the set expression
            if (finder != initialized_data.end())
              finder.merge(overlap);
            else
              to_add.insert(set_expr, overlap);
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
            subinit -= overlap;
            if (!subinit)
              break;
          }
          else if (union_size == expr->get_volume())
          {
            // New expression covers the old expression
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
          }
          else
          {
            // Union is bigger than both expression but not set_expr
            to_add.insert(union_expr, overlap);
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
            subinit -= overlap;
            if (!subinit)
              break;
          }
        }
        // Add new ones
        if (!to_add.empty())
        {
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                to_add.begin(); it != to_add.end(); it++)
            if (initialized_data.insert(it->first, it->second))
              it->first->add_nested_expression_reference(did);
        }
        // Delete after adding to keep expressions valid 
        if (!to_delete.empty())
        {
          for (std::vector<IndexSpaceExpression*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
          {
            if (to_add.find(*it) != to_add.end())
              continue;
            initialized_data.erase(*it);
            if ((*it)->remove_nested_expression_reference(did))
              delete (*it);
          }
        }
        // Add the new expression if we still have fields to add
        if (!!subinit && initialized_data.insert(expr, subinit))
          expr->add_nested_expression_reference(did);
      }
      else
      {
        // Remove all other expressions with overlapping fields
        if (!(user_mask * initialized_data.get_valid_mask()))
        {
          std::vector<IndexSpaceExpression*> to_delete;
          for (FieldMaskSet<IndexSpaceExpression>::iterator it = 
                initialized_data.begin(); it != initialized_data.end(); it++)
          {
            if (it->first == set_expr)
              continue;
            it.filter(user_mask);
            if (!it->second)
              to_delete.push_back(it->first);
          }
          if (!to_delete.empty())
          {
            for (std::vector<IndexSpaceExpression*>::const_iterator it =
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              initialized_data.erase(*it);
              if ((*it)->remove_nested_expression_reference(did))
                delete (*it);
            }
          }
        }
        if (initialized_data.insert(set_expr, user_mask))
          set_expr->add_nested_expression_reference(did);
      }
    }

    //--------------------------------------------------------------------------
    template<typename T>
    void EquivalenceSet::record_instances(IndexSpaceExpression *expr,
                                          const bool expr_covers,
                                          const FieldMask &record_mask,
                                          const FieldMaskSet<T> &target_insts)
    //--------------------------------------------------------------------------
    {
      bool rebuild_partial = false;
      if (expr_covers)
      {
        if (!(target_insts.get_valid_mask() - record_mask))
        {
          for (typename FieldMaskSet<T>::const_iterator it =
                target_insts.begin(); it != target_insts.end(); it++)
          {
            if (total_valid_instances.insert(it->first, it->second))
            {
              it->first->add_nested_valid_ref(did);
              // Check if this is a collective view we need to track
              if (it->first->is_collective_view() &&
                  collective_instances.insert(it->first->as_collective_view(),
                                              it->second))
                it->first->add_nested_resource_ref(did);
            }
            // Check to see if there are any copies of this to filter
            // from the partially valid instances
            ViewExprMaskSets::iterator finder =
              partial_valid_instances.find(it->first);
            if ((finder != partial_valid_instances.end()) &&
                !(finder->second.get_valid_mask() * it->second))
            {
              rebuild_partial = true;
              if (!(finder->second.get_valid_mask() - it->second))
              {
                // We're pruning everything so remove them all now
                for (FieldMaskSet<IndexSpaceExpression>::const_iterator eit =
                     finder->second.begin(); eit != finder->second.end(); eit++)
                  if (eit->first->remove_nested_expression_reference(did))
                    delete eit->first;
                // Remove the reference, no need to check for deletion
                // since we know we added the same reference above
                finder->first->remove_nested_valid_ref(did);
                partial_valid_instances.erase(finder);
              }
              else
              {
                // Filter out the ones we now subsume
                std::vector<IndexSpaceExpression*> to_delete;    
                for (FieldMaskSet<IndexSpaceExpression>::iterator eit =
                     finder->second.begin(); eit != finder->second.end(); eit++)
                {
                  eit.filter(it->second);
                  if (!eit->second)
                    to_delete.push_back(eit->first);
                }
                for (std::vector<IndexSpaceExpression*>::const_iterator eit =
                      to_delete.begin(); eit != to_delete.end(); eit++)
                {
                  finder->second.erase(*eit);
                  if ((*eit)->remove_nested_expression_reference(did))
                    delete (*eit);
                }
                if (finder->second.empty())
                {
                  // Remove the reference, no need to check for deletion
                  // since we know we added the same reference above
                  finder->first->remove_nested_valid_ref(did);
                  partial_valid_instances.erase(finder);
                }
                else
                  finder->second.tighten_valid_mask();
              }
            }
          } 
        }
        else
        {
          for (typename FieldMaskSet<T>::const_iterator it =
                target_insts.begin(); it != target_insts.end(); it++)
          {
            const FieldMask valid_mask = it->second & record_mask; 
            if (!valid_mask)
              continue;
            // Add it to the set
            if (total_valid_instances.insert(it->first, valid_mask))
            {
              it->first->add_nested_valid_ref(did);
              // Check if this is a collective view we need to track
              if (it->first->is_collective_view() &&
                  collective_instances.insert(it->first->as_collective_view(),
                                              valid_mask))
                it->first->add_nested_resource_ref(did);
            }
            // Check to see if there are any copies of this to filter
            // from the partially valid instances
            ViewExprMaskSets::iterator finder = 
              partial_valid_instances.find(it->first);
            if ((finder != partial_valid_instances.end()) &&
                !(finder->second.get_valid_mask() * valid_mask))
            {
              rebuild_partial = true;
              if (!(finder->second.get_valid_mask() - valid_mask))
              {
                // We're pruning everything so remove them all now
                for (FieldMaskSet<IndexSpaceExpression>::const_iterator eit =
                     finder->second.begin(); eit != finder->second.end(); eit++)
                  if (eit->first->remove_nested_expression_reference(did))
                    delete eit->first;
                // Remove the reference, no need to check for deletion
                // since we know we added the same reference above
                finder->first->remove_nested_valid_ref(did);
                partial_valid_instances.erase(finder);
              }
              else
              {
                // Filter out the ones we now subsume
                std::vector<IndexSpaceExpression*> to_delete;    
                for (FieldMaskSet<IndexSpaceExpression>::iterator eit =
                     finder->second.begin(); eit != finder->second.end(); eit++)
                {
                  eit.filter(valid_mask);
                  if (!eit->second)
                    to_delete.push_back(eit->first);
                }
                for (std::vector<IndexSpaceExpression*>::const_iterator eit =
                      to_delete.begin(); eit != to_delete.end(); eit++)
                {
                  finder->second.erase(*eit);
                  if ((*eit)->remove_nested_expression_reference(did))
                    delete (*eit);
                }
                if (finder->second.empty())
                {
                  // Remove the reference, no need to check for deletion
                  // since we know we added the same reference above
                  finder->first->remove_nested_valid_ref(did);
                  partial_valid_instances.erase(finder);
                }
                else
                  finder->second.tighten_valid_mask();
              }
            }
          }
        }
      }
      else
      {
        if (!(target_insts.get_valid_mask() - record_mask))
        {
          for (typename FieldMaskSet<T>::const_iterator it =
                target_insts.begin(); it != target_insts.end(); it++)
            if (record_partial_valid_instance(it->first, expr,
                                              it->second))
              rebuild_partial = true;
        }
        else
        {
          for (typename FieldMaskSet<T>::const_iterator it =
                target_insts.begin(); it != target_insts.end(); it++)
          {
            const FieldMask valid_mask = it->second & record_mask; 
            if (!valid_mask)
              continue;
            if (record_partial_valid_instance(it->first, expr,
                                              valid_mask))
              rebuild_partial = true;
          }
        }
      }
      if (rebuild_partial)
      {
        partial_valid_fields.clear();
        for (ViewExprMaskSets::const_iterator it =
              partial_valid_instances.begin(); it !=
              partial_valid_instances.end(); it++)
          partial_valid_fields |= it->second.get_valid_mask();
      }
    }

    //--------------------------------------------------------------------------
    template<typename T>
    void EquivalenceSet::record_unrestricted_instances(
                                          IndexSpaceExpression *expr,
                                          const bool expr_covers,
                                          FieldMask record_mask,
                                          const FieldMaskSet<T> &target_insts)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!(record_mask - restricted_fields));
#endif
      // Check to see if there are any restrictions which cover the whole
      // set and therefore we know that there are on partial coverings
      ExprViewMaskSets::const_iterator finder =
        restricted_instances.find(set_expr);
      if (finder != restricted_instances.end())
      {
        record_mask -= finder->second.get_valid_mask();
        if (!record_mask)
          return;
      }
      // The only fields left here are the partial restrictions
      FieldMaskSet<IndexSpaceExpression> restrictions;
      for (ExprViewMaskSets::const_iterator it = restricted_instances.begin();
            it != restricted_instances.end(); it++)
      {
        if (it == finder)
          continue;
        const FieldMask overlap = it->second.get_valid_mask() & record_mask;
        if (!overlap)
          continue;
        if (!expr_covers)
        {
          IndexSpaceExpression *overlap_expr =
            runtime->forest->intersect_index_spaces(expr, it->first);
          if (!overlap_expr->is_empty())
            restrictions.insert(overlap_expr, overlap);
        }
        else
          restrictions.insert(it->first, overlap);
      }
      // Sort these into grouped field sets so we can union them before
      // doing the subtraction to figure out what we can record
      LegionList<FieldSet<IndexSpaceExpression*> > restricted_sets;
      restrictions.compute_field_sets(record_mask, restricted_sets);
      bool need_partial_rebuild = false;
      for (LegionList<FieldSet<IndexSpaceExpression*> >::const_iterator
            rit = restricted_sets.begin(); rit != restricted_sets.end(); rit++)
      {
        IndexSpaceExpression *diff_expr = NULL;
        if (!rit->elements.empty())
        {
          IndexSpaceExpression *union_expr = 
            runtime->forest->union_index_spaces(rit->elements);
          diff_expr = runtime->forest->subtract_index_spaces(expr, union_expr);
        }
        else
          diff_expr = expr;
        if (!diff_expr->is_empty())
        {
          for (typename FieldMaskSet<T>::const_iterator it =
                target_insts.begin(); it != target_insts.end(); it++)
          {
            const FieldMask valid_mask = it->second & rit->set_mask; 
            if (!valid_mask)
              continue;
            if (record_partial_valid_instance(it->first, diff_expr, 
                                              valid_mask))
              need_partial_rebuild = true;
          }
        }
#ifdef DEBUG_LEGION
        record_mask -= rit->set_mask;
#endif
      }
#ifdef DEBUG_LEGION
      assert(!record_mask);
#endif
      if (need_partial_rebuild)
      {
        partial_valid_fields.clear();
        for (ViewExprMaskSets::const_iterator it = 
              partial_valid_instances.begin();  it !=
              partial_valid_instances.end(); it++)
          partial_valid_fields |= it->second.get_valid_mask();
      }
    }

    //--------------------------------------------------------------------------
    bool EquivalenceSet::record_partial_valid_instance(LogicalView *target,
                              IndexSpaceExpression *expr, FieldMask valid_mask, 
                              bool check_total_valid)
    //--------------------------------------------------------------------------
    {
      bool need_rebuild = false;
      if (check_total_valid)
      {
        FieldMaskSet<LogicalView>::const_iterator finder = 
          total_valid_instances.find(target);
        if (finder != total_valid_instances.end())
        {
          valid_mask -= finder->second;
          if (!valid_mask)
            return need_rebuild;
        }
      }
      partial_valid_fields |= valid_mask;
      ViewExprMaskSets::iterator finder = partial_valid_instances.find(target);
      if (finder != partial_valid_instances.end())
      {
        // See if we have any overlapping field expressions to add this to 
        if (!(valid_mask * finder->second.get_valid_mask()))
        {
          std::vector<IndexSpaceExpression*> to_delete;
          FieldMaskSet<IndexSpaceExpression> to_add;
          bool need_tighten = false;
          for (FieldMaskSet<IndexSpaceExpression>::iterator it =
                finder->second.begin(); it != finder->second.end(); it++)
          {
            const FieldMask overlap = it->second & valid_mask;
            if (!overlap)
              continue;
            IndexSpaceExpression *union_expr = 
              runtime->forest->union_index_spaces(it->first, expr);
            const size_t union_size = union_expr->get_volume();
#ifdef DEBUG_LEGION
            assert(union_size <= set_expr->get_volume());
#endif
            if (union_size == set_expr->get_volume())
            {
              // Hurray, we now cover the full expr so we can get
              // promoted up to the total valid instances
              it.filter(overlap);
              if (!it->second)
                to_delete.push_back(it->first);
              if (total_valid_instances.insert(target, overlap))
                target->add_nested_valid_ref(did);
              // No need for a collective instance check here since it
              // was already recorded in the partial valid instances
              need_tighten = true;
            }
            else if (union_size == expr->get_volume())
            {
              // We dominate the previous expression, so remove it
              // and put ourselves in
              it.filter(overlap);
              if (!it->second)
                to_delete.push_back(it->first);
              to_add.insert(expr, overlap);
            }
            else if (union_size > it->first->get_volume())
            {
              // Union dominates both so put it in instead
              it.filter(overlap);
              if (!it->second)
                to_delete.push_back(it->first);
              to_add.insert(union_expr, overlap);
            }
            // Else previous expr dominates so we can just leave it there
            valid_mask -= overlap;
            if (!valid_mask)
              break;
          }
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                to_add.begin(); it != to_add.end(); it++)
            if (finder->second.insert(it->first, it->second))
              it->first->add_nested_expression_reference(did);
          for (std::vector<IndexSpaceExpression*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
          {
            if (to_add.find(*it) != to_add.end())
              continue;
            finder->second.erase(*it);
            if ((*it)->remove_nested_expression_reference(did))
              delete (*it);
          }
          if (!!valid_mask && finder->second.insert(expr, valid_mask))
            expr->add_nested_expression_reference(did);
          if (need_tighten)
          {
            if (finder->second.empty())
            {
              // Wow! everything got promoted up to total valid
              // instances, lucky us, remove the old partial stuff
              finder->first->remove_nested_valid_ref(did);
              partial_valid_instances.erase(finder);
            }
            else
              finder->second.tighten_valid_mask();
            if (!partial_valid_instances.empty())
              need_rebuild = true;
            else
              partial_valid_fields.clear();
          }
        }
        else if (finder->second.insert(expr, valid_mask))
          expr->add_nested_expression_reference(did);
      }
      else
      {
        partial_valid_instances[target].insert(expr, valid_mask);
        target->add_nested_valid_ref(did);
        expr->add_nested_expression_reference(did);
        // Check to see if this is a collective view we need to record
        if (target->is_collective_view() &&
            collective_instances.insert(target->as_collective_view(),
                                        valid_mask))
          target->add_nested_resource_ref(did);
      }
      return need_rebuild;
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::filter_valid_instances(IndexSpaceExpression *expr,
                  const bool expr_covers, const FieldMask &filter_mask,
                  std::map<IndexSpaceExpression*,unsigned> *expr_refs_to_remove,
                  std::map<LogicalView*,unsigned> *view_refs_to_remove)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!!filter_mask);
#endif
      // Clear out any collective instances
      if (!(filter_mask * collective_instances.get_valid_mask()))
      {
        // Remove all the overlapping collective instances
        if (!!(collective_instances.get_valid_mask() - filter_mask))
        {
          std::vector<CollectiveView*> to_delete;
          for (FieldMaskSet<CollectiveView>::iterator it = 
                collective_instances.begin(); it != 
                collective_instances.end(); it++)
          {
            const FieldMask overlap = it->second & filter_mask;
            if (!overlap)
              continue;
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
          }
          if (!to_delete.empty())
          {
            for (std::vector<CollectiveView*>::const_iterator it = 
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              collective_instances.erase(*it);
              if ((*it)->remove_nested_resource_ref(did))
                delete (*it);
            }
          }
        }
        else
        {
          for (FieldMaskSet<CollectiveView>::const_iterator it =
                collective_instances.begin(); it !=
                collective_instances.end(); it++)
            if (it->first->remove_nested_resource_ref(did))
              delete it->first;
          collective_instances.clear();
        }
      }
      if (expr_covers)
      {
        // If the expr covers we can just filter everything
        if (!(filter_mask * total_valid_instances.get_valid_mask()))
        {
          // Clear out the total valid instances first
          std::vector<LogicalView*> to_delete;
          for (FieldMaskSet<LogicalView>::iterator it = 
                total_valid_instances.begin(); it != 
                total_valid_instances.end(); it++)
          {
            const FieldMask overlap = it->second & filter_mask;
            if (!overlap)
              continue;
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
          }
          if (!to_delete.empty())
          {
            for (std::vector<LogicalView*>::const_iterator it = 
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              total_valid_instances.erase(*it);
              if (view_refs_to_remove != NULL)
              {
                std::map<LogicalView*,unsigned>::iterator finder = 
                  view_refs_to_remove->find(*it);
                if (finder == view_refs_to_remove->end())
                  (*view_refs_to_remove)[*it] = 1;
                else
                  finder->second += 1;
              }
              else if ((*it)->remove_nested_valid_ref(did))
                delete (*it);
            }
          }
        }
        if (!(filter_mask * partial_valid_fields))
        {
          // Then clear out the partial valid instances
          std::vector<LogicalView*> to_delete;
          for (ViewExprMaskSets::iterator pit =
                partial_valid_instances.begin(); pit != 
                partial_valid_instances.end(); pit++)
          {
            const FieldMask &summary_mask = pit->second.get_valid_mask();
            if (summary_mask * filter_mask)
              continue;
            else if (!(summary_mask - filter_mask))
            {
              // Invalidating all the expressions
              for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                    pit->second.begin(); it != pit->second.end(); it++)
              {
                if (expr_refs_to_remove != NULL)
                {
                  std::map<IndexSpaceExpression*,unsigned>::iterator finder =
                    expr_refs_to_remove->find(it->first);
                  if (finder == expr_refs_to_remove->end())
                    (*expr_refs_to_remove)[it->first] = 1;
                  else
                    finder->second += 1;
                }
                else if (it->first->remove_nested_expression_reference(did))
                  delete it->first;
              }
              to_delete.push_back(pit->first);
            }
            else
            {
              // Only invalidating some of the expressions
              std::vector<IndexSpaceExpression*> to_erase;
              for (FieldMaskSet<IndexSpaceExpression>::iterator it =
                    pit->second.begin(); it != pit->second.end(); it++)
              {
                it.filter(filter_mask);
                if (!it->second)
                  to_erase.push_back(it->first);
              }
              if (!to_erase.empty())
              {
                for (std::vector<IndexSpaceExpression*>::const_iterator it = 
                      to_erase.begin(); it != to_erase.end(); it++)
                {
                  pit->second.erase(*it);
                  if (expr_refs_to_remove != NULL)
                  {
                    std::map<IndexSpaceExpression*,unsigned>::iterator finder =
                      expr_refs_to_remove->find(*it);
                    if (finder == expr_refs_to_remove->end())
                      (*expr_refs_to_remove)[*it] = 1;
                    else
                      finder->second += 1;
                  }
                  else if ((*it)->remove_nested_expression_reference(did))
                    delete (*it);
                }
              }
              pit->second.tighten_valid_mask();
            }
          }
          if (!to_delete.empty())
          {
            for (std::vector<LogicalView*>::const_iterator it = 
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              partial_valid_instances.erase(*it);
              if (view_refs_to_remove != NULL)
              {
                std::map<LogicalView*,unsigned>::iterator finder = 
                  view_refs_to_remove->find(*it);
                if (finder == view_refs_to_remove->end())
                  (*view_refs_to_remove)[*it] = 1;
                else
                  finder->second += 1;
              }
              else if ((*it)->remove_nested_valid_ref(did))
                delete (*it);
            }
          }
          partial_valid_fields -= filter_mask;
        }
      }
      else
      { 
        // If the expr does not cover then we have to do partial filtering
        // Filter any partial data first
        if (!(filter_mask * partial_valid_fields))
        {
          std::vector<LogicalView*> to_delete;
          FieldMask still_partial_valid;
          for (ViewExprMaskSets::iterator pit =
                partial_valid_instances.begin(); pit != 
                partial_valid_instances.end(); pit++)
          {
            FieldMask view_overlap = pit->second.get_valid_mask() & filter_mask;
            if (!view_overlap)
              continue;
            std::vector<IndexSpaceExpression*> to_erase;
            FieldMaskSet<IndexSpaceExpression> to_add;
            for (FieldMaskSet<IndexSpaceExpression>::iterator it =
                  pit->second.begin(); it != pit->second.end(); it++)
            {
              const FieldMask overlap = it->second & view_overlap;
              if (!overlap)
                continue;
              IndexSpaceExpression *diff = 
                runtime->forest->subtract_index_spaces(it->first, expr);
              if (diff->is_empty())
              {
                // filter expr covers, so remove it
                it.filter(overlap);
                if (!it->second)
                  to_erase.push_back(it->first);
              }
              else if (diff->get_volume() < it->first->get_volume())
              {
                // filter expr covers some, so these fields and make
                // the diff the new expression for these overlap fields
                it.filter(overlap);
                if (!it->second)
                  to_erase.push_back(it->first);
                to_add.insert(diff, overlap);
              }
              // else expr does not cover any so nothing to do here
              view_overlap -= overlap;
              if (!view_overlap)
                break;
            } 
            for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                  to_add.begin(); it != to_add.end(); it++)
              if (pit->second.insert(it->first, it->second))
                it->first->add_nested_expression_reference(did);
            // Deletions after adding to make sure to keep referenes around
            for (std::vector<IndexSpaceExpression*>::const_iterator it =
                  to_erase.begin(); it != to_erase.end(); it++)
            {
              // Don't delete if we just added it
              if (to_add.find(*it) != to_add.end())
                continue;
              FieldMaskSet<IndexSpaceExpression>::iterator finder =
                pit->second.find(*it);
#ifdef DEBUG_LEGION
              assert(finder != pit->second.end());
#endif
              if (!!finder->second)
                continue;
              pit->second.erase(finder);
              if (expr_refs_to_remove != NULL)
              {
                std::map<IndexSpaceExpression*,unsigned>::iterator finder2 =
                  expr_refs_to_remove->find(*it);
                if (finder2 == expr_refs_to_remove->end())
                  (*expr_refs_to_remove)[*it] = 1;
                else
                  finder2->second += 1;
              }
              else if ((*it)->remove_nested_expression_reference(did))
                delete (*it);
            }
            if (!pit->second.empty())
            {
              pit->second.tighten_valid_mask();
              still_partial_valid |= pit->second.get_valid_mask();
              // Check if this a collective view to record
              if (pit->first->is_collective_view() &&
                  collective_instances.insert(pit->first->as_collective_view(),
                    pit->second.get_valid_mask()))
                pit->first->add_nested_resource_ref(did);
            }
            else
              to_delete.push_back(pit->first);
          }
          for (std::vector<LogicalView*>::const_iterator it = 
                to_delete.begin(); it != to_delete.end(); it++)
          {
            partial_valid_instances.erase(*it);
            if (view_refs_to_remove != NULL)
            {
              std::map<LogicalView*,unsigned>::iterator finder = 
                view_refs_to_remove->find(*it);
              if (finder == view_refs_to_remove->end())
                (*view_refs_to_remove)[*it] = 1;
              else
                finder->second += 1;
            }
            else if ((*it)->remove_nested_valid_ref(did))
              delete (*it);
          }
          partial_valid_fields -= (filter_mask - still_partial_valid);
        }
        // Now we can filter the total valid instances back to the
        // partial valid instances
        if (!(filter_mask * total_valid_instances.get_valid_mask()))
        {
          std::vector<LogicalView*> to_delete;
          bool need_partial_rebuild = false;
          IndexSpaceExpression *diff_expr = NULL;
          for (FieldMaskSet<LogicalView>::iterator it =
                total_valid_instances.begin(); it != 
                total_valid_instances.end(); it++)
          {
            const FieldMask overlap = filter_mask & it->second;
            if (!overlap)
              continue;
            if (diff_expr == NULL)
            {
              diff_expr = runtime->forest->subtract_index_spaces(set_expr,expr);
#ifdef DEBUG_LEGION
              assert(!diff_expr->is_empty());
#endif
            }
            if (record_partial_valid_instance(it->first, diff_expr, 
                      overlap, false/*check total valid*/)) 
              need_partial_rebuild = true;
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
            // Check if this is a collective view to record
            else if (it->first->is_collective_view() &&
                collective_instances.insert(it->first->as_collective_view(),
                                            it->second))
              it->first->add_nested_resource_ref(did);
          }
          for (std::vector<LogicalView*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
          {
            if (view_refs_to_remove != NULL)
            {
              std::map<LogicalView*,unsigned>::iterator finder = 
                view_refs_to_remove->find(*it);
              if (finder == view_refs_to_remove->end())
                (*view_refs_to_remove)[*it] = 1;
              else
                finder->second += 1;
            }
            else if ((*it)->remove_nested_valid_ref(did))
              delete (*it);
            total_valid_instances.erase(*it);
          }
          total_valid_instances.tighten_valid_mask();
          if (need_partial_rebuild)
          {
            partial_valid_fields.clear();
            for (ViewExprMaskSets::const_iterator it =
                  partial_valid_instances.begin(); it !=
                  partial_valid_instances.end(); it++)
              partial_valid_fields |= it->second.get_valid_mask();
          }
        } 
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::filter_unrestricted_instances(
                             IndexSpaceExpression *expr, const bool expr_covers,
                             FieldMask filter_mask)
    //--------------------------------------------------------------------------
    {
      // Compute the expressions and field masks which are not restricted    
      // First remove any fields which are restricted for this set
      ExprViewMaskSets::const_iterator finder =
        restricted_instances.find(expr);
      if (finder != restricted_instances.end())
      {
        filter_mask -= finder->second.get_valid_mask();
        if (!filter_mask)
          return;
      }
      // Next see if there any full set restrictions which dominate our expr
      if (expr != set_expr)
      {
        finder = restricted_instances.find(set_expr);
        if (finder != restricted_instances.end())
        {
          filter_mask -= finder->second.get_valid_mask();
          if (!filter_mask)
            return;
        }
      }
      // If we're still here, then we now have to do the hard part of
      // computing the intefering expression sets
      FieldMaskSet<IndexSpaceExpression> restricted_sets;
      for (ExprViewMaskSets::const_iterator it = restricted_instances.begin();
            it != restricted_instances.end(); it++)
      {
        const FieldMask overlap = it->second.get_valid_mask() & filter_mask;
        if (!overlap)
          continue;
        IndexSpaceExpression *expr_overlap = 
          runtime->forest->intersect_index_spaces(it->first, expr);
        if (expr_overlap->is_empty())
          continue;
        if (expr_overlap->get_volume() == expr->get_volume())
        {
          // If this expression dominates the expr we are done
          filter_mask -= overlap;
          if (!filter_mask)
            return;
        }
        restricted_sets.insert(expr_overlap, overlap);
      }
#ifdef DEBUG_LEGION
      assert(!!filter_mask);
#endif
      // compute the field sets and take the field differences
      LegionList<FieldSet<IndexSpaceExpression*> > field_sets;
      restricted_sets.compute_field_sets(filter_mask, field_sets);
      for (LegionList<FieldSet<IndexSpaceExpression*> >::iterator it =
            field_sets.begin(); it != field_sets.end(); it++)
      {
        if (it->elements.empty())
        {
          filter_valid_instances(expr, expr_covers, it->set_mask);
          continue;
        }
        IndexSpaceExpression *union_expr =
          runtime->forest->union_index_spaces(it->elements);
        IndexSpaceExpression *diff_expr =
          runtime->forest->subtract_index_spaces(expr, union_expr);
        if (!diff_expr->is_empty())
          filter_valid_instances(diff_expr, false/*covers*/, it->set_mask);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::filter_reduction_instances(IndexSpaceExpression *expr,
                  const bool expr_covers, const FieldMask &filter_mask,
                  std::map<IndexSpaceExpression*,unsigned> *expr_refs_to_remove,
                  std::map<LogicalView*,unsigned> *view_refs_to_remove)
    //--------------------------------------------------------------------------
    {
      int fidx = filter_mask.find_first_set();
      while (fidx >= 0)
      {
        std::map<unsigned,std::list<
          std::pair<InstanceView*,IndexSpaceExpression*> > >::iterator
          finder = reduction_instances.find(fidx);
        if (finder != reduction_instances.end())
        {
          if (expr_covers)
          {
            for (std::list<std::pair<InstanceView*,IndexSpaceExpression*> >::
                  const_iterator it = finder->second.begin(); it !=
                  finder->second.end(); it++)
            {
              if (view_refs_to_remove != NULL)
              {
                std::map<LogicalView*,unsigned>::iterator finder = 
                  view_refs_to_remove->find(it->first);
                if (finder == view_refs_to_remove->end())
                  (*view_refs_to_remove)[it->first] = 1;
                else
                  finder->second += 1;
              }
              else if (it->first->remove_nested_valid_ref(did))
                delete it->first;
              if (expr_refs_to_remove != NULL)
              {
                std::map<IndexSpaceExpression*,unsigned>::iterator finder =
                  expr_refs_to_remove->find(it->second);
                if (finder == expr_refs_to_remove->end())
                  (*expr_refs_to_remove)[it->second] = 1;
                else
                  finder->second += 1;
              }
              else if (it->second->remove_nested_expression_reference(did))
                delete it->second;
            }
            reduction_instances.erase(finder);
            reduction_fields.unset_bit(fidx);
          }
          else
          {
            IndexSpaceExpression *full_diff = NULL;
            for (std::list<std::pair<InstanceView*,IndexSpaceExpression*> >::
                  iterator it = finder->second.begin(); it != 
                  finder->second.end(); /*nothing*/)
            {
              if (it->second == set_expr)
              {
                if (full_diff == NULL)
                {
                  full_diff = 
                    runtime->forest->subtract_index_spaces(set_expr, expr);
#ifdef DEBUG_LEGION
                  assert(!full_diff->is_empty());
#endif
                }
                full_diff->add_nested_expression_reference(did);
                if (expr_refs_to_remove != NULL)
                {
                  std::map<IndexSpaceExpression*,unsigned>::iterator finder =
                    expr_refs_to_remove->find(it->second);
                  if (finder == expr_refs_to_remove->end())
                    (*expr_refs_to_remove)[it->second] = 1;
                  else
                    finder->second += 1;
                }
                else if (it->second->remove_nested_expression_reference(did))
                  delete it->second;
                it->second = full_diff;
                it++;
              }
              else
              {
                IndexSpaceExpression *diff_expr = 
                  runtime->forest->subtract_index_spaces(it->second, expr);
                if (!diff_expr->is_empty())
                {
                  if (diff_expr->get_volume() < it->second->get_volume())
                  {
                    diff_expr->add_nested_expression_reference(did);
                    if (expr_refs_to_remove != NULL)
                    {
                      std::map<IndexSpaceExpression*,unsigned>::iterator finder =
                        expr_refs_to_remove->find(it->second);
                      if (finder == expr_refs_to_remove->end())
                        (*expr_refs_to_remove)[it->second] = 1;
                      else
                        finder->second += 1;
                    }
                    else if (it->second->remove_nested_expression_reference(
                                                                        did))
                      delete it->second;
                    it->second = diff_expr;
                  }
                  // Otherwise, no overlap so we keep going
                  it++;
                }
                else
                {
                  if (view_refs_to_remove != NULL)
                  {
                    std::map<LogicalView*,unsigned>::iterator finder = 
                      view_refs_to_remove->find(it->first);
                    if (finder == view_refs_to_remove->end())
                      (*view_refs_to_remove)[it->first] = 1;
                    else
                      finder->second += 1;
                  }
                  else if (it->first->remove_nested_valid_ref(did))
                    delete it->first;
                  if (expr_refs_to_remove != NULL)
                  {
                    std::map<IndexSpaceExpression*,unsigned>::iterator finder =
                      expr_refs_to_remove->find(it->second);
                    if (finder == expr_refs_to_remove->end())
                      (*expr_refs_to_remove)[it->second] = 1;
                    else
                      finder->second += 1;
                  }
                  else if (it->second->remove_nested_expression_reference(did))
                    delete it->second;
                  it = finder->second.erase(it);
                }
              }
            }
            if (finder->second.empty())
            {
              reduction_instances.erase(finder);
              reduction_fields.unset_bit(fidx);
            }
          }
        }
        fidx = filter_mask.find_next_set(fidx+1);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::update_set_internal(
                  CopyFillAggregator *&input_aggregator,
                  CopyFillGuard *previous_guard,
                  PhysicalAnalysis *analysis,
                  const RegionUsage &usage,
                  IndexSpaceExpression *expr, 
                  const bool expr_covers,
                  const FieldMask &user_mask,
                  const std::vector<PhysicalManager*> &target_instances,
                  const LegionVector<FieldMaskSet<InstanceView> > &target_views,
                  const std::vector<IndividualView*> &source_views,
                  const PhysicalTraceInfo &trace_info,
                  const bool record_valid, const bool record_release)
    //--------------------------------------------------------------------------
    {
      // Read-write or read-only
      // Issue fills and or copies to bring the target instances up to date
      make_instances_valid(input_aggregator, previous_guard, analysis,
                           false/*track*/, expr, expr_covers, user_mask, 
                           target_instances, target_views, source_views,
                           trace_info);
      const bool is_write = IS_WRITE(usage);
      const FieldMask reduce_mask = reduction_fields & user_mask;
      const FieldMask restricted_mask = restricted_fields & user_mask;
      if (!!reduce_mask)
      {
        // Apply any reductions 
        FieldMaskSet<IndexSpaceExpression> applied_reductions;  
        apply_reductions(target_instances, target_views, expr, expr_covers,
                         reduce_mask, input_aggregator, previous_guard,
                         analysis, false/*track*/, trace_info,
                         is_write ? NULL : &applied_reductions);
        // If we're writing we're going to do an invalidation there anyway
        // so no need to bother with doing the invalidation based on the
        // reductions that have been applied
        if (!applied_reductions.empty())
        {
#ifdef DEBUG_LEGION
          assert(!is_write);
#endif
          // See if covered the full expressions for invalidation
          FieldMaskSet<IndexSpaceExpression>::iterator finder = 
            applied_reductions.find(expr);
          if (finder != applied_reductions.end())
          {
            if (!!restricted_mask)
            {
              const FieldMask overlap = finder->second & restricted_mask;
              if (!!overlap)
              {
                filter_unrestricted_instances(expr, expr_covers, overlap);
                finder.filter(overlap);
              }
            }
            if (!!finder->second)
              filter_valid_instances(expr, expr_covers, finder->second);
            // Remove the expression reference that flowed back
            if (finder->first->remove_nested_expression_reference(did))
              delete finder->first;
            applied_reductions.erase(finder);
          }
          if (!applied_reductions.empty())
          {
            // Handle the partial cases here
            LegionList<FieldSet<IndexSpaceExpression*> > reduced_sets;
            applied_reductions.compute_field_sets(FieldMask(), reduced_sets);
            for (LegionList<FieldSet<IndexSpaceExpression*> >::iterator
                  it = reduced_sets.begin(); it != reduced_sets.end(); it++)
            {
              IndexSpaceExpression *union_expr = 
                runtime->forest->union_index_spaces(it->elements);
              const size_t union_size = union_expr->get_volume();
              const size_t set_size = set_expr->get_volume();
#ifdef DEBUG_LEGION
              assert(union_size <= set_size);
#endif
              const bool union_covers = (union_size == set_size);
              if (!!restricted_mask)
              {
                const FieldMask overlap = it->set_mask & restricted_mask;
                if (!!overlap)
                {
                  filter_unrestricted_instances(union_expr, union_covers, 
                                                overlap);
                  it->set_mask -= overlap;
                }
              }
              if (!!it->set_mask)
                filter_valid_instances(union_expr, union_covers,
                                       it->set_mask);
            }
            // Remove expression references that flowed back
            for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                  applied_reductions.begin(); it != 
                  applied_reductions.end(); it++)
              if (it->first->remove_nested_expression_reference(did))
                delete it->first;
          }
        }
      }
      if (is_write)
      {
        if (record_release)
        {
#ifdef DEBUG_LEGION
          assert(record_valid);
          assert(restricted_mask == user_mask);
#endif
          // Releases are a bit strange, we actually want to invalidate
          // all the current valid instances since we're making them all
          // restricted so their are no partial unrestricted cases
          filter_valid_instances(expr, expr_covers, user_mask);
        }
        else if (!!restricted_mask)
        {
          const FieldMask non_restricted_mask = user_mask - restricted_mask;
          if (!!non_restricted_mask)
            filter_valid_instances(expr, expr_covers, non_restricted_mask);
          filter_unrestricted_instances(expr, expr_covers, restricted_mask);
        }
        else
          filter_valid_instances(expr, expr_covers, user_mask);
      }
      // Finally record the valid instances that have been updated
      if (record_valid)
      {
        if (!!restricted_mask)
        {
          const FieldMask non_restricted = user_mask - restricted_mask;
          if (!!non_restricted)
          {
            for (unsigned idx = 0; idx < target_views.size(); idx++)
            {
              const FieldMaskSet<InstanceView> &targets = target_views[idx];
              if (non_restricted * targets.get_valid_mask())
                continue;
              record_instances(expr, expr_covers, non_restricted, targets);
            }
          }
          for (unsigned idx = 0; idx < target_views.size(); idx++)
          {
            const FieldMaskSet<InstanceView> &targets = target_views[idx];
            if (restricted_mask * targets.get_valid_mask())
              continue;
            record_unrestricted_instances(expr, expr_covers,
                                          restricted_mask, targets);
          }
        }
        else
        {
          for (unsigned idx = 0; idx < target_views.size(); idx++)
            record_instances(expr, expr_covers, user_mask, target_views[idx]);
        }
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::make_instances_valid(CopyFillAggregator *&aggregator,
                  CopyFillGuard *previous_guard,
                  PhysicalAnalysis *analysis,
                  const bool track_events,
                  IndexSpaceExpression *expr,
                  const bool expr_covers,
                  const FieldMask &update_mask,
                  const std::vector<PhysicalManager*> &target_instances,
                  const LegionVector<FieldMaskSet<InstanceView> > &target_views,
                  const std::vector<IndividualView*> &source_views,
                  const PhysicalTraceInfo &trace_info,
                  const bool skip_valid_check,
                  const ReductionOpID redop /*= 0*/,
                  CopyAcrossHelper *across_helper/*=NULL*/)
    //--------------------------------------------------------------------------
    {
      if (expr->is_empty())
        return;
      for (unsigned idx = 0; idx < target_views.size(); idx++)
      {
        if (target_views[idx].get_valid_mask() * update_mask)
          continue;
        for (FieldMaskSet<InstanceView>::const_iterator vit =
              target_views[idx].begin(); vit != target_views[idx].end(); vit++)
        {
          InstanceView *target = vit->first;
          FieldMask inst_mask = vit->second & update_mask;
          if (!inst_mask)
            continue;
          // If we can skip the check to see if the view is already valid
          if (!skip_valid_check)
          {
            // First check to see if the view is already marked as valid by name
            if (find_fully_valid_fields(target, inst_mask, expr, expr_covers))
              continue;
            // Next compute the partial valid expressions
            FieldMaskSet<IndexSpaceExpression> partial_valid_exprs;
            // If we're an individual view and there are potentially aliasing
            // collective views then we do an additional analysis here to see
            // if we are already valid for those other expression fields
            if (target->is_individual_view() && !collective_instances.empty() &&
                !(inst_mask * collective_instances.get_valid_mask()))
            {
              // We already know we're not valid by name here for these
              // field expressions, so just iterate the collective instances
              // and see if they contain the target and remove any 
              // expressions and fields which are already valid
              const DistributedID inst_did = 
                target->as_individual_view()->get_manager()->did;
              for (FieldMaskSet<CollectiveView>::const_iterator cit =
                    collective_instances.begin(); cit != 
                    collective_instances.end(); cit++)
              {
                // Check if it contains the instance
                if (!std::binary_search(cit->first->instances.begin(),
                      cit->first->instances.end(), inst_did))
                  continue;
                // Check if it overlaps with the fields
                if (cit->second * inst_mask)
                  continue;
                // Now do the expression checks
                if (find_fully_valid_fields(cit->first, inst_mask, 
                                            expr, expr_covers))
                  break;
                if (find_partial_valid_fields(cit->first, inst_mask,
                      expr, expr_covers, partial_valid_exprs))
                  break;
              }
              if (!inst_mask)
                continue;
            }
            // Do the check for any partial valid field expressions
            if (find_partial_valid_fields(target, inst_mask,
                  expr, expr_covers, partial_valid_exprs))
              continue;
            FieldMaskSet<IndexSpaceExpression> needed_exprs;
            if (!partial_valid_exprs.empty())
            {
              // Group expressions by fields since unions
              // and differences are expensive and hard to group later
              LegionList<FieldSet<IndexSpaceExpression*> > expr_groups;
              partial_valid_exprs.compute_field_sets(FieldMask(), expr_groups);
              // Clear this in case we want to use it later
              partial_valid_exprs.clear();
              // Compute differences for each of the field groups
              for (LegionList<FieldSet<IndexSpaceExpression*> >::const_iterator
                    it = expr_groups.begin(); it != expr_groups.end(); it++)
              {
                // No matter what we don't need to handle these fields
                // anymore for the full expression
                inst_mask -= it->set_mask;
                IndexSpaceExpression *valid_expr = 
                  (it->elements.size() == 1) ? *(it->elements.begin()) :
                  runtime->forest->union_index_spaces(it->elements);
                IndexSpaceExpression *needed_expr =
                  runtime->forest->subtract_index_spaces(expr, valid_expr);
                if (needed_expr->is_empty())
                  continue;
                needed_exprs.insert(needed_expr, it->set_mask);
              }
              if (needed_exprs.empty() && !inst_mask)
                continue;
            }
            // If the target is a collective view or there are collective
            // views that we might overlap with the target then we have to
            // do a much more sophisticated analysis to see if the data is
            // already valid by taking into account multiple views which 
            // all name the the same instance(s)
            if (target->is_collective_view())
            {
              // Collective view target case
              CollectiveView *collective = target->as_collective_view();
              // Welcome to hell. We have three dimensions of validity we
              // need to check for here before we decide to issue copies:
              // 1. fields
              // 2. index space expressions
              // 3. instances (aliasing with individual and collective views)
              // Prepare for extreme complexity...
              if (!!inst_mask)
                needed_exprs.insert(expr_covers ? set_expr : expr, inst_mask);
              // Use an aliased instance analysis to do any dynamic 
              // refinements as necessary for finding overlaps for 
              // different sets of instances
              MakeCollectiveValid alias_analysis(collective, needed_exprs);
              // See if we alias instances with any of the existing valid
              // views. The common case will be that we don't find any 
              // and we'll be able to issue the updates freely.
              const FieldMask &needed_mask = needed_exprs.get_valid_mask();
              alias_analysis.traverse_total(needed_mask, set_expr,
                                            total_valid_instances);
              if (!(needed_mask * partial_valid_fields))
                alias_analysis.traverse_partial(needed_mask,
                                                partial_valid_instances);
              LegionMap<InstanceView*,
                FieldMaskSet<IndexSpaceExpression> > updates;
              alias_analysis.visit_leaves(needed_mask, context, runtime->forest,
                  tree_id, updates);
              for (LegionMap<InstanceView*,
                    FieldMaskSet<IndexSpaceExpression> >::const_iterator 
                    uit = updates.begin(); uit != updates.end(); uit++)
                for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                      uit->second.begin(); it != uit->second.end(); it++)
                  issue_update_copies_and_fills(uit->first, 
                      target_instances[idx], source_views, aggregator,
                      previous_guard, analysis, track_events, it->first,
                      (it->first == set_expr) ? true : false/*expr covers*/,
                      it->second, trace_info, redop, across_helper);
              // Clear these since we've issued all our copies
              needed_exprs.clear();
              inst_mask.clear();
            }
            else
            {
              // At this point, any remaining needed_exprs are ones that
              // we can just issue the udpate copies and fills for from 
              // the original target instance
              for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                    needed_exprs.begin(); it != needed_exprs.end(); it++)
                issue_update_copies_and_fills(target, target_instances[idx],
                    source_views, aggregator, previous_guard, analysis,
                    track_events, it->first, false/*expr covers*/, it->second,
                    trace_info, redop, across_helper);
            }
          }
          // Whatever fields we have left here need updates for the whole expr
          if (!!inst_mask)
            issue_update_copies_and_fills(target, target_instances[idx],
                source_views, aggregator, previous_guard, analysis,
                track_events, expr, expr_covers, inst_mask, trace_info,
                redop, across_helper);
        }
      }
    }

    //--------------------------------------------------------------------------
    bool EquivalenceSet::find_fully_valid_fields(InstanceView *target,
                                                 FieldMask &inst_mask,
                                                 IndexSpaceExpression *expr,
                                                 const bool expr_covers) const
    //--------------------------------------------------------------------------
    {
      FieldMaskSet<LogicalView>::const_iterator total_finder = 
        total_valid_instances.find(target);
      if (total_finder != total_valid_instances.end())
      {
        inst_mask -= total_finder->second;
        if (!inst_mask)
          return true;
      }
      if (!expr_covers)
      {
        const FieldMask partial_overlap = partial_valid_fields & inst_mask;
        if (!!partial_overlap)
        {
          ViewExprMaskSets::const_iterator partial_finder =
            partial_valid_instances.find(target);
          if (partial_finder != partial_valid_instances.end())
          {
            FieldMaskSet<IndexSpaceExpression>::const_iterator expr_finder = 
              partial_finder->second.find(expr);
            if (expr_finder != partial_finder->second.end())
            {
              inst_mask -= expr_finder->second;
              if (!inst_mask)
                return true;
            }
          }
        }
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool EquivalenceSet::find_partial_valid_fields(InstanceView *target,
       FieldMask &inst_mask, IndexSpaceExpression *expr, const bool expr_covers,
       FieldMaskSet<IndexSpaceExpression> &partial_valid_exprs) const
    //--------------------------------------------------------------------------
    {
      ViewExprMaskSets::const_iterator partial_finder =
        partial_valid_instances.find(target);
      if (partial_finder != partial_valid_instances.end())
      {
        const FieldMask partial_valid = 
          inst_mask & partial_finder->second.get_valid_mask();
        if (!!partial_valid)
        {
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator eit =
                partial_finder->second.begin(); eit !=
                partial_finder->second.end(); eit++)
          {
            FieldMask overlap = eit->second & partial_valid;
            if (!overlap)
              continue;
            IndexSpaceExpression *expr_overlap;
            if (!expr_covers)
            {
              expr_overlap =
                runtime->forest->intersect_index_spaces(expr, eit->first);
              const size_t expr_volume = expr_overlap->get_volume();
              if (expr_volume == 0)
                continue;
              if (expr_volume == expr->get_volume())
              {
                // expression dominates us so we are valid
                inst_mask -= overlap;
                if (!inst_mask)
                  return true;
              }
              else if (expr_volume == eit->first->get_volume())
                expr_overlap = eit->first;
            }
            else // expr covers so we know it all intersects
              expr_overlap = eit->first;
            if (!(overlap * partial_valid_exprs.get_valid_mask()))
            {
              // If there are already some fields with expressions
              // (which can happen if this function is called more
              // than once for the same target), then we need to 
              // merge expressions for any overlapping fields
              FieldMaskSet<IndexSpaceExpression> to_add;
              std::vector<IndexSpaceExpression*> to_delete;
              for (FieldMaskSet<IndexSpaceExpression>::iterator it =
                    partial_valid_exprs.begin(); it !=
                    partial_valid_exprs.end(); it++)
              {
                const FieldMask prev_overlap = overlap & it->second;
                if (!prev_overlap)
                  continue;
                IndexSpaceExpression *union_expr =
                  runtime->forest->union_index_spaces(it->first, expr_overlap);
                to_add.insert(union_expr, prev_overlap);
                it.filter(prev_overlap);
                if (!it->second)
                  to_delete.push_back(it->first);
                overlap -= prev_overlap;
                if (!overlap)
                  break;
              }
              for (std::vector<IndexSpaceExpression*>::const_iterator it =
                    to_delete.begin(); it != to_delete.end(); it++)
                partial_valid_exprs.erase(*it);
              for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                    to_add.begin(); it != to_add.end(); it++)
                partial_valid_exprs.insert(it->first, it->second);
              if (!overlap)
                return false;
            }
            partial_valid_exprs.insert(expr_overlap, overlap);
          }
        }
      }
      return false;
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::issue_update_copies_and_fills(InstanceView *target,
                                          PhysicalManager *target_manager,
                               const std::vector<IndividualView*> &source_views,
                                          CopyFillAggregator *&aggregator,
                                          CopyFillGuard *previous_guard,
                                          PhysicalAnalysis *analysis,
                                          const bool track_events,
                                          IndexSpaceExpression *expr,
                                          const bool expr_covers,
                                          FieldMask update_mask,
                                          const PhysicalTraceInfo &trace_info,
                                          const ReductionOpID redop,
                                          CopyAcrossHelper *across_helper)
    //--------------------------------------------------------------------------
    {
      // Before we do anything, if the user has provided an ordering of
      // source views, then go through and attempt to issue copies from
      // them before we do anything else
      if (!source_views.empty())
      {
        FieldMaskSet<IndexSpaceExpression> remainders;
        remainders.insert(expr, update_mask);
        for (std::vector<IndividualView*>::const_iterator src_it = 
              source_views.begin(); src_it != source_views.end(); src_it++)
        {
          // Check to see if it is in the list of total valid instances
          FieldMaskSet<LogicalView>::const_iterator total_finder = 
            total_valid_instances.find(*src_it);
          if ((total_finder != total_valid_instances.end()) &&
              !(remainders.get_valid_mask() * total_finder->second))
          {
            std::vector<IndexSpaceExpression*> to_delete;
            for (FieldMaskSet<IndexSpaceExpression>::iterator it =
                  remainders.begin(); it != remainders.end(); it++)
            {
              const FieldMask overlap = it->second & total_finder->second;
              if (!overlap)
                continue;
              if (aggregator == NULL)
                aggregator = new CopyFillAggregator(runtime->forest, analysis,
                                                 previous_guard, track_events);
              aggregator->record_update(target, target_manager, *src_it,
                  overlap, it->first, trace_info, trace_info.recording ?
                  this : NULL, redop);
              it.filter(overlap);
              if (!it->second)
                to_delete.push_back(it->first);
            }
            if (!to_delete.empty())
            {
              for (std::vector<IndexSpaceExpression*>::const_iterator it =
                    to_delete.begin(); it != to_delete.end(); it++)
                remainders.erase(*it);
              if (remainders.empty())
                return;
              remainders.tighten_valid_mask();
            }
          }
          // Next check to see if the instance has partial valid expressions
          ViewExprMaskSets::const_iterator partial_finder =
            partial_valid_instances.find(*src_it);
          if ((partial_finder == partial_valid_instances.end()) ||
              (partial_finder->second.get_valid_mask() * 
               remainders.get_valid_mask()))
            continue;
          // Compute the joins of the two field mask sets to get pairs of
          // index space expressions with the same fields
          LegionMap<std::pair<IndexSpaceExpression*,IndexSpaceExpression*>,
                    FieldMask> join_expressions;
          unique_join_on_field_mask_sets(remainders, partial_finder->second, 
                                         join_expressions);
          bool need_tighten = false;
          for (LegionMap<std::pair<IndexSpaceExpression*,IndexSpaceExpression*>,          
                FieldMask>::const_iterator it = 
                join_expressions.begin(); it != join_expressions.end(); it++)
          {
            // Compute the intersection of the two index spaces 
            IndexSpaceExpression *overlap = 
              runtime->forest->intersect_index_spaces(it->first.first, 
                                                      it->first.second);
            const size_t overlap_size = overlap->get_volume();
            if (overlap_size == 0)
              continue;
            FieldMaskSet<IndexSpaceExpression>::iterator finder = 
              remainders.find(it->first.first);
#ifdef DEBUG_LEGION
            assert(finder != remainders.end());
#endif
            finder.filter(it->second);
            if (!finder->second)
              remainders.erase(finder);
            if (aggregator == NULL)
              aggregator = new CopyFillAggregator(runtime->forest, analysis,
                                                  previous_guard, track_events);
            if (overlap_size < it->first.first->get_volume())
            {
              if (overlap_size == it->first.second->get_volume())
                aggregator->record_update(target, target_manager,
                    *src_it, it->second, it->first.second, trace_info,
                    trace_info.recording ? this : NULL, redop);
              else
                aggregator->record_update(target, target_manager, *src_it,
                    it->second, overlap, trace_info, trace_info.recording ? 
                    this : NULL, redop);
              // Compute the difference to add to the remainders
              IndexSpaceExpression *diff = 
                runtime->forest->subtract_index_spaces(it->first.first,overlap);
              remainders.insert(diff, it->second);
            }
            else
            {
              // Covered the remainder expression
              aggregator->record_update(target, target_manager,
                  *src_it, it->second, it->first.first, trace_info,
                  trace_info.recording ? this : NULL, redop);
              if (remainders.empty())
                return;
              need_tighten = true;
            }
          }
          if (need_tighten)
            remainders.tighten_valid_mask();
        }
#ifdef DEBUG_LEGION
        assert(!remainders.empty());
#endif
        // It's too hard to track all the pairs of partial sets for
        // both the source and destination instances at the same
        // time, so we recurse on this method for any expressions that
        // are not the same as the original expression except this
        // time we will not have any sources to consider
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
              remainders.begin(); it != remainders.end(); it++)
        {
          if (it->first != expr)
          {
            const std::vector<IndividualView*> empty_sources;
            issue_update_copies_and_fills(target, target_manager, empty_sources, 
                      aggregator, previous_guard, analysis, track_events,
                      it->first, false/*covers*/, it->second, trace_info, 
                      redop, across_helper);
          }
          else // same expression so just keep the fields we need
            update_mask &= it->second;
        }
        // Fall through if we still have fields for this expression to handle
        if (!update_mask)
          return;
      }
      // We prefer bulk copies instead of lots of little copies, so do a quick
      // pass to see which fields we can find previous instances for that
      // completely cover our target without doing any intersection tests
      // If we find them then we'll just issue copies/fills from there, 
      // otherwise we'll build partial sets and do the expensive thing
      const FieldMask total_fields = 
        update_mask & total_valid_instances.get_valid_mask();
      if (!!total_fields)
      {
        if (aggregator == NULL)
          aggregator = new CopyFillAggregator(runtime->forest, analysis,
                                              previous_guard, track_events);
        if (total_fields != total_valid_instances.get_valid_mask())
        {
          // Compute selected instances that are valid for us
          FieldMaskSet<LogicalView> total_instances;
          for (FieldMaskSet<LogicalView>::const_iterator it = 
                total_valid_instances.begin(); it != 
                total_valid_instances.end(); it++)
          {
            const FieldMask overlap = it->second & update_mask;
            if (!overlap)
              continue;
            total_instances.insert(it->first, overlap);
          }
          aggregator->record_updates(target, target_manager, 
              total_instances, total_fields, expr, trace_info,
              trace_info.recording ? this : NULL, redop, across_helper);
        }
        else // Total valid instances covers everything!
          aggregator->record_updates(target, target_manager,
              total_valid_instances, total_fields, expr, trace_info,
              trace_info.recording ? this : NULL, redop, across_helper);
        update_mask -= total_fields;
        if (!update_mask)
          return;
      }
      // Now look through the partial valid instances for both instances
      // that cover us as well as partially valid instances
      FieldMaskSet<LogicalView> cover_instances;
      LegionMap<LogicalView*,
        FieldMaskSet<IndexSpaceExpression> > partial_instances;
      for (ViewExprMaskSets::const_iterator pit =
            partial_valid_instances.begin(); pit != 
            partial_valid_instances.end(); pit++)
      {
        if (pit->second.get_valid_mask() * update_mask)
          continue;
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
              pit->second.begin(); it != pit->second.end(); it++)
        {
          FieldMask overlap = it->second & update_mask;
          if (!overlap)
            continue;
          if (!expr_covers)
          {
            if (it->first != expr)
            {
              IndexSpaceExpression *expr_overlap =
                runtime->forest->intersect_index_spaces(it->first, expr);
              const size_t overlap_volume = expr_overlap->get_volume();
              if (overlap_volume > 0)
              {
                if (overlap_volume < expr->get_volume())
                {
                  // partial overlap, only record this if we do not
                  // have any covering instances since we always prefer
                  // total coverings to partial ones
                  if (!cover_instances.empty())
                    overlap -= cover_instances.get_valid_mask();
                  if (!!overlap)
                  {
                    if (overlap_volume == it->first->get_volume())
                      partial_instances[pit->first].insert(it->first, overlap);
                    else
                      partial_instances[pit->first].insert(expr_overlap,
                                                           overlap);
                  }
                }
                else
                  cover_instances.insert(pit->first, overlap);
              }
            }
            else
              cover_instances.insert(pit->first, overlap);
          }
          else // expr covers so everything is partial
            partial_instances[pit->first].insert(it->first, overlap);
        }
      }
      if (!cover_instances.empty())
      {
        if (aggregator == NULL)
          aggregator = new CopyFillAggregator(runtime->forest, analysis,
                                              previous_guard, track_events);
        aggregator->record_updates(target, target_manager, cover_instances,
            cover_instances.get_valid_mask(), expr, trace_info,
            trace_info.recording ? this : NULL, redop, across_helper);
        update_mask -= cover_instances.get_valid_mask();
        if (!update_mask)
          return;
      }
      // This is a horrible place to be, partial updates everywhere
      // so now we need to ask the mapper which order to do them in
      // Ask the copy fll aggregator to help us out with this since
      // its probably queried the mapper about this all before
      if (!partial_instances.empty())
      {
        if (aggregator == NULL)
          aggregator = new CopyFillAggregator(runtime->forest, analysis,
                                              previous_guard, track_events);
        aggregator->record_partial_updates(target, target_manager, 
                                 partial_instances, update_mask, expr,
                                 trace_info, trace_info.recording ? this : NULL,
                                 redop, across_helper);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::record_reductions(UpdateAnalysis &analysis,
                                           IndexSpaceExpression *expr,
                                           const bool expr_covers,
                                           const FieldMask &user_mask)
    //--------------------------------------------------------------------------
    {
      CopyFillAggregator *fill_aggregator = NULL;
      // See if we have an input aggregator that we can use now
      // for any fills that need to be done to initialize instances
      std::map<RtEvent,CopyFillAggregator*>::const_iterator finder = 
        analysis.input_aggregators.find(RtEvent::NO_RT_EVENT);
      if (finder != analysis.input_aggregators.end())
        fill_aggregator = finder->second; 
      FieldMask guard_fill_mask;
      for (unsigned idx = 0; idx < analysis.target_views.size(); idx++)
      {
        for (FieldMaskSet<InstanceView>::const_iterator rit =
              analysis.target_views[idx].begin(); rit != 
              analysis.target_views[idx].end(); rit++)
        {
          const FieldMask reduction_mask = rit->second & user_mask;
          if (!reduction_mask)
            continue;
          reduction_fields |= reduction_mask;
          InstanceView *red_view = rit->first;
          const ReductionOpID view_redop = red_view->get_redop(); 
          FillView *fill_view = red_view->get_redop_fill_view(); 
#ifdef DEBUG_LEGION
          assert(view_redop > 0);
          assert(view_redop == analysis.usage.redop);
          assert(partial_valid_instances.find(red_view) == 
                  partial_valid_instances.end());
#endif
          if (red_view->is_collective_view())
          {
            // Collective reduction view path
            AllreduceView *allreduce_view = red_view->as_allreduce_view();
            InitializeCollectiveReduction alias_analysis(allreduce_view,
                                                  runtime->forest, expr);
            int fidx = reduction_mask.find_first_set();
            while (fidx >= 0)
            {
              FieldMask reduce_mask;
              reduce_mask.set_bit(fidx);
              std::list<std::pair<InstanceView*,IndexSpaceExpression*> >
                &field_views = reduction_instances[fidx];
              for (std::list<std::pair<InstanceView*,
                                       IndexSpaceExpression*> >::iterator it =
                    field_views.begin(); it != field_views.end(); it++)
              {
                if (!allreduce_view->aliases(it->first))
                {
                  // Still need to check for the ABA problem 
                  // (see below for a more detailed comment)
                  if (it->first->get_redop() == view_redop)
                    continue;
                  bool failure = false;
                  if (!expr_covers)
                  {
                    IndexSpaceExpression *overlap_expr = 
                      runtime->forest->intersect_index_spaces(expr, it->second);
                    if (overlap_expr->is_empty())
                      continue;
                    alias_analysis.visit_leaves(reduce_mask,
                                                overlap_expr, failure);
                  }
                  else
                    alias_analysis.visit_leaves(reduce_mask,it->second,failure);
                  if (failure)
                    // If we make it here, report the ABA violation
                    REPORT_LEGION_FATAL(LEGION_FATAL_REDUCTION_ABA_PROBLEM,
                        "Unsafe re-use of reduction instance detected due "
                        "to alternating un-flushed reduction operations "
                        "%d and %d. Please report this use case to the "
                        "Legion developer's mailing list so that we can "
                        "help you address it.", view_redop,
                        it->first->get_redop())
                }
                else
                  alias_analysis.analyze(it->first, reduce_mask, it->second);
              }
              // Step to the next field
              fidx = reduction_mask.find_next_set(fidx+1);
            }
            // Record any fill operations that need to be performed as a result
            // and update the reduction instances with new reductions
            alias_analysis.visit_leaves(reduction_mask, context, analysis,
                fill_aggregator, fill_view, tree_id,
                this, did, reduction_instances);
          }
          else
          {
            // Individual reduction view path
            // Track the case where this reduction view is also
            // stored in the valid instances in which case we 
            // do not need to do any fills. This will only happen
            // if these fields are restricted because they are in
            // the total_valid_instances. 
            bool already_valid = false;
            if (!!restricted_fields && !(reduction_mask * restricted_fields) &&
                (total_valid_instances.find(red_view) != 
                 total_valid_instances.end()))
              already_valid = true;
            int fidx = reduction_mask.find_first_set();
            // Figure out which fields require a fill operation
            // in order initialize the reduction instances
            FieldMaskSet<IndexSpaceExpression> fill_exprs;
            while (fidx >= 0)
            {
              std::list<std::pair<InstanceView*,IndexSpaceExpression*> >
                &field_views = reduction_instances[fidx]; 
              // Scan through the reduction instances to see if we're
              // already in the list of valid reductions, if not then
              // we're going to need a fill to initialize the instance
              // Also check for the ABA problem on reduction instances
              // described in Legion issue #545 where we start out
              // with reductions of kind A, switch to reductions of
              // kind B, and then switch back to reductions of kind A
              // which will make it unsafe to re-use the instance
              bool found_covered = already_valid && 
                total_valid_instances[red_view].is_set(fidx);
              std::set<IndexSpaceExpression*> found_exprs;
              // We only need to do this check if it's not already-covered
              // In the case where we know that it is already covered
              // at this point it is restricted, so everything is being
              // flushed to it anyway
              if (!found_covered)
              {
                for (std::list<std::pair<InstanceView*,
                      IndexSpaceExpression*> >::iterator it =
                      field_views.begin(); it != field_views.end(); it++)
                {
                  if (!red_view->aliases(it->first))
                  {
                    if (!found_covered && found_exprs.empty())
                      continue;
                    if (it->first->get_redop() == view_redop)
                      continue;
                    // Check for intersection
                    if (found_covered)
                    {
                      if (!expr_covers && (expr != it->second))
                      {
                        IndexSpaceExpression *overlap = 
                          runtime->forest->intersect_index_spaces(expr, 
                                                                  it->second);
                        if (overlap->is_empty())
                          continue;
                      }
                    }
                    else
                    {
                      // Check each of the individual expressions for overlap
                      bool all_disjoint = true;
                      for (std::set<IndexSpaceExpression*>::const_iterator 
                            fit = found_exprs.begin(); 
                            fit != found_exprs.end(); fit++)
                      {
                        IndexSpaceExpression *overlap = 
                          runtime->forest->intersect_index_spaces(it->second,
                                                                  *fit);
                        if (overlap->is_empty())
                          continue;
                        all_disjoint = false;
                        break;
                      }
                      if (all_disjoint)
                        continue;
                    }
                    // If we make it here, report the ABA violation
                    REPORT_LEGION_FATAL(LEGION_FATAL_REDUCTION_ABA_PROBLEM,
                        "Unsafe re-use of reduction instance detected due "
                        "to alternating un-flushed reduction operations "
                        "%d and %d. Please report this use case to the "
                        "Legion developer's mailing list so that we can "
                        "help you address it.", view_redop,
                        it->first->get_redop())
                  }
                  else if (!found_covered)
                  {
                    if (!expr_covers)
                    {
                      if (expr != it->second)
                      {
                        IndexSpaceExpression *overlap = 
                          runtime->forest->intersect_index_spaces(expr,
                                                            it->second);
                        if (overlap->get_volume() < expr->get_volume())
                        {
                          found_exprs.insert(overlap);
                          // Promote this to be the union of the two
                          if (overlap->get_volume() < 
                              it->second->get_volume())
                          {
                            IndexSpaceExpression *union_expr =
                              runtime->forest->union_index_spaces(expr,
                                                            it->second);
                            union_expr->add_nested_expression_reference(did);
                            if (it->second->
                                remove_nested_expression_reference(did))
                              delete it->second;
                            it->second = union_expr;
                          }
                          else
                          {
                            expr->add_nested_expression_reference(did);
                            if (it->second->
                                remove_nested_expression_reference(did))
                              delete it->second;
                            it->second = expr;
                          }
                        }
                        else
                          found_covered = true;
                      }
                      else
                        found_covered = true;
                    }
                    else
                    {
                      if ((it->second != set_expr) &&
                          (it->second->get_volume() < set_expr->get_volume()))
                      {
                        found_exprs.insert(it->second);
                        // Promote this up to the full set expression
                        set_expr->add_nested_expression_reference(did);
                        // Since we're going to use the old expression, we 
                        // need to keep it live until the end of the task
                        it->second->add_base_expression_reference(
                                                               LIVE_EXPR_REF);
                        ImplicitReferenceTracker::record_live_expression(
                                                                  it->second);
                        // Now we can remove the previous live reference
                        if (it->second->
                            remove_nested_expression_reference(did))
                          delete it->second;
                        it->second = set_expr;
                      }
                      else
                        found_covered = true;
                    }
                  }
                }
                // See if there are any fill expressions that we need to do
                // These are also the expressions that we need to add to the
                // fields views set since they won't be described by prior
                // reductions already on the list
                if (!found_covered)
                {
                  FieldMask fill_mask;
                  fill_mask.set_bit(fidx);
                  if (!found_exprs.empty())
                  {
                    guard_fill_mask.set_bit(fidx);
                    // See if the union dominates the expression, if not
                    // put in the difference
                    IndexSpaceExpression *union_expr = 
                      runtime->forest->union_index_spaces(found_exprs);
                    if (union_expr->get_volume() < expr->get_volume())
                    {
                      IndexSpaceExpression *diff_expr =
                        runtime->forest->subtract_index_spaces(expr,
                                                          union_expr);
                      fill_exprs.insert(diff_expr, fill_mask);
                      red_view->add_nested_valid_ref(did);
                      diff_expr->add_nested_expression_reference(did);
                      field_views.push_back(std::make_pair(red_view,
                                                           diff_expr));
                    }
                  }
                  else
                  {
                    fill_exprs.insert(expr, fill_mask);
                    // No previous exprs, so record the full thing
                    red_view->add_nested_valid_ref(did);
                    expr->add_nested_expression_reference(did);
                    field_views.push_back(std::make_pair(red_view, expr));
                  }
                }
                else
                  guard_fill_mask.set_bit(fidx);
              }
              else
              {
                // This is already restricted, so just add it,
                // we'll be flushing it here shortly
                red_view->add_nested_valid_ref(did);
                expr->add_nested_expression_reference(did);
                field_views.push_back(std::make_pair(red_view, expr));
              }
#ifdef DEBUG_LEGION
              assert(!field_views.empty());
#endif
              fidx = reduction_mask.find_next_set(fidx+1);
            }
            if (!fill_exprs.empty())
            {
              if (fill_aggregator == NULL)
              {
                // Fill aggregators never need to wait for any other
                // aggregators since we know they won't depend on each other
                fill_aggregator = new CopyFillAggregator(runtime->forest,
                 &analysis, NULL/*no previous guard*/, false/*track events*/);
                analysis.input_aggregators[RtEvent::NO_RT_EVENT] = 
                  fill_aggregator;
              }
              // Record the fill operation on the aggregator
              for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                    fill_exprs.begin(); it != fill_exprs.end(); it++)
                fill_aggregator->record_fill(red_view, fill_view,
                                  it->second, it->first,
                                  PredEvent::NO_PRED_EVENT,
                                  analysis.trace_info.recording ? this : NULL);
              // Record this as a guard for later operations
              reduction_fill_guards.insert(fill_aggregator,
                                           fill_exprs.get_valid_mask());
#ifdef DEBUG_LEGION
              if (!fill_aggregator->record_guard_set(this,false/*read only*/))
                assert(false);
#else
              fill_aggregator->record_guard_set(this, false/*read only*/);
#endif
            }
          }
        }
      }
      // If we have any fills that were issued by a prior operation
      // that we need to reuse then check for them here. This is a
      // slight over-approximation for the mapping dependences because
      // we really only need to wait for fills to instances that we
      // care about it, but it should be minimal overhead and the
      // resulting event graph will still be precise
      if (!reduction_fill_guards.empty() && !!guard_fill_mask &&
          !(reduction_fill_guards.get_valid_mask() * guard_fill_mask))
      {
        for (FieldMaskSet<CopyFillGuard>::iterator it = 
              reduction_fill_guards.begin(); it != 
              reduction_fill_guards.end(); it++)
        {
          if (it->first == fill_aggregator)
            continue;
          const FieldMask guard_mask = guard_fill_mask & it->second;
          if (!guard_mask)
            continue;
          // No matter what record our dependences on the prior guards
#ifdef NON_AGGRESSIVE_AGGREGATORS
          analysis.guard_events.insert(it->first->effects_applied);
#else
          if (analysis.original_source == local_space)
            analysis.guard_events.insert(it->first->guard_postcondition);
          else
            analysis.guard_events.insert(it->first->effects_applied);
#endif
        }
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::apply_reductions(
                  const std::vector<PhysicalManager*> &target_instances,
                  const LegionVector<FieldMaskSet<InstanceView> > &target_views,
                  IndexSpaceExpression *expr, const bool expr_covers,
                  const FieldMask &reduction_mask,
                  CopyFillAggregator *&aggregator,
                  CopyFillGuard *previous_guard,
                  PhysicalAnalysis *analysis,
                  const bool track_events,
                  const PhysicalTraceInfo &trace_info,
                  FieldMaskSet<IndexSpaceExpression> *applied_exprs,
                  CopyAcrossHelper *across_helper/*= NULL*/)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!set_expr->is_empty());
      assert(target_instances.size() == target_views.size());
#endif
      for (unsigned idx = 0; idx < target_views.size(); idx++)
      {
        for (FieldMaskSet<InstanceView>::const_iterator it =
              target_views[idx].begin(); it != target_views[idx].end(); it++)
        {
          const FieldMask &inst_mask = it->second & reduction_mask;
          if (!inst_mask)
            continue;
          apply_reduction(it->first, target_instances[idx], expr, expr_covers,
            inst_mask, aggregator, previous_guard, analysis, track_events,
            trace_info, applied_exprs, across_helper);
        }
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::apply_restricted_reductions(
                            const FieldMaskSet<InstanceView> &reduction_targets,
                            IndexSpaceExpression *expr, const bool expr_covers,
                            const FieldMask &reduction_mask,
                            CopyFillAggregator *&aggregator,
                            CopyFillGuard *previous_guard,
                            PhysicalAnalysis *analysis,
                            const bool track_events,
                            const PhysicalTraceInfo &trace_info,
                            FieldMaskSet<IndexSpaceExpression> *applied_exprs)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!!reduction_targets.get_valid_mask());
      assert(!set_expr->is_empty());
#endif
      for (FieldMaskSet<InstanceView>::const_iterator it =
            reduction_targets.begin(); it != reduction_targets.end(); it++)
      {
        const FieldMask inst_mask = it->second & reduction_mask;
        if (!inst_mask)
          continue;
        apply_reduction(it->first, NULL/*no manager since this is restricted*/,
                        expr, expr_covers, inst_mask, aggregator,
                        previous_guard, analysis, track_events,
                        trace_info, applied_exprs, NULL/*across*/);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::apply_reduction(InstanceView *target,
                              PhysicalManager *target_manager,
                              IndexSpaceExpression *expr, 
                              const bool expr_covers,
                              const FieldMask &reduction_mask,
                              CopyFillAggregator *&aggregator,
                              CopyFillGuard *previous_guard,
                              PhysicalAnalysis *analysis,
                              const bool track_events,
                              const PhysicalTraceInfo &trace_info,
                              FieldMaskSet<IndexSpaceExpression> *applied_exprs,
                              CopyAcrossHelper *across_helper)
    //--------------------------------------------------------------------------
    {
      const bool track_exprs = (applied_exprs != NULL);
      const bool target_is_reduction = target->is_reduction_kind();
      int fidx = reduction_mask.find_first_set();
      while (fidx >= 0)
      {
        std::map<unsigned,std::list<
          std::pair<InstanceView*,IndexSpaceExpression*> > >::iterator 
            finder = reduction_instances.find(fidx);
#ifdef DEBUG_LEGION
        assert(finder != reduction_instances.end());
        assert(!finder->second.empty());
#endif 
        if (expr_covers)
        {
          // If the target is a reduction instance, check to see
          // that we at least have one reduction to apply
          if (target_is_reduction)
          {
            // Filter out all of our reductions
            for (std::list<std::pair<InstanceView*,
                           IndexSpaceExpression*> >::iterator it =
                  finder->second.begin(); it != 
                  finder->second.end(); /*nothing*/)
            {
              if (it->first == target)
              {
                if (it->first->remove_nested_valid_ref(did))
                  delete it->first;
                if (it->second->remove_nested_expression_reference(did))
                  delete it->second;
                it = finder->second.erase(it);
              }
              else
                it++;
            }
            if (finder->second.empty())
            {
              // Quick out if there was nothing to apply
              reduction_instances.erase(finder);
              reduction_fields.unset_bit(fidx);
              fidx = reduction_mask.find_next_set(fidx+1);
              continue;
            }
          }
          if (aggregator == NULL)
            aggregator = new CopyFillAggregator(runtime->forest, analysis,
                                                previous_guard, track_events);
          aggregator->record_reductions(target, target_manager, 
              finder->second, fidx, (across_helper == NULL) ? fidx : 
              across_helper->convert_src_to_dst(fidx),
            trace_info.recording ? this : NULL, across_helper);
          bool has_cover = false;
          for (std::list<std::pair<InstanceView*,
                IndexSpaceExpression*> >::const_iterator it =
                finder->second.begin(); it != finder->second.end(); it++)
          {
            if (it->second == set_expr)
              has_cover = true;
            if (it->first->remove_nested_valid_ref(did))
              delete it->first;
            // Only remove expression references here if we're not
            // tracking expressions
            if (!track_exprs &&
                it->second->remove_nested_expression_reference(did))
              delete it->second;
          }
          if (track_exprs)
          {
            // See if we find ourselves, if not just record all of them
            FieldMask expr_mask;
            expr_mask.set_bit(fidx);
            if (!has_cover)
            {
              // Expression references flow back but remove duplicates
              for (std::list<std::pair<InstanceView*,
                    IndexSpaceExpression*> >::const_iterator it =
                    finder->second.begin(); it != finder->second.end(); it++)
                if (!applied_exprs->insert(it->second, expr_mask) &&
                    it->second->remove_nested_expression_reference(did))
                  assert(false); // should never hit this
            }
            else
            {
              if (applied_exprs->insert(set_expr, expr_mask))
                set_expr->add_nested_expression_reference(did);
              // Now we can remove the remaining expression references
              for (std::list<std::pair<InstanceView*,
                    IndexSpaceExpression*> >::const_iterator it =
                    finder->second.begin(); it != finder->second.end(); it++)
                if (it->second->remove_nested_expression_reference(did))
                  delete it->second;
            }
          }
          // We applied all these reductions so we're done
          reduction_instances.erase(finder);
          reduction_fields.unset_bit(fidx);
        }
        else
        {
          bool has_cover = false;
          std::vector<
            std::pair<InstanceView*,IndexSpaceExpression*> > to_delete;
          std::list<
            std::pair<InstanceView*,IndexSpaceExpression*> > to_record;
          // expr does not cover so we need intersection tests
          for (std::list<std::pair<InstanceView*,IndexSpaceExpression*> >::
                iterator it = finder->second.begin();
                it != finder->second.end(); /*nothing*/)
          {
            if (target_is_reduction && (it->first == target))
            {
              to_delete.push_back(*it);
              it = finder->second.erase(it);
            }
            else if (it->second == expr)
            {
              to_record.push_back(*it);
              to_delete.push_back(*it);
              if (track_exprs)
                has_cover = true;
              it = finder->second.erase(it);
            }
            else if (it->second == set_expr)
            {
              to_record.push_back(std::make_pair(it->first, expr));
              if (track_exprs)
                has_cover = true;
              IndexSpaceExpression *remainder = 
                runtime->forest->subtract_index_spaces(set_expr, expr);
              remainder->add_nested_expression_reference(did);
              it->second = remainder;
              if (set_expr->remove_nested_expression_reference(did))
                assert(false); // should never hit this
              it++;
            }
            else
            {
              IndexSpaceExpression *overlap = 
                runtime->forest->intersect_index_spaces(expr, it->second);
              const size_t overlap_size = overlap->get_volume();
              if (overlap_size == 0)
              {
                it++;
                continue;
              }
              if (overlap_size == expr->get_volume())
              {
                to_record.push_back(std::make_pair(it->first, expr));
                if (track_exprs)
                  has_cover = true;
              }
              else
                to_record.push_back(std::make_pair(it->first, overlap));
              if (overlap_size == it->second->get_volume())
              {
                to_delete.push_back(*it);
                it = finder->second.erase(it);
              }
              else
              {
                IndexSpaceExpression *remainder = 
                  runtime->forest->subtract_index_spaces(it->second, expr);
                remainder->add_nested_expression_reference(did);
                if (it->second->remove_nested_expression_reference(did))
                  delete it->second;
                it->second = remainder;
                it++;
              }
            }
          }
          if (!to_record.empty())
          {
            if (aggregator == NULL)
              aggregator = new CopyFillAggregator(runtime->forest, analysis,
                                                  previous_guard, track_events);
            aggregator->record_reductions(target, target_manager, to_record,
                                    fidx, (across_helper == NULL) ? fidx : 
                                      across_helper->convert_src_to_dst(fidx),
                                    trace_info.recording ? this : NULL, 
                                    across_helper);
            if (track_exprs)
            {
              FieldMask expr_mask;
              expr_mask.set_bit(fidx);
              if (!has_cover)
              {
                for (std::list<std::pair<InstanceView*,
                      IndexSpaceExpression*> >::const_iterator it = 
                      to_record.begin(); it != to_record.end(); it++)
                  if (applied_exprs->insert(it->second, expr_mask))
                    it->second->add_nested_expression_reference(did);
              }
              else if (applied_exprs->insert(expr, expr_mask))
                expr->add_nested_expression_reference(did);
            }
          }
          if (!to_delete.empty())
          {
            for (std::vector<std::pair<InstanceView*,
                  IndexSpaceExpression*> >::const_iterator it =
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              if (it->first->remove_nested_valid_ref(did))
                delete it->first;
              if (it->second->remove_nested_expression_reference(did))
                delete it->second;
            }
          }
          if (finder->second.empty())
          {
            reduction_instances.erase(finder);
            reduction_fields.unset_bit(fidx);
          }
        }
        fidx = reduction_mask.find_next_set(fidx+1);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::copy_out(IndexSpaceExpression *expr, 
                     const bool expr_covers,
                     const FieldMask &restricted_mask, 
                     const std::vector<PhysicalManager*> &src_insts,
                     const LegionVector<FieldMaskSet<InstanceView> > &src_views,
                     PhysicalAnalysis *analysis,
                     const PhysicalTraceInfo &trace_info,
                     CopyFillAggregator *&aggregator)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(src_insts.size() == src_views.size());
#endif
      for (unsigned idx = 0; idx < src_views.size(); idx++)
      {
        const FieldMaskSet<InstanceView> &sources = src_views[idx];
        const FieldMask overlap = sources.get_valid_mask() & restricted_mask;
        if (!overlap)
          continue;
        copy_out(expr, expr_covers, restricted_mask, sources, analysis,
                 trace_info, aggregator);
      }
    }

    //--------------------------------------------------------------------------
    template<typename T>
    void EquivalenceSet::copy_out(IndexSpaceExpression *expr, 
                                  const bool expr_covers,
                                  const FieldMask &restricted_mask, 
                                  const FieldMaskSet<T> &src_insts,
                                  PhysicalAnalysis *analysis, 
                                  const PhysicalTraceInfo &trace_info,
                                  CopyFillAggregator *&aggregator)
    //--------------------------------------------------------------------------
    {
      if (expr->is_empty())
        return;
      // Iterate through the restrictions looking for overlaps
      for (ExprViewMaskSets::const_iterator rit = restricted_instances.begin();
            rit != restricted_instances.end(); rit++)
      {
        const FieldMask overlap = 
          rit->second.get_valid_mask() & restricted_mask;
        if (!overlap)
          continue;
        IndexSpaceExpression *overlap_expr = NULL;
        if (expr_covers)
          overlap_expr = rit->first;
        else if (rit->first == set_expr)
          overlap_expr = expr;
        else
        {
          IndexSpaceExpression *over = 
            runtime->forest->intersect_index_spaces(rit->first, expr);
          if (over->is_empty())
            continue;
          const size_t over_size = over->get_volume();
          if (over_size == expr->get_volume())
            overlap_expr = expr;
          else if (over_size == rit->first->get_volume())
            overlap_expr = rit->first;
          else
            overlap_expr = over;
        }
        // Find the restricted destination instances for these fields
        LegionMap<std::pair<InstanceView*,T*>,FieldMask> restricted_copies;
        unique_join_on_field_mask_sets(rit->second,src_insts,restricted_copies);
        if (restricted_copies.empty())
          continue;
        for (typename LegionMap<std::pair<InstanceView*,T*>,FieldMask>::
              const_iterator it = restricted_copies.begin();
              it != restricted_copies.end(); it++)
        {
          if (it->first.first == it->first.second)
            continue;
          if (aggregator == NULL)
            aggregator = new CopyFillAggregator(runtime->forest, analysis,
                                NULL/*no previous guard*/, true/*track*/);
          aggregator->record_update(it->first.first, NULL/*no manager*/,
              it->first.second, overlap, overlap_expr, trace_info,
              trace_info.recording ? this : NULL);
        }
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::acquire_restrictions(AcquireAnalysis &analysis, 
                                             IndexSpaceExpression *expr,
                                             const bool expr_covers, 
                                             const FieldMask &acquire_mask,
                                             std::set<RtEvent> &deferral_events,
                                             std::set<RtEvent> &applied_events,
                                             const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // Already holding the eq_lock from EquivalenceSet::analyze method
      std::vector<IndexSpaceExpression*> to_delete;
      std::map<IndexSpaceExpression*,IndexSpaceExpression*> to_add;
      // Now we need to lock the analysis if we're going to do this traversal
      AutoLock a_lock(analysis);
      for (ExprViewMaskSets::iterator eit = restricted_instances.begin();
            eit != restricted_instances.end(); eit++)
      {
        FieldMask overlap = eit->second.get_valid_mask() & acquire_mask;
        if (!overlap)
          continue;
        IndexSpaceExpression *overlap_expr = NULL;
        bool done_early = false;
        if (!expr_covers && (eit->first != expr))
        {
          overlap_expr = 
            runtime->forest->intersect_index_spaces(eit->first, expr);
          const size_t overlap_size = overlap_expr->get_volume();
          if (overlap_size == 0)
            continue;
          if (overlap_size == eit->first->get_volume())
          {
            overlap_expr = eit->first;
            if (overlap_size == expr->get_volume())
              done_early = (overlap == acquire_mask);
          }
          else if (overlap_size == expr->get_volume())
            overlap_expr = expr;
        }
        else
        {
          overlap_expr = eit->first;
          if (eit->first == expr)
            done_early = (overlap == acquire_mask);
        }
        ExprViewMaskSets::iterator release_finder =
          released_instances.find(overlap_expr);
        if (release_finder == released_instances.end())
        {
          overlap_expr->add_nested_expression_reference(did);
          released_instances[overlap_expr];
          release_finder = released_instances.find(overlap_expr);
        }
        if (overlap_expr == eit->first)
        {
          // Total covering of expressions
          // so remove instances no longer restricted
          if (overlap == eit->second.get_valid_mask())
          {
            // All instances are going to be released
            if (!release_finder->second.empty())
            {
              // Insert and remove duplicate references
              for (FieldMaskSet<InstanceView>::const_iterator it =
                    eit->second.begin(); it != eit->second.end(); it++)
              {
                analysis.record_instance(it->first, it->second);
                if (!release_finder->second.insert(it->first, it->second) &&
                    it->first->remove_nested_valid_ref(did))
                  assert(false); // should never delete this
              }
              eit->second.clear();
            }
            else
            {
              for (FieldMaskSet<InstanceView>::const_iterator it =
                    eit->second.begin(); it != eit->second.end(); it++)
                analysis.record_instance(it->first, it->second);
              release_finder->second.swap(eit->second);
            }
            to_delete.push_back(eit->first);
          }
          else
          {
            // Filter instances whose fields overlap
            std::vector<InstanceView*> to_erase;
            for (FieldMaskSet<InstanceView>::iterator it = 
                  eit->second.begin(); it != eit->second.end(); it++)
            {
              const FieldMask inst_overlap = overlap & it->second;
              if (!inst_overlap)
                continue;
              analysis.record_instance(it->first, inst_overlap);
              // Add it to the released instances
              if (release_finder->second.insert(it->first, inst_overlap))
                it->first->add_nested_valid_ref(did);
              // Remove it from here
              it.filter(inst_overlap);
              if (!it->second)
                to_erase.push_back(it->first);
              // Each field should only be represented by one instance
              overlap -= inst_overlap;
              if (!overlap)
                break;
            }
            for (std::vector<InstanceView*>::const_iterator it =
                  to_erase.begin(); it != to_erase.end(); it++)
              if ((*it)->remove_nested_valid_ref(did))
                delete (*it);
            if (!eit->second.empty())
              eit->second.tighten_valid_mask();
            else
              to_delete.push_back(eit->first);
          }
        }
        else
        {
          // Only partial covering, so compute the difference
          // and record that we'll pull valid instances from here
          to_add[eit->first] = 
            runtime->forest->subtract_index_spaces(eit->first, expr);
          // The intersection gets merged back into relased sets
          for (FieldMaskSet<InstanceView>::const_iterator it =
                eit->second.begin(); it != eit->second.end(); it++)
          {
            const FieldMask inst_overlap = overlap & it->second;
            if (!inst_overlap)
              continue;
            analysis.record_instance(it->first, inst_overlap);
            if (release_finder->second.insert(it->first, it->second))
              it->first->add_nested_valid_ref(did);
            // Each field should only be represented by one instance
            overlap -= inst_overlap;
            if (!overlap)
              break;
          }
        }
        // If expressions matched and we handled all the fields then
        // we can be done since we know there are no other overlaps
        if (done_early)
          break;
      }
      for (std::map<IndexSpaceExpression*,IndexSpaceExpression*>::const_iterator
            eit = to_add.begin(); eit != to_add.end(); eit++)
      {
        if (restricted_instances.find(eit->second) ==restricted_instances.end())
          eit->second->add_nested_expression_reference(did);
        FieldMaskSet<InstanceView> &old_insts =restricted_instances[eit->first];
        FieldMaskSet<InstanceView> &new_insts=restricted_instances[eit->second];
        if (!new_insts.empty() || !!(old_insts.get_valid_mask() & acquire_mask))
        {
          std::vector<InstanceView*> to_erase;
          for (FieldMaskSet<InstanceView>::iterator it =
                old_insts.begin(); it != old_insts.end(); it++)
          {
            const FieldMask overlap = it->second & acquire_mask;
            if (!overlap)
              continue;
            if (new_insts.insert(it->first, overlap))
              it->first->add_nested_valid_ref(did);
            it.filter(overlap);
            if (!it->second)
              to_erase.push_back(it->first);
          }
          for (std::vector<InstanceView*>::const_iterator it =
                to_erase.begin(); it != to_erase.end(); it++)
          {
            old_insts.erase(*it);
            if ((*it)->remove_nested_valid_ref(did))
              delete (*it);
          }
          if (old_insts.empty())
            to_delete.push_back(eit->first);
          else
            old_insts.tighten_valid_mask();
        }
        else
        {
          new_insts.swap(old_insts); 
          to_delete.push_back(eit->first);
        }
      }
      for (std::vector<IndexSpaceExpression*>::const_iterator it =
            to_delete.begin(); it != to_delete.end(); it++)
      {
        restricted_instances.erase(*it);
        if ((*it)->remove_nested_expression_reference(did))
          delete (*it);
      }
      restricted_fields.clear();
      for (ExprViewMaskSets::const_iterator it = restricted_instances.begin();
            it != restricted_instances.end(); it++)
        restricted_fields |= it->second.get_valid_mask();
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::release_restrictions(ReleaseAnalysis &analysis, 
                                             IndexSpaceExpression *expr,
                                             const bool expr_covers, 
                                             const FieldMask &release_mask,
                                             std::set<RtEvent> &deferral_events,
                                             std::set<RtEvent> &applied_events,
                                             const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // Already holding the eq_lock from EquivalenceSet::analyze method
      // We need to lock the analysis at this point
      AutoLock a_lock(analysis);
      // If the target views are empty then we are just restoring the
      // existing released instances, if we have target views then we
      // know what the restricted instaces are going to be but we still
      // need to filter out any previously released instances
      if (analysis.target_views.empty())
      {
        LegionMap<IndexSpaceExpression*,
                FieldMaskSet<InstanceView> > to_update;
        std::vector<IndexSpaceExpression*> to_delete;
        std::map<IndexSpaceExpression*,IndexSpaceExpression*> to_add;
        for (ExprViewMaskSets::iterator eit = released_instances.begin();
              eit != released_instances.end(); eit++)
        {
          FieldMask overlap = eit->second.get_valid_mask() & release_mask;
          if (!overlap)
            continue;
          IndexSpaceExpression *overlap_expr = NULL;
          if (!expr_covers && (eit->first != expr))
          {
            overlap_expr = 
              runtime->forest->intersect_index_spaces(eit->first, expr);
            const size_t overlap_size = overlap_expr->get_volume();
            if (overlap_size == 0)
              continue;
            if (overlap_size == eit->first->get_volume())
              overlap_expr = eit->first;
            else if (overlap_size == expr->get_volume())
              overlap_expr = expr;
          }
          else
            overlap_expr = eit->first;
          const bool overlap_covers = 
            (overlap_expr->get_volume() == set_expr->get_volume());
          if (overlap_expr == eit->first)
          {
            // Total covering of expressions
            // so move all instances back to being restricted
            std::vector<InstanceView*> to_erase;
            FieldMaskSet<InstanceView> &updates = to_update[eit->first];
            for (FieldMaskSet<InstanceView>::iterator it = 
                  eit->second.begin(); it != eit->second.end(); it++)
            {
              const FieldMask inst_overlap = overlap & it->second;
              if (!inst_overlap)
                continue;
              analysis.record_instance(it->first, inst_overlap);
              updates.insert(it->first, inst_overlap);
              // Record this as a restricted instance
              record_restriction(overlap_expr, overlap_covers, inst_overlap,
                                 it->first);
              // Remove it from here
              it.filter(inst_overlap);
              if (!it->second)
                to_erase.push_back(it->first);
              // Each field should only be represented by one instance
              overlap -= inst_overlap;
              if (!overlap)
                break;
            }
            for (std::vector<InstanceView*>::const_iterator it =
                  to_erase.begin(); it != to_erase.end(); it++)
            {
              eit->second.erase(*it);
              if ((*it)->remove_nested_valid_ref(did))
                delete (*it);
            }
            if (!eit->second.empty())
              eit->second.tighten_valid_mask();
            else
              to_delete.push_back(eit->first);
          }
          else
          {
            // Only partial covering, so compute the difference
            // and record that we'll pull valid instances from here
            to_add[eit->first] = 
              runtime->forest->subtract_index_spaces(eit->first, expr);
            FieldMaskSet<InstanceView> &updates = to_update[overlap_expr];
            // The intersection gets merged back into relased sets
            for (FieldMaskSet<InstanceView>::const_iterator it =
                  eit->second.begin(); it != eit->second.end(); it++)
            {
              const FieldMask inst_overlap = overlap & it->second;
              if (!inst_overlap)
                continue;
              analysis.record_instance(it->first, inst_overlap);
              updates.insert(it->first, inst_overlap);
              // Record this as a restricted instance
              record_restriction(overlap_expr, overlap_covers, inst_overlap,
                                 it->first);
              // Each field should only be represented by one instance
              overlap -= inst_overlap;
              if (!overlap)
                break;
            }
          }
        }
        // Record updates to the released sets
        for (std::map<IndexSpaceExpression*,IndexSpaceExpression*>::const_iterator
              eit = to_add.begin(); eit != to_add.end(); eit++)
        {
          if (released_instances.find(eit->first) == released_instances.end())
            eit->first->add_nested_expression_reference(did);
          FieldMaskSet<InstanceView> &new_insts = released_instances[eit->first];
          FieldMaskSet<InstanceView> &old_insts = released_instances[eit->second];
          if (!new_insts.empty() || !!(old_insts.get_valid_mask() & release_mask))
          {
            std::vector<InstanceView*> to_erase;
            for (FieldMaskSet<InstanceView>::iterator it =
                  old_insts.begin(); it != old_insts.end(); it++)
            {
              const FieldMask overlap = it->second & release_mask;
              if (!overlap)
                continue;
              if (new_insts.insert(it->first, overlap))
                it->first->add_nested_valid_ref(did);
              it.filter(overlap);
              if (!it->second)
                to_erase.push_back(it->first);
            }
            for (std::vector<InstanceView*>::const_iterator it =
                  to_erase.begin(); it != to_erase.end(); it++)
            {
              old_insts.erase(*it);
              if ((*it)->remove_nested_valid_ref(did))
                delete (*it);
            }
            if (old_insts.empty())
              to_delete.push_back(eit->first);
            else
              old_insts.tighten_valid_mask();
          }
          else
          {
            new_insts.swap(old_insts); 
            to_delete.push_back(eit->first);
          }
        }
        for (std::vector<IndexSpaceExpression*>::const_iterator it =
              to_delete.begin(); it != to_delete.end(); it++)
        {
          released_instances.erase(*it);
          if ((*it)->remove_nested_expression_reference(did))
            delete (*it);
        }
        // Now generate the copies for any updates to the restricted instances
        if (analysis.release_aggregator != NULL)
          analysis.release_aggregator->clear_update_fields();
        const RegionUsage release_usage(LEGION_READ_WRITE, LEGION_EXCLUSIVE, 0);
        for (LegionMap<IndexSpaceExpression*,
                       FieldMaskSet<InstanceView> >::const_iterator it =
              to_update.begin(); it != to_update.end(); it++)
        {
          // If we found all these views that means that they should all be
          // individual views since we can't have any restricted collective
          // views in non-control replicated settings and we can never be
          // here if we're in a control replicated context
          std::vector<PhysicalManager*> targets(it->second.size());
          LegionVector<FieldMaskSet<InstanceView> > views(it->second.size());
          unsigned index = 0;
          for (FieldMaskSet<InstanceView>::const_iterator vit =
                it->second.begin(); vit != it->second.end(); vit++, index++)
          {
#ifdef DEBUG_LEGION
            assert(vit->first->is_individual_view());
#endif
            IndividualView *view = vit->first->as_individual_view();
            targets[index] = view->get_manager();
            views[index].insert(vit->first, vit->second);
            update_set_internal(analysis.release_aggregator, NULL/*no guard*/,
                                &analysis, release_usage,
                                it->first, (it->first == set_expr),
                                it->second.get_valid_mask(), targets, views, 
                                analysis.source_views, analysis.trace_info,
                                true/*record valid*/, true/*record release*/);
            // Finally update the tracing postconditions now that we've recorded
            // any copies as part of the trace
            if (tracing_postconditions != NULL)
              tracing_postconditions->invalidate_all_but(vit->first, it->first,
                                                         vit->second);
          }
        }
      }
      else
      {
        // If we're not restoring the released instance then we should
        // record the actual instances that we are making restricted
        // Make sure that we don't have any overlapping restrictions
        filter_restricted_instances(expr, expr_covers, release_mask);
        // Make sure that we remove any old released instances
        filter_released_instances(expr, expr_covers, release_mask);
        for (unsigned idx = 0; idx < analysis.target_views.size(); idx++)
        {
          for (FieldMaskSet<InstanceView>::const_iterator it =
                analysis.target_views[idx].begin(); it != 
                analysis.target_views[idx].end(); it++)
          {
            // Record this as a restricted instance
            record_restriction(expr, expr_covers, it->second, it->first);
            // Update the tracing postconditions now that we've recorded
            // any copies as part of the trace
            if (tracing_postconditions != NULL)
              tracing_postconditions->invalidate_all_but(it->first, expr,
                                                         it->second);
          }
        }
        // Now generate the copies for any updates to the restricted instances
        if (analysis.release_aggregator != NULL)
          analysis.release_aggregator->clear_update_fields();
        const RegionUsage release_usage(LEGION_READ_WRITE, LEGION_EXCLUSIVE, 0);
        update_set_internal(analysis.release_aggregator, NULL/*no guard*/,
                            &analysis, release_usage, expr, expr_covers, 
                            release_mask, analysis.target_instances, 
                            analysis.target_views, analysis.source_views,
                            analysis.trace_info, true/*record valid*/,
                            true/*record release*/);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::record_restriction(IndexSpaceExpression *expr, 
                                            const bool expr_covers,
                                            const FieldMask &restrict_mask,
                                            InstanceView *restrict_view)
    //--------------------------------------------------------------------------
    {
      // This function only looks for merges of restrictions. It assumes
      // that restrictions are inherently non-overlapping and does not
      // attempt to enforce that there are no such overlaps

      // First see if we need to merge with any existing restrictions
      if (expr_covers)
      {
        // No need to check for merging, we should be independent
        ExprViewMaskSets::iterator restricted_finder =
          restricted_instances.find(set_expr);
        if (restricted_finder == restricted_instances.end())
        {
          set_expr->add_nested_expression_reference(did);
          restrict_view->add_nested_valid_ref(did);
          restricted_instances[set_expr].insert(restrict_view, restrict_mask);
        }
        else if (restricted_finder->second.insert(restrict_view, restrict_mask))
          restrict_view->add_nested_valid_ref(did);
      }
      else
      {
        // Check to see if we can union this expression with any others
        FieldMaskSet<IndexSpaceExpression> to_union;
        std::vector<IndexSpaceExpression*> to_delete;
        for (ExprViewMaskSets::iterator eit = restricted_instances.begin();
              eit != restricted_instances.end(); eit++)
        {
          FieldMaskSet<InstanceView>::iterator finder = 
            eit->second.find(restrict_view);
          if (finder == eit->second.end())
            continue;
          const FieldMask overlap = finder->second & restrict_mask;
          if (!overlap)
            continue;
          to_union.insert(eit->first, overlap);
          finder.filter(overlap);
          if (!finder->second)
          {
            if (finder->first->remove_nested_valid_ref(did))
              delete finder->first;
            eit->second.erase(finder);
            if (eit->second.empty())
              to_delete.push_back(eit->first);
          }
          else
            eit->second.tighten_valid_mask();
        }
        // Add in the new sets
        if (!to_union.empty())
        {
          LegionList<FieldSet<IndexSpaceExpression*> > expr_sets;
          to_union.compute_field_sets(FieldMask(), expr_sets);
          for (LegionList<FieldSet<IndexSpaceExpression*> >::iterator
                it = expr_sets.begin(); it != expr_sets.end(); it++)
          {
            it->elements.insert(expr);
            IndexSpaceExpression *union_expr = 
              runtime->forest->union_index_spaces(it->elements); 
            if (union_expr->get_volume() < set_expr->get_volume())
            {
              ExprViewMaskSets::iterator restricted_finder =
                restricted_instances.find(union_expr);
              if (restricted_finder == restricted_instances.end())
              {
                union_expr->add_nested_expression_reference(did);
                restrict_view->add_nested_valid_ref(did);
                restricted_instances[union_expr].insert(restrict_view, 
                                                      it->set_mask);
              }
              else if (restricted_finder->second.insert(restrict_view, 
                                                        it->set_mask))
                restrict_view->add_nested_valid_ref(did);
            }
            else
            {
              ExprViewMaskSets::iterator restricted_finder =
                restricted_instances.find(set_expr);
              if (restricted_finder == restricted_instances.end())
              {
                set_expr->add_nested_expression_reference(did);
                restrict_view->add_nested_valid_ref(did);
                restricted_instances[set_expr].insert(restrict_view, 
                                                      it->set_mask);
              }
              else if (restricted_finder->second.insert(restrict_view, 
                                                        it->set_mask))
                restrict_view->add_nested_valid_ref(did);
            }
          }
          const FieldMask remainder = restrict_mask - to_union.get_valid_mask();
          if (!!remainder)
          {
            ExprViewMaskSets::iterator restricted_finder =
              restricted_instances.find(expr);
            if (restricted_finder == restricted_instances.end())
            {
              expr->add_nested_expression_reference(did);
              restrict_view->add_nested_valid_ref(did);
              restricted_instances[expr].insert(restrict_view, remainder);
            }
            else if (restricted_finder->second.insert(restrict_view, remainder))
              restrict_view->add_nested_valid_ref(did);
          }
        }
        else
        {
          // Just record ourselves since there was nothing to merge
          ExprViewMaskSets::iterator restricted_finder =
            restricted_instances.find(expr);
          if (restricted_finder == restricted_instances.end())
          {
            expr->add_nested_expression_reference(did);
            restrict_view->add_nested_valid_ref(did);
            restricted_instances[expr].insert(restrict_view, restrict_mask);
          }
          else if (restricted_finder->second.insert(restrict_view, 
                                                    restrict_mask))
            restrict_view->add_nested_valid_ref(did);
        }
        for (std::vector<IndexSpaceExpression*>::const_iterator it =
              to_delete.begin(); it != to_delete.end(); it++)
        {
          restricted_instances.erase(*it);
          if ((*it)->remove_nested_expression_reference(did))
            delete (*it);
        }
      }
#ifdef DEBUG_LEGION
      assert(!restricted_instances.empty());
#endif
      // Always update the restricted fields
      restricted_fields |= restrict_mask;
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::update_reductions(const unsigned fidx, 
           std::list<std::pair<InstanceView*,IndexSpaceExpression*> > &updates)
    //--------------------------------------------------------------------------
    {
      if (updates.empty())
        return;
      // Check for equivalence to the dst and then add our references
      const size_t volume = set_expr->get_volume();
      for (std::list<std::pair<InstanceView*,IndexSpaceExpression*> >::iterator
            it = updates.begin(); it != updates.end(); it++)
      {
        it->first->add_nested_valid_ref(did);
        if (it->second->get_volume() == volume)
          it->second = set_expr;
        it->second->add_nested_expression_reference(did);
      }
      std::list<std::pair<InstanceView*,IndexSpaceExpression*> > &current =
        reduction_instances[fidx];
      current.splice(current.end(), updates);
      reduction_fields.set_bit(fidx);
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::update_released(IndexSpaceExpression *expr, 
                    const bool expr_covers, FieldMaskSet<InstanceView> &updates)
    //--------------------------------------------------------------------------
    {
      if (expr->get_volume() == set_expr->get_volume())
        expr = set_expr;
      ExprViewMaskSets::iterator finder = released_instances.find(expr);
      if (finder != released_instances.end())
      {
        for (FieldMaskSet<InstanceView>::const_iterator it =
              updates.begin(); it != updates.end(); it++)
          if (finder->second.insert(it->first, it->second))
            it->first->add_nested_valid_ref(did);
      }
      else
      {
        expr->add_nested_expression_reference(did);
        for (FieldMaskSet<InstanceView>::const_iterator it =
              updates.begin(); it != updates.end(); it++)
          it->first->add_nested_valid_ref(did);
        released_instances[expr].swap(updates);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::filter_initialized_data(IndexSpaceExpression *expr, 
                  const bool expr_covers, const FieldMask &filter_mask, 
                  std::map<IndexSpaceExpression*,unsigned> *expr_refs_to_remove)
    //--------------------------------------------------------------------------
    {
      if (initialized_data.empty() || 
          (filter_mask * initialized_data.get_valid_mask()))
        return;
      if (expr_covers)
      {
        if (!(initialized_data.get_valid_mask() - filter_mask))
        {
          // filter everything
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                initialized_data.begin(); it != initialized_data.end(); it++)
          {
            if (expr_refs_to_remove != NULL)
            {
              std::map<IndexSpaceExpression*,unsigned>::iterator finder =
                expr_refs_to_remove->find(it->first);
              if (finder == expr_refs_to_remove->end())
                (*expr_refs_to_remove)[it->first] = 1;
              else
                finder->second += 1;
            }
            else if (it->first->remove_nested_expression_reference(did))
              delete it->first;
          }
          initialized_data.clear();
        }
        else
        {
          // filter fields
          std::vector<IndexSpaceExpression*> to_delete;
          for (FieldMaskSet<IndexSpaceExpression>::iterator it =
                initialized_data.begin(); it != initialized_data.end(); it++)
          {
            it.filter(filter_mask);
            if (!it->second)
              to_delete.push_back(it->first);
          }
          if (!to_delete.empty())
          {
            for (std::vector<IndexSpaceExpression*>::const_iterator it =
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              initialized_data.erase(*it);
              if (expr_refs_to_remove != NULL)
              {
                std::map<IndexSpaceExpression*,unsigned>::iterator finder =
                  expr_refs_to_remove->find(*it);
                if (finder == expr_refs_to_remove->end())
                  (*expr_refs_to_remove)[*it] = 1;
                else
                  finder->second += 1;
              }
              else if ((*it)->remove_nested_expression_reference(did))
                delete (*it);
            }
          }
          if (!initialized_data.empty())
            initialized_data.tighten_valid_mask();
        }
      }
      else
      {
        // Filter on fields first and then on expressions
        std::vector<IndexSpaceExpression*> to_delete;
        FieldMaskSet<IndexSpaceExpression> to_add;
        for (FieldMaskSet<IndexSpaceExpression>::iterator it =
              initialized_data.begin(); it != initialized_data.end(); it++)
        {
          const FieldMask overlap = filter_mask & it->second;
          if (!overlap)
            continue;
          if (it->first != set_expr)
          {
            IndexSpaceExpression *intersection =
              runtime->forest->intersect_index_spaces(it->first, expr);
            const size_t volume = intersection->get_volume();
            if (volume == 0)
              continue;
            // We're removing this expression no matter what at this point
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
            // See if there are any remaining points left
            if (volume < it->first->get_volume())
            {
              IndexSpaceExpression *diff = 
                runtime->forest->subtract_index_spaces(it->first, intersection);
              to_add.insert(diff, overlap);
            }
          }
          else // special case for when we know that the expr is the set expr
          {
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
            to_add.insert(
              runtime->forest->subtract_index_spaces(it->first, expr), overlap);
          }
        }
        if (!to_add.empty())
        {
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                to_add.begin(); it != to_add.end(); it++)
            if (initialized_data.insert(it->first, it->second))
              it->first->add_nested_expression_reference(did);
        }
        if (!to_delete.empty())
        {
          for (std::vector<IndexSpaceExpression*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
          {
            if (to_add.find(*it) != to_add.end())
              continue;
            initialized_data.erase(*it);
            if (expr_refs_to_remove != NULL)
            {
              std::map<IndexSpaceExpression*,unsigned>::iterator finder =
                expr_refs_to_remove->find(*it);
              if (finder == expr_refs_to_remove->end())
                (*expr_refs_to_remove)[*it] = 1;
              else
                finder->second += 1;
            }
            else if ((*it)->remove_nested_expression_reference(did))
              delete (*it);
          }
        }
        if (!initialized_data.empty())
          initialized_data.tighten_valid_mask();
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::filter_restricted_instances(IndexSpaceExpression *expr,
                  const bool expr_covers, const FieldMask &filter_mask,
                  std::map<IndexSpaceExpression*,unsigned> *expr_refs_to_remove,
                  std::map<LogicalView*,unsigned> *view_refs_to_remove)
    //--------------------------------------------------------------------------
    {
      if (restricted_instances.empty() || (filter_mask * restricted_fields))
        return;
      if (expr_covers)
      {
        if (!(restricted_fields - filter_mask))
        {
          // filter everything
          for (ExprViewMaskSets::const_iterator rit =
                restricted_instances.begin(); rit != 
                restricted_instances.end(); rit++)
          {
            if (expr_refs_to_remove != NULL)
            {
              std::map<IndexSpaceExpression*,unsigned>::iterator finder =
                expr_refs_to_remove->find(rit->first);
              if (finder == expr_refs_to_remove->end())
                (*expr_refs_to_remove)[rit->first] = 1;
              else
                finder->second += 1;
            }
            else if (rit->first->remove_nested_expression_reference(did))
              delete rit->first;
            for (FieldMaskSet<InstanceView>::const_iterator it =
                  rit->second.begin(); it != rit->second.end(); it++)
            {
              if (view_refs_to_remove != NULL)
              {
                std::map<LogicalView*,unsigned>::iterator finder = 
                  view_refs_to_remove->find(it->first);
                if (finder == view_refs_to_remove->end())
                  (*view_refs_to_remove)[it->first] = 1;
                else
                  finder->second += 1;
              }
              else if (it->first->remove_nested_valid_ref(did))
                delete it->first;
            }
          }
          restricted_instances.clear();
          restricted_fields.clear();
        }
        else
        {
          // filter fields
          std::vector<IndexSpaceExpression*> to_delete;
          for (ExprViewMaskSets::iterator rit = restricted_instances.begin();
                rit != restricted_instances.end(); rit++)
          {
            if (!(rit->second.get_valid_mask() - filter_mask))
            {
              // delete all the views in this one
              for (FieldMaskSet<InstanceView>::const_iterator it =
                    rit->second.begin(); it != rit->second.end(); it++)
              {
                if (view_refs_to_remove != NULL)
                {
                  std::map<LogicalView*,unsigned>::iterator finder =
                    view_refs_to_remove->find(it->first);
                  if (finder == view_refs_to_remove->end())
                    (*view_refs_to_remove)[it->first] = 1;
                  else
                    finder->second += 1;
                }
                else if (it->first->remove_nested_valid_ref(did))
                  delete it->first;
              }
              to_delete.push_back(rit->first);
            }
            else
            {
              // filter views based on fields
              std::vector<InstanceView*> to_erase;
              for (FieldMaskSet<InstanceView>::iterator it =
                    rit->second.begin(); it != rit->second.end(); it++)
              {
                it.filter(filter_mask);
                if (!it->second)
                  to_erase.push_back(it->first);
              }
              for (std::vector<InstanceView*>::const_iterator it =
                    to_erase.begin(); it != to_erase.end(); it++)
              {
                if (view_refs_to_remove != NULL)
                {
                  std::map<LogicalView*,unsigned>::iterator finder =
                    view_refs_to_remove->find(*it);
                  if (finder == view_refs_to_remove->end())
                    (*view_refs_to_remove)[*it] = 1;
                  else
                    finder->second += 1;
                }
                else if ((*it)->remove_nested_valid_ref(did))
                  delete (*it);
              }
              if (rit->second.empty())
                to_delete.push_back(rit->first);
              else
                rit->second.tighten_valid_mask();
            }
          }
          for (std::vector<IndexSpaceExpression*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
          {
            restricted_instances.erase(*it);
            if (expr_refs_to_remove != NULL)
            {
              std::map<IndexSpaceExpression*,unsigned>::iterator finder =
                expr_refs_to_remove->find(*it);
              if (finder == expr_refs_to_remove->end())
                (*expr_refs_to_remove)[*it] = 1;
              else
                finder->second += 1;
            }
            else if ((*it)->remove_nested_expression_reference(did))
              delete (*it);
          }
          restricted_fields -= filter_mask;
        }
      }
      else
      {
        // Expression does not cover this equivalence set
        std::vector<IndexSpaceExpression*> to_delete;
        LegionMap<IndexSpaceExpression*,FieldMaskSet<InstanceView> > to_add;
        for (ExprViewMaskSets::iterator rit = restricted_instances.begin();
              rit != restricted_instances.end(); rit++)
        {
          if (rit->second.get_valid_mask() * filter_mask)
            continue;
          IndexSpaceExpression *intersection = (rit->first == set_expr) ? expr :
            runtime->forest->intersect_index_spaces(rit->first, expr);
          const size_t volume = intersection->get_volume();
          if (volume == 0)
            continue;
          if (volume == rit->first->get_volume())
          {
            // Covers the whole expression
            if (!(rit->second.get_valid_mask() - filter_mask))
            {
              // filter all of them
              for (FieldMaskSet<InstanceView>::const_iterator it =
                    rit->second.begin(); it != rit->second.end(); it++)
              {
                if (view_refs_to_remove != NULL)
                {
                  std::map<LogicalView*,unsigned>::iterator finder =
                    view_refs_to_remove->find(it->first);
                  if (finder == view_refs_to_remove->end())
                    (*view_refs_to_remove)[it->first] = 1;
                  else
                    finder->second += 1;
                }
                else if (it->first->remove_nested_valid_ref(did))
                  delete it->first;
              }
              to_delete.push_back(rit->first);
            }
            else
            {
              // fitler by fields
              std::vector<InstanceView*> to_erase;
              for (FieldMaskSet<InstanceView>::iterator it =
                    rit->second.begin(); it != rit->second.end(); it++)
              {
                it.filter(filter_mask);
                if (!it->second)
                  to_erase.push_back(it->first);
              }
              for (std::vector<InstanceView*>::const_iterator it = 
                    to_erase.begin(); it != to_erase.end(); it++)
              {
                rit->second.erase(*it);
                if (view_refs_to_remove != NULL)
                {
                  std::map<LogicalView*,unsigned>::iterator finder =
                    view_refs_to_remove->find(*it);
                  if (finder == view_refs_to_remove->end())
                    (*view_refs_to_remove)[*it] = 1;
                  else
                    finder->second += 1;
                }
                else if ((*it)->remove_nested_valid_ref(did))
                  delete (*it);
              }
              if (rit->second.empty())
                to_delete.push_back(rit->first);
              else
                rit->second.tighten_valid_mask();
            }
          }
          else
          {
            // Only covers part, so compute diff and put them in the add set
            IndexSpaceExpression *diff = 
              runtime->forest->subtract_index_spaces(rit->first, intersection);
            if (!(rit->second.get_valid_mask() - filter_mask))
            {
              // All the views are flowing into to_add
              LegionMap<IndexSpaceExpression*,
                FieldMaskSet<InstanceView> >::iterator finder = 
                  to_add.find(diff);
              if (finder != to_add.end())
              {
                // Deduplicate references in to add
                for (FieldMaskSet<InstanceView>::const_iterator it =
                      rit->second.begin(); it != rit->second.end(); it++)
                  if (!finder->second.insert(it->first, it->second) &&
                      it->first->remove_nested_valid_ref(did))
                    assert(false); // should never hit this
              }
              else
                to_add[diff].swap(rit->second);
              to_delete.push_back(rit->first);
            }
            else
            {
              // Filter by fields
              FieldMaskSet<InstanceView> &add_set = to_add[diff];
              std::vector<InstanceView*> to_erase;
              for (FieldMaskSet<InstanceView>::iterator it =
                    rit->second.begin(); it != rit->second.end(); it++)
              {
                const FieldMask overlap = filter_mask & it->second;
                if (!overlap)
                  continue;
                if (add_set.insert(it->first, overlap))
                  it->first->add_nested_valid_ref(did);
                it.filter(overlap);
                if (!it->second)
                  to_erase.push_back(it->first);
              }
              for (std::vector<InstanceView*>::const_iterator it = 
                    to_erase.begin(); it != to_erase.end(); it++)
              {
                rit->second.erase(*it);
                if (view_refs_to_remove != NULL)
                {
                  std::map<LogicalView*,unsigned>::iterator finder =
                    view_refs_to_remove->find(*it);
                  if (finder == view_refs_to_remove->end())
                    (*view_refs_to_remove)[*it] = 1;
                  else
                    finder->second += 1;
                }
                else if ((*it)->remove_nested_valid_ref(did))
                  delete (*it);
              }
              if (rit->second.empty())
                to_delete.push_back(rit->first);
              else
                rit->second.tighten_valid_mask();
            }
          }
        }
        for (LegionMap<IndexSpaceExpression*,
              FieldMaskSet<InstanceView> >::iterator ait =
              to_add.begin(); ait != to_add.end(); ait++)
        {
          ExprViewMaskSets::iterator finder =
            restricted_instances.find(ait->first);
          if (finder != restricted_instances.end())
          {
            ait->first->add_nested_expression_reference(did);
            restricted_instances[ait->first].swap(ait->second);
          }
          else
          {
            for (FieldMaskSet<InstanceView>::const_iterator it =
                  ait->second.begin(); it != ait->second.end(); it++)
              // remove duplicate references
              if (!finder->second.insert(it->first, it->second) &&
                  it->first->remove_nested_valid_ref(did))
                assert(false); // should never hit this
          }
        }
        for (std::vector<IndexSpaceExpression*>::const_iterator it =  
              to_delete.begin(); it != to_delete.end(); it++)
        {
          if (to_add.find(*it) != to_add.end())
            continue;
          restricted_instances.erase(*it);
          if (expr_refs_to_remove != NULL)
          {
            std::map<IndexSpaceExpression*,unsigned>::iterator finder =
              expr_refs_to_remove->find(*it);
            if (finder == expr_refs_to_remove->end())
              (*expr_refs_to_remove)[*it] = 1;
            else
              finder->second += 1;
          }
          else if ((*it)->remove_nested_expression_reference(did))
            delete (*it);
        }
        // Rebuild the restricted fields
        if (!restricted_instances.empty())
        {
          restricted_fields.clear();
          for (ExprViewMaskSets::const_iterator rit =
                restricted_instances.begin(); rit != 
                restricted_instances.end(); rit++)
            restricted_fields |= rit->second.get_valid_mask();
        }
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::filter_released_instances(IndexSpaceExpression *expr,
                  const bool expr_covers, const FieldMask &filter_mask,
                  std::map<IndexSpaceExpression*,unsigned> *expr_refs_to_remove,
                  std::map<LogicalView*,unsigned> *view_refs_to_remove)
    //--------------------------------------------------------------------------
    {
      if (released_instances.empty())
        return;
      if (expr_covers)
      {
        // filter fields
        std::vector<IndexSpaceExpression*> to_delete;
        for (ExprViewMaskSets::iterator rit = released_instances.begin();
              rit != released_instances.end(); rit++)
        {
          if (!(rit->second.get_valid_mask() - filter_mask))
          {
            // delete all the views in this one
            for (FieldMaskSet<InstanceView>::const_iterator it =
                  rit->second.begin(); it != rit->second.end(); it++)
            {
              if (view_refs_to_remove != NULL)
              {
                std::map<LogicalView*,unsigned>::iterator finder =
                  view_refs_to_remove->find(it->first);
                if (finder == view_refs_to_remove->end())
                  (*view_refs_to_remove)[it->first] = 1;
                else
                  finder->second += 1;
              }
              else if (it->first->remove_nested_valid_ref(did))
                delete it->first;
            }
            to_delete.push_back(rit->first);
          }
          else
          {
            // filter views based on fields
            std::vector<InstanceView*> to_erase;
            for (FieldMaskSet<InstanceView>::iterator it =
                  rit->second.begin(); it != rit->second.end(); it++)
            {
              it.filter(filter_mask);
              if (!it->second)
                to_erase.push_back(it->first);
            }
            for (std::vector<InstanceView*>::const_iterator it =
                  to_erase.begin(); it != to_erase.end(); it++)
            {
              if (view_refs_to_remove != NULL)
              {
                std::map<LogicalView*,unsigned>::iterator finder =
                  view_refs_to_remove->find(*it);
                if (finder == view_refs_to_remove->end())
                  (*view_refs_to_remove)[*it] = 1;
                else
                  finder->second += 1;
              }
              else if ((*it)->remove_nested_valid_ref(did))
                delete (*it);
            }
            if (rit->second.empty())
              to_delete.push_back(rit->first);
            else
              rit->second.tighten_valid_mask();
          }
        }
        for (std::vector<IndexSpaceExpression*>::const_iterator it =
              to_delete.begin(); it != to_delete.end(); it++)
        {
          released_instances.erase(*it);
          if (expr_refs_to_remove != NULL)
          {
            std::map<IndexSpaceExpression*,unsigned>::iterator finder =
              expr_refs_to_remove->find(*it);
            if (finder == expr_refs_to_remove->end())
              (*expr_refs_to_remove)[*it] = 1;
            else
              finder->second += 1;
          }
          else if ((*it)->remove_nested_expression_reference(did))
            delete (*it);
        }
      }
      else
      {
        // Expression does not cover this equivalence set
        std::vector<IndexSpaceExpression*> to_delete;
        LegionMap<IndexSpaceExpression*,FieldMaskSet<InstanceView> > to_add;
        for (ExprViewMaskSets::iterator rit = released_instances.begin();
              rit != released_instances.end(); rit++)
        {
          if (rit->second.get_valid_mask() * filter_mask)
            continue;
          IndexSpaceExpression *intersection = (rit->first == set_expr) ? expr :
            runtime->forest->intersect_index_spaces(rit->first, expr);
          const size_t volume = intersection->get_volume();
          if (volume == 0)
            continue;
          if (volume == rit->first->get_volume())
          {
            // Covers the whole expression
            if (!(rit->second.get_valid_mask() - filter_mask))
            {
              // filter all of them
              for (FieldMaskSet<InstanceView>::const_iterator it =
                    rit->second.begin(); it != rit->second.end(); it++)
              {
                if (view_refs_to_remove != NULL)
                {
                  std::map<LogicalView*,unsigned>::iterator finder =
                    view_refs_to_remove->find(it->first);
                  if (finder == view_refs_to_remove->end())
                    (*view_refs_to_remove)[it->first] = 1;
                  else
                    finder->second += 1;
                }
                else if (it->first->remove_nested_valid_ref(did))
                  delete it->first;
              }
              to_delete.push_back(rit->first);
            }
            else
            {
              // fitler by fields
              std::vector<InstanceView*> to_erase;
              for (FieldMaskSet<InstanceView>::iterator it =
                    rit->second.begin(); it != rit->second.end(); it++)
              {
                it.filter(filter_mask);
                if (!it->second)
                  to_erase.push_back(it->first);
              }
              for (std::vector<InstanceView*>::const_iterator it = 
                    to_erase.begin(); it != to_erase.end(); it++)
              {
                rit->second.erase(*it);
                if (view_refs_to_remove != NULL)
                {
                  std::map<LogicalView*,unsigned>::iterator finder =
                    view_refs_to_remove->find(*it);
                  if (finder == view_refs_to_remove->end())
                    (*view_refs_to_remove)[*it] = 1;
                  else
                    finder->second += 1;
                }
                else if ((*it)->remove_nested_valid_ref(did))
                  delete (*it);
              }
              if (rit->second.empty())
                to_delete.push_back(rit->first);
              else
                rit->second.tighten_valid_mask();
            }
          }
          else
          {
            // Only covers part, so compute diff and put them in the add set
            IndexSpaceExpression *diff = 
              runtime->forest->subtract_index_spaces(rit->first, intersection);
            if (!(rit->second.get_valid_mask() - filter_mask))
            {
              // All the views are flowing into to_add
              LegionMap<IndexSpaceExpression*,
                FieldMaskSet<InstanceView> >::iterator finder = 
                  to_add.find(diff);
              if (finder != to_add.end())
              {
                // Deduplicate references in to add
                for (FieldMaskSet<InstanceView>::const_iterator it =
                      rit->second.begin(); it != rit->second.end(); it++)
                  if (!finder->second.insert(it->first, it->second) &&
                      it->first->remove_nested_valid_ref(did))
                    assert(false); // should never hit this
              }
              else
                to_add[diff].swap(rit->second);
              to_delete.push_back(rit->first);
            }
            else
            {
              // Filter by fields
              FieldMaskSet<InstanceView> &add_set = to_add[diff];
              std::vector<InstanceView*> to_erase;
              for (FieldMaskSet<InstanceView>::iterator it =
                    rit->second.begin(); it != rit->second.end(); it++)
              {
                const FieldMask overlap = filter_mask & it->second;
                if (!overlap)
                  continue;
                if (add_set.insert(it->first, overlap))
                  it->first->add_nested_valid_ref(did);
                it.filter(overlap);
                if (!it->second)
                  to_erase.push_back(it->first);
              }
              for (std::vector<InstanceView*>::const_iterator it = 
                    to_erase.begin(); it != to_erase.end(); it++)
              {
                rit->second.erase(*it);
                if (view_refs_to_remove != NULL)
                {
                  std::map<LogicalView*,unsigned>::iterator finder =
                    view_refs_to_remove->find(*it);
                  if (finder == view_refs_to_remove->end())
                    (*view_refs_to_remove)[*it] = 1;
                  else
                    finder->second += 1;
                }
                else if ((*it)->remove_nested_valid_ref(did))
                  delete (*it);
              }
              if (rit->second.empty())
                to_delete.push_back(rit->first);
              else
                rit->second.tighten_valid_mask();
            }
          }
        }
        for (LegionMap<IndexSpaceExpression*,
              FieldMaskSet<InstanceView> >::iterator ait =
              to_add.begin(); ait != to_add.end(); ait++)
        {
          ExprViewMaskSets::iterator finder =
            released_instances.find(ait->first);
          if (finder != released_instances.end())
          {
            ait->first->add_nested_expression_reference(did);
            released_instances[ait->first].swap(ait->second);
          }
          else
          {
            for (FieldMaskSet<InstanceView>::const_iterator it =
                  ait->second.begin(); it != ait->second.end(); it++)
              // remove duplicate references
              if (!finder->second.insert(it->first, it->second) &&
                  it->first->remove_nested_valid_ref(did))
                assert(false); // should never hit this
          }
        }
        for (std::vector<IndexSpaceExpression*>::const_iterator it =  
              to_delete.begin(); it != to_delete.end(); it++)
        {
          if (to_add.find(*it) != to_add.end())
            continue;
          released_instances.erase(*it);
          if (expr_refs_to_remove != NULL)
          {
            std::map<IndexSpaceExpression*,unsigned>::iterator finder =
              expr_refs_to_remove->find(*it);
            if (finder == expr_refs_to_remove->end())
              (*expr_refs_to_remove)[*it] = 1;
            else
              finder->second += 1;
          }
          else if ((*it)->remove_nested_expression_reference(did))
            delete (*it);
        }
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::issue_across_copies(CopyAcrossAnalysis &analysis,
                                             const FieldMask &src_mask, 
                                             IndexSpaceExpression *expr,
                                             const bool expr_covers,
                                             std::set<RtEvent> &deferral_events,
                                             std::set<RtEvent> &applied_events,
                                             const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // Already holding the eq_lock from EquivalenceSet::analyze method
      // While you might think this could a read-only lock since
      // we're just reading meta-data, that's not quite right because
      // we need exclusive access to data structures in check_for_migration
      // We need to lock the analysis at this point
      AutoLock a_lock(analysis);
      check_for_uninitialized_data(analysis, expr, expr_covers, 
                                   src_mask, applied_events); 
      // See if there are any other predicate guard fields that we need
      // to have as preconditions before applying our owner updates
      if (!read_only_guards.empty() && 
          !(src_mask * read_only_guards.get_valid_mask()))
      {
        for (FieldMaskSet<CopyFillGuard>::iterator it = 
              read_only_guards.begin(); it != read_only_guards.end(); it++)
        {
          if (src_mask * it->second)
            continue;
          // No matter what record our dependences on the prior guards
#ifdef NON_AGGRESSIVE_AGGREGATORS
          analysis.guard_events.insert(it->first->effects_applied);
#else
          if (analysis.original_source == local_space)
            analysis.guard_events.insert(it->first->guard_postcondition);
          else
            analysis.guard_events.insert(it->first->effects_applied);
#endif
        }
      }
      // At this point we know we're going to need an aggregator since
      // this is an across copy and we have to be doing updates
      CopyFillAggregator *across_aggregator = analysis.get_across_aggregator();
      if (!analysis.perfect)
      {
        // The general case where fields don't align regardless of
        // whether we are doing a reduction across or not
#ifdef DEBUG_LEGION
        assert(!analysis.src_indexes.empty());
        assert(!analysis.dst_indexes.empty());
        assert(analysis.src_indexes.size() == analysis.dst_indexes.size());
        assert(analysis.across_helpers.size() == 
                analysis.target_instances.size());
#endif
        // First construct a map from dst indexes to src indexes 
        std::map<unsigned,unsigned> dst_to_src;
        for (unsigned idx = 0; idx < analysis.src_indexes.size(); idx++)
          dst_to_src[analysis.dst_indexes[idx]] = analysis.src_indexes[idx];
        // We want to group all the target views with their across helpers
        // so that we can issue them in bulk
        FieldMaskSet<CopyAcrossHelper> target_across;
        // We also need to convert the target views over to source fields
        LegionVector<FieldMaskSet<InstanceView> > converted_target_views(
                                            analysis.target_views.size());
        for (unsigned idx = 0; idx < analysis.target_views.size(); idx++)
        {
          const FieldMask &dst_mask =
            analysis.target_views[idx].get_valid_mask();
          // Compute a tmp mask based on the dst mask
          FieldMask source_mask;
          int fidx = dst_mask.find_first_set();
          while (fidx >= 0)
          {
            std::map<unsigned,unsigned>::const_iterator finder = 
              dst_to_src.find(fidx);
#ifdef DEBUG_LEGION
            assert(finder != dst_to_src.end());
#endif
            source_mask.set_bit(finder->second);
            fidx = dst_mask.find_next_set(fidx+1);
          }
          // This might not be the right equivalence set for all the
          // target instances, so filter down to the ones we apply to
          const FieldMask overlap = src_mask & source_mask;
          if (!overlap)
            continue;
          target_across.insert(analysis.across_helpers[idx], overlap);
          for (FieldMaskSet<InstanceView>::const_iterator it =
                analysis.target_views[idx].begin(); it !=
                analysis.target_views[idx].end(); it++)
          {
            const FieldMask converted = 
              analysis.across_helpers[idx]->convert_dst_to_src(it->second);
            converted_target_views[idx].insert(it->first, converted);
          }
        }
#ifdef DEBUG_LEGION
        assert(!target_across.empty());
#endif
        for (FieldMaskSet<CopyAcrossHelper>::const_iterator it =
              target_across.begin(); it != target_across.end(); it++)
        {
          make_instances_valid(across_aggregator, NULL/*no guard*/,
              &analysis, true/*track events*/, expr,
              expr_covers, it->second, analysis.target_instances, 
              converted_target_views, analysis.source_views, 
              analysis.trace_info, true/*skip check*/,
              analysis.redop, it->first);
          // Only need to check for reductions if we're not reducing since
          // the runtime prevents reductions-across with different reduction ops
          if ((analysis.redop == 0) && !!reduction_fields)
          {
            const FieldMask reduction_mask = reduction_fields & it->second;
            if (!!reduction_mask)
              apply_reductions(analysis.target_instances, converted_target_views,
                  expr, expr_covers, reduction_mask, across_aggregator, 
                  NULL/*no guard*/, &analysis, true/*track events*/, 
                  analysis.trace_info, NULL/*no applied exprs*/, it->first);
          }
        }
      }
      else
      {
        // Fields align when doing this copy across so use the general path
        make_instances_valid(across_aggregator, NULL/*no guard*/,
            &analysis, true/*track events*/, expr, expr_covers, src_mask,
            analysis.target_instances, analysis.target_views,
            analysis.source_views, analysis.trace_info,
            true/*skip check*/, analysis.redop);
        // Only need to check for reductions if we're not reducing since
        // the runtime prevents reductions-across with different reduction ops
        if ((analysis.redop == 0) && !!reduction_fields)
        {
          const FieldMask reduction_mask = src_mask & reduction_fields;
          if (!!reduction_mask)
            apply_reductions(analysis.target_instances, analysis.target_views,
                expr, expr_covers, reduction_mask, across_aggregator, 
                NULL/*no guard*/, &analysis, true/*track events*/,
                analysis.trace_info, NULL/*no need to track applied exprs*/);
        }
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::overwrite_set(OverwriteAnalysis &analysis, 
                                       IndexSpaceExpression *expr, 
                                       const bool expr_covers, 
                                       const FieldMask &overwrite_mask,
                                       std::set<RtEvent> &deferral_events,
                                       std::set<RtEvent> &applied_events,
                                       const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // Already holding the eq_lock from EquivalenceSet::analyze method
      // Now that we're ready to perform the analysis 
      // we need to lock the analysis 
      AutoLock a_lock(analysis);
      if (analysis.output_aggregator != NULL)
        analysis.output_aggregator->clear_update_fields();
      // Two different cases here depending on whether we have a precidate 
      if (analysis.true_guard.exists())
      {
        // This case happens with a predicated fill operation
        // (No other overwrite analyses can be predicated)
        // We have two options of what we can do here:
        // 1. Generate a phi view so we can still use the base instances
        //    just with extra predicated fills done to them
        // 2. We can eagerly issue fills to all the valid instances for
        //    this expression masked by the predicate value
        // Currently we opt for option 2. While phi views still exist
        // in the code base they are currently not being used. They are
        // complicated and add nesting to the names of instance views 
        // that would make them hard to reason about, so we prefer not
        // to use them if possible if we can avoid it. Hence we do the
        // second option and just issue predicated fill operations to 
        // all the total and partially valid instances in the equivalence
        // set that overlap without needing to change the state of the
        // equivalence set.
#ifdef DEBUG_LEGION
        // There should be no restrictions added for predicated fills
        assert(!analysis.add_restriction);
        assert(!analysis.views.empty());
        assert(analysis.reduction_views.empty());
#endif
        for (FieldMaskSet<LogicalView>::const_iterator it =
              analysis.views.begin(); it != analysis.views.end(); it++)
        {
#ifdef DEBUG_LEGION
          assert(it->first->is_fill_view());
#endif
          const FieldMask overlap = it->second & overwrite_mask;
          if (!overlap)
            continue;
          FillView *fill = it->first->as_fill_view();
          predicate_fill_all(fill, overlap, expr, expr_covers,
              analysis.true_guard, analysis.false_guard, &analysis,
              analysis.trace_info, analysis.output_aggregator);
        }
      }
      else
      {
        // In all cases we're going to remove any reductions we've overwriting
        const FieldMask reduce_filter = reduction_fields & overwrite_mask;
        if (!!reduce_filter)
          filter_reduction_instances(expr, expr_covers, reduce_filter);
        if (analysis.add_restriction || 
            !restricted_fields || (restricted_fields * overwrite_mask))
        {
          // Easy case, just filter everything and add the new view
          filter_valid_instances(expr, expr_covers, overwrite_mask);
          if (!analysis.views.empty())
            record_instances(expr, expr_covers, overwrite_mask, analysis.views);
        }
        else
        {
          // We overlap with some restricted fields so we can't filter
          // or update any restricted fields
          const FieldMask restricted_mask = overwrite_mask & restricted_fields;
          filter_unrestricted_instances(expr, expr_covers, restricted_mask);
          const FieldMask non_restricted = overwrite_mask - restricted_mask;
          if (!!non_restricted)
            filter_valid_instances(expr, expr_covers, non_restricted);
          if (!analysis.views.empty())
          {
            record_unrestricted_instances(expr, expr_covers, restricted_mask,
                                          analysis.views);
            copy_out(expr, expr_covers, restricted_mask, analysis.views,
                     &analysis, analysis.trace_info,analysis.output_aggregator);
            if (!!non_restricted)
              record_instances(expr, expr_covers, non_restricted, 
                               analysis.views);
          }
        }
        if (!analysis.reduction_views.empty())
        {
          for (FieldMaskSet<InstanceView>::const_iterator it =
                analysis.reduction_views.begin(); it != 
                analysis.reduction_views.end(); it++)
          {
            int fidx = it->second.find_first_set();
            while (fidx >= 0)
            {
              reduction_instances[fidx].push_back(
                  std::make_pair(it->first, expr)); 
              it->first->add_nested_valid_ref(did);
              expr->add_nested_expression_reference(did);
              fidx = it->second.find_next_set(fidx+1);
            }
          }
          reduction_fields |= analysis.reduction_views.get_valid_mask();
        }
        if (analysis.add_restriction)
        {
#ifdef DEBUG_LEGION
          assert(analysis.views.size() == 1);
          FieldMaskSet<LogicalView>::const_iterator it = analysis.views.begin();
          LogicalView *log_view = it->first;
          assert(log_view->is_instance_view());
          assert(!(overwrite_mask - it->second));
#else
          LogicalView *log_view = analysis.views.begin()->first;
#endif
          InstanceView *inst_view = log_view->as_instance_view();
          record_restriction(expr, expr_covers, overwrite_mask, inst_view);
          if (tracing_postconditions != NULL)
            tracing_postconditions->invalidate_all_but(inst_view, expr,
                                                       overwrite_mask);
        }
      }
      // Record that there is initialized data for this equivalence set
      update_initialized_data(expr, expr_covers, overwrite_mask);
      if (analysis.trace_info.recording)
      {
        if (tracing_postconditions == NULL)
          tracing_postconditions =
            new TraceViewSet(context, did, set_expr, tree_id);
        const RegionUsage usage(LEGION_WRITE_PRIV, LEGION_EXCLUSIVE, 0);
        for (FieldMaskSet<LogicalView>::const_iterator it =
              analysis.views.begin(); it != analysis.views.end(); it++)
          update_tracing_valid_views(it->first, expr, usage,
                                     it->second, true/*invalidates*/);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::predicate_fill_all(FillView *fill_view,
                                            const FieldMask &fill_mask,
                                            IndexSpaceExpression *expr,
                                            const bool expr_covers,
                                            const PredEvent true_guard,
                                            const PredEvent false_guard,
                                            PhysicalAnalysis *analysis,
                                            const PhysicalTraceInfo &trace_info,
                                            CopyFillAggregator *&aggregator)
    //--------------------------------------------------------------------------
    {
      // Importantly make sure we hold on to any references from deferred
      // views and expressions and until after we make the phi views and
      // have added the necessary references to them
      std::map<DeferredView*,unsigned> deferred_refs_to_remove;
      std::map<IndexSpaceExpression*,unsigned> expr_refs_to_remove;
      LegionMap<IndexSpaceExpression*,FieldMaskSet<DeferredView> > phi_views;
      // Do the partial valid views first
      if (!(fill_mask * partial_valid_fields))
      {
        std::vector<DeferredView*> to_delete;
        for (ViewExprMaskSets::iterator vit = partial_valid_instances.begin();
              vit != partial_valid_instances.end(); vit++)
        {
          if (fill_mask * vit->second.get_valid_mask())
            continue;
          if (vit->first->is_deferred_view())
          {
            DeferredView *deferred = vit->first->as_deferred_view();
            FieldMaskSet<IndexSpaceExpression> to_add;
            std::vector<IndexSpaceExpression*> to_remove;
            for (FieldMaskSet<IndexSpaceExpression>::iterator it =
                  vit->second.begin(); it != vit->second.end(); it++)
            {
              const FieldMask overlap = fill_mask & it->second;
              if (!overlap)
                continue;
              if (expr_covers)
              {
                phi_views[it->first].insert(deferred, overlap);
                it.filter(overlap);
                if (!it->second)
                  to_remove.push_back(it->first);
              }
              else
              {
                IndexSpaceExpression *fill_expr = 
                  runtime->forest->intersect_index_spaces(it->first, expr);
                if (fill_expr->is_empty())
                  continue;
                phi_views[fill_expr].insert(deferred, overlap); 
                // remove the fields from this expression no matter what
                it.filter(overlap);
                if (!it->second)
                  to_remove.push_back(it->first);
                if (fill_expr->get_volume() < it->first->get_volume())
                {
                  // if we only had a partial covering then put the
                  // difference back into the partial expressions
                  IndexSpaceExpression *diff_expr = 
                    runtime->forest->subtract_index_spaces(it->first,fill_expr);
                  to_add.insert(diff_expr, overlap);
                }
              }
            }
            if (!to_remove.empty())
            {
              for (std::vector<IndexSpaceExpression*>::const_iterator it =
                    to_remove.begin(); it != to_remove.end(); it++)
              {
                if (!to_add.empty() && (to_add.find(*it) != to_add.end()))
                  continue;
                vit->second.erase(*it);
                std::map<IndexSpaceExpression*,unsigned>::iterator finder =
                  expr_refs_to_remove.find(*it);
                if (finder == expr_refs_to_remove.end())
                  expr_refs_to_remove[*it] = 1;
                else
                  finder->second++;
              }
            }
            if (!to_add.empty())
            {
              for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                    to_add.begin(); it != to_add.end(); it++)
                if (vit->second.insert(it->first, it->second))
                  it->first->add_nested_expression_reference(did);
            }
            if (vit->second.empty())
              to_delete.push_back(deferred);
            else
              vit->second.tighten_valid_mask();
          }
          else
          {
            InstanceView *inst_view = vit->first->as_instance_view();
            // Physical instance so we can just record the predicated fills
            for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                  vit->second.begin(); it != vit->second.end(); it++)
            {
              const FieldMask overlap = fill_mask & it->second;
              if (!overlap)
                continue;
              IndexSpaceExpression *fill_expr = it->first;
              if (!expr_covers)
              {
                fill_expr = 
                  runtime->forest->intersect_index_spaces(it->first, expr);
                if (fill_expr->is_empty())
                  continue;
              }
              if (aggregator == NULL)
                aggregator = new CopyFillAggregator(runtime->forest, analysis,
                        NULL/*no previous guard*/, true/*track*/, true_guard);
              aggregator->record_fill(inst_view, fill_view, overlap, fill_expr,
                                      true_guard, this);
            }
          }
        }
        if (!to_delete.empty())
        {
          for (std::vector<DeferredView*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
          {
            partial_valid_instances.erase(*it);
            std::map<DeferredView*,unsigned>::iterator finder =
              deferred_refs_to_remove.find(*it);
            if (finder == deferred_refs_to_remove.end())
              deferred_refs_to_remove[*it] = 1;
            else
              finder->second++;
          }
        }
        // No need to rebuild the partial valid fields here since we 
        // didn't actually change them, we might have different sets of
        // views when we're done, but the same fields are all still represented
      }
      // Now do the total valid views
      bool need_partial_rebuild = false;
      if (!(fill_mask * total_valid_instances.get_valid_mask()))
      {
        std::vector<DeferredView*> to_delete;
        for (FieldMaskSet<LogicalView>::iterator it = 
              total_valid_instances.begin(); it != 
              total_valid_instances.end(); it++)
        {
          const FieldMask overlap = it->second & fill_mask;
          if (!overlap)
            continue;
          if (it->first->is_deferred_view())
          {
            // Record this in our set of phi views
            DeferredView *deferred = it->first->as_deferred_view();
            if (expr_covers)
              phi_views[set_expr].insert(deferred, overlap);
            else
              phi_views[expr].insert(deferred, overlap);
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(deferred);
            if (!expr_covers)
            {
              // If the predicated fill isn't covering then we need to
              // move this deferred view back to the partial valid views
              IndexSpaceExpression *diff = 
                runtime->forest->subtract_index_spaces(set_expr, expr);
              if (record_partial_valid_instance(it->first, diff, overlap,
                    false/*check total_valid*/))
                need_partial_rebuild = true;
            }
          }
          else
          {
            // Physical instance so we can just record the predicated fill
            if (aggregator == NULL)
              aggregator = new CopyFillAggregator(runtime->forest, analysis,
                      NULL/*no previous guard*/, true/*track*/, true_guard);
            aggregator->record_fill(it->first->as_instance_view(),
                                fill_view, overlap, expr, true_guard, this);
          }
        }
        if (!to_delete.empty())
        {
          for (std::vector<DeferredView*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
          {
            total_valid_instances.erase(*it);
            std::map<DeferredView*,unsigned>::iterator finder =
              deferred_refs_to_remove.find(*it);
            if (finder == deferred_refs_to_remove.end())
              deferred_refs_to_remove[*it] = 1;
            else
              finder->second++;
          }
        }
      }
      if (!phi_views.empty())
      {
        // Create the phi views and record them in the right data structure
        for (LegionMap<IndexSpaceExpression*,
              FieldMaskSet<DeferredView> >::iterator it =
              phi_views.begin(); it != phi_views.end(); it++)
        {
          const FieldMask phi_mask = it->second.get_valid_mask();
          FieldMaskSet<DeferredView> true_view;
          true_view.insert(fill_view, phi_mask);
          PhiView *phi_view = new PhiView(runtime,
              runtime->get_available_distributed_id(),
              true_guard, false_guard, std::move(true_view), 
              std::move(it->second));
          if (it->first == set_expr)
          {
            total_valid_instances.insert(phi_view, phi_mask);
            phi_view->add_nested_valid_ref(did);
          }
          else if (record_partial_valid_instance(phi_view, it->first,
                phi_mask, false/*check total valid*/))
            need_partial_rebuild = true;
        }
      }
      // Now that we've made our phi views and registered them we 
      // can remove the references that were being held
      if (!deferred_refs_to_remove.empty())
      {
        for (std::map<DeferredView*,unsigned>::const_iterator it =
              deferred_refs_to_remove.begin(); it !=
              deferred_refs_to_remove.end(); it++)
          if (it->first->remove_nested_valid_ref(did, it->second))
            delete it->first;
      }
      if (!expr_refs_to_remove.empty())
      {
        for (std::map<IndexSpaceExpression*,unsigned>::const_iterator it =
              expr_refs_to_remove.begin(); it !=
              expr_refs_to_remove.end(); it++)
          if (it->first->remove_nested_expression_reference(did, it->second))
            delete it->first;
      }
      if (need_partial_rebuild)
      {
        partial_valid_fields.clear();
        for (ViewExprMaskSets::const_iterator it =
              partial_valid_instances.begin(); it !=
              partial_valid_instances.end(); it++)
          partial_valid_fields |= it->second.get_valid_mask();
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::filter_set(FilterAnalysis &analysis, 
                                    IndexSpaceExpression *expr, 
                                    const bool expr_covers, 
                                    const FieldMask &filter_mask, 
                                    std::set<RtEvent> &deferral_events,
                                    std::set<RtEvent> &applied_events,
                                    const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // Already holding the eq_lock from EquivalenceSet::analyze method
      // No need to lock the analysis here since we're not going to change it
      if (analysis.filter_views.empty())
      {
        // This is the fast path for when we're just invalidating everything
        invalidate_state(expr, expr_covers, filter_mask, false/*record*/);
        return;
      }
      // We're filtering specific views from this set
      for (FieldMaskSet<InstanceView>::const_iterator fit =
            analysis.filter_views.begin(); fit != 
            analysis.filter_views.end(); fit++)
      {
        const FieldMask overlap = fit->second & filter_mask;
        if (!overlap)
          continue;
        filter_valid_instance(fit->first, expr, expr_covers, overlap);
        // Handle any aliasing with collective views
        if (fit->first->is_collective_view())
        {
          FieldMaskSet<InstanceView> to_filter;
          // Collective view case, need to check against all other views
          if (!(overlap * total_valid_instances.get_valid_mask()))
          {
            for (FieldMaskSet<LogicalView>::const_iterator it =
                  total_valid_instances.begin(); it !=
                  total_valid_instances.end(); it++)
            {
              if (!it->first->is_instance_view())
                continue;
              const FieldMask total_overlap = it->second & overlap;
              if (!total_overlap)
                continue;
              InstanceView *view = it->first->as_instance_view();
              if (!view->aliases(fit->first))
                continue;
              to_filter.insert(view, total_overlap);
            }
          }
          if (!(overlap * partial_valid_fields))
          {
            for (ViewExprMaskSets::const_iterator it =
                  partial_valid_instances.begin(); it !=
                  partial_valid_instances.end(); it++)
            {
              if (!it->first->is_instance_view())
                continue;
              const FieldMask partial_overlap = 
                it->second.get_valid_mask() & overlap;
              if (!partial_overlap)
                continue;
              InstanceView *view = it->first->as_instance_view();
              if (!view->aliases(fit->first))
                continue;
              to_filter.insert(view, partial_overlap);
            }
          }
          for (FieldMaskSet<InstanceView>::const_iterator it =
                to_filter.begin(); it != to_filter.end(); it++)
            filter_valid_instance(it->first, expr, expr_covers, it->second);
        }
        else if (!collective_instances.empty() &&
            !(collective_instances.get_valid_mask() * overlap))
        {
          // Individual view case, just check against the collective views
          for (FieldMaskSet<CollectiveView>::const_iterator cit =
                collective_instances.begin(); cit != 
                collective_instances.end(); cit++)
          {
            const FieldMask collective_overlap = cit->second & overlap;
            if (!collective_overlap)
              continue;
            if (!fit->first->aliases(cit->first))
              continue;
            filter_valid_instance(cit->first, expr, expr_covers,
                                  collective_overlap);
          }
        }
      }
      if (analysis.remove_restriction)
      {
#ifdef DEBUG_LEGION
        assert(!analysis.filter_views.empty());
#endif
        // Note there is no need to check for aliasing of instance views
        // here (e.g. between collective and non-collective) since we
        // should always be releasing the same views that we acquired
        // in restricted mode at this point
        for (FieldMaskSet<InstanceView>::const_iterator fit =
              analysis.filter_views.begin(); fit !=
              analysis.filter_views.end(); fit++)
        {
          const FieldMask inst_overlap = fit->second & filter_mask;
          if (!inst_overlap)
            continue;
          FieldMaskSet<IndexSpaceExpression> to_add;
          std::vector<IndexSpaceExpression*> to_delete;
          for (ExprViewMaskSets::iterator rit = restricted_instances.begin();
                rit != restricted_instances.end(); rit++)
          {
            FieldMaskSet<InstanceView>::iterator finder = 
              rit->second.find(fit->first);
            if (finder == rit->second.end())
              continue;
            const FieldMask overlap = finder->second & inst_overlap;
            if (!overlap)
              continue;
            if (!expr_covers && (rit->first != expr))
            {
              IndexSpaceExpression *expr_overlap = 
                runtime->forest->intersect_index_spaces(rit->first, expr);
              const size_t overlap_size = expr_overlap->get_volume();
              if (overlap_size == 0)
                continue;
              if (overlap_size < rit->first->get_volume())
              {
                // Did not cover all of it so we have to compute the diff
                IndexSpaceExpression *diff_expr = 
                  runtime->forest->subtract_index_spaces(rit->first, expr);
#ifdef DEBUG_LEGION
                assert(diff_expr != NULL);
#endif
                to_add.insert(diff_expr, overlap); 
              }
            }
            // If we get here, then we're definitely removing this 
            // restricted instances from this 
            finder.filter(overlap);
            if (!finder->second)
            {
              if (finder->first->remove_nested_valid_ref(did))
                delete finder->first;
              rit->second.erase(finder);
              if (rit->second.empty())
                to_delete.push_back(rit->first);
            }
          }
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                to_add.begin(); it != to_add.end(); it++)
          {
            ExprViewMaskSets::iterator finder =
              restricted_instances.find(it->first);
            if (finder == restricted_instances.end())
            {
              it->first->add_nested_expression_reference(did);
              fit->first->add_nested_valid_ref(did);
              restricted_instances[it->first].insert(fit->first, 
                                                     it->second);
            }
            else if (finder->second.insert(fit->first, it->second))
              fit->first->add_nested_valid_ref(did);
          }
          for (std::vector<IndexSpaceExpression*>::const_iterator it = 
                to_delete.begin(); it != to_delete.end(); it++)
          {
            ExprViewMaskSets::iterator finder =
              restricted_instances.find(*it);
#ifdef DEBUG_LEGION
            assert(finder != restricted_instances.end());
#endif
            // Check to see if it is still empty since we added things back
            if (!finder->second.empty())
              continue;
            restricted_instances.erase(finder);
            if ((*it)->remove_nested_expression_reference(did))
              delete (*it);
          }
        }
        // Rebuild the restricted fields
        restricted_fields.clear();
        for (ExprViewMaskSets::const_iterator it = restricted_instances.begin();
              it != restricted_instances.end(); it++)
          restricted_fields |= it->second.get_valid_mask();
        // If the data was restricted then we just removed the only
        // valid copy so we need to filter the initialized data
        filter_initialized_data(expr, expr_covers, filter_mask);
      }
      else
      {
        // Check to see if we still have initialized data for what we filtered
        if (!total_valid_instances.empty() || !partial_valid_instances.empty())
        {
          FieldMask to_check =
            filter_mask - total_valid_instances.get_valid_mask();
          if (!!to_check)
          {
            const FieldMask no_partial = to_check - partial_valid_fields;
            if (!!no_partial)
            {
              filter_initialized_data(expr, expr_covers, no_partial);
              to_check -= no_partial;
            }
            if (!!to_check)
            {
              FieldMaskSet<IndexSpaceExpression> to_filter;
              to_filter.insert(expr, to_check);
              for (ViewExprMaskSets::const_iterator pit =
                    partial_valid_instances.begin(); pit !=
                    partial_valid_instances.end(); pit++)
              {
                if (to_check * pit->second.get_valid_mask())
                  continue;
                LegionMap<std::pair<IndexSpaceExpression*,
                  IndexSpaceExpression*>,FieldMask> filter_sets;
                unique_join_on_field_mask_sets(to_filter, 
                                pit->second, filter_sets);
                for (LegionMap<std::pair<IndexSpaceExpression*,
                      IndexSpaceExpression*>,FieldMask>::const_iterator
                      it = filter_sets.begin(); it != filter_sets.end(); it++)
                {
                  IndexSpaceExpression *diff = 
                    runtime->forest->subtract_index_spaces(
                        it->first.first, it->first.second);
                  if (diff->get_volume() == it->first.first->get_volume())
                    continue;
                  FieldMaskSet<IndexSpaceExpression>::iterator finder =
                    to_filter.find(it->first.first);
#ifdef DEBUG_LEGION
                  assert(finder != to_filter.end());
#endif
                  finder.filter(it->second);
                  if (!finder->second)
                    to_filter.erase(finder);
                  if (!diff->is_empty())
                    to_filter.insert(diff, it->second);
                  else
                    to_check -= it->second;
                }
                if (to_filter.empty())
                  break;
              }
              if (!to_filter.empty())
              {
                for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                      to_filter.begin(); it != to_filter.end(); it++)
                {
                  const bool covers =
                    (it->first->get_volume() == set_expr->get_volume());
                  filter_initialized_data(it->first, covers, it->second);
                }
              }
            }
          }
        }
        else // everything empty so filter the whole set
          filter_initialized_data(set_expr, true/*covers*/, filter_mask);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::filter_valid_instance(InstanceView *to_filter,
                                               IndexSpaceExpression *expr,
                                               const bool expr_covers,
                                               const FieldMask &filter_mask)
    //--------------------------------------------------------------------------
    {
      ViewExprMaskSets::iterator part_finder =
        partial_valid_instances.find(to_filter);
      if (part_finder != partial_valid_instances.end())
      {
        FieldMask part_overlap = 
          part_finder->second.get_valid_mask() & filter_mask;
        if (!!part_overlap)
        {
          FieldMaskSet<IndexSpaceExpression> to_add;
          std::vector<IndexSpaceExpression*> to_delete;
          for (FieldMaskSet<IndexSpaceExpression>::iterator it = 
               part_finder->second.begin(); it != 
               part_finder->second.end(); it++)
          {
            const FieldMask overlap = it->second & part_overlap;
            if (!overlap)
              continue;
            if (!expr_covers && (it->first != expr))
            {
              IndexSpaceExpression *expr_overlap = 
                runtime->forest->intersect_index_spaces(it->first, expr);
              const size_t expr_size = expr_overlap->get_volume();
              if (expr_size == 0)
                continue;
              if (expr_size < it->first->get_volume())
              {
                IndexSpaceExpression *diff_expr = 
                  runtime->forest->subtract_index_spaces(it->first, 
                                                         expr_overlap);
#ifdef DEBUG_LEGION
                assert(!diff_expr->is_empty());
#endif
                to_add.insert(diff_expr, overlap);
              }
            }
            // cover at least some if it so this expression will be removed
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
            // Field overlaps should only occur once
            part_overlap -= overlap;
            if (!part_overlap)
              break;
          }
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                to_add.begin(); it != to_add.end(); it++)
            if (part_finder->second.insert(it->first, it->second))
              it->first->add_nested_expression_reference(did);
          // Deletions after adds to keep references around
          if (!to_delete.empty())
          {
            for (std::vector<IndexSpaceExpression*>::const_iterator it =
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              FieldMaskSet<IndexSpaceExpression>::iterator finder =
                part_finder->second.find(*it);
#ifdef DEBUG_LEGION
              assert(finder != part_finder->second.end());
#endif
              if (!!finder->second)
                continue;
              part_finder->second.erase(*it);
              if ((*it)->remove_nested_expression_reference(did))
                delete (*it);
            }
          }
          if (part_finder->second.empty())
          {
            if (part_finder->first->remove_nested_valid_ref(did))
              delete part_finder->first;
            partial_valid_instances.erase(part_finder);
          }
          else
            part_finder->second.tighten_valid_mask();
          // Rebuild the partial valid fields
          partial_valid_fields.clear();
          if (!partial_valid_instances.empty())
          {
            for (ViewExprMaskSets::const_iterator it =
                  partial_valid_instances.begin(); it !=
                  partial_valid_instances.end(); it++)
              partial_valid_fields |= it->second.get_valid_mask();
          }
        }
      }
      FieldMaskSet<LogicalView>::iterator total_finder = 
        total_valid_instances.find(to_filter);
      if (total_finder != total_valid_instances.end())
      {
        const FieldMask total_overlap = total_finder->second & filter_mask;
        if (!!total_overlap)
        {
          if (!expr_covers)
          {
            // Compute the difference and store it in the partial valid fields
            IndexSpaceExpression *diff_expr = 
              runtime->forest->subtract_index_spaces(set_expr, expr);
#ifdef DEBUG_LEGION
            assert(!diff_expr->is_empty());
#endif
            if (record_partial_valid_instance(to_filter, diff_expr,
                        total_overlap, false/*check total valid*/))
            {
              // Need to rebuild the partial valid fields
              partial_valid_fields.clear();
              for (ViewExprMaskSets::const_iterator it =
                    partial_valid_instances.begin(); it !=
                    partial_valid_instances.end(); it++)
                partial_valid_fields |= it->second.get_valid_mask();
            }
          }
          total_finder.filter(total_overlap);
          if (!total_finder->second)
          {
            if (total_finder->first->remove_nested_valid_ref(did))
              delete total_finder->first;
            total_valid_instances.erase(total_finder);
          }
          total_valid_instances.tighten_valid_mask();
        }
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::clone_set(CloneAnalysis &analysis, 
                                   IndexSpaceExpression *expr, 
                                   const bool expr_covers, 
                                   const FieldMask &clone_mask, 
                                   std::set<RtEvent> &deferral_events,
                                   std::vector<RtEvent> &applied_events,
                                   const bool already_deferred)
    //--------------------------------------------------------------------------
    {
      // Already holding the eq_lock from EquivalenceSet::analyze method
      for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
            analysis.sources.begin(); it != analysis.sources.end(); it++)
      {
        // Skip cloning to ourselves
        if (it->first == this)
          continue;
        // Check that the fields overlap 
        const FieldMask overlap = clone_mask & it->second;
        if (!overlap)
          continue;
        // Check that the expressions overlap
        IndexSpaceExpression *overlap_expr = 
          runtime->forest->intersect_index_spaces(expr, it->first->set_expr);
        if (overlap_expr->is_empty())
          continue;
        it->first->clone_to_local(this, overlap, overlap_expr, applied_events,
            false/*invalidate overlap*/, true/*forward to owner*/,
            false/*record invalidate*/);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::update_tracing_valid_views(LogicalView *view,
                                                    IndexSpaceExpression *expr,
                                                    const RegionUsage &usage,
                                                    const FieldMask &user_mask,
                                                    const bool invalidates)
    //--------------------------------------------------------------------------
    {
      // No need for the lock here since we should be called from a copy
      // fill aggregator that is being built while already holding the lock
      if (HAS_READ(usage) && !IS_DISCARD(usage))
      {
        FieldMaskSet<IndexSpaceExpression> not_dominated;
        if (view->is_reduction_kind())
        {
          if (tracing_anticonditions != NULL)
            tracing_anticonditions->dominates(view, expr,
                                              user_mask, not_dominated);
          else
            not_dominated.insert(expr, user_mask);
        }
        else
        {
          if (tracing_postconditions != NULL)
            tracing_postconditions->dominates(view, expr,
                                              user_mask, not_dominated);
          else
            not_dominated.insert(expr, user_mask);
        }
        if ((tracing_preconditions == NULL) && !not_dominated.empty())
          tracing_preconditions =
            new TraceViewSet(context, did, set_expr, tree_id);
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
              not_dominated.begin(); it != not_dominated.end(); it++)
          tracing_preconditions->insert(view, it->first, it->second);
        if (view->is_reduction_kind())
        {
          // Invalidate this reduction view since we read it
          if (tracing_postconditions != NULL)
            tracing_postconditions->invalidate(view, expr, user_mask);
          return;
        }
        // Do not record read-only postconditions
        if (IS_READ_ONLY(usage))
          return;
      }
#ifdef DEBUG_LEGION
      assert(HAS_WRITE(usage));
#endif
      if (tracing_postconditions != NULL)
      {
        if (invalidates)
          tracing_postconditions->invalidate_all_but(view, expr, user_mask);
      }
      else
        tracing_postconditions =
          new TraceViewSet(context, did, set_expr, tree_id);
      tracing_postconditions->insert(view, expr, user_mask);
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::update_tracing_anti_views(LogicalView *view,
                                                   IndexSpaceExpression *expr, 
                                                   const FieldMask &mask) 
    //--------------------------------------------------------------------------
    {
      if (tracing_anticonditions == NULL)
        tracing_anticonditions =
          new TraceViewSet(context, did, set_expr, tree_id);
      tracing_anticonditions->insert(view, expr, mask);
    }

    //--------------------------------------------------------------------------
    RtEvent EquivalenceSet::capture_trace_conditions(TraceConditionSet *target,
                        AddressSpaceID target_space, IndexSpaceExpression *expr,
                        const FieldMask &mask, RtUserEvent ready_event)
    //--------------------------------------------------------------------------
    {
      AutoLock eq(eq_lock);    
      // This always needs to be sent to the owner to handle the case where
      // we are figuring out which shard owns each precondition expression
      // We can only deduplicate if they go to the same place
      if (!is_logical_owner())
      {
        if (!ready_event.exists())
          ready_event = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(did);
          rez.serialize(target);
          rez.serialize(target_space);
          expr->pack_expression(rez, logical_owner_space);
          rez.serialize(mask);
          rez.serialize(ready_event);
        }
        runtime->send_equivalence_set_capture_request(logical_owner_space, rez);
        return ready_event;
      }
      // If we get here then we are the ones to do the analysis
      TraceViewSet *previews = NULL;
      TraceViewSet *antiviews = NULL;
      TraceViewSet *postviews = NULL;
      // Compute the views to send back
      if (tracing_preconditions != NULL)
      {
        previews = new TraceViewSet(context, 0/*no owner*/, set_expr, tree_id);
        tracing_preconditions->find_overlaps(*previews, expr, 
                                             (expr == set_expr), mask);
      }
      if (tracing_anticonditions != NULL)
      {
        antiviews = new TraceViewSet(context, 0/*no owner*/, set_expr, tree_id);
        tracing_anticonditions->find_overlaps(*antiviews, expr,
                                             (expr == set_expr), mask);
      }
      if (tracing_postconditions != NULL)
      {
        postviews = new TraceViewSet(context, 0/*no owner*/, set_expr, tree_id);
        tracing_postconditions->find_overlaps(*postviews, expr,
                                             (expr == set_expr), mask);
      }
      // Return the results
      RtEvent result = ready_event;
      if (target_space != local_space)
      {
#ifdef DEBUG_LEGION
        assert(ready_event.exists());
#endif
        // Send back the results to the target node
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(target);
          set_expr->pack_expression(rez, target_space);
          rez.serialize(tree_id);
          if (previews != NULL)
            previews->pack(rez, target_space, true/*pack references*/);
          else
            rez.serialize<size_t>(0);
          if (antiviews != NULL)
            antiviews->pack(rez, target_space, true/*pack references*/);
          else
            rez.serialize<size_t>(0);
          if (postviews != NULL)
            postviews->pack(rez, target_space, true/*pack references*/);
          else
            rez.serialize<size_t>(0);
          rez.serialize(ready_event);
        }
        runtime->send_equivalence_set_capture_response(target_space, rez);
        if (previews != NULL)
          delete previews;
        if (antiviews != NULL)
          delete antiviews;
        if (postviews != NULL)
          delete postviews;
      }
      else
      {
        std::set<RtEvent> ready_events;
        target->receive_capture(previews, antiviews, postviews, ready_events);
        if (!ready_events.empty())
        {
          if (ready_event.exists())
            Runtime::trigger_event(ready_event, 
                Runtime::merge_events(ready_events));
          else
            result = Runtime::merge_events(ready_events);
        }
        else if (ready_event.exists())
          Runtime::trigger_event(ready_event);
      }
      if (tracing_preconditions != NULL)
      {
        tracing_preconditions->invalidate_all_but(NULL, expr, mask);
        if (tracing_preconditions->empty())
        {
          delete tracing_preconditions;
          tracing_preconditions = NULL;
        }
      }
      if (tracing_anticonditions != NULL)
      {
        tracing_anticonditions->invalidate_all_but(NULL, expr, mask);
        if (tracing_anticonditions->empty())
        {
          delete tracing_anticonditions;
          tracing_anticonditions = NULL;
        }
      }
      if (tracing_postconditions != NULL)
      {
        tracing_postconditions->invalidate_all_but(NULL, expr, mask);
        if (tracing_postconditions->empty())
        {
          delete tracing_postconditions;
          tracing_postconditions = NULL;
        }
      }
      return result;
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::remove_read_only_guard(CopyFillGuard *guard)
    //--------------------------------------------------------------------------
    {
      AutoLock eq(eq_lock);
      // If we're no longer the logical owner then it's because we were
      // migrated and there should be no guards so we're done
      if (read_only_guards.empty())
        return;
      // We could get here when we're not the logical owner if we've unpacked
      // ourselves but haven't become the owner yet, in which case we still
      // need to prune ourselves out of the list
      FieldMaskSet<CopyFillGuard>::iterator finder = 
        read_only_guards.find(guard);
      // It's also possible that the equivalence set is migrated away and
      // then migrated back before this guard is removed in which case we
      // won't find it in the update guards and can safely ignore it
      if (finder == read_only_guards.end())
        return;
      const bool should_tighten = !!finder->second;
      read_only_guards.erase(finder);
      if (should_tighten)
        read_only_guards.tighten_valid_mask();
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::remove_reduction_fill_guard(CopyFillGuard *guard)
    //--------------------------------------------------------------------------
    {
      AutoLock eq(eq_lock);
      // If we're no longer the logical owner then it's because we were
      // migrated and there should be no guards so we're done
      if (reduction_fill_guards.empty())
        return;
      // We could get here when we're not the logical owner if we've unpacked
      // ourselves but haven't become the owner yet, in which case we still
      // need to prune ourselves out of the list
      FieldMaskSet<CopyFillGuard>::iterator finder = 
        reduction_fill_guards.find(guard);
      // It's also possible that the equivalence set is migrated away and
      // then migrated back before this guard is removed in which case we
      // won't find it in the update guards and can safely ignore it
      if (finder == reduction_fill_guards.end())
        return;
      const bool should_tighten = !!finder->second;
      reduction_fill_guards.erase(finder);
      if (should_tighten)
        reduction_fill_guards.tighten_valid_mask();
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_replication_request(
                   Deserializer &derez, Runtime *runtime, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      RtEvent ready_event;
      EquivalenceSet *set = 
        runtime->find_or_request_equivalence_set(did, ready_event);
      size_t num_spaces;
      derez.deserialize(num_spaces);
      CollectiveMapping *mapping = NULL;
      if (num_spaces > 0)
      {
        mapping = new CollectiveMapping(derez, num_spaces);
        mapping->add_reference();
      }
      if (ready_event.exists() && !ready_event.has_triggered())
        ready_event.wait();
      set->replicate_logical_owner_space(source, mapping, true/*need lock*/);
      if ((mapping != NULL) && mapping->remove_reference())
        delete mapping;
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_replication_response(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      RtEvent ready_event;
      EquivalenceSet *set = 
        runtime->find_or_request_equivalence_set(did, ready_event);
      AddressSpaceID owner;
      derez.deserialize(owner);

      if (ready_event.exists() && !ready_event.has_triggered())
        ready_event.wait();
      set->process_replication_response(owner);
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_replication_invalidation(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      RtEvent ready_event;
      EquivalenceSet *set = 
        runtime->find_or_request_equivalence_set(did, ready_event);
      RtUserEvent done_event;
      derez.deserialize(done_event);

      std::vector<RtEvent> applied_events;
      if (ready_event.exists() && !ready_event.has_triggered())
        ready_event.wait();
      set->process_replication_invalidation(applied_events);
      if (!applied_events.empty())
        Runtime::trigger_event(done_event,
            Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(done_event);
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::send_equivalence_set(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(is_owner());
      // We should have had a request for this already
      assert(!has_remote_instance(target));
#endif
      // If the target is in the collective mapping then we don't need to
      // bother sending the result since that just means that something
      // requested the equivalence set on a remote node before the equivalence
      // set creation could propagate there yet
      if ((collective_mapping != NULL) && collective_mapping->contains(target))
        return;
      update_remote_instances(target);
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        set_expr->pack_expression(rez, target);
        rez.serialize(tree_id);
        rez.serialize(context->get_replication_id());
        rez.serialize(context->did);
        // There be dragons here!
        // In the case where we first make a new equivalence set on a
        // remote node that is about to be the owner, we can't mark it
        // as the owner until it receives all an unpack_state or 
        // unpack_migration message which provides it valid meta-data
        // Therefore we'll tell it that we're the owner which will 
        // create a cycle in the forwarding graph. This won't matter for
        // unpack_migration as it's going to overwrite the data in the
        // equivalence set anyway, but for upack_state, we'll need to 
        // recognize when to break the cycle. Effectively whenever we
        // send an update to a remote node that we can tell has never
        // been the owner before (and therefore can't have migrated)
        // we know that we should just do the unpack there. This will
        // break the cycle and allow forward progress. Analysis messages
        // may go round and round a few times, but they have lower
        // priority and therefore shouldn't create a livelock.
        AutoLock eq(eq_lock,1,false/*exclusive*/);
        // is_ready tests whether this set expression has been set
        // it might not be in the case of an output region and we
        // don't want to block testing is_empty in that case if it 
        // hasn't been set
        if (set_expr->is_set() && !set_expr->is_empty())
        {
          if (target == logical_owner_space)
            rez.serialize(local_space);
          else
            rez.serialize(logical_owner_space);
        }
        else
          rez.serialize(logical_owner_space);
      }
      runtime->send_equivalence_set_response(target, rez);
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_equivalence_set_request(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      AddressSpaceID source;
      derez.deserialize(source);
      DistributedCollectable *dc = runtime->find_distributed_collectable(did);
#ifdef DEBUG_LEGION
      EquivalenceSet *set = dynamic_cast<EquivalenceSet*>(dc);
      assert(set != NULL);
#else
      EquivalenceSet *set = static_cast<EquivalenceSet*>(dc);
#endif
      set->send_equivalence_set(source);
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_equivalence_set_response(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      IndexSpaceExpression *expr =
        IndexSpaceExpression::unpack_expression(derez, runtime->forest, 
                                        runtime->determine_owner(did));
      RegionTreeID tid;
      derez.deserialize(tid);
      DistributedID repl_id, ctx_did;
      derez.deserialize(repl_id);
      derez.deserialize(ctx_did);
      AddressSpaceID logical_owner;
      derez.deserialize(logical_owner);
      
      InnerContext *context = NULL;
      if (repl_id > 0)
      {
        // See if there is a local shard manager
        ShardManager *manager = 
          runtime->find_shard_manager(repl_id, true/*can fail*/);
        if (manager != NULL)
          context = manager->find_local_context();
      }
      if (context == NULL)
      {
        RtEvent ctx_ready;
        context = runtime->find_or_request_inner_context(ctx_did, ctx_ready);
        if (ctx_ready.exists() && !ctx_ready.has_triggered())
          ctx_ready.wait();
      }
      void *location = runtime->find_or_create_pending_collectable_location<
                                                        EquivalenceSet>(did);
      EquivalenceSet *set = new(location) EquivalenceSet(runtime, did, 
          logical_owner, expr, tid, context,false/*register now*/);
      // Once construction is complete then we do the registration
      set->register_with_runtime();
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::clone_from(const AddressSpaceID target_space,
                                    EquivalenceSet *src, const FieldMask &mask,
                                    IndexSpaceExpression *clone_expr,
                                    const bool forward_to_owner,
                                    const bool record_invalidate,
                                    std::vector<RtEvent> &applied_events,
                                    const bool invalidate_overlap)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(this != src);
      assert(src->tree_id == tree_id);
#endif
      if (target_space == local_space)
      {
        AutoLock eq(eq_lock); 
        src->clone_to_local(this, mask, clone_expr, applied_events,
            invalidate_overlap, forward_to_owner, record_invalidate);
      }
      else
        src->clone_to_remote(did, target_space, set_expr, clone_expr,
             mask, applied_events, invalidate_overlap, 
             forward_to_owner, record_invalidate);
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::update_owner(const AddressSpaceID new_logical_owner)
    //--------------------------------------------------------------------------
    {
      AutoLock eq(eq_lock);
#ifdef DEBUG_LEGION
      // We should never be told that we're the new owner this way
      assert(new_logical_owner != local_space);
#endif
      // If we are the owner then we know this update is stale so ignore it
      if (!is_logical_owner())
        logical_owner_space = new_logical_owner;
    }

    //--------------------------------------------------------------------------
    RtEvent EquivalenceSet::make_owner(AddressSpaceID new_owner, RtEvent pre)
    //--------------------------------------------------------------------------
    {
      if (new_owner != local_space)
      {
        const RtUserEvent done = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(did);
          rez.serialize(pre);
          rez.serialize(done);
        }
        runtime->send_equivalence_set_make_owner(new_owner, rez);
        return done;
      }
      if (pre.exists() && !pre.has_triggered())
      {
        const DeferMakeOwnerArgs args(this);
        return runtime->issue_runtime_meta_task(args, 
            LG_LATENCY_DEFERRED_PRIORITY, pre);
      }
      // If we make it here then we can finally mark ourselves the owner
      AutoLock eq(eq_lock);
      logical_owner_space = local_space;
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_make_owner(const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferMakeOwnerArgs *dargs = (const DeferMakeOwnerArgs*)args;
      dargs->set->make_owner(dargs->set->local_space, RtEvent::NO_RT_EVENT);
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_make_owner(Deserializer &derez,
                                                      Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      RtEvent ready;
      EquivalenceSet *set = runtime->find_or_request_equivalence_set(did,ready);
      RtEvent precondition;
      derez.deserialize(precondition);
      RtUserEvent done;
      derez.deserialize(done);

      if ((ready.exists() && !ready.has_triggered()) ||
          (precondition.exists() && !precondition.has_triggered()))
      {
        const DeferMakeOwnerArgs args(set);
        Runtime::trigger_event(done,
            runtime->issue_runtime_meta_task(args, LG_LATENCY_DEFERRED_PRIORITY,
              Runtime::merge_events(ready, precondition)));
      }
      else
        Runtime::trigger_event(done, 
            set->make_owner(set->local_space, precondition));
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_owner_update(Deserializer &derez,
                                                        Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      RtEvent ready;
      EquivalenceSet *set = runtime->find_or_request_equivalence_set(did,ready);
      AddressSpaceID new_owner;
      derez.deserialize(new_owner);
      RtUserEvent done;
      derez.deserialize(done);
      if (ready.exists() && !ready.has_triggered())
        ready.wait();
      set->update_owner(new_owner);
      Runtime::trigger_event(done);
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_migration(Deserializer &derez,
                                        Runtime *runtime, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      RtEvent ready;
      EquivalenceSet *set = runtime->find_or_request_equivalence_set(did,ready);

      std::vector<RtEvent> ready_events;
      if (ready.exists() && !ready.has_triggered())
        ready.wait();
      set->unpack_state_and_apply(derez, source, false/*forward*/,ready_events);
      // Check to see if we're ready or we need to defer this
      if (!ready_events.empty())
        set->make_owner(runtime->address_space, 
            Runtime::merge_events(ready_events));
      else
        set->make_owner(runtime->address_space);
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::pack_state(Serializer &rez,const AddressSpaceID target,
                    DistributedID target_did, IndexSpaceExpression *target_expr,
                    IndexSpaceExpression *expr, const bool expr_covers, 
                    const FieldMask &mask, const bool pack_guards,
                    const bool pack_invalidates)
    //--------------------------------------------------------------------------
    {
      LegionMap<IndexSpaceExpression*,FieldMaskSet<LogicalView> > valid_updates;
      FieldMaskSet<IndexSpaceExpression> initialized_updates, invalid_updates;
      std::map<unsigned,std::list<
        std::pair<InstanceView*,IndexSpaceExpression*> > > reduction_updates;
      LegionMap<IndexSpaceExpression*,FieldMaskSet<InstanceView> >
        restricted_updates, released_updates;
      FieldMaskSet<CopyFillGuard> read_only_guards, reduction_fill_guards;
      TraceViewSet *precondition_updates = NULL;
      TraceViewSet *anticondition_updates = NULL;
      TraceViewSet *postcondition_updates = NULL;
      find_overlap_updates(expr, expr_covers, mask, pack_invalidates,
                           valid_updates, initialized_updates, 
                           invalid_updates, reduction_updates, 
                           restricted_updates, released_updates,
                           pack_guards ? &read_only_guards : NULL, 
                           pack_guards ? &reduction_fill_guards : NULL, 
                           precondition_updates, anticondition_updates, 
                           postcondition_updates, target_did, target_expr);
      pack_updates(rez, target, valid_updates, initialized_updates,
           invalid_updates, reduction_updates, restricted_updates, 
           released_updates, &read_only_guards, &reduction_fill_guards, 
           precondition_updates, anticondition_updates,
           postcondition_updates, true/*pack refs*/);
      if (precondition_updates != NULL)
        delete precondition_updates;
      if (anticondition_updates != NULL)
        delete anticondition_updates;
      if (postcondition_updates != NULL)
        delete postcondition_updates;
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::pack_updates(Serializer &rez,
              const AddressSpaceID target,
              const LegionMap<IndexSpaceExpression*,
                  FieldMaskSet<LogicalView> > &valid_updates,
              const FieldMaskSet<IndexSpaceExpression> &initialized_updates,
              const FieldMaskSet<IndexSpaceExpression> &invalidated_updates,
              const std::map<unsigned,std::list<std::pair<InstanceView*,
                  IndexSpaceExpression*> > > &reduction_updates,
              const LegionMap<IndexSpaceExpression*,
                  FieldMaskSet<InstanceView> > &restricted_updates,
              const LegionMap<IndexSpaceExpression*,
                  FieldMaskSet<InstanceView> > &released_updates,
              const FieldMaskSet<CopyFillGuard> *read_only_updates,
              const FieldMaskSet<CopyFillGuard> *reduction_fill_updates,
              const TraceViewSet *precondition_updates,
              const TraceViewSet *anticondition_updates,
              const TraceViewSet *postcondition_updates,
              const bool pack_references)
    //--------------------------------------------------------------------------
    {
      rez.serialize<size_t>(valid_updates.size());
      for (LegionMap<IndexSpaceExpression*,
            FieldMaskSet<LogicalView> >::const_iterator vit =
            valid_updates.begin(); vit != valid_updates.end(); vit++)
      {
        vit->first->pack_expression(rez, target);
        rez.serialize<size_t>(vit->second.size());
        for (FieldMaskSet<LogicalView>::const_iterator it =
              vit->second.begin(); it != vit->second.end(); it++)
        {
          rez.serialize(it->first->did);
          rez.serialize(it->second);
          if (pack_references)
            it->first->pack_valid_ref();
        }
      }
      rez.serialize<size_t>(initialized_updates.size());
      for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
            initialized_updates.begin(); it != initialized_updates.end(); it++)
      {
        it->first->pack_expression(rez, target);
        rez.serialize(it->second);
      }
      rez.serialize<size_t>(invalidated_updates.size());
      for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
            invalidated_updates.begin(); it != invalidated_updates.end(); it++)
      {
        it->first->pack_expression(rez, target);
        rez.serialize(it->second);
      }
      rez.serialize<size_t>(reduction_updates.size());
      for (std::map<unsigned,std::list<std::pair<InstanceView*,
            IndexSpaceExpression*> > >::const_iterator rit =
            reduction_updates.begin(); rit != reduction_updates.end(); rit++)
      {
        rez.serialize(rit->first);
        rez.serialize<size_t>(rit->second.size());
        for (std::list<std::pair<InstanceView*,
              IndexSpaceExpression*> >::const_iterator it =
              rit->second.begin(); it != rit->second.end(); it++)
        {
          rez.serialize(it->first->did);
          it->second->pack_expression(rez, target);
          if (pack_references)
            it->first->pack_valid_ref();
        }
      }
      rez.serialize<size_t>(restricted_updates.size());
      for (LegionMap<IndexSpaceExpression*,
            FieldMaskSet<InstanceView> >::const_iterator rit =
            restricted_updates.begin(); rit != 
            restricted_updates.end(); rit++)
      {
        rit->first->pack_expression(rez, target);
        rez.serialize<size_t>(rit->second.size());
        for (FieldMaskSet<InstanceView>::const_iterator it =
              rit->second.begin(); it != rit->second.end(); it++)
        {
          rez.serialize(it->first->did);
          rez.serialize(it->second);
          if (pack_references)
            it->first->pack_valid_ref();
        }
      }
      rez.serialize<size_t>(released_updates.size());
      for (LegionMap<IndexSpaceExpression*,
            FieldMaskSet<InstanceView> >::const_iterator rit =
            released_updates.begin(); rit != released_updates.end(); rit++)
      {
        rit->first->pack_expression(rez, target);
        rez.serialize<size_t>(rit->second.size());
        for (FieldMaskSet<InstanceView>::const_iterator it =
              rit->second.begin(); it != rit->second.end(); it++)
        {
          rez.serialize(it->first->did);
          rez.serialize(it->second);
          if (pack_references)
            it->first->pack_valid_ref();
        }
      }
      if ((read_only_updates != NULL) && !read_only_updates->empty())
      {
        rez.serialize<size_t>(read_only_updates->size());
        for (FieldMaskSet<CopyFillGuard>::const_iterator it =
              read_only_updates->begin(); it != read_only_updates->end(); it++)
        {
          it->first->pack_guard(rez);
          rez.serialize(it->second);
        }
      }
      else
        rez.serialize<size_t>(0);
      if ((reduction_fill_updates != NULL) && !reduction_fill_updates->empty())
      {
        rez.serialize<size_t>(reduction_fill_updates->size());
        for (FieldMaskSet<CopyFillGuard>::const_iterator it =
              reduction_fill_updates->begin(); it != 
              reduction_fill_updates->end(); it++)
        {
          it->first->pack_guard(rez);
          rez.serialize(it->second);
        }
      }
      else
        rez.serialize<size_t>(0);
      if (precondition_updates != NULL)
        precondition_updates->pack(rez, target, pack_references); 
      else
        rez.serialize<size_t>(0);
      if (anticondition_updates != NULL)
        anticondition_updates->pack(rez, target, pack_references); 
      else
        rez.serialize<size_t>(0);
      if (postcondition_updates != NULL)
        postcondition_updates->pack(rez, target, pack_references); 
      else
        rez.serialize<size_t>(0);
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::unpack_state_and_apply(Deserializer &derez,
                       const AddressSpaceID source, const bool forward_to_owner,
                       std::vector<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      LegionMap<IndexSpaceExpression*,FieldMaskSet<LogicalView> > valid_updates;
      FieldMaskSet<IndexSpaceExpression> initialized_updates, invalid_updates;
      std::map<unsigned,std::list<
        std::pair<InstanceView*,IndexSpaceExpression*> > > reduction_updates;
      LegionMap<IndexSpaceExpression*,FieldMaskSet<InstanceView> >
        restricted_updates, released_updates;
      FieldMaskSet<CopyFillGuard> read_only_updates, reduction_fill_updates;
      std::set<RtEvent> ready_events;
      size_t num_valid;
      derez.deserialize(num_valid);
      for (unsigned idx1 = 0; idx1 < num_valid; idx1++)
      {
        IndexSpaceExpression *expr = 
          IndexSpaceExpression::unpack_expression(derez,runtime->forest,source);
        size_t num_views;
        derez.deserialize(num_views);
        FieldMaskSet<LogicalView> &views = valid_updates[expr];
        for (unsigned idx2 = 0; idx2 < num_views; idx2++)
        {
          DistributedID did;
          derez.deserialize(did);
          RtEvent ready;
          LogicalView *view = runtime->find_or_request_logical_view(did, ready);
          if (ready.exists() && !ready.has_triggered())
            ready_events.insert(ready);
          FieldMask mask;
          derez.deserialize(mask);
          views.insert(view, mask);
        }
      }
      size_t num_initialized;
      derez.deserialize(num_initialized);
      for (unsigned idx = 0; idx < num_initialized; idx++)
      {
        IndexSpaceExpression *expr = 
          IndexSpaceExpression::unpack_expression(derez,runtime->forest,source);
        FieldMask mask;
        derez.deserialize(mask);
        initialized_updates.insert(expr, mask);
      }
      size_t num_invalidated;
      derez.deserialize(num_invalidated);
      for (unsigned idx = 0; idx < num_invalidated; idx++)
      {
        IndexSpaceExpression *expr = 
          IndexSpaceExpression::unpack_expression(derez,runtime->forest,source);
        FieldMask mask;
        derez.deserialize(mask);
        invalid_updates.insert(expr, mask);
      }
      size_t num_reductions;
      derez.deserialize(num_reductions);
      for (unsigned idx1 = 0; idx1 < num_reductions; idx1++)
      {
        unsigned fidx;
        derez.deserialize(fidx);
        size_t num_views;
        derez.deserialize(num_views);
        std::list<std::pair<InstanceView*,IndexSpaceExpression*> > 
          &reductions = reduction_updates[fidx];
        for (unsigned idx2 = 0; idx2 < num_views; idx2++)
        {
          DistributedID did;
          derez.deserialize(did);
          RtEvent ready;
          LogicalView *view = runtime->find_or_request_logical_view(did, ready);
          if (ready.exists() && !ready.has_triggered())
            ready_events.insert(ready);
          IndexSpaceExpression *expr = 
            IndexSpaceExpression::unpack_expression(derez,
                                                    runtime->forest, source);
          reductions.push_back(std::pair<InstanceView*,IndexSpaceExpression*>(
                static_cast<InstanceView*>(view), expr));
        }
      }
      size_t num_restrictions;
      derez.deserialize(num_restrictions);
      for (unsigned idx1 = 0; idx1 < num_restrictions; idx1++)
      {
        IndexSpaceExpression *expr = 
          IndexSpaceExpression::unpack_expression(derez,runtime->forest,source);
        size_t num_views;
        derez.deserialize(num_views);
        FieldMaskSet<InstanceView> &restrictions = restricted_updates[expr];
        for (unsigned idx2 = 0; idx2 < num_views; idx2++)
        {
          DistributedID did;
          derez.deserialize(did);
          RtEvent ready;
          LogicalView *view = runtime->find_or_request_logical_view(did, ready);
          if (ready.exists() && !ready.has_triggered())
            ready_events.insert(ready);
          FieldMask mask;
          derez.deserialize(mask);
          restrictions.insert(static_cast<InstanceView*>(view), mask);
        }
      }
      size_t num_releases;
      derez.deserialize(num_releases);
      for (unsigned idx1 = 0; idx1 < num_releases; idx1++)
      {
        IndexSpaceExpression *expr = 
          IndexSpaceExpression::unpack_expression(derez,runtime->forest,source);
        size_t num_views;
        derez.deserialize(num_views);
        FieldMaskSet<InstanceView> &releases = released_updates[expr];
        for (unsigned idx2 = 0; idx2 < num_views; idx2++)
        {
          DistributedID did;
          derez.deserialize(did);
          RtEvent ready;
          LogicalView *view = runtime->find_or_request_logical_view(did, ready);
          if (ready.exists() && !ready.has_triggered())
            ready_events.insert(ready);
          FieldMask mask;
          derez.deserialize(mask);
          releases.insert(static_cast<InstanceView*>(view), mask);
        }
      }
      size_t num_read_only_guards;
      derez.deserialize(num_read_only_guards);
      if (num_read_only_guards)
      {
        // Need to hold the lock here to prevent copy fill guard
        // deletions from removing this before we've registered it
        AutoLock eq(eq_lock);
        for (unsigned idx = 0; idx < num_read_only_guards; idx++)
        {
          CopyFillGuard *guard = 
            CopyFillGuard::unpack_guard(derez, runtime, this);
          FieldMask guard_mask;
          derez.deserialize(guard_mask);
          if (guard != NULL)
          {
            read_only_guards.insert(guard, guard_mask);
            read_only_updates.insert(guard, guard_mask);
          }
        }
      }
      size_t num_reduction_fill_guards;
      derez.deserialize(num_reduction_fill_guards);
      if (num_reduction_fill_guards)
      {
        // Need to hold the lock here to prevent copy fill guard
        // deletions from removing this before we've registered it
        AutoLock eq(eq_lock);
        for (unsigned idx = 0; idx < num_reduction_fill_guards; idx++)
        {
          CopyFillGuard *guard = 
            CopyFillGuard::unpack_guard(derez, runtime, this);
          FieldMask guard_mask;
          derez.deserialize(guard_mask);
          if (guard != NULL)
          {
            reduction_fill_guards.insert(guard, guard_mask);
            reduction_fill_updates.insert(guard, guard_mask);
          }
        }
      }
      size_t num_preconditions;
      derez.deserialize(num_preconditions);
      TraceViewSet *precondition_updates = NULL;
      if (num_preconditions > 0)
      {
        precondition_updates =
          new TraceViewSet(context, did, set_expr, tree_id);
        precondition_updates->unpack(derez, num_preconditions, 
                                     source, ready_events);
      }
      size_t num_anticonditions;
      derez.deserialize(num_anticonditions);
      TraceViewSet *anticondition_updates = NULL;
      if (num_anticonditions > 0)
      {
        anticondition_updates =
          new TraceViewSet(context, did, set_expr, tree_id);
        anticondition_updates->unpack(derez, num_anticonditions, 
                                     source, ready_events);
      }
      size_t num_postconditions;
      derez.deserialize(num_postconditions);
      TraceViewSet *postcondition_updates = NULL;
      if (num_postconditions > 0)
      {
        postcondition_updates =
          new TraceViewSet(context, did, set_expr, tree_id);
        postcondition_updates->unpack(derez, num_postconditions, 
                                     source, ready_events);
      }
      if (!ready_events.empty())
      {
        const RtEvent ready_event = Runtime::merge_events(ready_events);
        if (ready_event.exists() && !ready_event.has_triggered())
        {
          // Defer this until it is ready to be performed
          DeferApplyStateArgs args(this, forward_to_owner, applied_events, 
              valid_updates, initialized_updates, invalid_updates, 
              reduction_updates, restricted_updates, released_updates,
              read_only_updates, reduction_fill_updates, precondition_updates,
              anticondition_updates, postcondition_updates);
          runtime->issue_runtime_meta_task(args, 
              LG_LATENCY_DEFERRED_PRIORITY, ready_event);
          return;
        }
      }
      // All the views are ready so we can add them now
      apply_state(valid_updates, initialized_updates, invalid_updates,
                  reduction_updates, restricted_updates, released_updates,
                  precondition_updates, anticondition_updates,
                  postcondition_updates, &read_only_updates, 
                  &reduction_fill_updates, applied_events, true/*need lock*/,
                  forward_to_owner, true/*unpack references*/);
    }

    //--------------------------------------------------------------------------
    EquivalenceSet::DeferApplyStateArgs::DeferApplyStateArgs(EquivalenceSet *s,
                                       bool forward,
                                       std::vector<RtEvent> &applied_events,
                                       ExprLogicalViews &valid,
                                       FieldMaskSet<IndexSpaceExpression> &init,
                                       FieldMaskSet<IndexSpaceExpression> &invd,
                                       ExprReductionViews &reductions,
                                       ExprInstanceViews &restricted,
                                       ExprInstanceViews &released,
                                       FieldMaskSet<CopyFillGuard> &read_only,
                                       FieldMaskSet<CopyFillGuard> &reduc_fill,
                                       TraceViewSet *preconditions,
                                       TraceViewSet *anticonditions,
                                       TraceViewSet *postconditions)
      : LgTaskArgs<DeferApplyStateArgs>(implicit_provenance), set(s),
        valid_updates(new LegionMap<IndexSpaceExpression*,
            FieldMaskSet<LogicalView> >()),
        initialized_updates(new FieldMaskSet<IndexSpaceExpression>()),
        invalidated_updates(new FieldMaskSet<IndexSpaceExpression>()),
        reduction_updates(new std::map<unsigned,std::list<std::pair<
            InstanceView*,IndexSpaceExpression*> > >()),
        restricted_updates(new LegionMap<IndexSpaceExpression*,
            FieldMaskSet<InstanceView> >()),
        released_updates(new LegionMap<IndexSpaceExpression*,
            FieldMaskSet<InstanceView> >()), 
        read_only_updates(new FieldMaskSet<CopyFillGuard>()),
        reduction_fill_updates(new FieldMaskSet<CopyFillGuard>()),
        precondition_updates(preconditions),
        anticondition_updates(anticonditions),
        postcondition_updates(postconditions),
        done_event(Runtime::create_rt_user_event()), forward_to_owner(forward)
    //--------------------------------------------------------------------------
    {
      for (ExprLogicalViews::const_iterator it =
            valid.begin(); it != valid.end(); it++)
        it->first->add_base_expression_reference(META_TASK_REF);
      valid_updates->swap(valid);
      for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
            init.begin(); it != init.end(); it++)
        it->first->add_base_expression_reference(META_TASK_REF);
      initialized_updates->swap(init);
      for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
            invd.begin(); it != invd.end(); it++)
        it->first->add_base_expression_reference(META_TASK_REF);
      invalidated_updates->swap(invd);
      for (ExprReductionViews::const_iterator rit =
            reductions.begin(); rit != reductions.end(); rit++)
        for (std::list<std::pair<InstanceView*,IndexSpaceExpression*> >::
              const_iterator it = rit->second.begin();
              it != rit->second.end(); it++)
          it->second->add_base_expression_reference(META_TASK_REF);
      reduction_updates->swap(reductions);
      for (ExprInstanceViews::const_iterator it =
            restricted.begin(); it != restricted.end(); it++)
        it->first->add_base_expression_reference(META_TASK_REF);
      restricted_updates->swap(restricted);
      for (ExprInstanceViews::const_iterator it =
            released.begin(); it != released.end(); it++)
        it->first->add_base_expression_reference(META_TASK_REF);
      released_updates->swap(released);
      read_only_updates->swap(read_only);
      reduction_fill_updates->swap(reduc_fill);
      applied_events.push_back(done_event);
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::DeferApplyStateArgs::release_references(void) const
    //--------------------------------------------------------------------------
    {
      for (ExprLogicalViews::const_iterator it =
            valid_updates->begin(); it != valid_updates->end(); it++)
        if (it->first->remove_base_expression_reference(META_TASK_REF))
          delete it->first;
      for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
           initialized_updates->begin(); it != initialized_updates->end(); it++)
        if (it->first->remove_base_expression_reference(META_TASK_REF))
          delete it->first;
      for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
           invalidated_updates->begin(); it != invalidated_updates->end(); it++)
        if (it->first->remove_base_expression_reference(META_TASK_REF))
          delete it->first;
      for (ExprReductionViews::const_iterator rit =
            reduction_updates->begin(); rit != reduction_updates->end(); rit++)
        for (std::list<std::pair<InstanceView*,IndexSpaceExpression*> >::
              const_iterator it = rit->second.begin(); 
              it != rit->second.end(); it++)
          if (it->second->remove_base_expression_reference(META_TASK_REF))
            delete it->second;
      for (ExprInstanceViews::const_iterator it =
            restricted_updates->begin(); it != restricted_updates->end(); it++)
        if (it->first->remove_base_expression_reference(META_TASK_REF))
          delete it->first;
      for (ExprInstanceViews::const_iterator it =
            released_updates->begin(); it != released_updates->end(); it++)
        if (it->first->remove_base_expression_reference(META_TASK_REF))
          delete it->first;
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_apply_state(const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferApplyStateArgs *dargs = (const DeferApplyStateArgs*)args;
      std::vector<RtEvent> applied_events;
      dargs->set->apply_state(*(dargs->valid_updates), 
          *(dargs->initialized_updates), *(dargs->invalidated_updates),
          *(dargs->reduction_updates), *(dargs->restricted_updates),
          *(dargs->released_updates), dargs->precondition_updates, 
          dargs->anticondition_updates, dargs->postcondition_updates,
          dargs->read_only_updates, dargs->reduction_fill_updates, 
          applied_events, true/*needs lock*/, 
          dargs->forward_to_owner, true/*unpack refs*/);
      if (!applied_events.empty())
        Runtime::trigger_event(dargs->done_event, 
            Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(dargs->done_event);
      dargs->release_references();
      delete dargs->valid_updates;
      delete dargs->initialized_updates;
      delete dargs->invalidated_updates;
      delete dargs->reduction_updates;
      delete dargs->restricted_updates;
      delete dargs->released_updates;
      delete dargs->read_only_updates;
      delete dargs->reduction_fill_updates;
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::invalidate_state(IndexSpaceExpression *expr,
       const bool expr_covers, const FieldMask &mask, bool record_invalidations)
    //--------------------------------------------------------------------------
    {
      filter_valid_instances(expr, expr_covers, mask); 
      filter_reduction_instances(expr, expr_covers, mask); 
      filter_initialized_data(expr, expr_covers, mask);
      filter_restricted_instances(expr, expr_covers, mask);
      filter_released_instances(expr, expr_covers, mask);
      if (tracing_preconditions != NULL)
      {
        tracing_preconditions->invalidate_all_but(NULL, expr, mask);
        if (tracing_preconditions->empty())
        {
          delete tracing_preconditions;
          tracing_preconditions = NULL;
        }
      }
      if (tracing_anticonditions != NULL)
      {
        tracing_anticonditions->invalidate_all_but(NULL, expr, mask);
        if (tracing_anticonditions->empty())
        {
          delete tracing_anticonditions;
          tracing_anticonditions = NULL;
        }
      }
      if (tracing_postconditions != NULL)
      {
        tracing_postconditions->invalidate_all_but(NULL, expr, mask);
        if (tracing_postconditions->empty())
        {
          delete tracing_postconditions;
          tracing_postconditions = NULL;
        }
      }
      if (record_invalidations)
      {
        if (!(mask * partial_invalidations.get_valid_mask()))
        {
#ifdef DEBUG_LEGION
          // Should never invalidate things twice
          assert(!expr_covers);
#endif
          FieldMask remaining = mask;
          FieldMaskSet<IndexSpaceExpression> to_add;
          std::vector<IndexSpaceExpression*> to_delete;
          for (FieldMaskSet<IndexSpaceExpression>::iterator it =
                partial_invalidations.begin(); it !=
                partial_invalidations.end(); it++)
          {
            const FieldMask overlap = remaining & it->second;
            if (!overlap)
              continue;
            IndexSpaceExpression *union_expr = 
              runtime->forest->union_index_spaces(it->first, expr);
            const size_t union_volume = union_expr->get_volume();
#ifdef DEBUG_LEGION
            // There shouldn't have been any overlap here
            assert(union_volume == 
                (it->first->get_volume() + expr->get_volume()));
#endif
            if (union_volume == set_expr->get_volume())
              to_add.insert(set_expr, overlap);
            else
              to_add.insert(union_expr, overlap);
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
            remaining -= overlap;
            if (!remaining)
              break;
          }
          for (std::vector<IndexSpaceExpression*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
          {
            partial_invalidations.erase(*it);
            if ((*it)->remove_nested_expression_reference(did))
              delete (*it);
          }
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                to_add.begin(); it != to_add.end(); it++)
            if (partial_invalidations.insert(it->first, it->second))
              it->first->add_nested_expression_reference(did);
          if (!!remaining && partial_invalidations.insert(expr, remaining))
            expr->add_nested_expression_reference(did);
        }
        else if (partial_invalidations.insert(expr, mask))
          expr->add_nested_expression_reference(did);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::clone_to_local(EquivalenceSet *dst, FieldMask mask,
                     IndexSpaceExpression *overlap,
                     std::vector<RtEvent> &applied_events, 
                     const bool invalidate_overlap, const bool forward_to_owner,
                     const bool record_invalidate)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(dst->tree_id == tree_id);
#endif
      // Lock in exclusive mode if we're doing an invalidate
      AutoLock eq(eq_lock, invalidate_overlap ? 0 : 1, invalidate_overlap);
      if (!is_logical_owner())
      {
        const RtUserEvent done_event = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(did);
          rez.serialize(dst->did);
          rez.serialize(local_space);
          dst->set_expr->pack_expression(rez, logical_owner_space);
          overlap->pack_expression(rez, logical_owner_space);
          rez.serialize(mask);
          rez.serialize(done_event);
          rez.serialize<bool>(invalidate_overlap);
          rez.serialize<bool>(forward_to_owner);
          rez.serialize<bool>(record_invalidate);
        }
        runtime->send_equivalence_set_clone_request(logical_owner_space, rez);
        applied_events.push_back(done_event);
        return;
      }
      // If we get here, we're performing the clone locally for these fields
      LegionMap<IndexSpaceExpression*,FieldMaskSet<LogicalView> > valid_updates;
      FieldMaskSet<IndexSpaceExpression> initialized_updates, invalid_updates;
      std::map<unsigned,std::list<
        std::pair<InstanceView*,IndexSpaceExpression*> > > reduction_updates;
      LegionMap<IndexSpaceExpression*,FieldMaskSet<InstanceView> >
        restricted_updates, released_updates;
      TraceViewSet *precondition_updates = NULL;
      TraceViewSet *anticondition_updates = NULL;
      TraceViewSet *postcondition_updates = NULL;
      if (!set_expr->is_empty())
      {
        const size_t overlap_volume = overlap->get_volume();
#ifdef DEBUG_LEGION
        assert(overlap_volume > 0);
#endif
        const bool overlap_covers = (overlap_volume == set_expr->get_volume());
        find_overlap_updates(overlap, overlap_covers, mask, false/*invalids*/,
                             valid_updates, initialized_updates,
                             invalid_updates, reduction_updates, 
                             restricted_updates, released_updates,
                             NULL/*guards*/,NULL/*guards*/,
                             precondition_updates, anticondition_updates,
                             postcondition_updates, dst->did, dst->set_expr);
      }
      else if (dst->set_expr->is_empty())
        find_overlap_updates(set_expr, true/*covers*/, mask, false/*invalids*/,
                             valid_updates, initialized_updates,
                             invalid_updates, reduction_updates, 
                             restricted_updates, released_updates,
                             NULL/*guards*/,NULL/*guards*/,
                             precondition_updates, anticondition_updates,
                             postcondition_updates, dst->did, dst->set_expr);
      // We hold the lock so calling back into the destination is safe
      dst->apply_state(valid_updates, initialized_updates, invalid_updates,
            reduction_updates, restricted_updates, released_updates, 
            precondition_updates, anticondition_updates, postcondition_updates,
            NULL/*guards*/, NULL/*guards*/, applied_events, false/*no lock*/,
            forward_to_owner, false/*unpack references*/);
      if (invalidate_overlap)
      { 
        if (!set_expr->is_empty())
        {
          const bool overlap_covers = 
            (overlap->get_volume() == set_expr->get_volume());
          invalidate_state(overlap, overlap_covers, mask, record_invalidate);
        }
        else
          invalidate_state(set_expr, true/*cover*/, mask, record_invalidate);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::clone_to_remote(DistributedID target, 
                     AddressSpaceID target_space, 
                     IndexSpaceExpression *target_expr, 
                     IndexSpaceExpression *overlap,
                     FieldMask mask, std::vector<RtEvent> &applied_events, 
                     const bool invalidate_overlap, const bool forward_to_owner,
                     const bool record_invalidate)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!overlap->is_empty());
#endif
      const size_t overlap_volume = overlap->get_volume();
      const size_t set_volume = set_expr->get_volume();
      const bool overlap_covers = (overlap_volume == set_volume); 
      if (overlap_covers)
        overlap = set_expr;
      // Lock in exclusive mode if we're doing an invalidate
      AutoLock eq(eq_lock, invalidate_overlap ? 0 : 1, invalidate_overlap);
      if (!is_logical_owner())
      {
        const RtUserEvent done_event = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(did);
          rez.serialize(target);
          rez.serialize(target_space);
          target_expr->pack_expression(rez, logical_owner_space);
          overlap->pack_expression(rez, logical_owner_space);
          rez.serialize(mask);
          rez.serialize(done_event);
          rez.serialize<bool>(invalidate_overlap);
          rez.serialize<bool>(forward_to_owner);
          rez.serialize<bool>(record_invalidate);
        }
        runtime->send_equivalence_set_clone_request(logical_owner_space, rez);
        applied_events.push_back(done_event);
      }
      else
      {
        // If we make it here, then we've got valid data for the all the fields
        const RtUserEvent done_event = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(target);
          rez.serialize(local_space);
          rez.serialize(done_event);
          rez.serialize<bool>(forward_to_owner);
          pack_state(rez, target_space, target, target_expr, overlap,
            overlap_covers, mask, false/*pack guards*/, false/*pack invalids*/);
        }
        runtime->send_equivalence_set_clone_response(target_space, rez);
        if (invalidate_overlap)
        {
          if (!set_expr->is_empty())
            invalidate_state(overlap, overlap_covers, mask, record_invalidate);
          else
            invalidate_state(set_expr, true/*cover*/, mask, record_invalidate);
        }
        applied_events.push_back(done_event);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::find_overlap_updates(
              IndexSpaceExpression *overlap_expr, const bool overlap_covers, 
              const FieldMask &mask, const bool find_invalidates,
              LegionMap<IndexSpaceExpression*,
                  FieldMaskSet<LogicalView> > &valid_updates,
              FieldMaskSet<IndexSpaceExpression> &initialized_updates,
              FieldMaskSet<IndexSpaceExpression> &invalidated_updates,
              std::map<unsigned,std::list<std::pair<InstanceView*,
                  IndexSpaceExpression*> > > &reduction_updates,
              LegionMap<IndexSpaceExpression*,
                  FieldMaskSet<InstanceView> > &restricted_updates,
              LegionMap<IndexSpaceExpression*,
                  FieldMaskSet<InstanceView> > &released_updates,
              FieldMaskSet<CopyFillGuard> *read_only_guard_updates,
              FieldMaskSet<CopyFillGuard> *reduction_fill_guard_updates,
              TraceViewSet *&precondition_updates,
              TraceViewSet *&anticondition_updates,
              TraceViewSet *&postcondition_updates,
              DistributedID target_did, IndexSpaceExpression *target_expr) const
    //--------------------------------------------------------------------------
    {
      // Get updates from the total valid instances
      if (!total_valid_instances.empty() && 
          !(mask * total_valid_instances.get_valid_mask()))
      {
        if (!!(total_valid_instances.get_valid_mask() - mask))
        {
          // Need to filter on fields
          for (FieldMaskSet<LogicalView>::const_iterator it =
                total_valid_instances.begin(); it != 
                total_valid_instances.end(); it++)
          {
            const FieldMask overlap = mask & it->second;
            if (!overlap)
              continue;
            if (overlap_covers)
              valid_updates[set_expr].insert(it->first, overlap);
            else
              valid_updates[overlap_expr].insert(it->first, overlap);
          }
        }
        else
        {
          if (overlap_covers)
            valid_updates[set_expr] = total_valid_instances;
          else
            valid_updates[overlap_expr] = total_valid_instances;
        }
      }
      // Get updates from the partial valid instances
      if (!partial_valid_instances.empty() && !(mask * partial_valid_fields))
      {
        if (!!(partial_valid_fields - mask))
        {
          // Need to filter on fields
          for (ViewExprMaskSets::const_iterator pit =
                partial_valid_instances.begin(); pit !=
                partial_valid_instances.end(); pit++)
          {
            if (pit->second.get_valid_mask() * mask)
              continue;
            for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                  pit->second.begin(); it != pit->second.end(); it++)
            {
              const FieldMask overlap = mask & it->second;
              if (!overlap)
                continue;
              if (!overlap_covers)
              {
                // Check for expression overlap
                IndexSpaceExpression *intersection = 
                  runtime->forest->intersect_index_spaces(overlap_expr, 
                                                          it->first);
                const size_t volume = intersection->get_volume();
                if (volume == 0)
                  continue;
                if (volume == overlap_expr->get_volume())
                  valid_updates[overlap_expr].insert(pit->first, overlap);
                else if (volume == it->first->get_volume())
                  valid_updates[it->first].insert(pit->first, overlap);
                else
                  valid_updates[intersection].insert(pit->first, overlap);
              }
              else
                valid_updates[it->first].insert(pit->first, overlap);
            }
          }
        }
        else
        {
          // No filtering on fields, just check expressions if necessary
          for (ViewExprMaskSets::const_iterator pit =
                partial_valid_instances.begin(); pit !=
                partial_valid_instances.end(); pit++)
          {
            for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                  pit->second.begin(); it != pit->second.end(); it++)
            {
              if (!overlap_covers)
              {
                // Check for expression overlap
                IndexSpaceExpression *intersection = 
                  runtime->forest->intersect_index_spaces(overlap_expr, 
                                                          it->first);
                const size_t volume = intersection->get_volume();
                if (volume == 0)
                  continue;
                if (volume == overlap_expr->get_volume())
                  valid_updates[overlap_expr].insert(pit->first, it->second);
                else if (volume == it->first->get_volume())
                  valid_updates[it->first].insert(pit->first, it->second);
                else
                  valid_updates[intersection].insert(pit->first, it->second);
              }
              else
                valid_updates[it->first].insert(pit->first, it->second);
            }
          }
        }
      }
      // Get updates on the initialized data
      if (!initialized_data.empty() && 
          !(mask * initialized_data.get_valid_mask()))
      {
        if (!overlap_covers || !!(initialized_data.get_valid_mask() - mask))
        {
          for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
                initialized_data.begin(); it != initialized_data.end(); it++)
          {
            const FieldMask overlap = mask & it->second;
            if (!overlap)
              continue;
            if (!overlap_covers)
            {
              IndexSpaceExpression *intersection = 
                runtime->forest->intersect_index_spaces(it->first,overlap_expr);
              const size_t volume = intersection->get_volume();
              if (volume == 0)
                continue;
              if (volume == overlap_expr->get_volume())
                initialized_updates.insert(overlap_expr, overlap);
              else if (volume == it->first->get_volume())
                initialized_updates.insert(it->first, overlap);
              else
                initialized_updates.insert(intersection, overlap);
            }
            else
              initialized_updates.insert(it->first, overlap);
          }
        }
        else
          initialized_updates = initialized_data;
      }
      if (find_invalidates && !partial_invalidations.empty())
      {
#ifdef DEBUG_LEGION
        // The only time we should be packing partial invalidations 
        // should be if we're moving the entire equivalence set
        assert(overlap_covers);
        assert(!(partial_invalidations.get_valid_mask() - mask));
#endif
        invalidated_updates = partial_invalidations;
      }
      // Get updates from the reductions
      if (!reduction_instances.empty() && !(mask * reduction_fields))
      {
        for (std::map<unsigned,std::list<std::pair<InstanceView*,
              IndexSpaceExpression*> > >::const_iterator rit =
              reduction_instances.begin(); rit != 
              reduction_instances.end(); rit++)
        {
          if (!mask.is_set(rit->first))
            continue;
          std::list<std::pair<InstanceView*,IndexSpaceExpression*> > &updates =
            reduction_updates[rit->first];
          updates = rit->second;
          if (!overlap_covers)
          {
            for (std::list<std::pair<InstanceView*,IndexSpaceExpression*> >::
                 iterator it = updates.begin(); it != updates.end();/*nothing*/)
            {
              if (it->second == set_expr)
              {
                it->second = overlap_expr;
                it++;
              }
              else
              {
                IndexSpaceExpression *intersection = 
                  runtime->forest->intersect_index_spaces(it->second, 
                                                          overlap_expr);
                const size_t volume = intersection->get_volume();
                if (volume > 0)
                {
                  if (volume == overlap_expr->get_volume())
                    it->second = overlap_expr;
                  else if (volume < it->second->get_volume())
                    it->second = intersection;
                  it++;
                }
                else
                  it = updates.erase(it);
              }
            }
          }
        }
      }
      // Get updates from the restricted instances
      if (!restricted_instances.empty() && !(mask * restricted_fields))
      {
        for (ExprViewMaskSets::const_iterator rit =
              restricted_instances.begin(); rit != 
              restricted_instances.end(); rit++)
        {
          if (mask * rit->second.get_valid_mask())
            continue;
          IndexSpaceExpression *restricted_overlap = rit->first;
          if (!overlap_covers)
          {
            IndexSpaceExpression *intersection = 
              runtime->forest->intersect_index_spaces(rit->first, overlap_expr);
            const size_t volume = intersection->get_volume();
            if (volume == 0)
              continue;
            if (volume == overlap_expr->get_volume())
              restricted_overlap = overlap_expr;
            else if (volume < rit->first->get_volume())
              restricted_overlap = intersection;
          }
          FieldMaskSet<InstanceView> &updates = 
            restricted_updates[restricted_overlap]; 
          for (FieldMaskSet<InstanceView>::const_iterator it =
                rit->second.begin(); it != rit->second.end(); it++)
          {
            const FieldMask overlap = mask & it->second;
            if (!overlap)
              continue;
            updates.insert(it->first, overlap);
          }
        }
      }
      // Get updates from the released instances
      if (!released_instances.empty())
      {
        for (ExprViewMaskSets::const_iterator rit = released_instances.begin();
              rit != released_instances.end(); rit++)
        {
          if (mask * rit->second.get_valid_mask())
            continue;
          IndexSpaceExpression *released_overlap = rit->first;
          if (!overlap_covers)
          {
            IndexSpaceExpression *intersection = 
              runtime->forest->intersect_index_spaces(rit->first, overlap_expr);
            const size_t volume = intersection->get_volume();
            if (volume == 0)
              continue;
            if (volume == overlap_expr->get_volume())
              released_overlap = overlap_expr;
            else if (volume < rit->first->get_volume())
              released_overlap = intersection;
          }
          FieldMaskSet<InstanceView> &updates = 
            released_updates[released_overlap]; 
          for (FieldMaskSet<InstanceView>::const_iterator it =
                rit->second.begin(); it != rit->second.end(); it++)
          {
            const FieldMask overlap = mask & it->second;
            if (!overlap)
              continue;
            updates.insert(it->first, overlap);
          }
        }
      }
      // There is something really scary here so be very careful
      // It might look like we have read-only guards even though
      // read_only_guard_updates is NULL. You might think that this
      // is very bad because we should be capturing those guards.
      // This should not be necessary though because the guards are
      // very conservative with these equivalence sets and they span
      // the whole equivalence set, even when the updates we care
      // about here might only be for a subset of the equivalence
      // set. Therefore it should be safe to ignore them in this case.
      if (!read_only_guards.empty() && 
          (read_only_guard_updates != NULL) &&
          !(mask * read_only_guards.get_valid_mask()))
      {
        for (FieldMaskSet<CopyFillGuard>::const_iterator it =
              read_only_guards.begin(); it != read_only_guards.end(); it++)
        {
          const FieldMask overlap = mask & it->second;
          if (!overlap)
            continue;
          read_only_guard_updates->insert(it->first, overlap);
        }
      }
      // See same "scary" comment above because it applies here too
      if (!reduction_fill_guards.empty() &&
          (reduction_fill_guard_updates != NULL) &&
          !(mask * reduction_fill_guards.get_valid_mask()))
      {
        for (FieldMaskSet<CopyFillGuard>::const_iterator it =
              reduction_fill_guards.begin(); it != 
              reduction_fill_guards.end(); it++)
        {
          const FieldMask overlap = mask & it->second;
          if (!overlap)
            continue;
          reduction_fill_guard_updates->insert(it->first, overlap);
        }
      }
      if (tracing_preconditions != NULL)
      {
        if (precondition_updates == NULL)
        {
          precondition_updates = 
            new TraceViewSet(context, target_did, target_expr, tree_id);
          tracing_preconditions->find_overlaps(*precondition_updates,
                                 overlap_expr, overlap_covers, mask);
          if (precondition_updates->empty())
          {
            delete precondition_updates;
            precondition_updates = NULL;
          }
        }
        else
          tracing_preconditions->find_overlaps(*precondition_updates,
                                 overlap_expr, overlap_covers, mask);
      }
      if (tracing_anticonditions != NULL)
      {
        if (anticondition_updates == NULL)
        {
          anticondition_updates =
            new TraceViewSet(context, target_did, target_expr, tree_id);
          tracing_anticonditions->find_overlaps(*anticondition_updates,
                                  overlap_expr, overlap_covers, mask);
          if (anticondition_updates->empty())
          {
            delete anticondition_updates;
            anticondition_updates = NULL;
          }
        }
        else
          tracing_anticonditions->find_overlaps(*anticondition_updates,
                                  overlap_expr, overlap_covers, mask);
      }
      if (tracing_postconditions != NULL)
      {
        if (postcondition_updates == NULL)
        {
          postcondition_updates = 
            new TraceViewSet(context, target_did, target_expr, tree_id);
          tracing_postconditions->find_overlaps(*postcondition_updates,
                                  overlap_expr, overlap_covers, mask);
          if (postcondition_updates->empty())
          {
            delete postcondition_updates;
            postcondition_updates = NULL;
          }
        }
        else
          tracing_postconditions->find_overlaps(*postcondition_updates,
                                  overlap_expr, overlap_covers, mask);
      }
    }

    //--------------------------------------------------------------------------
    void EquivalenceSet::apply_state(LegionMap<IndexSpaceExpression*,
                      FieldMaskSet<LogicalView> > &valid_updates,
                  FieldMaskSet<IndexSpaceExpression> &initialized_updates,
                  FieldMaskSet<IndexSpaceExpression> &invalidated_updates,
                  std::map<unsigned,std::list<std::pair<InstanceView*,
                      IndexSpaceExpression*> > > &reduction_updates,
                  LegionMap<IndexSpaceExpression*,
                      FieldMaskSet<InstanceView> > &restricted_updates,
                  LegionMap<IndexSpaceExpression*,
                      FieldMaskSet<InstanceView> > &released_updates,
                  TraceViewSet *precondition_updates, 
                  TraceViewSet *anticondition_updates,
                  TraceViewSet *postcondition_updates,
                  FieldMaskSet<CopyFillGuard> *read_only_guard_updates,
                  FieldMaskSet<CopyFillGuard> *reduction_fill_guard_updates,
                  std::vector<RtEvent> &applied_events, 
                  const bool needs_lock, const bool forward_to_owner,
                  const bool unpack_references)
    //--------------------------------------------------------------------------
    {
      if (needs_lock)
      {
        AutoLock eq(eq_lock);
        apply_state(valid_updates, initialized_updates, invalidated_updates,
                    reduction_updates, restricted_updates, released_updates,
                    precondition_updates, anticondition_updates, 
                    postcondition_updates, read_only_guard_updates, 
                    reduction_fill_guard_updates, applied_events,
                    false/*needs lock*/, forward_to_owner, unpack_references);
        return;
      }
      if (forward_to_owner && !is_logical_owner())
      {
        const RtUserEvent done_event = Runtime::create_rt_user_event();
        // Filter out any guard updates that have been pruned out
        // while we were not holding the lock. We know they've been
        // pruned because they will no longer be in the update_guards 
        if (read_only_guard_updates != NULL)
        {
          std::vector<CopyFillGuard*> to_delete;
          for (FieldMaskSet<CopyFillGuard>::const_iterator it =
                read_only_guard_updates->begin(); it != 
                read_only_guard_updates->end(); it++)
            if (read_only_guards.find(it->first) == read_only_guards.end())
              to_delete.push_back(it->first);
          if (!to_delete.empty())
          {
            for (std::vector<CopyFillGuard*>::const_iterator it =
                  to_delete.begin(); it != to_delete.end(); it++)
              read_only_guard_updates->erase(*it);
          }
        }
        if (reduction_fill_guard_updates != NULL)
        {
          std::vector<CopyFillGuard*> to_delete;
          for (FieldMaskSet<CopyFillGuard>::const_iterator it =
                reduction_fill_guard_updates->begin(); it != 
                reduction_fill_guard_updates->end(); it++)
            if (reduction_fill_guards.find(it->first) == 
                reduction_fill_guards.end())
              to_delete.push_back(it->first);
          if (!to_delete.empty())
          {
            for (std::vector<CopyFillGuard*>::const_iterator it =
                  to_delete.begin(); it != to_delete.end(); it++)
              reduction_fill_guard_updates->erase(*it);
          }
        }
        // Forward this on to the logical owner
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(did);
          rez.serialize(local_space);
          rez.serialize(done_event);
          rez.serialize<bool>(true); // forward to owner
          pack_updates(rez, logical_owner_space, valid_updates, 
                     initialized_updates, invalidated_updates,
                     reduction_updates, restricted_updates,
                     released_updates, read_only_guard_updates, 
                     reduction_fill_guard_updates, precondition_updates,
                     anticondition_updates, postcondition_updates, 
                     !unpack_references);
        }
        runtime->send_equivalence_set_clone_response(logical_owner_space, rez);
        applied_events.push_back(done_event);
        return;
      }
      const size_t dst_volume = set_expr->get_volume();
      for (LegionMap<IndexSpaceExpression*,
            FieldMaskSet<LogicalView> >::const_iterator it = 
            valid_updates.begin(); it != valid_updates.end(); it++)
      {
        if (it->first->get_volume() == dst_volume)
          record_instances(set_expr, true/*covers*/,
              it->second.get_valid_mask(), it->second);
        else
          record_instances(it->first, false/*covers*/,
              it->second.get_valid_mask(), it->second);
        if (unpack_references)
        {
          for (FieldMaskSet<LogicalView>::const_iterator vit =
                it->second.begin(); vit != it->second.end(); vit++)
            vit->first->unpack_valid_ref();
        }
      }
      for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
            initialized_updates.begin(); it != initialized_updates.end(); it++)
      {
        if (it->first->get_volume() == dst_volume)
          update_initialized_data(set_expr, true/*covers*/, it->second);
        else
          update_initialized_data(it->first,false/*covers*/,it->second);
      }
      if (!invalidated_updates.empty())
      {
#ifdef DEBUG_LEGION
        // The only reaon we are moving partial invalidations is if we're
        // moving the entire equivalence sets so this should be empty and
        // we should just be able to swap it in
        assert(partial_invalidations.empty());
#endif
        partial_invalidations.swap(invalidated_updates);
        for (FieldMaskSet<IndexSpaceExpression>::const_iterator it =
              partial_invalidations.begin(); it != 
              partial_invalidations.end(); it++)
          it->first->add_nested_expression_reference(did);
      }
      for (std::map<unsigned,std::list<
            std::pair<InstanceView*,IndexSpaceExpression*> > >::iterator
            it = reduction_updates.begin(); it != reduction_updates.end(); it++)
      {
        if (unpack_references)
        {
          // Need to make a local copy here since this can destroy the list
          std::vector<InstanceView*> local_reductions;
          local_reductions.reserve(it->second.size());
          for (std::list<std::pair<InstanceView*,
                                   IndexSpaceExpression*> >::const_iterator
                vit = it->second.begin(); vit != it->second.end(); vit++)
            local_reductions.push_back(vit->first);
          // Update the reductions
          update_reductions(it->first, it->second);
          // Then unpack our valid references
          for (std::vector<InstanceView*>::const_iterator it =
                local_reductions.begin(); it != local_reductions.end(); it++)
            (*it)->unpack_valid_ref();
        }
        else
          update_reductions(it->first, it->second);
      }
      for (LegionMap<IndexSpaceExpression*,
            FieldMaskSet<InstanceView> >::const_iterator rit =
            restricted_updates.begin(); rit != restricted_updates.end(); rit++)
      {
        const bool covers = (rit->first->get_volume() == dst_volume);
        for (FieldMaskSet<InstanceView>::const_iterator it =
              rit->second.begin(); it != rit->second.end(); it++)
        {
          record_restriction(covers ? set_expr : rit->first, covers,
                             it->second, it->first);
          if (unpack_references)
            it->first->unpack_valid_ref();
        }
      }
      for (LegionMap<IndexSpaceExpression*,
            FieldMaskSet<InstanceView> >::iterator it =
            released_updates.begin(); it != released_updates.end(); it++)
      {
        update_released(it->first, (it->first->get_volume() == dst_volume),
                        it->second);
        if (unpack_references)
        {
          for (FieldMaskSet<InstanceView>::const_iterator vit =
                it->second.begin(); vit != it->second.end(); vit++)
            vit->first->unpack_valid_ref();
        }
      }
      if (precondition_updates != NULL)
      {
#ifdef DEBUG_LEGION
        assert(precondition_updates->owner_did == did);
        assert(precondition_updates->expression == set_expr);
        assert(precondition_updates->tree_id == tree_id);
#endif
        if (tracing_preconditions == NULL)
        {
          tracing_preconditions = precondition_updates;
          if (unpack_references)
            tracing_preconditions->unpack_references();
        }
        else
        {
          precondition_updates->merge(*tracing_preconditions);
          if (unpack_references)
            precondition_updates->unpack_references();
          delete precondition_updates;
        }
      }
      if (anticondition_updates != NULL)
      {
#ifdef DEBUG_LEGION
        assert(anticondition_updates->owner_did == did);
        assert(anticondition_updates->expression == set_expr);
        assert(anticondition_updates->tree_id == tree_id);
#endif
        if (tracing_anticonditions == NULL)
        {
          tracing_anticonditions = anticondition_updates;
          if (unpack_references)
            tracing_anticonditions->unpack_references();
        }
        else
        {
          anticondition_updates->merge(*tracing_anticonditions);
          if (unpack_references)
            anticondition_updates->unpack_references();
          delete anticondition_updates;
        }
      }
      if (postcondition_updates != NULL)
      {
#ifdef DEBUG_LEGION
        assert(postcondition_updates->owner_did == did);
        assert(postcondition_updates->expression == set_expr);
        assert(postcondition_updates->tree_id == tree_id);
#endif
        if (tracing_postconditions == NULL)
        {
          tracing_postconditions = postcondition_updates;
          if (unpack_references)
            tracing_postconditions->unpack_references();
        }
        else
        {
          postcondition_updates->merge(*tracing_postconditions);
          if (unpack_references)
            postcondition_updates->unpack_references();
          delete postcondition_updates;
        }
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_clone_request(Deserializer &derez,
                                        Runtime *runtime, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      RtEvent ready;
      EquivalenceSet *set = runtime->find_or_request_equivalence_set(did,ready);
      DistributedID target;
      derez.deserialize(target);
      AddressSpaceID target_space;
      derez.deserialize(target_space);
      IndexSpaceExpression *target_expr =
        IndexSpaceExpression::unpack_expression(derez, runtime->forest, source);
      IndexSpaceExpression *overlap =
        IndexSpaceExpression::unpack_expression(derez, runtime->forest, source);
      FieldMask mask;
      derez.deserialize(mask);
      RtUserEvent done_event;
      derez.deserialize(done_event);
      bool invalidate_overlap, forward_to_owner, record_invalidate;
      derez.deserialize<bool>(invalidate_overlap);
      derez.deserialize<bool>(forward_to_owner);
      derez.deserialize<bool>(record_invalidate);
      std::vector<RtEvent> applied_events;   
      if (ready.exists() && !ready.has_triggered())
        ready.wait();
      // Add a reference to make sure we don't race with sending the response
      set->add_base_resource_ref(RUNTIME_REF);
      if (target_space == runtime->address_space)
      {
        // We've been sent back to the owner node
        EquivalenceSet *dst = 
          runtime->find_or_request_equivalence_set(target, ready);
        if (ready.exists() && !ready.has_triggered())
          ready.wait();
        dst->clone_from(target_space, set, mask, overlap, forward_to_owner,
                    record_invalidate, applied_events, invalidate_overlap);
      }
      else
        set->clone_to_remote(target, target_space, target_expr, overlap, mask,
            applied_events, invalidate_overlap, forward_to_owner,
            record_invalidate);
      if (!applied_events.empty())
        Runtime::trigger_event(done_event, 
            Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(done_event);
      if (set->remove_base_resource_ref(RUNTIME_REF))
        delete set;
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_clone_response(Deserializer &derez,
                                                          Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      RtEvent ready;
      EquivalenceSet *set = runtime->find_or_request_equivalence_set(did,ready);
      AddressSpaceID source;
      derez.deserialize(source);
      RtUserEvent done_event;
      derez.deserialize(done_event);
      bool forward_to_owner;
      derez.deserialize(forward_to_owner);

      std::vector<RtEvent> applied_events;
      if (ready.exists() && !ready.has_triggered())
        ready.wait();
      set->unpack_state_and_apply(derez,source,forward_to_owner,applied_events);
      if (!applied_events.empty())
        Runtime::trigger_event(done_event, 
            Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(done_event);
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_capture_request(Deserializer &derez,
                                        Runtime *runtime, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      RtEvent ready;
      EquivalenceSet *set = runtime->find_or_request_equivalence_set(did,ready);
      TraceConditionSet *target;
      derez.deserialize(target);
      AddressSpaceID target_space;
      derez.deserialize(target_space);
      IndexSpaceExpression *expr = 
        IndexSpaceExpression::unpack_expression(derez, runtime->forest, source);
      FieldMask mask;
      derez.deserialize(mask);
      RtUserEvent ready_event;
      derez.deserialize(ready_event);
      if (ready.exists() && !ready.has_triggered())
        ready.wait();
      set->capture_trace_conditions(target,target_space,expr,mask,ready_event);
    }

    //--------------------------------------------------------------------------
    /*static*/ void EquivalenceSet::handle_capture_response(Deserializer &derez,
                                        Runtime *runtime, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      TraceConditionSet *target;
      derez.deserialize(target);
      IndexSpaceExpression *expr =
        IndexSpaceExpression::unpack_expression(derez, runtime->forest, source);
      RegionTreeID tid;
      derez.deserialize(tid);
      TraceViewSet *previews = NULL;
      TraceViewSet *antiviews = NULL;
      TraceViewSet *postviews = NULL;
      size_t num_previews;
      derez.deserialize(num_previews);
      std::set<RtEvent> ready_events;
      if (num_previews > 0)
      {
        previews = new TraceViewSet(target->context, 0/*no owner*/, expr, tid);
        previews->unpack(derez, num_previews, source, ready_events); 
      }
      size_t num_antiviews;
      derez.deserialize(num_antiviews);
      if (num_antiviews > 0)
      {
        antiviews = new TraceViewSet(target->context, 0/*no owner*/, expr, tid);
        antiviews->unpack(derez, num_antiviews, source, ready_events);
      }
      size_t num_postviews;
      derez.deserialize(num_postviews);
      if (num_postviews > 0)
      {
        postviews = new TraceViewSet(target->context, 0/*no owner*/, expr, tid);
        postviews->unpack(derez, num_postviews, source, ready_events);
      }
      RtUserEvent done_event;
      derez.deserialize(done_event);
#ifdef DEBUG_LEGION
      assert(done_event.exists());
#endif
      // Wait for the views to be ready before recording them
      if (!ready_events.empty())
      {
        const RtEvent wait_on = Runtime::merge_events(ready_events);
        ready_events.clear();
        if (wait_on.exists() && !wait_on.has_triggered())
          wait_on.wait();
      }
      target->receive_capture(previews, antiviews, postviews, ready_events); 
      if (previews != NULL)
        previews->unpack_references();
      if (antiviews != NULL)
        antiviews->unpack_references();
      if (postviews != NULL)
        postviews->unpack_references();
      if (!ready_events.empty())
        Runtime::trigger_event(done_event, Runtime::merge_events(ready_events));
      else
        Runtime::trigger_event(done_event);
    }

#if 0
    /////////////////////////////////////////////////////////////
    // Pending Equivalence Set 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PendingEquivalenceSet::PendingEquivalenceSet(RegionNode *node, 
                                                 InnerContext *ctx) 
      : region_node(node), context(ctx), new_set(NULL)
    //--------------------------------------------------------------------------
    {
      region_node->add_base_resource_ref(PENDING_REFINEMENT_REF);
    }

    //--------------------------------------------------------------------------
    PendingEquivalenceSet::~PendingEquivalenceSet(void)
    //--------------------------------------------------------------------------
    {
      if ((new_set != NULL) && 
          new_set->remove_base_gc_ref(PENDING_REFINEMENT_REF))
        delete new_set;
      for (FieldMaskSet<EquivalenceSet>::const_iterator it =
            previous_sets.begin(); it != previous_sets.end(); it++)
        if (it->first->remove_base_gc_ref(PENDING_REFINEMENT_REF))
          delete it->first;
      if (region_node->remove_base_resource_ref(PENDING_REFINEMENT_REF))
        delete region_node;
    }

    //--------------------------------------------------------------------------
    void PendingEquivalenceSet::record_previous(EquivalenceSet *set,
                                                const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      for (FieldMaskSet<EquivalenceSet>::const_iterator it =
            previous_sets.begin(); it != previous_sets.end(); it++)
        assert((set->region_node != it->first->region_node) ||
            (mask * it->second));
      if (previous_sets.insert(set, mask))
        set->add_base_gc_ref(PENDING_REFINEMENT_REF);
    }

    //--------------------------------------------------------------------------
    void PendingEquivalenceSet::record_all(VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(previous_sets.empty());
#endif
      version_info.swap(previous_sets);
      for (FieldMaskSet<EquivalenceSet>::const_iterator it =
            previous_sets.begin(); it != previous_sets.end(); it++)
        it->first->add_base_gc_ref(PENDING_REFINEMENT_REF);
    }

    //--------------------------------------------------------------------------
    EquivalenceSet* PendingEquivalenceSet::compute_refinement(
                                        AddressSpaceID suggested_owner, 
                                        Runtime *runtime,
                                        std::set<RtEvent> &ready_events)
    //--------------------------------------------------------------------------
    {
      if (new_set == NULL)
      {
        new_set = new EquivalenceSet(runtime, 
            runtime->get_available_distributed_id(),
            suggested_owner, region_node, context, true/*register now*/);
        new_set->add_base_gc_ref(PENDING_REFINEMENT_REF);
        std::set<RtEvent> preconditions;
        for (FieldMaskSet<EquivalenceSet>::const_iterator it =
              previous_sets.begin(); it != previous_sets.end(); it++)
          new_set->clone_from(suggested_owner, it->first, it->second, 
                              false/*forward to owner*/, preconditions);
        if (!preconditions.empty())
          clone_event = new_set->make_owner(suggested_owner, 
              Runtime::merge_events(preconditions));
        else
          clone_event = new_set->make_owner(suggested_owner);
      }
      if (clone_event.exists())
      {
        if (!clone_event.has_triggered())
          ready_events.insert(clone_event);
        else
          clone_event = RtEvent::NO_RT_EVENT;
      }
      return new_set;
    }

    //--------------------------------------------------------------------------
    bool PendingEquivalenceSet::finalize(void)
    //--------------------------------------------------------------------------
    {
      if (!clone_event.exists() || clone_event.has_triggered())
      {
        for (FieldMaskSet<EquivalenceSet>::const_iterator it =
              previous_sets.begin(); it != previous_sets.end(); it++)
          if (it->first->remove_base_gc_ref(PENDING_REFINEMENT_REF))
            delete it->first;
        previous_sets.clear();
        // Indicate that we can delete this now
        return true;
      }
      else
      {
        // Launch a meta-task to remove these references and delete
        // the object once we know that the clone event has triggered
        DeferFinalizePendingSetArgs args(this);
        region_node->context->runtime->issue_runtime_meta_task(args,
            LG_LATENCY_DEFERRED_PRIORITY, clone_event);
        // Do no delete this now
        return false;
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void PendingEquivalenceSet::handle_defer_finalize(
                                                               const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferFinalizePendingSetArgs *dargs = 
        (const DeferFinalizePendingSetArgs*)args;
      delete dargs->pending;
    }
#endif

    /////////////////////////////////////////////////////////////
    // Equivalence Set Tracker
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    EqSetTracker::EqSetTracker(LocalLock &lock)
      : tracker_lock(lock), pending_equivalence_sets(NULL),
        equivalence_sets_ready(NULL), waiting_infos(NULL),
        creation_requests(NULL), creation_rectangles(NULL),
        creation_sources(NULL), remaining_responses(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    EqSetTracker::~EqSetTracker(void) 
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(equivalence_sets.empty());
      assert(pending_equivalence_sets == NULL);
      assert(equivalence_sets_ready == NULL);
      assert(waiting_infos == NULL);
      assert(current_subscriptions.empty());
      assert(!pending_invalidations);
      assert(creation_requests == NULL);
      assert(creation_rectangles == NULL);
      assert(creation_sources == NULL);
      assert(remaining_responses == NULL);
#endif
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::record_equivalence_sets(InnerContext *context,
                                const FieldMask &mask,
                                FieldMaskSet<EquivalenceSet> &eq_sets,
                                FieldMaskSet<EqKDTree> &to_create,
                                std::map<EqKDTree*,Domain> &creation_rects,
                                std::map<EquivalenceSet*,
                                  LegionMap<Domain,FieldMask> > &creation_srcs,
                                FieldMaskSet<EqKDTree> &new_subscriptions,
                                unsigned new_references, AddressSpaceID source,
                                unsigned total_responses,
                                std::vector<RtEvent> &ready_events)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(total_responses > 0);
#endif
      LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > create_now;
      LegionMap<Domain,FieldMask> create_now_rectangles;
      std::map<EquivalenceSet*,LegionMap<Domain,FieldMask> > create_now_sources;
      // If we have just one response, we can just move things over
      if ((total_responses == 1) && !to_create.empty())
      {
        for (std::map<EqKDTree*,Domain>::const_iterator it =
              creation_rects.begin(); it != creation_rects.end(); it++)
        {
#ifdef DEBUG_LEGION
          assert(to_create.find(it->first) != to_create.end());
#endif
          LegionMap<Domain,FieldMask>::iterator finder = 
            create_now_rectangles.find(it->second);
          if (finder != create_now_rectangles.end())
            finder->second |= to_create[it->first];
          else
            create_now_rectangles[it->second] = to_create[it->first];
        }
        create_now[source].swap(to_create);
        create_now_sources.swap(creation_srcs);
      }
      {
        AutoLock t_lock(tracker_lock);
#ifdef DEBUG_LEGION
        // A little sanity check, we should never be receiving racy
        // creations for fields that were also being invalidated because
        // mapping dependences should guarantee non-overlapping of these
        // two processes
        if (!!pending_invalidations)
        {
          if (!create_now_rectangles.empty())
          {
            for (LegionMap<Domain,FieldMask>::const_iterator it =
                  create_now_rectangles.begin(); it != 
                  create_now_rectangles.end(); it++)
              assert(pending_invalidations * it->second);
          }
          else
            assert(pending_invalidations * to_create.get_valid_mask());
        }
#endif
        // Record pending equivalence sets
        if (!eq_sets.empty())
        {
          if (pending_equivalence_sets == NULL)
          {
            pending_equivalence_sets = new FieldMaskSet<EquivalenceSet>();
            pending_equivalence_sets->swap(eq_sets);
          }
          else
          {
            for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                  eq_sets.begin(); it != eq_sets.end(); it++)
              pending_equivalence_sets->insert(it->first, it->second);
          }
        }
        if (new_references > 0)
          add_subscription_reference(new_references);
        // Record subscription owners
        if (!new_subscriptions.empty())
        {
          FieldMaskSet<EqKDTree> &subscriptions = current_subscriptions[source];
          if (subscriptions.empty())
            subscriptions.swap(new_subscriptions);
          else
            record_subscriptions(source, new_subscriptions);
        }
        if (!to_create.empty())
          // Even though we're going to make these equivalence sets, we'll
          // still effectively be recorded as subscribers of these nodes
          // once we're done with the creation
          record_subscriptions(source, to_create);
        if (!create_now.empty())
          // Also handle the case where we already swapped to_create into
          // the create_now data structure
          record_subscriptions(source, create_now[source]);
        // Record any creations that we need to perform
        if (total_responses > 1)
        {
          if (!to_create.empty())
            record_creation_sets(to_create,creation_rects,source,creation_srcs);
          // Now see which fields are done
          FieldMask remaining = mask;
          if (remaining_responses != NULL)
          {
            for (LegionMap<unsigned,FieldMask>::iterator it = 
                  remaining_responses->begin(); it != 
                  remaining_responses->end(); /*nothing*/)
            {
              const FieldMask overlap = remaining & it->second;
              if (!overlap)
              {
                it++;
                continue;
              }
              if (it->first > 1) // Update it lower down the entries
                (*remaining_responses)[it->first-1] |= overlap;
              else
                // We've seen the last response for these fields
                // so we can now merge things over to be handled
                extract_creation_sets(overlap, create_now, 
                    create_now_rectangles, create_now_sources);
              it->second -= overlap;
              if (!it->second)
              {
                LegionMap<unsigned,FieldMask>::iterator to_delete = it++;
                remaining_responses->erase(to_delete);
              }
              else
                it++;
              remaining -= overlap;
              if (!remaining)
                break;
            }
            if (remaining_responses->empty())
            {
              delete remaining_responses;
              remaining_responses = NULL;
            }
          }
          if (!!remaining)
          {
            if (remaining_responses == NULL)
              remaining_responses = new LegionMap<unsigned,FieldMask>();
            (*remaining_responses)[total_responses-1] |= remaining;
          }
        }
      }
      // See if we have any equivalence sets for us to create right now
      if (!create_now.empty())
        create_new_equivalence_sets(context, ready_events,
            create_now, create_now_rectangles, create_now_sources);
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::record_output_subscriptions(AddressSpaceID source,
                                      FieldMaskSet<EqKDTree> &new_subscriptions)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!new_subscriptions.empty());
#endif
      AutoLock t_lock(tracker_lock);
      FieldMaskSet<EqKDTree> &subscriptions = current_subscriptions[source];
      if (!subscriptions.empty())
      {
        for (FieldMaskSet<EqKDTree>::const_iterator it =
              new_subscriptions.begin(); it != new_subscriptions.end(); it++)
        {
#ifdef DEBUG_LEGION
          assert((subscriptions.find(it->first) == subscriptions.end()) ||
              (subscriptions.find(it->first)->second * it->second));
#endif
          subscriptions.insert(it->first, it->second);
        }
      }
      else
        subscriptions.swap(new_subscriptions);
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::record_subscriptions(AddressSpaceID source,
                                const FieldMaskSet<EqKDTree> &new_subscriptions)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!new_subscriptions.empty());
#endif
      FieldMaskSet<EqKDTree> &subscriptions = current_subscriptions[source];
      if (!subscriptions.empty())
      {
        for (FieldMaskSet<EqKDTree>::const_iterator it =
              new_subscriptions.begin(); it != new_subscriptions.end(); it++)
        {
#ifdef DEBUG_LEGION
          assert((subscriptions.find(it->first) == subscriptions.end()) ||
              (subscriptions.find(it->first)->second * it->second));
#endif
          subscriptions.insert(it->first, it->second);
        }
      }
      else
        subscriptions = new_subscriptions;
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::record_creation_sets(FieldMaskSet<EqKDTree> &to_create,
        std::map<EqKDTree*,Domain> &creation_rects, AddressSpaceID source,
        std::map<EquivalenceSet*,LegionMap<Domain,FieldMask> > &creation_srcs)
    //--------------------------------------------------------------------------
    {
      // Lock held by caller
      // Save these into the creation requests 
      for (std::map<EqKDTree*,Domain>::const_iterator it =
            creation_rects.begin(); it != creation_rects.end(); it++)
      {
#ifdef DEBUG_LEGION
        assert(to_create.find(it->first) != to_create.end());
#endif
        if (creation_rectangles != NULL)
        {
          LegionMap<Domain,FieldMask>::iterator finder = 
            creation_rectangles->find(it->second);
          if (finder != creation_rectangles->end())
            finder->second |= to_create[it->first];
          else
            (*creation_rectangles)[it->second] = to_create[it->first];
        }
        else
        {
          creation_rectangles = new LegionMap<Domain,FieldMask>();
          (*creation_rectangles)[it->second] = to_create[it->first];
        }
      }
      if (creation_requests == NULL)
        creation_requests = 
          new LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >();
      FieldMaskSet<EqKDTree> &requests = (*creation_requests)[source];
      if (!requests.empty())
      {
        for (FieldMaskSet<EqKDTree>::const_iterator it =
              to_create.begin(); it != to_create.end(); it++)
          requests.insert(it->first, it->second);
      }
      else
        requests.swap(to_create);
      // Save the creation sources
      if (creation_sources != NULL)
      {
        for (std::map<EquivalenceSet*,
                      LegionMap<Domain,FieldMask> >::iterator sit =
              creation_srcs.begin(); sit != creation_srcs.end(); sit++)
        {
          std::map<EquivalenceSet*,LegionMap<Domain,FieldMask> >::iterator
            finder = creation_sources->find(sit->first);
          if (finder != creation_sources->end())
          {
            for (LegionMap<Domain,FieldMask>::const_iterator it =
                  sit->second.begin(); it != sit->second.end(); it++)
              finder->second[it->first] |= it->second;
          }
          else
            (*creation_sources)[sit->first].swap(sit->second);
        }
      }
      else
      {
        creation_sources =
          new std::map<EquivalenceSet*,LegionMap<Domain,FieldMask> >();
        creation_sources->swap(creation_srcs);
      }
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::extract_creation_sets(const FieldMask &mask,
     LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > &create_now,
     LegionMap<Domain,FieldMask> &create_now_rectangles,
     std::map<EquivalenceSet*,LegionMap<Domain,FieldMask> > &create_now_sources)
    //--------------------------------------------------------------------------
    {
      // Lock held by caller
#ifdef DEBUG_LEGION
      assert(equivalence_sets_ready != NULL);
#endif
      const size_t outstanding_requests = equivalence_sets_ready->size();
#ifdef DEBUG_LEGION
      assert(outstanding_requests > 0);
#endif
      if (outstanding_requests > 1)
      {
        // Pull out the entries just for our fields
        if (creation_requests != NULL)
        {
          for (LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::iterator
                cit = creation_requests->begin(); 
                cit != creation_requests->end(); /*nothing*/)
          {
            if (mask * cit->second.get_valid_mask())
              cit++;
            else if (mask == cit->second.get_valid_mask())
            {
              create_now[cit->first].swap(cit->second);
              LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::iterator
                to_delete = cit++;
              creation_requests->erase(to_delete);
            }
            else
            {
              std::vector<EqKDTree*> to_delete;
              for (FieldMaskSet<EqKDTree>::iterator it =
                    cit->second.begin(); it != cit->second.end(); it++)
              {
                const FieldMask overlap = mask & it->second;
                if (!overlap)
                  continue;
                create_now[cit->first].insert(it->first, overlap);
                it.filter(overlap);
                if (!it->second)
                  to_delete.push_back(it->first);
              }
              for (std::vector<EqKDTree*>::const_iterator it =
                    to_delete.begin(); it != to_delete.end(); it++)
                cit->second.erase(*it);
              if (cit->second.empty())
              {
                LegionMap<AddressSpaceID,
                  FieldMaskSet<EqKDTree> >::iterator to_delete = cit++;
                creation_requests->erase(to_delete);
              }
              else
              {
                cit->second.tighten_valid_mask();
                cit++;
              }
            }
          }
          if (creation_requests->empty())
          {
            delete creation_requests;
            creation_requests = NULL;
          }
        }
        if (creation_rectangles != NULL)
        {
          for (LegionMap<Domain,FieldMask>::iterator it =
                creation_rectangles->begin(); it != 
                creation_rectangles->end(); /*nothing*/)
          {
            const FieldMask overlap = mask & it->second;
            if (!overlap)
            {
              it++;
              continue;
            }
            create_now_rectangles[it->first] = overlap;
            it->second -= overlap;
            if (!it->second)
            {
              LegionMap<Domain,FieldMask>::iterator to_delete = it++;
              creation_rectangles->erase(to_delete);
            }
            else
              it++;
          }
          if (creation_rectangles->empty())
          {
            delete creation_rectangles;
            creation_rectangles = NULL;
          }
        }
        if (creation_sources != NULL)
        {
          for (std::map<EquivalenceSet*,
                        LegionMap<Domain,FieldMask> >::iterator sit =
                creation_sources->begin(); sit != 
                creation_sources->end(); /*nothing*/)
          {
            for (LegionMap<Domain,FieldMask>::iterator it =
                  sit->second.begin(); it != 
                  sit->second.end(); /*nothing*/)
            {
              const FieldMask overlap = mask & it->second;
              if (!!overlap)
              {
                create_now_sources[sit->first][it->first] = overlap;
                it->second -= overlap;
                if (!it->second)
                {
                  LegionMap<Domain,FieldMask>::iterator to_delete = it++;
                  sit->second.erase(to_delete);
                }
                else
                  it++;
              }
              else
                it++;
            }
            if (sit->second.empty())
            {
              std::map<EquivalenceSet*,
                LegionMap<Domain,FieldMask> >::iterator to_delete = sit++;
              creation_sources->erase(to_delete);
            }
            else
              sit++;
          }
          if (creation_sources->empty())
          {
            delete creation_sources;
            creation_sources = NULL;
          }
        }
      }
      else
      {
        // If we're just doing one compute call then we
        // know we're going to do this for everything
        if (creation_requests != NULL)
        {
          creation_requests->swap(create_now);
          delete creation_requests;
          creation_requests = NULL;
        }
        if (creation_rectangles != NULL)
        {
          creation_rectangles->swap(create_now_rectangles);
          delete creation_rectangles;
          creation_rectangles = NULL;
        }
        if (creation_sources != NULL)
        {
          creation_sources->swap(create_now_sources);
          delete creation_sources;
          creation_sources = NULL;
        }
      }
    }

    //--------------------------------------------------------------------------
    EqSetTracker::SourceState::~SourceState(void)
    //--------------------------------------------------------------------------
    {
      if ((source_expr != NULL) && 
          source_expr->remove_base_expression_reference(DISJOINT_COMPLETE_REF))
        delete source_expr;
    }

    //--------------------------------------------------------------------------
    IndexSpaceExpression* EqSetTracker::SourceState::get_expression(void) const
    //--------------------------------------------------------------------------
    {
      return source_expr;
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::SourceState::set_expression(IndexSpaceExpression *expr)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(source_expr == NULL);
#endif
      source_expr = expr;
      source_expr->add_base_expression_reference(DISJOINT_COMPLETE_REF);
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::create_new_equivalence_sets(InnerContext *context,
         std::vector<RtEvent> &ready_events,
         LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > &create_now,
         LegionMap<Domain,FieldMask> &create_now_rectangles,
         std::map<EquivalenceSet*,LegionMap<Domain,FieldMask> > &creation_srcs)
    //--------------------------------------------------------------------------
    {
      Runtime *runtime = context->runtime;
      // Compute the rectangle sets for all the source equivalence sets
      // Also track which source equivalence sets have a unique set of fields
      // as we can use those to check for dominating the new set of rectangles
      // so we might be able to skip making a new equivalence set
      FieldMaskSet<EquivalenceSet> unique_sources;
      std::map<EquivalenceSet*,LegionList<SourceState> > set_sources;
      {
        FieldMask multiple_sources;
        for (std::map<EquivalenceSet*,
                      LegionMap<Domain,FieldMask> >::const_iterator
              eit = creation_srcs.begin(); eit != creation_srcs.end(); eit++)
        {
          LegionList<SourceState> &src_rects = set_sources[eit->first];
          compute_field_sets(FieldMask(), eit->second, src_rects);
          FieldMask src_fields;
          for (LegionList<SourceState>::const_iterator it =
                src_rects.begin(); it != src_rects.end(); it++)
            src_fields |= it->set_mask;
          if (!!multiple_sources)
          {
            src_fields -= multiple_sources;
            if (!src_fields)
              continue;
          }
          // Now deduplicate with prior fields
          if (!(src_fields * unique_sources.get_valid_mask()))
          {
            std::vector<EquivalenceSet*> to_delete;
            for (FieldMaskSet<EquivalenceSet>::iterator it =
                  unique_sources.begin(); it != unique_sources.end(); it++)
            {
              const FieldMask overlap = src_fields & it->second;
              if (!overlap)
                continue;
              multiple_sources |= overlap;
              it.filter(overlap);
              if (!it->second)
                to_delete.push_back(it->first);
              src_fields -= overlap;
              if (!src_fields)
                break;
            }
            for (std::vector<EquivalenceSet*>::const_iterator it =
                  to_delete.begin(); it != to_delete.end(); it++)
              unique_sources.erase(*it);
            if (!src_fields)
              continue;
          }
          // We only allow the unique source optimization when the 
          // equivalence set comes from the same context. If it 
          // doesn't then we don't have any way to scope its
          // invalidation set down to just this set of points safely
          if (eit->first->context->get_depth() == context->get_depth())
            unique_sources.insert(eit->first, src_fields);
          else
            multiple_sources |= src_fields;
        }
      }
      // Sort the rectangles into field sets and make an equivlaence set
      // for each of them if their sources are not the same
      LegionList<FieldSet<Domain> > rectangle_sets;
      // In release mode we just leave the unverse mask empty as that tells
      // the field sets routine to just make sets for the represented field
      // In debug mode, we populate the universe mask to make sure it aligns
      // with all the kd-nodes that we're supposed to target
      FieldMask universe_mask;
#ifdef DEBUG_LEGION
      for (LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::const_iterator
            it = create_now.begin(); it != create_now.end(); it++)
        universe_mask |= it->second.get_valid_mask();
#endif
      compute_field_sets(universe_mask, create_now_rectangles, rectangle_sets);
      FieldMaskSet<EquivalenceSet> pending_sets;
      IndexSpaceExpression *tracker_expr = get_tracker_expression();
      for (LegionList<FieldSet<Domain> >::iterator rit =
            rectangle_sets.begin(); rit != rectangle_sets.end(); rit++)
      {
#ifdef DEBUG_LEGION
        assert(!rit->elements.empty());
#endif
        // Check for the case where we have an existing equivalence set
        // that already has all the data that we need in which case there
        // is no point in making a new one and copying the data if we can
        // just reuse the existing one
        if (!(rit->set_mask * unique_sources.get_valid_mask()) &&
            check_for_congruent_source_equivalence_sets(*rit, runtime,
              ready_events, pending_sets, unique_sources,
              create_now, set_sources))
          continue;
        // First we need an expression for this equivalence set
        // If the total volume of all the rectangles is the same as the
        // volume of our index space then we know that its the same as
        // our index space so we can reuse it, otherwise we'll need to
        // create a new index space expression to use for the set
        IndexSpaceExpression *expr =
          tracker_expr->create_from_rectangles(rit->elements);
        // Extract the things that we need to notify about this set
        LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > to_notify;
        if (rectangle_sets.size() == 1)
        {
          to_notify.swap(create_now);
          // But swap our local space back out if there is one
          LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::iterator 
            finder = to_notify.find(runtime->address_space);
          if (finder != to_notify.end())
          {
            create_now[runtime->address_space].swap(finder->second);
            to_notify.erase(finder);
          }
        }
        else
          extract_remote_notifications(rit->set_mask, 
              runtime->address_space, create_now, to_notify);
        // Next compute the CollectiveMapping for the equivalence set
        std::vector<AddressSpaceID> spaces;
        spaces.reserve(to_notify.size());
        for (LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::const_iterator
              it = to_notify.begin(); it != to_notify.end(); it++)
          spaces.push_back(it->first);
        // Make sure we include our local space too
        if (!spaces.empty() && !std::binary_search(spaces.begin(), 
                            spaces.end(), runtime->address_space))
        {
          spaces.push_back(runtime->address_space);
          std::sort(spaces.begin(), spaces.end());
        }
        CollectiveMapping *mapping = NULL;
        if (spaces.size() > 1)
          mapping = 
            new CollectiveMapping(spaces, runtime->legion_collective_radix);
        const DistributedID did = runtime->get_available_distributed_id();
        EquivalenceSet *set = new EquivalenceSet(runtime, did,
            runtime->address_space/*logical owner*/, expr, 
            get_region_tree_id(), context, true/*register*/, mapping);
        pending_sets.insert(set, rit->set_mask);
        // Clone any meta-data from the source equivalence sets to
        // bring this new equivalence set up to date 
        RtEvent ready;
        if (!set_sources.empty())
        {
          ready = initialize_new_equivalence_set(set, rit->set_mask, 
                                                 runtime, set_sources);
          if (ready.exists())
            ready_events.push_back(ready);
        }
        // Notify any equivalence set kd tree nodes about the new set 
        if (mapping != NULL)
        {
          // Broadcast this out to all the spaces and have them notify
          // their EqKDTree objects
          std::vector<AddressSpaceID> children;
          mapping->get_children(runtime->address_space, 
                                runtime->address_space, children);
#ifdef DEBUG_LEGION
          assert(!children.empty());
#endif
          // See if this is an index space node for easy packing
          // otherwise we're going to pack all the rectangles so
          // we can make an expression on the remote nodes
          IndexSpaceNode *node = NULL;
          if (expr == tracker_expr)
            node = dynamic_cast<IndexSpaceNode*>(expr);
          for (std::vector<AddressSpaceID>::const_iterator cit =
                children.begin(); cit != children.end(); cit++)
          {
            const RtUserEvent notified = Runtime::create_rt_user_event();
            Serializer rez;
            {
              RezCheck z(rez);
              rez.serialize(this);
              rez.serialize(did);
              if (node == NULL)
              {
                rez.serialize(IndexSpace::NO_SPACE);
                rez.serialize(rit->elements.size());
                for (std::set<Domain>::const_iterator it =
                      rit->elements.begin(); it != rit->elements.end(); it++)
                  rez.serialize(*it);
              }
              else
                rez.serialize(node->handle);
              rez.serialize(set->tree_id);
              context->pack_task_context(rez);
              mapping->pack(rez);
              rez.serialize(ready);
              rez.serialize<size_t>(to_notify.size());
              for (LegionMap<AddressSpaceID,
                             FieldMaskSet<EqKDTree> >::const_iterator nit =
                    to_notify.begin(); nit != to_notify.end(); nit++)
              {
                rez.serialize(nit->first);
                rez.serialize(nit->second.size());
                for (FieldMaskSet<EqKDTree>::const_iterator it =
                      nit->second.begin(); it != nit->second.end(); it++)
                {
                  rez.serialize(it->first);
                  rez.serialize(it->second);
                }
              }
              rez.serialize(notified);
            }
            runtime->send_equivalence_set_creation(*cit, rez);
            ready_events.push_back(notified);
          }
        }
        // Notify any local EqKDTree objects
        LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::iterator
          finder = create_now.find(runtime->address_space);
        if (finder != create_now.end())
        {
          std::vector<EqKDTree*> to_delete;
          for (FieldMaskSet<EqKDTree>::iterator it =
                finder->second.begin(); it != finder->second.end(); it++)
          {
            const FieldMask overlap = rit->set_mask & it->second;
            if (!overlap)
              continue;
            it->first->record_equivalence_set(set, overlap, ready,
                                    this, runtime->address_space);
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
          }
          for (std::vector<EqKDTree*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
            finder->second.erase(*it);
          if (finder->second.empty())
            create_now.erase(finder);
        }
      }
      // Retake the lock and record these pending equivalence sets
      AutoLock t_lock(tracker_lock); 
      if (pending_equivalence_sets == NULL)
      {
        pending_equivalence_sets = new FieldMaskSet<EquivalenceSet>();
        pending_equivalence_sets->swap(pending_sets);
      }
      else
      {
        for (FieldMaskSet<EquivalenceSet>::const_iterator it =
              pending_sets.begin(); it != pending_sets.end(); it++)
          pending_equivalence_sets->insert(it->first, it->second);
      }
    }

    //--------------------------------------------------------------------------
    bool EqSetTracker::check_for_congruent_source_equivalence_sets(
        FieldSet<Domain> &dest, Runtime *runtime,
        std::vector<RtEvent> &ready_events,
        FieldMaskSet<EquivalenceSet> &pending_sets,
        FieldMaskSet<EquivalenceSet> &unique_sources,
        LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > &create_now,
        std::map<EquivalenceSet*,LegionList<SourceState> > &set_sources)
    //--------------------------------------------------------------------------
    {
      size_t destination_volume = 0;
      for (std::set<Domain>::const_iterator it =
            dest.elements.begin(); it != dest.elements.end(); it++)
        destination_volume += it->get_volume();
      std::vector<EquivalenceSet*> to_remove;
      const AddressSpaceID local_space = runtime->address_space;
      for (FieldMaskSet<EquivalenceSet>::iterator eit =
            unique_sources.begin(); eit != unique_sources.end(); eit++)
      {
        const FieldMask src_mask = dest.set_mask & eit->second;
        if (!src_mask)
          continue; 
        std::map<EquivalenceSet*,LegionList<SourceState> >::iterator
          source_finder = set_sources.find(eit->first);
#ifdef DEBUG_LEGION
        assert(source_finder != set_sources.end());
#endif
        for (LegionList<SourceState>::iterator sit = 
              source_finder->second.begin(); sit !=
              source_finder->second.end(); /*nothing*/)
        {
          const FieldMask overlap = src_mask & sit->set_mask;
          if (!overlap)
          {
            sit++;
            continue;
          }
          if (sit->source_volume == 0)
          {
#ifdef DEBUG_LEGION
            assert(!sit->elements.empty());
#endif
            for (std::set<Domain>::const_iterator it =
                  sit->elements.begin(); it != sit->elements.end(); it++)
              sit->source_volume += it->get_volume();
          }
#ifdef DEBUG_LEGION
          assert(sit->source_volume <= destination_volume);
#endif
          if (sit->source_volume == destination_volume)
          {
            // This equivalence set is already valid for all the points
            // of the overlapping fields
            // Get the set of EqKDTree nodes that we need to notify
            LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > to_notify;
            extract_remote_notifications(overlap, runtime->address_space,
                                         create_now, to_notify);
            std::vector<AddressSpaceID> spaces;
            spaces.reserve(to_notify.size());
            for (LegionMap<AddressSpaceID,
                           FieldMaskSet<EqKDTree> >::const_iterator
                  it = to_notify.begin(); it != to_notify.end(); it++)
              spaces.push_back(it->first);
            if (!spaces.empty())
            {
              // Make sure we include our local space too
              spaces.push_back(local_space);
              std::sort(spaces.begin(), spaces.end());
              CollectiveMapping mapping(spaces, 
                  runtime->legion_collective_radix);
              std::vector<AddressSpaceID> children;
              mapping.get_children(local_space, local_space, children);
              for (std::vector<AddressSpaceID>::const_iterator cit =
                    children.begin(); cit != children.end(); cit++)
              {
                const RtUserEvent notified = Runtime::create_rt_user_event();
                Serializer rez;
                {
                  RezCheck z(rez);
                  // Reference still held by at least one EqKDTree 
                  // for which this is in the previous set
                  rez.serialize(eit->first->did);
                  rez.serialize(this);
                  rez.serialize(local_space);
                  mapping.pack(rez);
                  rez.serialize<size_t>(to_notify.size());
                  for (LegionMap<AddressSpaceID,
                                 FieldMaskSet<EqKDTree> >::const_iterator nit =
                        to_notify.begin(); nit != to_notify.end(); nit++)
                  {
                    if (nit->first == local_space)
                      continue;
                    rez.serialize(nit->first);
                    rez.serialize(nit->second.size());
                    for (FieldMaskSet<EqKDTree>::const_iterator it =
                          nit->second.begin(); it != nit->second.end(); it++)
                    {
#ifdef DEBUG_LEGION
                      assert(!(it->second - overlap));
#endif
                      rez.serialize(it->first);
                      rez.serialize(it->second);
                    }
                  }
                  rez.serialize(notified);
                }
                runtime->send_equivalence_set_reuse(*cit, rez);
                ready_events.push_back(notified);
              }
            }
            // Check to see if there are any local notifications to perform
            LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::iterator 
              local_finder = create_now.find(local_space);
            if ((local_finder != create_now.end()) && 
                !(overlap * local_finder->second.get_valid_mask()))
            {
              std::vector<EqKDTree*> to_delete;
              for (FieldMaskSet<EqKDTree>::iterator it = 
                    local_finder->second.begin(); it != 
                    local_finder->second.end(); it++)
              {
                const FieldMask local_overlap = overlap & it->second;
                if (!local_overlap)
                  continue;
                it->first->record_equivalence_set(eit->first, local_overlap,
                    RtEvent::NO_RT_EVENT, this, local_space);
                it.filter(local_overlap);
                if (!it->second)
                  to_delete.push_back(it->first);
              }
              if (to_delete.size() < local_finder->second.size())
              {
                for (std::vector<EqKDTree*>::const_iterator it =
                      to_delete.begin(); it != to_delete.end(); it++)
                  local_finder->second.erase(*it);
                local_finder->second.tighten_valid_mask();
              }
              else if (to_delete.size() == local_finder->second.size())
                create_now.erase(local_finder);
            }
            pending_sets.insert(eit->first, overlap);
            dest.set_mask -= overlap;
            sit->set_mask -= overlap;
            if (!sit->set_mask)
            {
              LegionList<SourceState>::iterator to_delete = sit++;
              source_finder->second.erase(to_delete);
            }
            else
              sit++;
          }
          else
            sit++;
        }
        if (source_finder->second.empty())
          set_sources.erase(source_finder);
        eit.filter(src_mask);
        if (!eit->second)
          to_remove.push_back(eit->first);
      }
      for (std::vector<EquivalenceSet*>::const_iterator it =
            to_remove.begin(); it != to_remove.end(); it++)
        unique_sources.erase(*it);
      return !dest.set_mask;
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::extract_remote_notifications(const FieldMask &mask,
          AddressSpaceID local_space,
          LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > &create_now,
          LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > &to_notify)
    //--------------------------------------------------------------------------
    {
      for (LegionMap<AddressSpaceID,
                     FieldMaskSet<EqKDTree> >::iterator cit =
            create_now.begin(); cit != create_now.end(); /*nothing*/)
      {
        if (cit->first == local_space)
        {
          cit++;
          continue;
        }
        if (mask * cit->second.get_valid_mask())
        {
          cit++;
          continue;
        }
        FieldMaskSet<EqKDTree> &notify = to_notify[cit->first];
        if (mask != cit->second.get_valid_mask())
        {
          std::vector<EqKDTree*> to_delete;
          for (FieldMaskSet<EqKDTree>::iterator it =
                cit->second.begin(); it != cit->second.end(); it++)
          {
            const FieldMask overlap = mask & it->second;
            if (!overlap)
              continue;
            notify.insert(it->first, overlap);
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
          }
          for (std::vector<EqKDTree*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
            cit->second.erase(*it);
        }
        else
          notify.swap(cit->second);
        if (cit->second.empty())
        {
          LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::iterator
            to_delete = cit++;
          create_now.erase(to_delete);
        }
        else
          cit++;
      }
    }

    //--------------------------------------------------------------------------
    RtEvent EqSetTracker::initialize_new_equivalence_set(EquivalenceSet *target,
           const FieldMask &mask, Runtime *runtime, 
           std::map<EquivalenceSet*,LegionList<SourceState> > &set_sources)
    //--------------------------------------------------------------------------
    {
      std::vector<RtEvent> ready_events;
      for (std::map<EquivalenceSet*,LegionList<SourceState> >::iterator eit =
            set_sources.begin(); eit != set_sources.end(); /*nothing*/)
      {
        for (LegionList<SourceState>::iterator sit =
              eit->second.begin(); sit != eit->second.end(); /*nothing*/)
        {
          const FieldMask overlap = mask & sit->set_mask;
          if (!overlap)
          {
            sit++;
            continue;
          }
          IndexSpaceExpression *expression = sit->get_expression();
          if (expression == NULL)
          {
            IndexSpaceExpression *intersection = 
              runtime->forest->intersect_index_spaces(
                  eit->first->set_expr, target->set_expr);
            if (intersection->get_volume() == target->set_expr->get_volume())
              intersection = target->set_expr;
            else if (intersection->get_volume() == 
                eit->first->set_expr->get_volume())
              intersection = eit->first->set_expr;
            expression = intersection->create_from_rectangles(sit->elements);
            sit->set_expression(expression);
          }
          // We only record the partial invalidation if we're cloning between
          // two equivalence sets in the same context. Note that because of
          // control replication you can't just check that both sets are from
          // the same equivalence set, but what you can do is check to see if
          // they are at the same depth in the task tree
          const bool record_invalidate = 
            (target->context->get_depth() == eit->first->context->get_depth());
          target->clone_from(runtime->address_space, eit->first, overlap,
              expression, false/*forward to owner*/, record_invalidate,
              ready_events, true/*invalidate overlap*/);
          sit->set_mask -= overlap;
          if (!sit->set_mask)
          {
            LegionList<SourceState>::iterator to_delete = sit++;
            eit->second.erase(to_delete);
          }
          else
            sit++;
        }
        if (eit->second.empty())
        {
          std::map<EquivalenceSet*,LegionList<SourceState> >::iterator
            to_delete = eit++;
          set_sources.erase(to_delete);
        }
        else
          eit++;
      }
      if (ready_events.empty())
        return RtEvent::NO_RT_EVENT;
      else
        return Runtime::merge_events(ready_events);
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::record_pending_equivalence_set(EquivalenceSet *set,
                                                      const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      AutoLock t_lock(tracker_lock);
      if (pending_equivalence_sets == NULL)
        pending_equivalence_sets = new FieldMaskSet<EquivalenceSet>();
      pending_equivalence_sets->insert(set, mask);
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::finalize_equivalence_sets(RtUserEvent done_event,
        InnerContext *context, Runtime *runtime, unsigned parent_req_index,
        IndexSpaceExpression *expr, UniqueID opid)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock t_lock(tracker_lock);
#ifdef DEBUG_LEGION
        assert(equivalence_sets_ready != NULL);
#endif
        LegionMap<RtUserEvent,FieldMask>::iterator finder =
          equivalence_sets_ready->find(done_event);
#ifdef DEBUG_LEGION
        assert(finder != equivalence_sets_ready->end());
#endif
        // Check for the pending invalidations case. This occurs due to 
        // false aliasing in the Equivalence Set KD tree, were we might
        // have found some equivalence sets for a subset of points for
        // this analysis, but some other invalidation came through for
        // an independent set of points which required refining the 
        // node in the equivalence set KD tree. This will invalidate
        // the subscriptions so we'll need to recompute the equivalence 
        // sets. Note that we're guaranteed that this is only false
        // aliasing because of the logical dependence analysis which 
        // implies if there were true aliasing then there would have been
        // a mapping dependence that prevented this race.
        FieldMask invalidated = finder->second & pending_invalidations;
        while (!!invalidated)
        {
          // We have pending invalidations that we deferred until we had
          // seen all the responses for the compute_equivalence_sets call
          // We've now seen all the responses, so we need to do the actual
          // work of the invalidations now that we're in a consistent state
          // First invalidate any subscriptions 
          LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > to_cancel;
          find_cancellations(invalidated, to_cancel);
          // Then we can prune states out of the pending equivalence sets
          if ((pending_equivalence_sets != NULL) &&
              !(invalidated * pending_equivalence_sets->get_valid_mask()))
          {
            std::vector<EquivalenceSet*> to_delete;
            for (FieldMaskSet<EquivalenceSet>::iterator it =
                  pending_equivalence_sets->begin(); it !=
                  pending_equivalence_sets->end(); it++)
            {
              it.filter(invalidated);
              if (!it->second)
                to_delete.push_back(it->first);
            }
            if (!to_delete.empty())
            {
              if (to_delete.size() < pending_equivalence_sets->size())
              {
                for (std::vector<EquivalenceSet*>::const_iterator it =
                      to_delete.begin(); it != to_delete.end(); it++)
                  pending_equivalence_sets->erase(*it);
                pending_equivalence_sets->tighten_valid_mask();
              }
              else
              {
                delete pending_equivalence_sets;
                pending_equivalence_sets = NULL;
              }
            }
            else
              pending_equivalence_sets->tighten_valid_mask();
          }
          pending_invalidations -= invalidated;
          t_lock.release();
          // Perform any cancellations now that we've released the lock
          if (!to_cancel.empty())
          {
            std::vector<RtEvent> cancelled_events;
            cancel_subscriptions(runtime, to_cancel, &cancelled_events);
            // Make sure all the cancellations are done before we try
            // again so we don't double count references
            if (!cancelled_events.empty())
            {
              const RtEvent wait_on = Runtime::merge_events(cancelled_events);
              wait_on.wait();
            }
          }
          // Now redo the computation
          const RtEvent ready = context->compute_equivalence_sets(this,
              runtime->address_space, parent_req_index, expr, invalidated);
          if (ready.exists() && !ready.has_triggered())
          {
            // Launch task to finalize the sets once they are ready
            LgFinalizeEqSetsArgs args(this, done_event, opid,
                context, parent_req_index, expr);
            runtime->issue_runtime_meta_task(args, 
                               LG_LATENCY_DEFERRED_PRIORITY, ready);
            // We did the continuation for this so there's nothing
            // more to do here
            return;
          }
          t_lock.reacquire();
          invalidated = finder->second & pending_invalidations;
        }
        // If there are any pending equivalence sets, move them into 
        // the actual equivalence sets
        if ((pending_equivalence_sets != NULL) &&
            !(finder->second * pending_equivalence_sets->get_valid_mask()))
        {
          std::vector<EquivalenceSet*> to_delete;
          for (FieldMaskSet<EquivalenceSet>::iterator it = 
                pending_equivalence_sets->begin(); it !=
                pending_equivalence_sets->end(); it++)
          {
            // Once it's valid for any field then it's valid for all of them
            if (it->second * finder->second)
              continue;
            if (equivalence_sets.insert(it->first, it->second))
              it->first->add_base_gc_ref(VERSION_MANAGER_REF);
            to_delete.push_back(it->first);
          }
          if (!to_delete.empty())
          {
            if (to_delete.size() < pending_equivalence_sets->size())
            {
              for (std::vector<EquivalenceSet*>::const_iterator it =
                    to_delete.begin(); it != to_delete.end(); it++)
                pending_equivalence_sets->erase(*it);
              pending_equivalence_sets->tighten_valid_mask();
            }
            else
            {
              delete pending_equivalence_sets;
              pending_equivalence_sets = NULL;
            }
          }
        }
        if (waiting_infos != NULL)
        {
          std::vector<VersionInfo*> to_delete;
          for (FieldMaskSet<VersionInfo>::iterator wit =
                waiting_infos->begin(); wit != waiting_infos->end(); wit++)
          {
            const FieldMask info_overlap = wit->second & finder->second;
            if (!info_overlap)
              continue;
            record_equivalence_sets(wit->first, info_overlap);
            wit.filter(info_overlap);
            if (!wit->second)
              to_delete.push_back(wit->first);
          }
          if (!to_delete.empty())
          {
            if (to_delete.size() < waiting_infos->size())
            {
              for (std::vector<VersionInfo*>::const_iterator it =
                    to_delete.begin(); it != to_delete.end(); it++)
                waiting_infos->erase(*it);
              waiting_infos->tighten_valid_mask();
            }
            else
            {
              delete waiting_infos;
              waiting_infos = NULL;
            }
          }
        }
        // We can relax the mask for the equivalence sets here so we don't
        // recompute in the case that we are empty
        equivalence_sets.relax_valid_mask(finder->second);
        equivalence_sets_ready->erase(finder);
        if (equivalence_sets_ready->empty())
        {
          delete equivalence_sets_ready;
          equivalence_sets_ready = NULL;
        }
      }
      // At this point we're done so we can trigger the done event
      Runtime::trigger_event(done_event);
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::record_equivalence_sets(VersionInfo *version_info,
                                               const FieldMask &mask) const
    //--------------------------------------------------------------------------
    {
      for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
            equivalence_sets.begin(); it != equivalence_sets.end(); it++)
      {
        const FieldMask overlap = it->second & mask;
        if (!overlap)
          continue;
        version_info->record_equivalence_set(it->first, overlap);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void EqSetTracker::handle_pending_equivalence_set(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      RtEvent ready;
      EquivalenceSet *set = runtime->find_or_request_equivalence_set(did,ready);
      size_t num_trackers;
      derez.deserialize(num_trackers);
      for (unsigned idx = 0; idx < num_trackers; idx++)
      {
        EqSetTracker *tracker;
        derez.deserialize(tracker);
        FieldMask mask;
        derez.deserialize(mask);
        tracker->record_pending_equivalence_set(set, mask);
      }
      RtUserEvent recorded_event;
      derez.deserialize(recorded_event);
      // Then pending sets can't be processed until the equivalence set is
      // actually ready so chaing these events here
      Runtime::trigger_event(recorded_event, ready);
    }

#if 0
    //--------------------------------------------------------------------------
    bool EqSetTracker::finish_subscription(EqKDTree *owner,
                                    AddressSpaceID space, const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      bool remove_reference;
      {
        AutoLock t_lock(tracker_lock);
        

        std::map<std::pair<EqKDTree*,AddressSpaceID>,unsigned>::iterator
          finder = subscription_owners.find(key);
#ifdef DEBUG_LEGION
        assert(finder != subscription_owners.end());
        assert(finder->second > 0);
#endif
        if (--finder->second == 0)
          subscription_owners.erase(finder);
        remove_reference = subscription_owners.empty();
      }
      // Do this last in case we delete ourselves
      if (remove_reference)
        return remove_subscription_reference();
      else
        return false;
    }
#endif

    //--------------------------------------------------------------------------
    /*static*/ void EqSetTracker::handle_equivalence_set_creation(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      EqSetTracker *owner;
      derez.deserialize(owner);
      DistributedID did;
      derez.deserialize(did);
      IndexSpace handle;
      derez.deserialize(handle);
      std::vector<Domain> rectangles;
      if (!handle.exists())
      {
        size_t num_rectangles;
        derez.deserialize(num_rectangles);
        rectangles.resize(num_rectangles);
        for (unsigned idx = 0; idx < num_rectangles; idx++)
          derez.deserialize(rectangles[idx]);
      }
      RegionTreeID tid;
      derez.deserialize(tid);
      RtEvent ctx_ready;
      InnerContext *context =
        InnerContext::unpack_task_context(derez, runtime, ctx_ready);
      size_t num_spaces;
      derez.deserialize(num_spaces);
      CollectiveMapping *mapping = new CollectiveMapping(derez, num_spaces);
      RtEvent ready_event;
      derez.deserialize(ready_event);
      // Make sure that we'll know when this is triggered
      ready_event.subscribe();
      derez.deserialize(num_spaces);
      LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > to_notify;
      for (unsigned idx1 = 0; idx1 < num_spaces; idx1++)
      {
        AddressSpaceID space;
        derez.deserialize(space);
        FieldMaskSet<EqKDTree> &trees = to_notify[space];
        size_t num_trees;
        derez.deserialize(num_trees);
        for (unsigned idx2 = 0; idx2 < num_trees; idx2++)
        {
          EqKDTree *tree;
          derez.deserialize(tree);
          FieldMask mask;
          derez.deserialize(mask);
          trees.insert(tree, mask);
        }
      }
      RtUserEvent notified_event;
      derez.deserialize(notified_event);

      LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::iterator local_finder =
        to_notify.find(runtime->address_space);
#ifdef DEBUG_LEGION
      assert(local_finder != to_notify.end());
#endif
      // Send it off to any children nodes
      std::vector<RtEvent> notified_events;
      std::vector<AddressSpaceID> children;
      const AddressSpaceID root = runtime->determine_owner(did);
      mapping->get_children(root, runtime->address_space, children);
      if (!children.empty())
      {
        // Make this a broadcast tree of ready events so not everyone
        // is subscribing to the owner node
        const RtUserEvent local_ready = Runtime::create_rt_user_event();
        Runtime::trigger_event(local_ready, ready_event);
        for (std::vector<AddressSpaceID>::const_iterator cit =
              children.begin(); cit != children.end(); cit++)
        {
          const RtUserEvent notified = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(owner);
            rez.serialize(did);
            rez.serialize(handle);
            if (!handle.exists())
            {
              rez.serialize<size_t>(rectangles.size());
              for (unsigned idx = 0; idx < rectangles.size(); idx++)
                rez.serialize(rectangles[idx]);
            }
            rez.serialize(tid);
            if (ctx_ready.exists() && !ctx_ready.has_triggered())
            {
              ctx_ready.wait();
              ctx_ready = RtEvent::NO_RT_EVENT;
            }
            context->pack_task_context(rez);
            mapping->pack(rez);
            rez.serialize(local_ready);
            rez.serialize<size_t>(to_notify.size() - 1);
            for (LegionMap<AddressSpaceID,
                           FieldMaskSet<EqKDTree> >::const_iterator nit =
                  to_notify.begin(); nit != to_notify.end(); nit++)
            {
              if (nit == local_finder)
                continue;
              rez.serialize(nit->first);
              rez.serialize(nit->second.size());
              for (FieldMaskSet<EqKDTree>::const_iterator it =
                    nit->second.begin(); it != nit->second.end(); it++)
              {
                rez.serialize(it->first);
                rez.serialize(it->second);
              }
            }
            rez.serialize(notified);
          }
          runtime->send_equivalence_set_creation(*cit, rez);
          notified_events.push_back(notified);
        }
      }
      // Make the equivalence set
      IndexSpaceExpression *expr = handle.exists() ? 
        runtime->forest->get_node(handle) :
        local_finder->second.begin()->first->create_from_rectangles(
                                          runtime->forest, rectangles);
      if (ctx_ready.exists() && !ctx_ready.has_triggered())
        ctx_ready.wait();
      void *location = 
        runtime->find_or_create_pending_collectable_location<EquivalenceSet>(
          did);
      EquivalenceSet *set = new(location) EquivalenceSet(runtime, did, root, 
          expr, tid, context, false/*register now*/, mapping);
      // Once construction is complete then we do the registration
      set->register_with_runtime();
      // Register it with any local trees 
      for (FieldMaskSet<EqKDTree>::const_iterator it =
            local_finder->second.begin(); it != 
            local_finder->second.end(); it++)
        it->first->record_equivalence_set(set, it->second, ready_event,
                                          owner, root);
      to_notify.erase(local_finder);
      // Trigger the event that we're done
      if (!notified_events.empty())
        Runtime::trigger_event(notified_event,
            Runtime::merge_events(notified_events));
      else
        Runtime::trigger_event(notified_event);
    }

    //--------------------------------------------------------------------------
    /*static*/ void EqSetTracker::handle_equivalence_set_reuse(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      RtEvent ready;
      EquivalenceSet *set = runtime->find_or_request_equivalence_set(did,ready);
      EqSetTracker *owner;
      derez.deserialize(owner);
      AddressSpaceID owner_space;
      derez.deserialize(owner_space);
      size_t num_spaces;
      derez.deserialize(num_spaces);
      CollectiveMapping mapping(derez, num_spaces);
      size_t num_notifications;
      derez.deserialize(num_notifications);
      LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > to_notify;
      for (unsigned idx1 = 0; idx1 < num_notifications; idx1++)
      {
        AddressSpaceID space;
        derez.deserialize(space);
        FieldMaskSet<EqKDTree> &trees = to_notify[space];
        size_t num_trees;
        derez.deserialize(num_trees);
        for (unsigned idx2 = 0; idx2 < num_trees; idx2++)
        {
          EqKDTree *tree;
          derez.deserialize(tree);
          FieldMask mask;
          derez.deserialize(mask);
          trees.insert(tree, mask);
        }
      }
      RtUserEvent done_event;
      derez.deserialize(done_event);

      std::vector<AddressSpaceID> children;
      mapping.get_children(owner_space, runtime->address_space, children);
      std::vector<RtEvent> done_events;
      for (std::vector<AddressSpaceID>::const_iterator cit =
            children.begin(); cit != children.end(); cit++)
      {
        const RtUserEvent notified = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z2(rez);
          // Reference still held by at least one EqKDTree 
          // for which this is in the previous set
          rez.serialize(did);
          rez.serialize(owner);
          rez.serialize(owner_space);
          mapping.pack(rez);
          rez.serialize<size_t>(to_notify.size() - 1);
          for (LegionMap<AddressSpaceID,
                         FieldMaskSet<EqKDTree> >::const_iterator nit =
                to_notify.begin(); nit != to_notify.end(); nit++)
          {
            if (nit->first == runtime->address_space)
              continue;
            rez.serialize(nit->first);
            rez.serialize(nit->second.size());
            for (FieldMaskSet<EqKDTree>::const_iterator it =
                  nit->second.begin(); it != nit->second.end(); it++)
            {
              rez.serialize(it->first);
              rez.serialize(it->second);
            }
          }
          rez.serialize(notified);
        }
        runtime->send_equivalence_set_reuse(*cit, rez);
        done_events.push_back(notified);
      }
#ifdef DEBUG_LEGION
      assert(to_notify.find(runtime->address_space) != to_notify.end());
#endif
      FieldMaskSet<EqKDTree> &local_trees = to_notify[runtime->address_space];
      if (ready.exists() && !ready.has_triggered())
        ready.wait();
      for (FieldMaskSet<EqKDTree>::const_iterator it =
            local_trees.begin(); it != local_trees.end(); it++)
        it->first->record_equivalence_set(set, it->second,
            RtEvent::NO_RT_EVENT, owner, owner_space);
      if (!done_events.empty())
        Runtime::trigger_event(done_event, Runtime::merge_events(done_events));
      else
        Runtime::trigger_event(done_event);
    }

    //--------------------------------------------------------------------------
    unsigned EqSetTracker::invalidate_equivalence_sets(Runtime *runtime,
                                       const FieldMask &mask,
                                       EqKDTree *tree, AddressSpaceID source,
                                       std::vector<RtEvent> &invalidated_events)
    //--------------------------------------------------------------------------
    {
      unsigned source_references_to_remove = 0;
      LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > to_cancel;
      {
        FieldMask remaining = mask;
        AutoLock t_lock(tracker_lock);
        // If we've already recorded a pending invalidation for any of these
        // fields then there's nothing more we need to do here
        if (!!pending_invalidations)
        {
          remaining -= pending_invalidations;
          if (!remaining)
            return source_references_to_remove;
        }
        // Invalidations are not precise so we need to handle the case where
        // we get an invalidation in the middle of computing our equivalence
        // sets and record that we need to do the invalidation later
        if (equivalence_sets_ready != NULL)
        {
          FieldMask to_filter;
          for (LegionMap<RtUserEvent,FieldMask>::const_iterator it =
                equivalence_sets_ready->begin(); it !=
                equivalence_sets_ready->end(); it++)
          {
            const FieldMask overlap = remaining & it->second;
            if (!overlap)
              continue;
            to_filter |= overlap;
            remaining -= overlap;
            if (!remaining)
              break;
          }
          if (!!to_filter)
          {
#ifdef DEBUG_LEGION
            // A little sanity check, we should never be receiving racy
            // invalidations for fields that we are also trying to create
            // equivalence sets for because mapping dependence analysis 
            // should guarantee non-overlapping of these index spaces
            if (creation_rectangles != NULL)
            {
              for (LegionMap<Domain,FieldMask>::const_iterator it =
                    creation_rectangles->begin(); it !=
                    creation_rectangles->end(); it++)
                assert(to_filter * it->second);
            }
#endif
            pending_invalidations |= to_filter;
            if (!remaining)
              return source_references_to_remove;
          }
        }
        // See if we won the race to invalidating the source
        LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::iterator finder =
          current_subscriptions.find(source);
        if (finder != current_subscriptions.end())
        {
          FieldMaskSet<EqKDTree>::iterator source_finder = 
            finder->second.find(tree);
          if (source_finder != finder->second.end())
          {
            const FieldMask overlap = source_finder->second & remaining;
            if (!!overlap)
            {
              source_references_to_remove += overlap.pop_count();
              source_finder.filter(overlap);
              if (!source_finder->second)
              {
                finder->second.erase(source_finder);
                if (finder->second.empty())
                  current_subscriptions.erase(finder);
                else
                  finder->second.tighten_valid_mask();
              }
              else
                finder->second.tighten_valid_mask();
            }
          }
        }
        // Now go through and invalidate all the other subscriptions to cancel
        find_cancellations(remaining, to_cancel);
        if (!(remaining * equivalence_sets.get_valid_mask()))
        {
          std::vector<EquivalenceSet*> to_delete;
          for (FieldMaskSet<EquivalenceSet>::iterator it =
                equivalence_sets.begin(); it != equivalence_sets.end(); it++)
          {
            it.filter(remaining);
            if (!it->second)
              to_delete.push_back(it->first);
          }
          if (!to_delete.empty())
          {
            const ReferenceSource source_kind = get_reference_source_kind();
            for (std::vector<EquivalenceSet*>::const_iterator it =
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              equivalence_sets.erase(*it);
              if ((*it)->remove_base_gc_ref(source_kind))
                delete (*it);
            }
          }
          equivalence_sets.tighten_valid_mask();
        }
      }
      if (!to_cancel.empty())
        cancel_subscriptions(runtime, to_cancel, &invalidated_events);
      return source_references_to_remove;
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::find_cancellations(const FieldMask &mask,
                   LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > &to_cancel)
    //--------------------------------------------------------------------------
    {
      // Lock held from caller
      for (LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::iterator cit =
            current_subscriptions.begin(); cit != 
            current_subscriptions.end(); /*nothing*/)
      {
        if (cit->second.get_valid_mask() * mask)
        {
          cit++;
          continue;
        }
        if (!!(cit->second.get_valid_mask() - mask))
        {
          // Selectively filter matches
          std::vector<EqKDTree*> to_delete;
          FieldMaskSet<EqKDTree> &invalidations = to_cancel[cit->first];
          for (FieldMaskSet<EqKDTree>::iterator it =
                cit->second.begin(); it != cit->second.end(); it++)
          {
            const FieldMask overlap = it->second & mask;
            if (!overlap)
              continue;
            invalidations.insert(it->first, overlap);
            it.filter(overlap);
            if (!it->second)
              to_delete.push_back(it->first);
          }
          for (std::vector<EqKDTree*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
            cit->second.erase(*it);
        }
        else // Filtering all the fields so we can swap it out 
          to_cancel[cit->first].swap(cit->second);
        if (cit->second.empty())
        {
          LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::iterator
            delete_it = cit++;
          current_subscriptions.erase(delete_it);
        }
        else
        {
          cit->second.tighten_valid_mask();
          cit++;
        }
      }
    }

    //--------------------------------------------------------------------------
    void EqSetTracker::cancel_subscriptions(Runtime *runtime,
        const LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > &to_cancel,
        std::vector<RtEvent> *cancelled_events)
    //--------------------------------------------------------------------------
    {
      const AddressSpaceID local_space = runtime->address_space;
      for (LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> >::const_iterator
            cit = to_cancel.begin(); cit != to_cancel.end(); cit++)
      {
        if (cit->first != local_space)
        {
          Serializer rez;
          {
            RezCheck z(rez);
#ifdef DEBUG_LEGION
            assert(!cit->second.empty());
#endif
            rez.serialize<size_t>(cit->second.size());
            rez.serialize(this);
            for (FieldMaskSet<EqKDTree>::const_iterator it =
                  cit->second.begin(); it != cit->second.end(); it++)
            {
              rez.serialize(it->first);
              rez.serialize(it->second);
            }
            if (cancelled_events != NULL)
            {
              const RtUserEvent cancelled = Runtime::create_rt_user_event();
              rez.serialize(cancelled);
              cancelled_events->push_back(cancelled);
            }
            else
              rez.serialize(RtUserEvent::NO_RT_USER_EVENT);
          }
          runtime->send_cancel_equivalence_sets_subscription(cit->first, rez);
        }
        else
        {
          unsigned references_to_remove = 0;
          for (FieldMaskSet<EqKDTree>::const_iterator it =
                cit->second.begin(); it != cit->second.end(); it++)
          {
            references_to_remove +=
              it->first->cancel_subscription(this, local_space, it->second);
            if (it->first->remove_reference(it->second.pop_count()))
              delete it->first;
          }
          if ((references_to_remove > 0) && 
              remove_subscription_reference(references_to_remove))
            assert(false); // should never end up deleting ourselves
        }
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void EqSetTracker::handle_cancel_subscription(
                   Deserializer &derez, Runtime *runtime, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      size_t num_subscribers;
      derez.deserialize(num_subscribers);
      if (num_subscribers > 0)
      {
        EqSetTracker *owner;
        derez.deserialize(owner);
        unsigned references_to_remove = 0;
        for (unsigned idx = 0; idx < num_subscribers; idx++)
        {
          EqKDTree *tree;
          derez.deserialize(tree);
          FieldMask mask;
          derez.deserialize(mask);
          references_to_remove += 
            tree->cancel_subscription(owner, source, mask);
          if (tree->remove_reference(mask.pop_count()))
            delete tree;
        }
        if (references_to_remove > 0)
        {
          Serializer rez;
          {
            RezCheck z2(rez);
            rez.serialize<size_t>(0); // nothing to filter
            rez.serialize(owner);
            rez.serialize(references_to_remove);
          }
          runtime->send_invalidate_equivalence_sets_subscription(source, rez);
        }
        RtUserEvent cancelled;
        derez.deserialize(cancelled);
        if (cancelled.exists())
          Runtime::trigger_event(cancelled);
      }
      else
      {
        EqKDTree *subscriber;
        derez.deserialize(subscriber);
        unsigned references_to_remove;
        derez.deserialize(references_to_remove);
        if (subscriber->remove_reference(references_to_remove))
          delete subscriber;
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void EqSetTracker::invalidate_subscriptions(
        Runtime *runtime, EqKDTree *owner,
        LegionMap<AddressSpaceID,FieldMaskSet<EqSetTracker> > &subscribers,
        std::vector<RtEvent> &invalidated_events)
    //--------------------------------------------------------------------------
    {
      const AddressSpaceID local_space = runtime->address_space;
      for (LegionMap<AddressSpaceID,FieldMaskSet<EqSetTracker> >::const_iterator
            sit = subscribers.begin(); sit != subscribers.end(); sit++)
      {
        if (sit->first != local_space)
        {
          const RtUserEvent invalidated = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
#ifdef DEBUG_LEGION
            assert(!sit->second.empty());
#endif
            rez.serialize<size_t>(sit->second.size());
            rez.serialize(owner);
            for (FieldMaskSet<EqSetTracker>::const_iterator it =
                  sit->second.begin(); it != sit->second.end(); it++)
            {
              rez.serialize(it->first);
              rez.serialize(it->second);
            }
            rez.serialize(invalidated);
          }
          runtime->send_invalidate_equivalence_sets_subscription(sit->first,
                                                                 rez);
          invalidated_events.push_back(invalidated);
        }
        else
        {
          unsigned references_to_remove = 0;
          for (FieldMaskSet<EqSetTracker>::const_iterator it = 
                sit->second.begin(); it != sit->second.end(); it++)
          {
            references_to_remove += it->first->invalidate_equivalence_sets(
                runtime, it->second, owner, runtime->address_space, 
                invalidated_events);
            if (it->first->remove_subscription_reference(
                                  it->second.pop_count()))
              delete it->first;
          }
          if ((references_to_remove > 0) && 
              owner->remove_reference(references_to_remove))
            delete owner;
        }
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void EqSetTracker::handle_invalidate_subscription(
                   Deserializer &derez, Runtime *runtime, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      size_t num_subscribers;
      derez.deserialize(num_subscribers);
      if (num_subscribers > 0)
      {
        EqKDTree *owner;
        derez.deserialize(owner);
        unsigned references_to_remove = 0;
        std::vector<RtEvent> invalidated_events;
        for (unsigned idx = 0; idx < num_subscribers; idx++)
        {
          EqSetTracker *tracker;
          derez.deserialize(tracker);
          FieldMask mask;
          derez.deserialize(mask);
          references_to_remove += tracker->invalidate_equivalence_sets(
              runtime, mask, owner, source, invalidated_events);
          if (tracker->remove_subscription_reference(mask.pop_count()))
            delete tracker;
        }
        RtUserEvent invalidated;
        derez.deserialize(invalidated);
        if (!invalidated_events.empty())
          Runtime::trigger_event(invalidated, 
              Runtime::merge_events(invalidated_events));
        else
          Runtime::trigger_event(invalidated);
        if (references_to_remove > 0)
        {
          Serializer rez;
          {
            RezCheck z2(rez);
            rez.serialize<size_t>(0); // num subscribers
            rez.serialize(owner);
            rez.serialize(references_to_remove);
          }
          runtime->send_cancel_equivalence_sets_subscription(source, rez);
        }
      }
      else
      {
        EqSetTracker *subscriber;
        derez.deserialize(subscriber);
        unsigned references_to_remove;
        derez.deserialize(references_to_remove);
        if (subscriber->remove_subscription_reference(references_to_remove))
          delete subscriber;
      }
    }

    //--------------------------------------------------------------------------
    EqSetTracker::LgFinalizeEqSetsArgs::LgFinalizeEqSetsArgs(EqSetTracker *t,
        RtUserEvent c, UniqueID uid, InnerContext *ctx, unsigned index,
        IndexSpaceExpression *e)
      : LgTaskArgs<LgFinalizeEqSetsArgs>(uid), tracker(t), compute(c),
        context(ctx), expr(e), parent_req_index(index)
    //--------------------------------------------------------------------------
    {
      context->add_base_gc_ref(META_TASK_REF);
      expr->add_base_expression_reference(META_TASK_REF);
    }

    //--------------------------------------------------------------------------
    /*static*/ void EqSetTracker::handle_finalize_eq_sets(const void *args, 
                                                          Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      const LgFinalizeEqSetsArgs *fargs = (const LgFinalizeEqSetsArgs*)args;
      fargs->tracker->finalize_equivalence_sets(fargs->compute,
          fargs->context, runtime, fargs->parent_req_index, fargs->expr,
          fargs->provenance);
      if (fargs->context->remove_base_gc_ref(META_TASK_REF))
        delete fargs->context;
      if (fargs->expr->remove_base_expression_reference(META_TASK_REF))
        delete fargs->expr;
    }

    /////////////////////////////////////////////////////////////
    // Version Manager 
    ///////////////////////////////////////////////////////////// 

    //--------------------------------------------------------------------------
    VersionManager::VersionManager(RegionTreeNode *n, ContextID c)
      : EqSetTracker(manager_lock), ctx(c), node(n),runtime(n->context->runtime)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    VersionManager::~VersionManager(void)
    //--------------------------------------------------------------------------
    {
#if 0
      assert(!disjoint_complete);
      assert(disjoint_complete_children.empty());
      assert(disjoint_complete_children_shards.empty());
      assert(refinement_subscriptions.empty());
      assert(subscription_owners.empty());
#endif
    }

    //--------------------------------------------------------------------------
    void VersionManager::perform_versioning_analysis(InnerContext *context,
                 VersionInfo *version_info, RegionNode *region_node,
                 const FieldMask &version_mask, Operation *op, unsigned index,
                 unsigned parent_req_index, IndexSpace root_space,
                 std::set<RtEvent> &ready_events, RtEvent *output_region_ready,
                 bool collective_rendezvous)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(node == region_node);
#endif
      if (output_region_ready != NULL)
      {
#ifdef DEBUG_LEGION
        assert(!collective_rendezvous);
#endif
        // This is a special case for output regions
        // Make a new equivalence set and record it in the current set
        // and then we are done, we'll register this equivalence set
        // with the EqKDTree later once we actually know the bounds
        // Note that by registering it we're allowing others to find it
        // here even before they can compute it in the EqKDTree which is
        // an important optimization for many applications
        const DistributedID did = runtime->get_available_distributed_id();
        EquivalenceSet *set = new EquivalenceSet(runtime, did,
            runtime->address_space/*logical owner*/, region_node->row_source,
            region_node->handle.get_tree_id(), context, true/*register*/);
        version_info->record_equivalence_set(set, version_mask);
        // Launch a meta-task to register this equivalence set with
        // EqKDTree once the index space domain is ready
        RtUserEvent done_event = Runtime::create_rt_user_event();
        FinalizeOutputEquivalenceSetArgs args(this, op->get_unique_op_id(),
            context, parent_req_index, set, root_space, done_event);
        runtime->issue_runtime_meta_task(args, LG_LATENCY_DEFERRED_PRIORITY,
            region_node->row_source->get_ready_event());
        *output_region_ready = done_event;
        AutoLock m_lock(manager_lock);
#ifdef DEBUG_LEGION
        assert(version_mask * equivalence_sets.get_valid_mask());
#endif
        if (equivalence_sets.insert(set, version_mask))
          set->add_base_gc_ref(VERSION_MANAGER_REF);
        return;
      }
      // If we don't have equivalence classes for this region yet we 
      // either need to compute them or request them from the owner
      FieldMask remaining_mask(version_mask);
      bool has_waiter = false;
      {
        AutoLock m_lock(manager_lock,1,false/*exclusive*/);
        // Check to see if any computations of equivalence sets are in progress
        // If so we'll skip out early and go down the slow path which should
        // be a fairly rare thing to do
        if (equivalence_sets_ready != NULL)
        {
          for (LegionMap<RtUserEvent,FieldMask>::const_iterator it =
                equivalence_sets_ready->begin(); it != 
                equivalence_sets_ready->end(); it++)
          {
            if (remaining_mask * it->second)
              continue;
            // Skip out earlier if we have at least one thing to wait
            // for since we're going to have to go down the slow path
            has_waiter = true;
            break;
          }
        }
        // If we have a waiter, then don't bother doing this
        if (!has_waiter)
        {
          // Get any fields that are already ready
          if ((version_info != NULL) &&
              !(version_mask * equivalence_sets.get_valid_mask()))
            record_equivalence_sets(version_info, version_mask);
          remaining_mask -= equivalence_sets.get_valid_mask();
          // If we got all our fields then we are done
          if (!remaining_mask)
            return;
        }
      }
      // Retake the lock in exclusive mode and make sure we don't lose the race
      RtUserEvent compute_event;
      {
        FieldMask waiting_mask;
        AutoLock m_lock(manager_lock);
        if (equivalence_sets_ready != NULL)
        {
          for (LegionMap<RtUserEvent,FieldMask>::const_iterator it =
                equivalence_sets_ready->begin(); it != 
                equivalence_sets_ready->end(); it++)
          {
            const FieldMask overlap = remaining_mask & it->second;
            if (!overlap)
              continue;
            ready_events.insert(it->first);
            waiting_mask |= overlap;
          }
          if (!!waiting_mask)
            remaining_mask -= waiting_mask;
        }
        // Get any fields that are already ready
        // Have to do this after looking for pending equivalence sets
        // to make sure we don't have pending outstanding requests
        if (!(remaining_mask * equivalence_sets.get_valid_mask()))
        {
          if (version_info != NULL)
            record_equivalence_sets(version_info, remaining_mask);
          remaining_mask -= equivalence_sets.get_valid_mask();
          // If we got all our fields here and we're not waiting 
          // on any other computations then we're done
          if (!remaining_mask && !waiting_mask)
            return;
        }
        // If we still have remaining fields then we need to
        // do this computation ourselves
        if (!!remaining_mask)
        {
          compute_event = Runtime::create_rt_user_event();
          if (equivalence_sets_ready == NULL)
            equivalence_sets_ready = new LegionMap<RtUserEvent,FieldMask>();
          equivalence_sets_ready->insert(
              std::make_pair(compute_event,remaining_mask));
          ready_events.insert(compute_event);
          waiting_mask |= remaining_mask;
        }
#ifdef DEBUG_LEGION
        assert(!!waiting_mask);
#endif
        // Record that our version info is waiting for these fields
        if (version_info != NULL)
        {
          if (waiting_infos == NULL)
            waiting_infos = new FieldMaskSet<VersionInfo>();
          waiting_infos->insert(version_info, waiting_mask);
        }
      }
      if (compute_event.exists())
      {
        // Bounce this computation off the context so that we know
        // that we are on the right node to perform it
        const RtEvent ready = context->compute_equivalence_sets(this, 
                        runtime->address_space, parent_req_index,
                        region_node->row_source, remaining_mask, root_space);
        if (ready.exists() && !ready.has_triggered())
        {
          // Launch task to finalize the sets once they are ready
          LgFinalizeEqSetsArgs args(this, compute_event, 
              op->get_unique_op_id(), context, parent_req_index,
              region_node->row_source);
          runtime->issue_runtime_meta_task(args, 
                             LG_LATENCY_DEFERRED_PRIORITY, ready);
        }
        else
          finalize_equivalence_sets(compute_event, context, runtime,
              parent_req_index, region_node->row_source, 
              op->get_unique_op_id());
      }
    } 

    //--------------------------------------------------------------------------
    RtEvent VersionManager::finalize_output_equivalence_set(EquivalenceSet *set,
                                                      InnerContext *context,
                                                      unsigned parent_req_index,
                                                      IndexSpace root_space)
    //--------------------------------------------------------------------------
    {
      FieldMask set_mask;
      {
        AutoLock m_lock(manager_lock,1,false/*exclusive*/);
        FieldMaskSet<EquivalenceSet>::const_iterator finder =
          equivalence_sets.find(set);
#ifdef DEBUG_LEGION
        assert(finder != equivalence_sets.end());
#endif
        set_mask = finder->second;
      }
      return context->record_output_equivalence_set(this,
          runtime->address_space, parent_req_index, set, set_mask, root_space);
    } 

#if 0
    //--------------------------------------------------------------------------
    void VersionManager::record_subscription(VersionManager *owner,
                                             AddressSpaceID space)
    //--------------------------------------------------------------------------
    {
      bool add_reference;
      {
        const std::pair<VersionManager*,AddressSpaceID> key(owner, space);
        AutoLock m_lock(manager_lock);
        add_reference = subscription_owners.empty();
        std::map<std::pair<VersionManager*,AddressSpaceID>,unsigned>::iterator
          finder = subscription_owners.find(key);
        if (finder == subscription_owners.end())
          subscription_owners[key] = 1;
        else
          finder->second++;
      }
      if (add_reference)
        node->add_base_resource_ref(VERSION_MANAGER_REF);
    }
#endif

    //--------------------------------------------------------------------------
    RegionTreeID VersionManager::get_region_tree_id(void) const
    //--------------------------------------------------------------------------
    {
      return node->get_tree_id();
    }

    //--------------------------------------------------------------------------
    IndexSpaceExpression* VersionManager::get_tracker_expression(void) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(node->is_region());
#endif
      return node->as_region_node()->row_source;
    }

    //--------------------------------------------------------------------------
    void VersionManager::add_subscription_reference(unsigned count)
    //--------------------------------------------------------------------------
    {
      // This implicitly keeps us alive
      node->add_base_resource_ref(VERSION_MANAGER_REF, count);
    }

    //--------------------------------------------------------------------------
    bool VersionManager::remove_subscription_reference(unsigned count)
    //--------------------------------------------------------------------------
    {
      if (node->remove_base_resource_ref(VERSION_MANAGER_REF, count))
        delete node;
      // Never directly delete ourselves
      return false;
    }

#if 0
    //--------------------------------------------------------------------------
    bool VersionManager::cancel_subscription(EqSetTracker *subscriber,
                                             AddressSpaceID space)
    //--------------------------------------------------------------------------
    {
      AutoLock m_lock(manager_lock);
      LegionMap<AddressSpaceID,FieldMaskSet<EqSetTracker> >::iterator
        refinement_finder = refinement_subscriptions.find(space);
      if (refinement_finder == refinement_subscriptions.end())
        return false;
      FieldMaskSet<EqSetTracker>::iterator finder =
        refinement_finder->second.find(subscriber);
      if (finder == refinement_finder->second.end())
        return false;
      refinement_finder->second.erase(finder);
      if (refinement_finder->second.empty())
        refinement_subscriptions.erase(refinement_finder);
      return true;
    }

    //--------------------------------------------------------------------------
    void VersionManager::record_equivalence_set(EquivalenceSet *set,
                                                const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      AutoLock m_lock(manager_lock);
      if (equivalence_sets.insert(set, mask))
        set->add_base_resource_ref(VERSION_MANAGER_REF);
    }

    //--------------------------------------------------------------------------
    void VersionManager::record_pending_equivalence_set(EquivalenceSet *set,
                                                        const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      AutoLock m_lock(manager_lock);
#ifdef DEBUG_LEGION
      assert(mask * disjoint_complete);
#endif
      pending_equivalence_sets.insert(set, mask);
    }

    //--------------------------------------------------------------------------
    void VersionManager::invalidate_equivalence_sets(const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      AutoLock m_lock(manager_lock);
#if 0
      // This is very subtle so pay attention!
      // If you invalidate any of the equivalence sets for a version manager
      // that is not part of the disjoint complete refinement then you have
      // to invalidate ALL the equivalence sets with the same fields since 
      // we use the presence of any equivalence set with a field to indicate
      // that this VersionManager has an up-to-date copy of the equivalence
      // sets corresponding to this logical region.
#endif
      if (mask * equivalence_sets.get_valid_mask())
        return;
      std::vector<EquivalenceSet*> to_delete;
      for (FieldMaskSet<EquivalenceSet>::iterator it =
            equivalence_sets.begin(); it != equivalence_sets.end(); it++)
      {
        it.filter(mask);
        if (!it->second)
          to_delete.push_back(it->first);
      }
      for (std::vector<EquivalenceSet*>::const_iterator it =
            to_delete.begin(); it != to_delete.end(); it++)
      {
        equivalence_sets.erase(*it);
        if ((*it)->remove_base_resource_ref(VERSION_MANAGER_REF))
          assert(false); // should never end up deleting this here
      }
      equivalence_sets.tighten_valid_mask();
    }
#endif 

    //--------------------------------------------------------------------------
    void VersionManager::finalize_manager(void)
    //--------------------------------------------------------------------------
    {
      // We need to remove any tracked equivalence sets that we have
      FieldMaskSet<EquivalenceSet> to_remove;
      LegionMap<AddressSpaceID,FieldMaskSet<EqKDTree> > to_cancel;
      {
        AutoLock m_lock(manager_lock);
#ifdef DEBUG_LEGION
        // All these other resource should already be empty by the time
        // we are being finalized
        assert(pending_equivalence_sets == NULL);
        assert(waiting_infos == NULL);
        assert(equivalence_sets_ready == NULL);
#if 0
        assert(!disjoint_complete);
        assert(disjoint_complete_children.empty());
        assert(disjoint_complete_children_shards.empty());
#endif
#endif
        if (!equivalence_sets.empty())
          to_remove.swap(equivalence_sets);
        else if (current_subscriptions.empty())
          return;
        to_cancel.swap(current_subscriptions);
      }
#ifdef DEBUG_LEGION
      assert(node->is_region());
#endif
      if (!to_cancel.empty())
        cancel_subscriptions(runtime, to_cancel);
      for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
            to_remove.begin(); it != to_remove.end(); it++)
      {
#ifdef DEBUG_LEGION
        // This would be a valid assertion except for cases with control
        // replication where there is another node that owns the equivalence
        // set and we just happen to have a copy of it here
        //assert((it->first->region_node != node) ||
        //        it->first->region_node->row_source->is_empty());
#endif
        if (it->first->remove_base_gc_ref(VERSION_MANAGER_REF))
          delete it->first;
      }
    }

#if 0
    //--------------------------------------------------------------------------
    void VersionManager::add_node_disjoint_complete_ref(void) const
    //--------------------------------------------------------------------------
    {
      node->get_row_source()->add_base_valid_ref(DISJOINT_COMPLETE_REF);
    }

    //--------------------------------------------------------------------------
    void VersionManager::remove_node_disjoint_complete_ref(void) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
#ifndef NDEBUG
      const bool delete_node =
#endif
#endif
      node->get_row_source()->remove_base_valid_ref(DISJOINT_COMPLETE_REF);
#ifdef DEBUG_LEGION
      assert(!delete_node); // should never be directly deleting our node
#endif
    }

    //--------------------------------------------------------------------------
    void VersionManager::initialize_versioning_analysis(EquivalenceSet *set,
                                                        const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      // No need for the lock here since we know this is initialization
#ifdef DEBUG_LEGION
      assert(!!mask);
      assert(disjoint_complete * mask);
#endif
      if (!disjoint_complete)
        add_node_disjoint_complete_ref();
      disjoint_complete |= mask;
      if (equivalence_sets.insert(set, mask))
      {
        set->add_base_gc_ref(DISJOINT_COMPLETE_REF);
        set->add_base_resource_ref(VERSION_MANAGER_REF);
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::initialize_nonexclusive_virtual_analysis(
                                       const FieldMask &mask,
                                       const FieldMaskSet<EquivalenceSet> &sets)
    //--------------------------------------------------------------------------
    {
      // No need for the lock here since we know this is initialization
#ifdef DEBUG_LEGION
      assert(!!mask);
      assert(disjoint_complete * sets.get_valid_mask());
#endif
      // We'll pretend like we're the root of the equivalence set tree
      // here even though we don't actually own these sets, we're just
      // marking it so that any analyses stop here. The logical analysis
      // will ensure that we are never refined
      if (!disjoint_complete)
        add_node_disjoint_complete_ref();
      disjoint_complete |= mask;
      for (FieldMaskSet<EquivalenceSet>::const_iterator it =
            sets.begin(); it != sets.end(); it++)
        if (equivalence_sets.insert(it->first, it->second))
        {
          it->first->add_base_gc_ref(DISJOINT_COMPLETE_REF);
          it->first->add_base_resource_ref(VERSION_MANAGER_REF);
        }
    }

    //--------------------------------------------------------------------------
    void VersionManager::compute_equivalence_sets(IndexSpaceExpression *expr,
                                          EqSetTracker *target,
                                          const AddressSpaceID target_space,
                                          FieldMask mask,
                                          std::set<RtEvent> &ready_events,
                                          FieldMaskSet<PartitionNode> &children,
                                          FieldMask &parent_traversal,
                                          const bool downward_only)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(node->is_region());
#endif
      bool new_subscriber = false;
      FieldMaskSet<EquivalenceSet> to_record;
      {
        AutoLock m_lock(manager_lock);
        if (!downward_only)
        {
          // Keep going up the tree for any fields which we haven't
          // found the refinement for yet
          parent_traversal = mask - disjoint_complete;
          // Any remaining fields we can start analyzing this node
          mask -= parent_traversal;
          if (!mask)
            return;
        }
#ifdef DEBUG_LEGION
        assert(!(mask - disjoint_complete));
        assert(disjoint_complete_children_shards.empty());
#endif
        // See if there are any children to traversa
        if (!disjoint_complete_children.empty() &&
            !(mask * disjoint_complete_children.get_valid_mask()))
        {
          for (FieldMaskSet<RegionTreeNode>::const_iterator it =
                disjoint_complete_children.begin(); it !=
                disjoint_complete_children.end(); it++)
          {
            const FieldMask overlap = mask & it->second;
            if (!overlap)
              continue;
            children.insert(it->first->as_partition_node(), overlap);
            mask -= overlap;
            if (!mask)
              return;
          }
        }
        // At this point we're done with the symbolic analysis so we
        // can actually test the expression for emptiness
        if ((expr != node->as_region_node()->row_source) && expr->is_empty())
          return;
        // If we make it here then we should have equivalence sets for
        // all these remaining fields
#ifdef DEBUG_LEGION
        assert(!equivalence_sets.empty() ||
                node->as_region_node()->row_source->is_empty());
        assert(!(mask - equivalence_sets.get_valid_mask()) ||
                node->as_region_node()->row_source->is_empty());
#endif
        if (target_space != runtime->address_space)
        {
          FieldMaskSet<EquivalenceSet> to_send;
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                equivalence_sets.begin(); it != equivalence_sets.end(); it++)
          {
            const FieldMask overlap = mask & it->second;
            if (!overlap)
              continue;
#ifdef DEBUG_LEGION
            // We should only be sending equivalence sets that we
            // know we are the owners of
            // This used to be a valid assertion, but is no longer a valid
            // assertion if we have virtual-mapped parent region with
            // read-only privileges because then we have equivalence sets
            // which only overlap our region here
            //assert(it->first->region_node == node);
#endif
            to_send.insert(it->first, overlap);
            mask -= overlap;
            if (!mask)
              break;
          }
          if (!to_send.empty())
          {
            // Record that we have a refinement tracker
            new_subscriber = refinement_subscriptions[target_space].insert(
                                          target, to_send.get_valid_mask());
            const RtUserEvent done = Runtime::create_rt_user_event();
            Serializer rez;
            {
              RezCheck z(rez);
              rez.serialize(target);
              rez.serialize<bool>(new_subscriber);
              if (new_subscriber)
                rez.serialize(this);
              rez.serialize<size_t>(to_send.size());
              for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                    to_send.begin(); it != to_send.end(); it++)
              {
                rez.serialize(it->first->did);
                rez.serialize(it->second);
              }
              rez.serialize(done);
            }
            runtime->send_compute_equivalence_sets_response(target_space, rez);
            ready_events.insert(done);
          }
        }
        else if (target != this)
        {
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                equivalence_sets.begin(); it != equivalence_sets.end(); it++)
          {
            const FieldMask overlap = mask & it->second;
            if (!overlap)
              continue;
#ifdef DEBUG_LEGION
            // We should only be sending equivalence sets that we
            // know we are the owners of
            // This used to be a valid assertion, but is no longer a valid
            // assertion if we have virtual-mapped parent region with
            // read-only privileges because then we have equivalence sets
            // which only overlap our region here
            //assert(it->first->region_node == node);
#endif
            to_record.insert(it->first, overlap);
            mask -= overlap;
            if (!mask)
              break;
          }
          if (!to_record.empty())
            // Record that we have a refinement tracker
            new_subscriber = refinement_subscriptions[target_space].insert(
                                        target, to_record.get_valid_mask());
        }
      }
      if (!to_record.empty())
      {
        if (new_subscriber)
          target->record_subscription(this, runtime->address_space);
        for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
              to_record.begin(); it != to_record.end(); it++)
          target->record_equivalence_set(it->first, it->second);
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::compute_equivalence_sets(const ContextID ctx,
                                          IndexSpaceExpression *expr,
                                          EqSetTracker *target,
                                          const AddressSpaceID target_space,
                                          FieldMask mask, InnerContext *context,
                                          const UniqueID opid,
                                          const AddressSpaceID original_source,
                                          std::set<RtEvent> &ready_events,
                                          FieldMaskSet<PartitionNode> &children,
                                          FieldMask &parent_traversal,
                                          std::set<RtEvent> &deferral_events,
                                          const bool downward_only)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(node->is_region());
#endif
      FieldMask request_mask;
      RtUserEvent request_ready;
      RegionNode *region_node = node->as_region_node();
      if (downward_only)
      {
        AutoLock m_lock(manager_lock);
        // Check to see if we need to ask the context to finalize the
        // creation of any equivalence sets at this level
        // Check to see first if there are any pending computations
        // for disjoint complete equivalence sets
        if (!disjoint_complete_ready.empty())
        {
          std::vector<RtEvent> to_delete;
          for (LegionMap<RtEvent,FieldMask>::const_iterator it =
                disjoint_complete_ready.begin(); it != 
                disjoint_complete_ready.end(); it++)
          {
            if (it->first.has_triggered())
            {
              to_delete.push_back(it->first);
              continue;
            }
            if (it->second * mask)
              continue;
            deferral_events.insert(it->first);
            mask -= it->second;
            if (!mask)
              break;
          }
          if (!to_delete.empty())
          {
            for (std::vector<RtEvent>::const_iterator it = 
                  to_delete.begin(); it != to_delete.end(); it++)
              disjoint_complete_ready.erase(*it);
          }
        }
        // Now see if there are any fields left for which we still
        // need to perform any requests to finalize equivalence sets
        if (!!mask)
        {
          request_mask = mask - disjoint_complete;
          if (!!request_mask)
          {
            // We'll defer this computation until it is ready
            mask -= request_mask;
            request_ready = Runtime::create_rt_user_event();
            disjoint_complete_ready[request_ready] = request_mask;
            deferral_events.insert(request_ready);
          }
        }
      }
      // Release the lock before doing the call out to the context
      if (!!request_mask)
      {
#ifdef DEBUG_LEGION
        assert(request_ready.exists());
        assert(region_node->parent != NULL);
#endif
        // Record that this is now a disjoint complete child for invalidation
        // We propagate once when we make the pending refinement object and
        // again here because this might be a pending refinement from a remote
        // shard in a control replication context, so we have to do it again
        // in order to be sure that the parent knows about it
        region_node->parent->propagate_refinement(ctx,region_node,request_mask);
        // Ask the context to fill in the disjoint complete sets here
        if (context->finalize_disjoint_complete_sets(region_node, this,
                    request_mask, opid, original_source, request_ready) &&
            request_ready.has_triggered())
        {
          // Special case where the context did this right away
          // so we can restore the request fields and remove the
          // deferral event as we know that it no longer matters
          mask |= request_mask;
          deferral_events.erase(request_ready);
        }
        // If we don't have any local fields remaining then we are done
        if (!mask)
          return;
      }
      // If we have deferral events then save this traversal for another time
      if (!deferral_events.empty())
        return;
      bool new_subscriber = false;
      FieldMaskSet<EquivalenceSet> to_record;
      {
        // Do the local analysis on our owned equivalence sets
        AutoLock m_lock(manager_lock);
        if (!downward_only)
        {
          if (!disjoint_complete)
          {
            parent_traversal = mask;
            return;
          }
          parent_traversal = mask - disjoint_complete;
          if (!!parent_traversal)
          {
            mask -= parent_traversal;
            if (!mask)
              return;
          }
        }
        if (!disjoint_complete_children.empty())
        {
          const FieldMask children_overlap = mask & 
            disjoint_complete_children.get_valid_mask();
          if (!!children_overlap)
          {
            for (FieldMaskSet<RegionTreeNode>::const_iterator it = 
                  disjoint_complete_children.begin(); it !=
                  disjoint_complete_children.end(); it++)
            {
              const FieldMask overlap = mask & it->second;
              if (!overlap)
                continue;
#ifdef DEBUG_LEGION
              assert(!it->first->is_region());
#endif
              children.insert(it->first->as_partition_node(), overlap);
              mask -= overlap;
              if (!mask)
                return;
            }
          }
        }
        // At this point we're done with the symbolic analysis so we
        // can actually test the expression for emptiness
        if ((expr != node->as_region_node()->row_source) && expr->is_empty())
          return;
        // If we make it here then we should have equivalence sets for
        // all these remaining fields
#ifdef DEBUG_LEGION
        assert(!equivalence_sets.empty() ||
                region_node->row_source->is_empty());
        assert(!(mask - equivalence_sets.get_valid_mask()) ||
                region_node->row_source->is_empty());
#endif
        // If we have any pending disjoint complete ready events then 
        // we need to record dependences on them as well
        if (!disjoint_complete_ready.empty())
        {
          for (LegionMap<RtEvent,FieldMask>::const_iterator it =
                disjoint_complete_ready.begin(); it !=
                disjoint_complete_ready.end(); it++)
            if (!(mask * it->second))
              ready_events.insert(it->first); 
        }
#ifdef DEBUG_LEGION
        FieldMask observed_sets;
#endif
        if (target_space != runtime->address_space)
        {
          FieldMaskSet<EquivalenceSet> to_send;
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                equivalence_sets.begin(); it != equivalence_sets.end(); it++)
          {
            const FieldMask overlap = mask & it->second;
            if (!overlap)
              continue;
#ifdef DEBUG_LEGION
            // We should only be sending equivalence sets that we
            // know we are the owners of
            // This used to be a valid assertion, but is no longer a valid
            // assertion if we have virtual-mapped parent region with
            // read-only privileges because then we have equivalence sets
            // which only overlap our region here
            //assert(it->first->region_node == node);
            observed_sets |= overlap;
#endif
            to_send.insert(it->first, overlap);
          }
          if (!to_send.empty())
          {
            // Record that we have a refinement tracker
            new_subscriber = refinement_subscriptions[target_space].insert(
                                          target, to_send.get_valid_mask());
            const RtUserEvent done = Runtime::create_rt_user_event();
            Serializer rez;
            {
              RezCheck z(rez);
              rez.serialize(target);
              rez.serialize<bool>(new_subscriber);
              if (new_subscriber)
                rez.serialize(this);
              rez.serialize<size_t>(to_send.size());
              for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                    to_send.begin(); it != to_send.end(); it++)
              {
                rez.serialize(it->first->did);
                rez.serialize(it->second);
              }
              rez.serialize(done);
            }
            runtime->send_compute_equivalence_sets_response(target_space, rez);
            ready_events.insert(done);
            
          }
        }
        else if (target != this)
        {
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                equivalence_sets.begin(); it != equivalence_sets.end(); it++)
          {
            const FieldMask overlap = mask & it->second;
            if (!overlap)
              continue;
#ifdef DEBUG_LEGION
            // We should only be sending equivalence sets that we
            // know we are the owners of
            // This used to be a valid assertion, but is no longer a valid
            // assertion if we have virtual-mapped parent region with
            // read-only privileges because then we have equivalence sets
            // which only overlap our region here
            //assert(it->first->region_node == node);
            observed_sets |= overlap;
#endif
            to_record.insert(it->first, overlap);
          }
          if (!to_record.empty())
            // Record that we have a refinement tracker
            new_subscriber = refinement_subscriptions[target_space].insert(
                                        target, to_record.get_valid_mask());
        }
#ifdef DEBUG_LEGION
        else
          observed_sets = mask;
        assert(observed_sets == mask);
#endif
      }
      if (!to_record.empty())
      {
        if (new_subscriber)
          target->record_subscription(this, runtime->address_space);
        for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
              to_record.begin(); it != to_record.end(); it++)
          target->record_equivalence_set(it->first, it->second);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void VersionManager::handle_compute_equivalence_sets_response(
                   Deserializer &derez, Runtime *runtime, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      EqSetTracker *target;
      derez.deserialize(target);
      bool new_subscriber;
      derez.deserialize<bool>(new_subscriber);
      if (new_subscriber)
      {
        VersionManager *owner;
        derez.deserialize(owner);
        target->record_subscription(owner, source);
      }
      size_t num_sets;
      derez.deserialize(num_sets);
      std::set<RtEvent> ready_events;
      for (unsigned idx = 0; idx < num_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        FieldMask eq_mask;
        derez.deserialize(eq_mask);
        RtEvent ready;
        EquivalenceSet *set = 
          runtime->find_or_request_equivalence_set(did, ready);
        if (ready.exists() && !ready.has_triggered())
        {
          target->record_pending_equivalence_set(set, eq_mask);
          ready_events.insert(ready);
        }
        else
          target->record_equivalence_set(set, eq_mask);
      }
      RtUserEvent done_event;
      derez.deserialize(done_event);
      if (!ready_events.empty())
        Runtime::trigger_event(done_event, Runtime::merge_events(ready_events));
      else
        Runtime::trigger_event(done_event);
    }

    //--------------------------------------------------------------------------
    void VersionManager::record_refinement(EquivalenceSet *set, 
                                           const FieldMask &mask, 
                                           FieldMask &parent_mask)
    //--------------------------------------------------------------------------
    {
      AutoLock m_lock(manager_lock);
#ifdef DEBUG_LEGION
      assert(!!mask);
      // There should not be any other equivalence sets for these fields
      // This is a valid assertion in general, but not with control replication
      // where you can get two merge close ops updating subsets
      //assert(equivalence_sets.get_valid_mask() * mask);
#ifndef NDEBUG
      {
        FieldMaskSet<EquivalenceSet>::const_iterator finder =
          equivalence_sets.find(set);
        assert((finder == equivalence_sets.end()) ||
                (finder->second * mask));
      }
#endif
#endif
      if (equivalence_sets.insert(set, mask))
      {
        set->add_base_gc_ref(DISJOINT_COMPLETE_REF);
        set->add_base_resource_ref(VERSION_MANAGER_REF);
      }
      else
      {
        // Need to check whether we need to add the first valid reference
        // See if there were any previous fields that were valid
        FieldMaskSet<EquivalenceSet>::const_iterator finder =
          equivalence_sets.find(set);
#ifdef DEBUG_LEGION
        assert(finder != equivalence_sets.end());
#endif
        const FieldMask previous = finder->second - mask;
        if (previous * disjoint_complete)
          set->add_base_gc_ref(DISJOINT_COMPLETE_REF);
      }
      parent_mask = mask;
      if (!!disjoint_complete)
        parent_mask -= disjoint_complete;
      else
        add_node_disjoint_complete_ref();
      disjoint_complete |= mask;
    }

    //--------------------------------------------------------------------------
    void VersionManager::record_empty_refinement(const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      AutoLock m_lock(manager_lock);
#ifdef DEBUG_LEGION
      assert(!!mask);
      assert(node->is_region());
      assert(node->as_region_node()->row_source->is_empty());
#endif
      if (!disjoint_complete)
        add_node_disjoint_complete_ref();
      disjoint_complete |= mask;
    }

    //--------------------------------------------------------------------------
    void VersionManager::record_refinement(ShardedColorMap *map,
                                  const FieldMask &mask, FieldMask &parent_mask)
    //--------------------------------------------------------------------------
    {
      if (disjoint_complete_children_shards.insert(map, mask))
        map->add_reference();
      parent_mask = mask;
      if (!!disjoint_complete)
        parent_mask -= disjoint_complete;
      else
        add_node_disjoint_complete_ref();
      disjoint_complete |= mask;
    }

    //--------------------------------------------------------------------------
    void VersionManager::compute_equivalence_sets(IndexSpaceExpression *expr,
                                            const FieldMask &mask, 
                                            FieldMask &parent_traversal, 
                                            FieldMaskSet<RegionNode> &children,
                                            std::map<ShardID,
                                              LegionMap<LegionColor,
                                                  FieldMask> > &shard_children,
                                            const bool downward_only,
                                            const bool expr_covers) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!node->is_region());
#endif
      FieldMask local_mask;
      const FieldMask &child_mask = downward_only ? mask : local_mask;
      if (!downward_only)
      {
        local_mask = mask; 
        AutoLock m_lock(manager_lock,1,false/*exclusive*/);
        parent_traversal = mask - disjoint_complete;
        if (!!parent_traversal)
        {
          local_mask -= parent_traversal;
          if (!local_mask)
            return;
        }
      }
      // Find the colors of all the interfering children for this expression
      IndexPartNode *partition = node->as_partition_node()->row_source;
      std::vector<LegionColor> interfering_children;
      if (expr_covers)
      {
        // Expr covers so all the children interfere
        interfering_children.reserve(partition->total_children);
        for (ColorSpaceIterator itr(partition); itr; itr++)
          interfering_children.push_back(*itr);
      }
      else
        partition->find_interfering_children(expr, interfering_children);
#ifdef DEBUG_LEGION
      // These are sorted so we can use binary search for lookup
      assert(std::is_sorted(interfering_children.begin(),
            interfering_children.end()));
#endif
      // Do the look-up to find all the local children and any shard
      LegionMap<LegionColor,FieldMask> partial_colors;
      // children that we might need to find
      AutoLock m_lock(manager_lock,1,false/*exclusive*/);
      for (FieldMaskSet<RegionTreeNode>::const_iterator it =
            disjoint_complete_children.begin(); it !=
            disjoint_complete_children.end(); it++)
      {
        const LegionColor child_color = it->first->get_color();
        // If it's not a color that we need then we're done
        std::vector<LegionColor>::iterator finder =
          std::lower_bound(interfering_children.begin(), 
              interfering_children.end(), child_color);
        // We didn't find it if either of these are true
        if ((finder == interfering_children.end()) ||
            (*finder != child_color))
          continue;
        // Remove it from the vector since we don't need it anymore
        FieldMask overlap = child_mask & it->second;
        if (!overlap)
          continue;
        children.insert(it->first->as_region_node(), overlap);
        if (overlap != child_mask)
        {
          // If not all the fields are described then we better have
          // something that will tell us which shard to go to
          const FieldMask &remainder = child_mask - overlap;
#ifdef DEBUG_LEGION
          assert(!(remainder - 
                disjoint_complete_children_shards.get_valid_mask()));
#endif
          partial_colors[child_color] = remainder;
        }
        interfering_children.erase(finder);
        if (interfering_children.empty())
          break;
      }
      if (!interfering_children.empty())
      {
#ifdef DEBUG_LEGION
        assert(!(child_mask -
              disjoint_complete_children_shards.get_valid_mask()));
#endif
        // For all these colors we do them for all the child fields
        // since we didn't see them at all when looking at the
        // disjoint and complete children
        for (FieldMaskSet<ShardedColorMap>::const_iterator cit =
              disjoint_complete_children_shards.begin(); cit !=
              disjoint_complete_children_shards.end(); cit++)
        {
          const FieldMask overlap = child_mask & cit->second;
          if (!overlap)
            continue;
          for (std::vector<LegionColor>::const_iterator it =
                interfering_children.begin(); it !=
                interfering_children.end(); it++)
          {
            ShardID shard = cit->first->at(*it);
            shard_children[shard][*it] |= child_mask;
          }
        }
      }
      if (!partial_colors.empty())
      {
        for (LegionMap<LegionColor,FieldMask>::iterator cit =
              partial_colors.begin(); cit != partial_colors.end(); cit++)
        {
          for (FieldMaskSet<ShardedColorMap>::const_iterator it =
                disjoint_complete_children_shards.begin(); it !=
                disjoint_complete_children_shards.end(); it++)
          {
            const FieldMask overlap = it->second & cit->second;
            if (!overlap)
              continue;
            ShardID shard = it->first->at(cit->first);
            shard_children[shard][cit->first] |= overlap;
            cit->second -= overlap;
            if (!cit->second)
              break;
          }
#ifdef DEBUG_LEGION
          assert(!cit->second);
#endif
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::propagate_refinement(RegionTreeNode *child,
                                              const FieldMask &mask, 
                                              FieldMask &parent_mask) 
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!!mask);
#endif
      AutoLock m_lock(manager_lock);
      if (child != NULL)
      {
        // We can get duplicate propagations of refinements here because
        // of control replication. See VersionManager::compute_equivalence_sets 
        if (disjoint_complete_children.insert(child, mask))
          child->add_base_gc_ref(VERSION_MANAGER_REF);
      }
      parent_mask = mask;
      if (!!disjoint_complete)
        parent_mask -= disjoint_complete;
      else
        add_node_disjoint_complete_ref();
      disjoint_complete |= mask;
    }

    //--------------------------------------------------------------------------
    void VersionManager::invalidate_refinement(InnerContext &context,
                                      const FieldMask &mask, bool self,
                                      FieldMaskSet<RegionTreeNode> &to_traverse,
                                      LegionMap<AddressSpaceID,
                                        SubscriberInvalidations> &subscribers,
                                      std::vector<EquivalenceSet*> &to_release,
                                      bool nonexclusive_virtual_mapping_root)
    //--------------------------------------------------------------------------
    {
      AutoLock m_lock(manager_lock);     
 #ifdef DEBUG_LEGION
      assert(to_traverse.empty());
      assert(!(mask - disjoint_complete));
#endif
      FieldMask children_overlap;
      if (!disjoint_complete_children.empty())
        children_overlap = mask & disjoint_complete_children.get_valid_mask();
      if (self)
      {
        disjoint_complete -= mask;
        if (!disjoint_complete)
          remove_node_disjoint_complete_ref();
        // Invalidate any equivalence sets that we might have at this node
        if (!equivalence_sets.empty() && 
            !(mask * equivalence_sets.get_valid_mask()))
        {
          std::vector<EquivalenceSet*> to_delete;
#ifdef DEBUG_LEGION
          assert(node->is_region());
#endif
          FieldMask untrack_mask;
          // Handle the nasty case where there is just one equivalence set
          // and the index space is empty so the summary valid mask is aliased
          if ((equivalence_sets.size() == 1) &&
              node->as_region_node()->row_source->is_empty())
          {
            FieldMaskSet<EquivalenceSet>::iterator finder =
              equivalence_sets.begin();
            const FieldMask overlap = finder->second & mask;
            if (!!overlap)
            {
              finder.filter(overlap);
              untrack_mask |= overlap;
              // Remove this if the only remaining fields are not refinements
              if (!finder->second || (finder->second * disjoint_complete))
              {
                // Remove this entirely from the set
                // The version manager resource reference flows back
                to_delete.push_back(finder->first);
                // Record this to be released once all the effects are applied
                to_release.push_back(finder->first);
              }
            }
          }
          else
          {
            // This is the common case
            for (FieldMaskSet<EquivalenceSet>::iterator it = 
                  equivalence_sets.begin(); it != equivalence_sets.end(); it++)
            {
              // Skip any nodes that are not even part of a refinement 
              // Unless we are a non-exclusive virtual mapping root in
              // which case we do still want to invalidate these
              if ((it->first->region_node != node) && 
                  !nonexclusive_virtual_mapping_root)
                continue;
              FieldMask overlap = it->second & mask;
              if (!overlap)
                continue;
              // If we have disjoint complete children then we do not actually
              // own these equivalence sets (we're just another observer) so
              // we can't actually remove them, just ignore them
              if (!!children_overlap)
              {
                overlap -= children_overlap;
                if (!overlap)
                  continue;
              }
              untrack_mask |= overlap; 
              it.filter(overlap);
              if (!it->second)
              {
                to_delete.push_back(it->first);
                // Record this to be released once all the effects are applied
                to_release.push_back(it->first);
              }
            }
          }
          if (!!untrack_mask && !refinement_subscriptions.empty())
            filter_refinement_subscriptions(untrack_mask, subscribers);
          if (!to_delete.empty())
          {
            for (std::vector<EquivalenceSet*>::const_iterator it =
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              equivalence_sets.erase(*it);
              // Remove our version manager reference here, this shouldn't
              // end up deleting the equivalence set though since the 
              // to_release data structure still holds a disjoin-complete ref
              if ((*it)->remove_base_resource_ref(VERSION_MANAGER_REF))
                assert(false);
            }
          }
          equivalence_sets.tighten_valid_mask();
        }
      }
      if (!!children_overlap)
      {
        std::vector<RegionTreeNode*> to_delete;
        for (FieldMaskSet<RegionTreeNode>::iterator it = 
              disjoint_complete_children.begin(); it != 
              disjoint_complete_children.end(); it++)
        {
          const FieldMask overlap = mask & it->second;
          if (!overlap)
            continue;
          it.filter(overlap);
          to_traverse.insert(it->first, overlap);
          // Add a reference for to_traverse
          it->first->add_base_gc_ref(VERSION_MANAGER_REF);
          if (!it->second)
            to_delete.push_back(it->first);
        }
        if (!to_delete.empty())
        {
          for (std::vector<RegionTreeNode*>::const_iterator it = 
                to_delete.begin(); it != to_delete.end(); it++)
          {
            disjoint_complete_children.erase(*it);
            if ((*it)->remove_base_gc_ref(VERSION_MANAGER_REF))
              delete (*it);
          }
        }
        disjoint_complete_children.tighten_valid_mask();
      }
      if (!disjoint_complete_children_shards.empty() &&
          !(mask * disjoint_complete_children_shards.get_valid_mask()))
      {
        std::vector<ShardedColorMap*> to_delete;
        for (FieldMaskSet<ShardedColorMap>::iterator it =
              disjoint_complete_children_shards.begin(); it !=
              disjoint_complete_children_shards.end(); it++)
        {
          it.filter(mask);
          if (!it->second)
            to_delete.push_back(it->first);
        }
        if (!to_delete.empty())
        {
          for (std::vector<ShardedColorMap*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
          {
            disjoint_complete_children_shards.erase(*it);
            if ((*it)->remove_reference())
              delete (*it);
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::filter_refinement_subscriptions(const FieldMask &mask,
                 LegionMap<AddressSpaceID,SubscriberInvalidations> &subscribers)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(subscribers.empty());
#endif
      for (LegionMap<AddressSpaceID,FieldMaskSet<EqSetTracker> >::iterator 
            ait = refinement_subscriptions.begin(); 
            ait != refinement_subscriptions.end(); /*nothing*/)
      {
        const FieldMask space_overlap = ait->second.get_valid_mask() & mask;
        if (!space_overlap)
        {
          ait++;
          continue;
        }
        SubscriberInvalidations &to_untrack = subscribers[ait->first];
        to_untrack.delete_all = true;
        if (space_overlap != ait->second.get_valid_mask())
        {
          std::vector<EqSetTracker*> to_delete;
          for (FieldMaskSet<EqSetTracker>::iterator it =
                ait->second.begin(); it != ait->second.end(); it++)
          {
            const FieldMask overlap = it->second & space_overlap;
            if (!overlap)
              continue;
            to_untrack.subscribers.insert(it->first, overlap);
            it.filter(overlap);
            if (!it->second)
            {
              to_delete.push_back(it->first);
              if (!to_untrack.delete_all)
                to_untrack.finished.push_back(it->first);
            }
            else if (to_untrack.delete_all)
            {
              to_untrack.delete_all = false;
              if (to_untrack.subscribers.size() > 1)
              {
                to_untrack.finished.reserve(to_untrack.subscribers.size() - 1);
                for (FieldMaskSet<EqSetTracker>::const_iterator sit =
                      to_untrack.subscribers.begin(); sit !=
                      to_untrack.subscribers.end(); sit++)
                {
                  if (sit->first == it->first)
                    continue;
                  to_untrack.finished.push_back(sit->first);
                }
              }
            }
          }
          for (std::vector<EqSetTracker*>::const_iterator it =
                to_delete.begin(); it != to_delete.end(); it++)
            ait->second.erase(*it);
          if (ait->second.empty())
          {
            LegionMap<AddressSpaceID,FieldMaskSet<EqSetTracker> >::iterator
              delete_it = ait++;
            refinement_subscriptions.erase(delete_it);
          }
          else
          {
            ait->second.tighten_valid_mask();
            ait++;
          }
        }
        else
        {
          to_untrack.subscribers.swap(ait->second);
          LegionMap<AddressSpaceID,FieldMaskSet<EqSetTracker> >::iterator
            delete_it = ait++;
          refinement_subscriptions.erase(delete_it);
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::merge(VersionManager &src, 
                             std::set<RegionTreeNode*> &to_traverse,
                             LegionMap<AddressSpaceID,
                              SubscriberInvalidations> &subscribers,
                             const std::vector<ShardID> *shard_to_shard_mapping)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(node == src.node);
      assert(!!src.disjoint_complete);
#endif
      if (!src.equivalence_sets.empty())
      {
        FieldMask untrack_mask;
        for (FieldMaskSet<EquivalenceSet>::iterator it = 
              src.equivalence_sets.begin(); it != 
              src.equivalence_sets.end(); it++)
        {
          if (it->first->region_node != node)
          {
            if (it->first->remove_base_resource_ref(VERSION_MANAGER_REF))
              delete it->first;
            continue;
          }
#ifdef DEBUG_LEGION
          assert(!(it->second - src.disjoint_complete));
#endif
          untrack_mask |= it->second; 
          // Figure out whether we've already recorded this equivalence set
          FieldMaskSet<EquivalenceSet>::iterator finder = 
            equivalence_sets.find(it->first);
          if (finder != equivalence_sets.end())
          {
            // Check to see if we have recorded a valid reference yet
            // for this equivalence set
            if (!(finder->second * disjoint_complete))
            {
              // Remove the duplicate references
              if (it->first->remove_base_gc_ref(DISJOINT_COMPLETE_REF))
                assert(false); // should never end up deleting this
            }
            finder.merge(it->second);
            if (it->first->remove_base_resource_ref(VERSION_MANAGER_REF))
              assert(false); // should never end up deleting this
          }
          else
            // Did not have this before so just insert it
            // References flow back
            equivalence_sets.insert(it->first, it->second);
        }
        if (!!untrack_mask && !refinement_subscriptions.empty())
            filter_refinement_subscriptions(untrack_mask, subscribers);
        src.equivalence_sets.clear();
      }
      // The disjoint complete node ref can propagate here but we need
      // to deduplicate if we already have a reference
      if (!!disjoint_complete && !!src.disjoint_complete)
        src.remove_node_disjoint_complete_ref();
      disjoint_complete |= src.disjoint_complete;
      src.disjoint_complete.clear();
      if (!src.disjoint_complete_children.empty())
      {
        std::vector<LegionColor> new_children;
        for (FieldMaskSet<RegionTreeNode>::const_iterator it = 
              src.disjoint_complete_children.begin(); it !=
              src.disjoint_complete_children.end(); it++)
        {
          to_traverse.insert(it->first);
          // Remove duplicate references if it is already there
          // otherwise the references flow to the destination
          if (!disjoint_complete_children.insert(it->first, it->second))
          {
            it->first->remove_base_gc_ref(VERSION_MANAGER_REF);
            if (!disjoint_complete_children_shards.empty())
              new_children.push_back(it->first->get_color());
          }
        }
        src.disjoint_complete_children.clear();
        if (!new_children.empty())
        {
          FieldMaskSet<ShardedColorMap> new_maps;
          for (FieldMaskSet<ShardedColorMap>::const_iterator it =
                disjoint_complete_children_shards.begin(); it !=
                disjoint_complete_children_shards.end(); it++)
          {
            std::unordered_map<LegionColor,ShardID> new_mapping = 
              it->first->color_shards;
            for (std::vector<LegionColor>::const_iterator cit =
                  new_children.begin(); cit != new_children.end(); cit++)
              new_mapping.erase(*cit);
            if (!new_mapping.empty())
            {
              ShardedColorMap *new_map = 
                new ShardedColorMap(std::move(new_mapping));
              new_map->add_reference();
              new_maps.insert(new_map, it->second);
            }
            if (it->first->remove_reference())
              delete it->first;
          }
          disjoint_complete_children_shards.swap(new_maps);
        }
      }
      if (!src.disjoint_complete_children_shards.empty())
      {
        if (shard_to_shard_mapping != NULL)
        {
          // Need to convert these on the way over
          std::vector<LegionColor> child_colors;
          for (FieldMaskSet<ShardedColorMap>::iterator sit =
                src.disjoint_complete_children_shards.begin(); sit !=
                src.disjoint_complete_children_shards.end(); sit++)
          {
            sit.filter(disjoint_complete_children_shards.get_valid_mask());
            if (!!sit->second)
            {
              // Need to convert on the way over between shards
              // and also check for any children that we already have
              std::unordered_map<LegionColor,ShardID> new_mapping;
              for (std::unordered_map<LegionColor,ShardID>::const_iterator it =
                    sit->first->color_shards.begin(); it !=
                    sit->first->color_shards.end(); it++)
              {
                if (child_colors.empty() && !disjoint_complete_children.empty())
                {
                  child_colors.reserve(disjoint_complete_children.size());
                  for (FieldMaskSet<RegionTreeNode>::const_iterator it =
                        disjoint_complete_children.begin(); it !=
                        disjoint_complete_children.end(); it++)
                    child_colors.push_back(it->first->get_color());
                  std::sort(child_colors.begin(), child_colors.end());
                }
                // If we already have the child then we no longer need it
                if (std::binary_search(child_colors.begin(),
                      child_colors.end(), it->first))
                  continue;
                if (!shard_to_shard_mapping->empty())
                {
#ifdef DEBUG_LEGION
                  assert(it->second < shard_to_shard_mapping->size());
#endif
                  new_mapping[it->first] = 
                    shard_to_shard_mapping->at(it->second);
                }
                else
                {
                  // identity case
                  new_mapping[it->first] = it->second;
                }
              }
              if (!new_mapping.empty())
              {
                ShardedColorMap *new_map = 
                  new ShardedColorMap(std::move(new_mapping)); 
                new_map->add_reference();
                disjoint_complete_children_shards.insert(new_map, sit->second);
              }
            }
            if (sit->first->remove_reference())
              delete sit->first;
          }
        }
        else
        {
          // We can just clear these out
          for (FieldMaskSet<ShardedColorMap>::iterator it =
                src.disjoint_complete_children_shards.begin(); it !=
                src.disjoint_complete_children_shards.end(); it++)
            if (it->first->remove_reference())
              delete it->first;
        }
        src.disjoint_complete_children_shards.clear();
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::convert(VersionManager &src,
                             std::set<RegionTreeNode*> &to_traverse,
                             LegionMap<AddressSpaceID,
                               SubscriberInvalidations> &subscribers,
                             const std::vector<ShardID> *shard_to_shard_mapping)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(src.node == node);
      assert(!disjoint_complete);
      assert(!!src.disjoint_complete || (src.node->get_parent() == NULL));
      assert(equivalence_sets.empty());
      assert(disjoint_complete_children.empty());
      assert(disjoint_complete_children_shards.empty());
#endif
      // The disjoint complete node ref can propagate here but we need
      // to deduplicate if we already have a reference since it's about
      // to be overwritten by the reference from the src
      if (!!disjoint_complete)
        remove_node_disjoint_complete_ref();
      disjoint_complete = src.disjoint_complete;
      src.disjoint_complete.clear();
      if (!src.equivalence_sets.empty())
      {
        FieldMask untrack_mask;
        for (FieldMaskSet<EquivalenceSet>::iterator it = 
              src.equivalence_sets.begin(); it != 
              src.equivalence_sets.end(); it++)
        {
          if (it->first->region_node != node)
          {
            if (it->first->remove_base_resource_ref(VERSION_MANAGER_REF))
              delete it->first;
            continue;
          }
#ifdef DEBUG_LEGION
          assert(!(it->second - disjoint_complete));
#endif
          // reference flows back
          if (!equivalence_sets.insert(it->first, it->second))
            assert(false); // should never already be there
          untrack_mask |= it->second;
        }
        if (!!untrack_mask && !refinement_subscriptions.empty())
          filter_refinement_subscriptions(untrack_mask, subscribers);
        src.equivalence_sets.clear();
      }
      disjoint_complete_children.swap(src.disjoint_complete_children);
      for (FieldMaskSet<RegionTreeNode>::const_iterator it = 
            disjoint_complete_children.begin(); it != 
            disjoint_complete_children.end(); it++)
        to_traverse.insert(it->first);
      disjoint_complete_children_shards.swap(
          src.disjoint_complete_children_shards);
      if (!src.disjoint_complete_children_shards.empty())
      {
        if ((shard_to_shard_mapping == NULL) ||
            !shard_to_shard_mapping->empty())
        {
          for (FieldMaskSet<ShardedColorMap>::iterator sit =
                src.disjoint_complete_children_shards.begin(); sit !=
                src.disjoint_complete_children_shards.end(); sit++)
          {
            if (shard_to_shard_mapping != NULL)
            {
              std::unordered_map<LegionColor,ShardID> new_mapping;
              for (std::unordered_map<LegionColor,ShardID>::const_iterator it =
                    sit->first->color_shards.begin(); it !=
                    sit->first->color_shards.end(); it++)
              {
#ifdef DEBUG_LEGION
                assert(it->second < shard_to_shard_mapping->size());
#endif
                new_mapping[it->first] = shard_to_shard_mapping->at(it->second);
              }
              ShardedColorMap *new_map = 
                new ShardedColorMap(std::move(new_mapping));
              new_map->add_reference();
              disjoint_complete_children_shards.insert(new_map, sit->second);
            }
            if (sit->first->remove_reference())
              delete sit->first;
          }
          src.disjoint_complete_children_shards.clear();
        }
        else // identity case
          src.disjoint_complete_children_shards.swap(
              disjoint_complete_children_shards);
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::pack_manager(Serializer &rez, const bool invalidate,
                          std::map<LegionColor,RegionTreeNode*> &to_traverse,
                          LegionMap<AddressSpaceID,
                            SubscriberInvalidations> &subscribers)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!!disjoint_complete || 
          (node->is_region() && (node->as_region_node()->parent == NULL)));
#endif
      // No need for the lock here, should not be racing with anyone
      if (!equivalence_sets.empty())
      {
        const FieldMask eq_overlap = 
          equivalence_sets.get_valid_mask() & disjoint_complete;
        FieldMask untrack_mask;
        if (eq_overlap == disjoint_complete)
        {
          // We're sending all the equivalence sets
          rez.serialize<size_t>(equivalence_sets.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                equivalence_sets.begin(); it != equivalence_sets.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
            // Add a remote valid reference on these nodes to keep
            // them live until we can add on remotely.
            it->first->pack_global_ref();
            if (invalidate)
            {
              it->first->remove_base_gc_ref(DISJOINT_COMPLETE_REF);
              untrack_mask |= it->second; 
              if (it->first->remove_base_resource_ref(VERSION_MANAGER_REF))
                delete it->first;
            }
          }
        }
        else if (!!eq_overlap)
        {
          // Count how many equivalence sets we need to send back
          std::vector<EquivalenceSet*> to_send;
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                equivalence_sets.begin(); it != equivalence_sets.end(); it++)
          {
            if (it->first->region_node != node)
            {
              if (invalidate && 
                  it->first->remove_base_resource_ref(VERSION_MANAGER_REF))
                delete it->first;
            }
            else
            {
              to_send.push_back(it->first);
              // Add a remote valid reference on these nodes to keep
              // them live until we can add on remotely.
              it->first->pack_global_ref();
              if (invalidate)
              {
                it->first->remove_base_gc_ref(DISJOINT_COMPLETE_REF);
                untrack_mask |= it->second;
                if (it->first->remove_base_resource_ref(VERSION_MANAGER_REF))
                  delete it->first;
              }
            }
          }
          rez.serialize<size_t>(to_send.size());
          for (std::vector<EquivalenceSet*>::const_iterator it = 
                to_send.begin(); it != to_send.end(); it++)
          {
            rez.serialize((*it)->did);
            rez.serialize(equivalence_sets[*it]);
          }
        }
        else if (invalidate)
        {
          // Invalidate all the equivalence sets since none of them are going
          rez.serialize<size_t>(0);
          for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
                equivalence_sets.begin(); it != equivalence_sets.end(); it++)
          {
#ifdef DEBUG_LEGION
            assert(node != it->first->region_node);
#endif
            if (it->first->remove_base_resource_ref(VERSION_MANAGER_REF))
              delete it->first;
          }
        }
        if (!!untrack_mask && !refinement_subscriptions.empty())
          filter_refinement_subscriptions(untrack_mask, subscribers);
      }
      else
        rez.serialize<size_t>(0);
      if (!!disjoint_complete)
        node->get_row_source()->pack_valid_ref();
      rez.serialize(disjoint_complete);
      rez.serialize<size_t>(disjoint_complete_children.size());
      for (FieldMaskSet<RegionTreeNode>::const_iterator it = 
            disjoint_complete_children.begin(); it !=
            disjoint_complete_children.end(); it++)
      {
        const LegionColor child_color = it->first->get_color();
        rez.serialize(child_color);
        rez.serialize(it->second);
        to_traverse[child_color] = it->first;
        // Return reference for to_traverse
        it->first->add_base_resource_ref(VERSION_MANAGER_REF);
        if (invalidate && 
            it->first->remove_base_gc_ref(VERSION_MANAGER_REF))
          assert(false); // should never get here
      }
      rez.serialize<size_t>(disjoint_complete_children_shards.size());
      for (FieldMaskSet<ShardedColorMap>::const_iterator it =
            disjoint_complete_children_shards.begin(); it !=
            disjoint_complete_children_shards.end(); it++)
      {
        it->first->pack(rez);
        it->second.serialize(rez);
        if (invalidate && it->first->remove_reference())
          delete it->first;
      }
      if (invalidate)
      {
        if (!!disjoint_complete)
          remove_node_disjoint_complete_ref();
        disjoint_complete.clear();
        disjoint_complete_children.clear();
        disjoint_complete_children_shards.clear();
        equivalence_sets.clear();
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::unpack_manager(Deserializer &derez, 
                             std::map<LegionColor,RegionTreeNode*> &to_traverse)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!disjoint_complete);
      assert(disjoint_complete_children.empty());
      assert(equivalence_sets.empty());
#endif
      // No need for the lock here, we should not be racing with anyone
      size_t num_equivalence_sets;
      derez.deserialize(num_equivalence_sets);
      std::set<RtEvent> ready_events;
      for (unsigned idx = 0; idx < num_equivalence_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        FieldMask eq_mask;
        derez.deserialize(eq_mask);
        RtEvent ready_event;
        EquivalenceSet *set = 
          runtime->find_or_request_equivalence_set(did, ready_event);
        equivalence_sets.insert(set, eq_mask);
        if (ready_event.exists())
          ready_events.insert(ready_event);
      }
      derez.deserialize(disjoint_complete);
      if (!!disjoint_complete)
      {
        add_node_disjoint_complete_ref();
        node->get_row_source()->unpack_valid_ref();
      }
      size_t num_children;
      derez.deserialize(num_children);
      for (unsigned idx = 0; idx < num_children; idx++)
      {
        LegionColor child_color;
        derez.deserialize(child_color);
        FieldMask child_mask;
        derez.deserialize(child_mask);
        RegionTreeNode *child = node->get_tree_child(child_color);
        disjoint_complete_children.insert(child, child_mask);
        child->add_base_gc_ref(VERSION_MANAGER_REF);
        to_traverse[child_color] = child;
      }
      size_t num_shard_children;
      derez.deserialize(num_shard_children);
      for (unsigned idx = 0; idx < num_shard_children; idx++)
      {
        ShardedColorMap *map = ShardedColorMap::unpack(derez);
        FieldMask mask;
        mask.deserialize(derez);
        if (disjoint_complete_children_shards.insert(map, mask))
          map->add_reference();
      }
      if (!ready_events.empty())
      {
        const RtEvent wait_on = Runtime::merge_events(ready_events);
        if (wait_on.exists() && !wait_on.has_triggered())
          wait_on.wait();
      }
      // Update the references on all our equivalence sets
      for (FieldMaskSet<EquivalenceSet>::const_iterator it = 
            equivalence_sets.begin(); it != equivalence_sets.end(); it++)
      {
#ifdef DEBUG_LEGION
        assert(it->first->region_node == node);
#endif
        it->first->add_base_gc_ref(DISJOINT_COMPLETE_REF);
        it->first->add_base_resource_ref(VERSION_MANAGER_REF);
        it->first->unpack_global_ref();
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::print_physical_state(RegionTreeNode *node,
                                              const FieldMask &capture_mask,
                                              TreeStateLogger *logger)
    //--------------------------------------------------------------------------
    {
      logger->log("Equivalence Sets:");
      logger->down();
      // TODO: log equivalence sets
      assert(false);
      logger->up();
    } 
#endif 

    //--------------------------------------------------------------------------
    /*static*/ void VersionManager::handle_finalize_output_eq_set(
                                                               const void *args)
    //--------------------------------------------------------------------------
    {
      const FinalizeOutputEquivalenceSetArgs *fargs =
        (const FinalizeOutputEquivalenceSetArgs*)args;
      RtEvent done = fargs->proxy_this->finalize_output_equivalence_set(
        fargs->set, fargs->context, fargs->parent_req_index, fargs->root_space);
      Runtime::trigger_event(fargs->done_event, done);
      if (fargs->set->remove_base_gc_ref(META_TASK_REF))
        delete fargs->set;
    }

    /////////////////////////////////////////////////////////////
    // RegionTreePath 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RegionTreePath::RegionTreePath(void) 
      : min_depth(0), max_depth(0)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    void RegionTreePath::initialize(unsigned min, unsigned max)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(min <= max);
#endif
      min_depth = min;
      max_depth = max;
      path.resize(max_depth+1, INVALID_COLOR);
    }

    //--------------------------------------------------------------------------
    void RegionTreePath::register_child(unsigned depth, 
                                        const LegionColor color)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(min_depth <= depth);
      assert(depth <= max_depth);
#endif
      path[depth] = color;
    }

    //--------------------------------------------------------------------------
    void RegionTreePath::clear(void)
    //--------------------------------------------------------------------------
    {
      path.clear();
      min_depth = 0;
      max_depth = 0;
    }

#ifdef DEBUG_LEGION
    //--------------------------------------------------------------------------
    bool RegionTreePath::has_child(unsigned depth) const
    //--------------------------------------------------------------------------
    {
      assert(min_depth <= depth);
      assert(depth <= max_depth);
      return (path[depth] != INVALID_COLOR);
    }

    //--------------------------------------------------------------------------
    LegionColor RegionTreePath::get_child(unsigned depth) const
    //--------------------------------------------------------------------------
    {
      assert(min_depth <= depth);
      assert(depth <= max_depth);
      assert(has_child(depth));
      return path[depth];
    }
#endif

    /////////////////////////////////////////////////////////////
    // InstanceRef 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    InstanceRef::InstanceRef(bool comp)
      : manager(NULL), local(true)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    InstanceRef::InstanceRef(const InstanceRef &rhs)
      : valid_fields(rhs.valid_fields), manager(rhs.manager), local(rhs.local)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    InstanceRef::InstanceRef(InstanceManager *man, const FieldMask &m)
      : valid_fields(m), manager(man), local(true)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    InstanceRef::~InstanceRef(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    InstanceRef& InstanceRef::operator=(const InstanceRef &rhs)
    //--------------------------------------------------------------------------
    {
      valid_fields = rhs.valid_fields;
      local = rhs.local;
      manager = rhs.manager;
      return *this;
    }

    //--------------------------------------------------------------------------
    bool InstanceRef::operator==(const InstanceRef &rhs) const
    //--------------------------------------------------------------------------
    {
      if (valid_fields != rhs.valid_fields)
        return false;
      if (manager != rhs.manager)
        return false;
      return true;
    }

    //--------------------------------------------------------------------------
    bool InstanceRef::operator!=(const InstanceRef &rhs) const
    //--------------------------------------------------------------------------
    {
      return !(*this == rhs);
    }

    //--------------------------------------------------------------------------
    MappingInstance InstanceRef::get_mapping_instance(void) const
    //--------------------------------------------------------------------------
    {
      return MappingInstance(manager);
    }

    //--------------------------------------------------------------------------
    bool InstanceRef::is_virtual_ref(void) const
    //--------------------------------------------------------------------------
    {
      if (manager == NULL)
        return true;
      return manager->is_virtual_manager(); 
    }

    //--------------------------------------------------------------------------
    void InstanceRef::add_resource_reference(ReferenceSource source) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(manager != NULL);
#endif
      manager->add_base_resource_ref(source);
    }

    //--------------------------------------------------------------------------
    void InstanceRef::remove_resource_reference(ReferenceSource source) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(manager != NULL);
#endif
      if (manager->remove_base_resource_ref(source))
        delete manager;
    }

    //--------------------------------------------------------------------------
    bool InstanceRef::acquire_valid_reference(ReferenceSource source) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(manager != NULL);
#endif
      return manager->as_physical_manager()->acquire_instance(source);
    }

    //--------------------------------------------------------------------------
    void InstanceRef::add_valid_reference(ReferenceSource source) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(manager != NULL);
#endif
      manager->as_physical_manager()->add_base_valid_ref(source);
    }

    //--------------------------------------------------------------------------
    void InstanceRef::remove_valid_reference(ReferenceSource source) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(manager != NULL);
#endif
      if (manager->as_physical_manager()->remove_base_valid_ref(source))
        delete manager;
    }

    //--------------------------------------------------------------------------
    Memory InstanceRef::get_memory(void) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(manager != NULL);
#endif
      if (!manager->is_physical_manager())
        return Memory::NO_MEMORY;
      return manager->as_physical_manager()->get_memory();
    }

    //--------------------------------------------------------------------------
    PhysicalManager* InstanceRef::get_physical_manager(void) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(manager != NULL);
      assert(manager->is_physical_manager());
#endif
      return manager->as_physical_manager();
    }

    //--------------------------------------------------------------------------
    bool InstanceRef::is_field_set(FieldID fid) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(manager != NULL);
#endif
      unsigned index = manager->field_space_node->get_field_index(fid);
      return valid_fields.is_set(index);
    }

    //--------------------------------------------------------------------------
    LegionRuntime::Accessor::RegionAccessor<
      LegionRuntime::Accessor::AccessorType::Generic> 
        InstanceRef::get_accessor(void) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(manager != NULL);
#endif
      return manager->get_accessor();
    }

    //--------------------------------------------------------------------------
    LegionRuntime::Accessor::RegionAccessor<
      LegionRuntime::Accessor::AccessorType::Generic>
        InstanceRef::get_field_accessor(FieldID fid) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(manager != NULL);
#endif
      return manager->get_field_accessor(fid);
    }

    //--------------------------------------------------------------------------
    void InstanceRef::pack_reference(Serializer &rez) const
    //--------------------------------------------------------------------------
    {
      rez.serialize(valid_fields);
      if (manager != NULL)
        rez.serialize(manager->did);
      else
        rez.serialize<DistributedID>(0);
    }

    //--------------------------------------------------------------------------
    void InstanceRef::unpack_reference(Runtime *runtime,
                                       Deserializer &derez, RtEvent &ready)
    //--------------------------------------------------------------------------
    {
      derez.deserialize(valid_fields);
      DistributedID did;
      derez.deserialize(did);
      if (did == 0)
        return;
      manager = runtime->find_or_request_instance_manager(did, ready);
      local = false;
    } 

    /////////////////////////////////////////////////////////////
    // InstanceSet 
    /////////////////////////////////////////////////////////////
    
    //--------------------------------------------------------------------------
    InstanceSet::CollectableRef& InstanceSet::CollectableRef::operator=(
                                         const InstanceSet::CollectableRef &rhs)
    //--------------------------------------------------------------------------
    {
      valid_fields = rhs.valid_fields;
      local = rhs.local;
      manager = rhs.manager;
      return *this;
    }

    //--------------------------------------------------------------------------
    InstanceSet::InstanceSet(size_t init_size /*=0*/)
      : single((init_size <= 1)), shared(false)
    //--------------------------------------------------------------------------
    {
      if (init_size == 0)
        refs.single = NULL;
      else if (init_size == 1)
      {
        refs.single = new CollectableRef();
        refs.single->add_reference();
      }
      else
      {
        refs.multi = new InternalSet(init_size);
        refs.multi->add_reference();
      }
    }

    //--------------------------------------------------------------------------
    InstanceSet::InstanceSet(const InstanceSet &rhs)
      : single(rhs.single)
    //--------------------------------------------------------------------------
    {
      // Mark that the other one is sharing too
      if (single)
      {
        refs.single = rhs.refs.single;
        if (refs.single == NULL)
        {
          shared = false;
          return;
        }
        shared = true;
        rhs.shared = true;
        refs.single->add_reference();
      }
      else
      {
        refs.multi = rhs.refs.multi;
        shared = true;
        rhs.shared = true;
        refs.multi->add_reference();
      }
    }

    //--------------------------------------------------------------------------
    InstanceSet::~InstanceSet(void)
    //--------------------------------------------------------------------------
    {
      if (single)
      {
        if ((refs.single != NULL) && refs.single->remove_reference())
          delete (refs.single);
      }
      else
      {
        if (refs.multi->remove_reference())
          delete refs.multi;
      }
    }

    //--------------------------------------------------------------------------
    InstanceSet& InstanceSet::operator=(const InstanceSet &rhs)
    //--------------------------------------------------------------------------
    {
      // See if we need to delete our current one
      if (single)
      {
        if ((refs.single != NULL) && refs.single->remove_reference())
          delete (refs.single);
      }
      else
      {
        if (refs.multi->remove_reference())
          delete refs.multi;
      }
      // Now copy over the other one
      single = rhs.single; 
      if (single)
      {
        refs.single = rhs.refs.single;
        if (refs.single != NULL)
        {
          shared = true;
          rhs.shared = true;
          refs.single->add_reference();
        }
        else
          shared = false;
      }
      else
      {
        refs.multi = rhs.refs.multi;
        shared = true;
        rhs.shared = true;
        refs.multi->add_reference();
      }
      return *this;
    }

    //--------------------------------------------------------------------------
    void InstanceSet::make_copy(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(shared);
#endif
      if (single)
      {
        if (refs.single != NULL)
        {
          CollectableRef *next = 
            new CollectableRef(*refs.single);
          next->add_reference();
          if (refs.single->remove_reference())
            delete (refs.single);
          refs.single = next;
        }
      }
      else
      {
        InternalSet *next = new InternalSet(*refs.multi);
        next->add_reference();
        if (refs.multi->remove_reference())
          delete refs.multi;
        refs.multi = next;
      }
      shared = false;
    }

    //--------------------------------------------------------------------------
    bool InstanceSet::operator==(const InstanceSet &rhs) const
    //--------------------------------------------------------------------------
    {
      if (single != rhs.single)
        return false;
      if (single)
      {
        if (refs.single == rhs.refs.single)
          return true;
        if (((refs.single == NULL) && (rhs.refs.single != NULL)) ||
            ((refs.single != NULL) && (rhs.refs.single == NULL)))
          return false;
        return ((*refs.single) == (*rhs.refs.single));
      }
      else
      {
        if (refs.multi->vector.size() != rhs.refs.multi->vector.size())
          return false;
        for (unsigned idx = 0; idx < refs.multi->vector.size(); idx++)
        {
          if (refs.multi->vector[idx] != rhs.refs.multi->vector[idx])
            return false;
        }
        return true;
      }
    }

    //--------------------------------------------------------------------------
    bool InstanceSet::operator!=(const InstanceSet &rhs) const
    //--------------------------------------------------------------------------
    {
      return !((*this) == rhs);
    }

    //--------------------------------------------------------------------------
    InstanceRef& InstanceSet::operator[](unsigned idx)
    //--------------------------------------------------------------------------
    {
      if (shared)
        make_copy();
      if (single)
      {
#ifdef DEBUG_LEGION
        assert(idx == 0);
        assert(refs.single != NULL);
#endif
        return *(refs.single);
      }
#ifdef DEBUG_LEGION
      assert(idx < refs.multi->vector.size());
#endif
      return refs.multi->vector[idx];
    }

    //--------------------------------------------------------------------------
    const InstanceRef& InstanceSet::operator[](unsigned idx) const
    //--------------------------------------------------------------------------
    {
      // No need to make a copy if shared here since this is read-only
      if (single)
      {
#ifdef DEBUG_LEGION
        assert(idx == 0);
        assert(refs.single != NULL);
#endif
        return *(refs.single);
      }
#ifdef DEBUG_LEGION
      assert(idx < refs.multi->vector.size());
#endif
      return refs.multi->vector[idx];
    }

    //--------------------------------------------------------------------------
    bool InstanceSet::empty(void) const
    //--------------------------------------------------------------------------
    {
      if (single && (refs.single == NULL))
        return true;
      else if (!single && refs.multi->empty())
        return true;
      return false;
    }

    //--------------------------------------------------------------------------
    size_t InstanceSet::size(void) const
    //--------------------------------------------------------------------------
    {
      if (single)
      {
        if (refs.single == NULL)
          return 0;
        return 1;
      }
      if (refs.multi == NULL)
        return 0;
      return refs.multi->vector.size();
    }

    //--------------------------------------------------------------------------
    void InstanceSet::resize(size_t new_size)
    //--------------------------------------------------------------------------
    {
      if (single)
      {
        if (new_size == 0)
        {
          if ((refs.single != NULL) && refs.single->remove_reference())
            delete (refs.single);
          refs.single = NULL;
          shared = false;
        }
        else if (new_size > 1)
        {
          // Switch to multi
          InternalSet *next = new InternalSet(new_size);
          if (refs.single != NULL)
          {
            next->vector[0] = *(refs.single);
            if (refs.single->remove_reference())
              delete (refs.single);
          }
          next->add_reference();
          refs.multi = next;
          single = false;
          shared = false;
        }
        else if (refs.single == NULL)
        {
          // New size is 1 but we were empty before
          CollectableRef *next = new CollectableRef();
          next->add_reference();
          refs.single = next;
          single = true;
          shared = false;
        }
      }
      else
      {
        if (new_size == 0)
        {
          if (refs.multi->remove_reference())
            delete refs.multi;
          refs.single = NULL;
          single = true;
          shared = false;
        }
        else if (new_size == 1)
        {
          CollectableRef *next = 
            new CollectableRef(refs.multi->vector[0]);
          if (refs.multi->remove_reference())
            delete (refs.multi);
          next->add_reference();
          refs.single = next;
          single = true;
          shared = false;
        }
        else
        {
          size_t current_size = refs.multi->vector.size();
          if (current_size != new_size)
          {
            if (shared)
            {
              // Make a copy
              InternalSet *next = new InternalSet(new_size);
              // Copy over the elements
              for (unsigned idx = 0; idx < 
                   ((current_size < new_size) ? current_size : new_size); idx++)
                next->vector[idx] = refs.multi->vector[idx];
              if (refs.multi->remove_reference())
                delete refs.multi;
              next->add_reference();
              refs.multi = next;
              shared = false;
            }
            else
            {
              // Resize our existing vector
              refs.multi->vector.resize(new_size);
            }
          }
          // Size is the same so there is no need to do anything
        }
      }
    }

    //--------------------------------------------------------------------------
    void InstanceSet::clear(void)
    //--------------------------------------------------------------------------
    {
      // No need to copy since we are removing our references and not mutating
      if (single)
      {
        if ((refs.single != NULL) && refs.single->remove_reference())
          delete (refs.single);
        refs.single = NULL;
      }
      else
      {
        if (shared)
        {
          // Small optimization here, if we're told to delete it, we know
          // that means we were the last user so we can re-use it
          if (refs.multi->remove_reference())
          {
            // Put a reference back on it since we're reusing it
            refs.multi->add_reference();
            refs.multi->vector.clear();
          }
          else
          {
            // Go back to single
            refs.multi = NULL;
            single = true;
          }
        }
        else
          refs.multi->vector.clear();
      }
      shared = false;
    }

    //--------------------------------------------------------------------------
    void InstanceSet::swap(InstanceSet &rhs)
    //--------------------------------------------------------------------------
    {
      // Swap references
      {
        InternalSet *other = rhs.refs.multi;
        rhs.refs.multi = refs.multi;
        refs.multi = other;
      }
      // Swap single
      {
        bool other = rhs.single;
        rhs.single = single;
        single = other;
      }
      // Swap shared
      {
        bool other = rhs.shared;
        rhs.shared = shared;
        shared = other;
      }
    }

    //--------------------------------------------------------------------------
    void InstanceSet::add_instance(const InstanceRef &ref)
    //--------------------------------------------------------------------------
    {
      if (single)
      {
        // No need to check for shared, we're going to make new things anyway
        if (refs.single != NULL)
        {
          // Make the new multi version
          InternalSet *next = new InternalSet(2);
          next->vector[0] = *(refs.single);
          next->vector[1] = ref;
          if (refs.single->remove_reference())
            delete (refs.single);
          next->add_reference();
          refs.multi = next;
          single = false;
          shared = false;
        }
        else
        {
          refs.single = new CollectableRef(ref);
          refs.single->add_reference();
        }
      }
      else
      {
        if (shared)
          make_copy();
        refs.multi->vector.push_back(ref);
      }
    }

    //--------------------------------------------------------------------------
    bool InstanceSet::is_virtual_mapping(void) const
    //--------------------------------------------------------------------------
    {
      if (empty())
        return true;
      if (size() > 1)
        return false;
      return refs.single->is_virtual_ref();
    }

    //--------------------------------------------------------------------------
    void InstanceSet::pack_references(Serializer &rez) const
    //--------------------------------------------------------------------------
    {
      if (single)
      {
        if (refs.single == NULL)
        {
          rez.serialize<size_t>(0);
          return;
        }
        rez.serialize<size_t>(1);
        refs.single->pack_reference(rez);
      }
      else
      {
        rez.serialize<size_t>(refs.multi->vector.size());
        for (unsigned idx = 0; idx < refs.multi->vector.size(); idx++)
          refs.multi->vector[idx].pack_reference(rez);
      }
    }

    //--------------------------------------------------------------------------
    void InstanceSet::unpack_references(Runtime *runtime, Deserializer &derez, 
                                        std::set<RtEvent> &ready_events)
    //--------------------------------------------------------------------------
    {
      size_t num_refs;
      derez.deserialize(num_refs);
      if (num_refs == 0)
      {
        // No matter what, we can just clear out any references we have
        if (single)
        {
          if ((refs.single != NULL) && refs.single->remove_reference())
            delete (refs.single);
          refs.single = NULL;
        }
        else
        {
          if (refs.multi->remove_reference())
            delete refs.multi;
          single = true;
        }
      }
      else if (num_refs == 1)
      {
        // If we're in multi, go back to single
        if (!single)
        {
          if (refs.multi->remove_reference())
            delete refs.multi;
          refs.multi = NULL;
          single = true;
        }
        // Now we can unpack our reference, see if we need to make one
        if (refs.single == NULL)
        {
          refs.single = new CollectableRef();
          refs.single->add_reference();
        }
        RtEvent ready;
        refs.single->unpack_reference(runtime, derez, ready);
        if (ready.exists())
          ready_events.insert(ready);
      }
      else
      {
        // If we're in single, go to multi
        // otherwise resize our multi for the appropriate number of references
        if (single)
        {
          if ((refs.single != NULL) && refs.single->remove_reference())
            delete (refs.single);
          refs.multi = new InternalSet(num_refs);
          refs.multi->add_reference();
          single = false;
        }
        else
          refs.multi->vector.resize(num_refs);
        // Now do the unpacking
        for (unsigned idx = 0; idx < num_refs; idx++)
        {
          RtEvent ready;
          refs.multi->vector[idx].unpack_reference(runtime, derez, ready);
          if (ready.exists())
            ready_events.insert(ready);
        }
      }
      // We are always not shared when we are done
      shared = false;
    }

    //--------------------------------------------------------------------------
    void InstanceSet::add_resource_references(ReferenceSource source) const
    //--------------------------------------------------------------------------
    {
      if (single)
      {
        if (refs.single != NULL)
          refs.single->add_resource_reference(source);
      }
      else
      {
        for (unsigned idx = 0; idx < refs.multi->vector.size(); idx++)
          refs.multi->vector[idx].add_resource_reference(source);
      }
    }

    //--------------------------------------------------------------------------
    void InstanceSet::remove_resource_references(ReferenceSource source) const
    //--------------------------------------------------------------------------
    {
      if (single)
      {
        if (refs.single != NULL)
          refs.single->remove_resource_reference(source);
      }
      else
      {
        for (unsigned idx = 0; idx < refs.multi->vector.size(); idx++)
          refs.multi->vector[idx].remove_resource_reference(source);
      }
    }

    //--------------------------------------------------------------------------
    bool InstanceSet::acquire_valid_references(ReferenceSource source) const
    //--------------------------------------------------------------------------
    {
      if (single)
      {
        if (refs.single != NULL)
          return refs.single->acquire_valid_reference(source);
      }
      else
      {
        for (unsigned idx = 0; idx < refs.multi->vector.size(); idx++)
          if (!refs.multi->vector[idx].acquire_valid_reference(source))
            return false;
      }
      return true;
    }

    //--------------------------------------------------------------------------
    void InstanceSet::add_valid_references(ReferenceSource source) const
    //--------------------------------------------------------------------------
    {
      if (single)
      {
        if (refs.single != NULL)
          refs.single->add_valid_reference(source);
      }
      else
      {
        for (unsigned idx = 0; idx < refs.multi->vector.size(); idx++)
          refs.multi->vector[idx].add_valid_reference(source);
      }
    }

    //--------------------------------------------------------------------------
    void InstanceSet::remove_valid_references(ReferenceSource source) const
    //--------------------------------------------------------------------------
    {
      if (single)
      {
        if (refs.single != NULL)
          refs.single->remove_valid_reference(source);
      }
      else
      {
        for (unsigned idx = 0; idx < refs.multi->vector.size(); idx++)
          refs.multi->vector[idx].remove_valid_reference(source);
      }
    }

    //--------------------------------------------------------------------------
    LegionRuntime::Accessor::RegionAccessor<
      LegionRuntime::Accessor::AccessorType::Generic> InstanceSet::
                                           get_field_accessor(FieldID fid) const
    //--------------------------------------------------------------------------
    {
      if (single)
      {
#ifdef DEBUG_LEGION
        assert(refs.single != NULL);
#endif
        return refs.single->get_field_accessor(fid);
      }
      else
      {
        for (unsigned idx = 0; idx < refs.multi->vector.size(); idx++)
        {
          const InstanceRef &ref = refs.multi->vector[idx];
          if (ref.is_field_set(fid))
            return ref.get_field_accessor(fid);
        }
        assert(false);
        return refs.multi->vector[0].get_field_accessor(fid);
      }
    }

  }; // namespace Internal 
}; // namespace Legion

