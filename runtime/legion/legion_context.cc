/* Copyright 2023 Stanford University, NVIDIA Corporation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <thread>
#include "legion/runtime.h"
#include "legion/legion_tasks.h"
#include "legion/legion_trace.h"
#include "legion/legion_context.h"
#include "legion/legion_instances.h"
#include "legion/legion_views.h"
#include "legion/legion_replication.h"
#include "realm/id.h"

#define SWAP_PART_KINDS(k1, k2) \
  {                             \
    PartitionKind temp = k1;    \
    k1 = k2;                    \
    k2 = temp;                  \
  }

namespace Legion {
  namespace Internal {

    LEGION_EXTERN_LOGGER_DECLARATIONS

    /////////////////////////////////////////////////////////////
    // Task Context 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    TaskContext::TaskContext(Runtime *rt, SingleTask *owner, int d,
                             const std::vector<RegionRequirement> &reqs,
                             const std::vector<OutputRequirement> &out_reqs,
                             DistributedID id, bool perform_registration,
                             bool inline_t, bool implicit_t,
                             CollectiveMapping *mapping)
      : DistributedCollectable(rt, id, perform_registration, mapping),
        owner_task(owner), regions(reqs), output_reqs(out_reqs), depth(d),
        executing_processor(Processor::NO_PROC), inlined_tasks(0),
        overhead_profiler(NULL), implicit_profiler(NULL), task_executed(false),
        mutable_priority(false), children_complete_invoked(false),
        children_commit_invoked(false), inline_task(inline_t),
        implicit_task(implicit_t)
    //--------------------------------------------------------------------------
    {
      if (implicit_task && (runtime->profiler != NULL))
        implicit_profiler = new ImplicitProfiler();
    }

    //--------------------------------------------------------------------------
    TaskContext::~TaskContext(void)
    //--------------------------------------------------------------------------
    {
      // Clean up any local variables that we have
      if (!task_local_variables.empty())
      {
        for (std::map<LocalVariableID,
                      std::pair<void*,void (*)(void*)> >::iterator it = 
              task_local_variables.begin(); it != 
              task_local_variables.end(); it++)
        {
          if (it->second.second != NULL)
            (*it->second.second)(it->second.first);
        }
      }
      if (overhead_profiler != NULL)
        delete overhead_profiler;
      if (implicit_profiler != NULL)
        delete implicit_profiler;
    }

    //--------------------------------------------------------------------------
    Task* TaskContext::get_task(void)
    //--------------------------------------------------------------------------
    {
      return owner_task;
    }

    //--------------------------------------------------------------------------
    UniqueID TaskContext::get_unique_id(void) const
    //--------------------------------------------------------------------------
    {
      return owner_task->get_unique_id();
    }

    //--------------------------------------------------------------------------
    InnerContext* TaskContext::find_parent_context(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(owner_task != NULL);
#endif
      return owner_task->get_context();
    }

    //--------------------------------------------------------------------------
    bool TaskContext::is_leaf_context(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    bool TaskContext::is_inner_context(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

#ifdef LEGION_USE_LIBDL
    //--------------------------------------------------------------------------
    void TaskContext::perform_global_registration_callbacks(
                     Realm::DSOReferenceImplementation *dso, const void *buffer,
                     size_t buffer_size, bool withargs, size_t dedup_tag,
                     RtEvent local_done, RtEvent global_done,
                     std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      // Send messages to all the other nodes to perform it
      for (AddressSpaceID space = 0; 
            space < runtime->total_address_spaces; space++)
      {
        if (space == runtime->address_space)
          continue;
        runtime->send_registration_callback(space, dso, global_done,
            preconditions, buffer, buffer_size, withargs, 
            true/*deduplicate*/, dedup_tag);
      }
    }
#endif

    //--------------------------------------------------------------------------
    void TaskContext::print_once(FILE *f, const char *message) const
    //--------------------------------------------------------------------------
    {
      fprintf(f, "%s", message);
    }

    //--------------------------------------------------------------------------
    void TaskContext::log_once(Realm::LoggerMessage &message) const
    //--------------------------------------------------------------------------
    {
      // Do nothing, just don't deactivate it
    }

    //--------------------------------------------------------------------------
    Future TaskContext::from_value(const void *value, size_t size,
                           bool owned, Provenance *provenance, bool shard_local) 
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      Future result(new FutureImpl(this, runtime, true/*register*/,
            runtime->get_available_distributed_id(), provenance));
      // Set the future result
      FutureInstance *instance = NULL;
      if (size > 0)
      {
        if (owned)
        {
          const Realm::ExternalMemoryResource resource(
              reinterpret_cast<uintptr_t>(value), size, true/*read only*/);
          instance = new FutureInstance(value, size,
              true/*own allocation*/, resource.clone(), 
              FutureInstance::free_host_memory, executing_processor);
        }
        else
          instance = copy_to_future_inst(value, size);
      }
      result.impl->set_result(ApEvent::NO_AP_EVENT, instance);
      return result;
    }

    //--------------------------------------------------------------------------
    Future TaskContext::from_value(const void *buffer, size_t size, bool owned,
                       const Realm::ExternalInstanceResource &resource,
                       void (*freefunc)(const Realm::ExternalInstanceResource&),
                       Provenance *provenance, bool shard_local)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      Future result(new FutureImpl(this, runtime, true/*register*/,
            runtime->get_available_distributed_id(), provenance));
      FutureInstance *instance = new FutureInstance(buffer, size,
          owned, resource.clone(), freefunc);
      result.impl->set_result(ApEvent::NO_AP_EVENT, instance);
      return result;
    }

    //--------------------------------------------------------------------------
    Future TaskContext::consensus_match(const void *input, void *output,
               size_t num_elements, size_t element_size, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      // No need to do a match here, there is just one shard
      const size_t future_size = sizeof(num_elements);
      // We only need this if-statement to guard against nullptr cases
      // to make undefined behavior checkers happy since the C++ standard
      // is woefully underspecified as usual.
      if (num_elements > 0)
        memcpy(output, input, num_elements*future_size);
      Future result(new FutureImpl(this, runtime, true/*register*/,
            runtime->get_available_distributed_id(), provenance));
      result.impl->set_local(&num_elements, future_size);
      return result;
    }

    //--------------------------------------------------------------------------
    VariantID TaskContext::register_variant(
            const TaskVariantRegistrar &registrar, const void *user_data,
            size_t user_data_size, const CodeDescriptor &desc, size_t ret_size,
            bool has_ret_size, VariantID vid, bool check_task_id)
    //--------------------------------------------------------------------------
    {
      return runtime->register_variant(registrar, user_data, user_data_size,
          desc,ret_size,has_ret_size,vid,check_task_id,false/*check context*/);
    }

    //--------------------------------------------------------------------------
    TraceID TaskContext::generate_dynamic_trace_id(void)
    //--------------------------------------------------------------------------
    {
      return runtime->generate_dynamic_trace_id(false/*check context*/);
    }

    //--------------------------------------------------------------------------
    MapperID TaskContext::generate_dynamic_mapper_id(void)
    //--------------------------------------------------------------------------
    {
      return runtime->generate_dynamic_mapper_id(false/*check context*/);
    }

    //--------------------------------------------------------------------------
    ProjectionID TaskContext::generate_dynamic_projection_id(void)
    //--------------------------------------------------------------------------
    {
      return runtime->generate_dynamic_projection_id(false/*check context*/);
    }

    //--------------------------------------------------------------------------
    ShardingID TaskContext::generate_dynamic_sharding_id(void)
    //--------------------------------------------------------------------------
    {
      return runtime->generate_dynamic_sharding_id(false/*check context*/);
    }

    //--------------------------------------------------------------------------
    TaskID TaskContext::generate_dynamic_task_id(void)
    //--------------------------------------------------------------------------
    {
      return runtime->generate_dynamic_task_id(false/*check context*/);
    }

    //--------------------------------------------------------------------------
    ReductionOpID TaskContext::generate_dynamic_reduction_id(void)
    //--------------------------------------------------------------------------
    {
      return runtime->generate_dynamic_reduction_id(false/*check context*/);
    }

    //--------------------------------------------------------------------------
    CustomSerdezID TaskContext::generate_dynamic_serdez_id(void)
    //--------------------------------------------------------------------------
    {
      return runtime->generate_dynamic_serdez_id(false/*check context*/);
    }

    //--------------------------------------------------------------------------
    bool TaskContext::perform_semantic_attach(const char *func, unsigned kind,
        const void *arg, size_t arglen, SemanticTag tag, const void *buffer,
        size_t size, bool is_mutable, bool &global, 
        const void *arg2, size_t arg2len)
    //--------------------------------------------------------------------------
    {
      return true;
    }

    //--------------------------------------------------------------------------
    void TaskContext::post_semantic_attach(void)
    //--------------------------------------------------------------------------
    {
      // Nothing to do here
    }

    //--------------------------------------------------------------------------
    void TaskContext::add_output_region(const OutputRequirement &req,
                                        const InstanceSet &instances,
                                        bool global,
                                        bool valid)
    //--------------------------------------------------------------------------
    {
      size_t index = output_regions.size();
      OutputRegionImpl *impl = new OutputRegionImpl(index,
                                                    req,
                                                    instances,
                                                    this,
                                                    runtime,
                                                    global,
                                                    valid);
      output_regions.push_back(OutputRegion(impl));
    }

    //--------------------------------------------------------------------------
    PhysicalRegion TaskContext::get_physical_region(unsigned idx)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(idx < regions.size()); // should be one of our original regions
#endif
      return physical_regions[idx];
    } 

    //--------------------------------------------------------------------------
    OutputRegion TaskContext::get_output_region(unsigned idx) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(idx < output_regions.size()); //should be one of our output regions
#endif
      return output_regions[idx];
    }

    //--------------------------------------------------------------------------
    void TaskContext::finalize_output_regions(void)
    //--------------------------------------------------------------------------
    {
      for (unsigned idx = 0; idx < output_regions.size(); ++idx)
      {
        // Check if the task returned data for all the output fields
        // before we fianlize this output region.
        OutputRegion &output_region = output_regions[idx];
        FieldID unbound_field = 0;
        if (!output_region.impl->is_complete(unbound_field))
        {
          REPORT_LEGION_ERROR(ERROR_UNBOUND_OUTPUT_REGION,
            "Task %s (UID %lld) did not return any instance for field %d "
            "of output requirement %u",
            owner_task->get_task_name(), owner_task->get_unique_id(),
            unbound_field, idx);
        }
        output_region.impl->finalize();
      }
      // Clear this to remove references in output region data structures
      output_regions.clear();
    }

    //--------------------------------------------------------------------------
    void TaskContext::raise_poison_exception(void)
    //--------------------------------------------------------------------------
    {
      // TODO: handle poisoned task
      assert(false);
    }

    //--------------------------------------------------------------------------
    void TaskContext::raise_region_exception(PhysicalRegion region,bool nuclear)
    //--------------------------------------------------------------------------
    {
      // TODO: handle region exception
      assert(false);
    }

    //--------------------------------------------------------------------------
    bool TaskContext::safe_cast(RegionTreeForest *forest, IndexSpace handle,
                                const void *realm_point, TypeTag type_tag)
    //--------------------------------------------------------------------------
    {
      // Check to see if we already have the pointer for the node
      std::map<IndexSpace,IndexSpaceNode*>::const_iterator finder =
        safe_cast_spaces.find(handle);
      if (finder == safe_cast_spaces.end())
      {
        safe_cast_spaces[handle] = forest->get_node(handle);
        finder = safe_cast_spaces.find(handle);
      }
      return finder->second->contains_point(realm_point, type_tag);
    }

    //--------------------------------------------------------------------------
    bool TaskContext::is_region_mapped(unsigned idx)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(idx < physical_regions.size());
#endif
      return physical_regions[idx].is_mapped();
    }

    //--------------------------------------------------------------------------
    void TaskContext::record_padded_fields(VariantImpl *variant)
    //--------------------------------------------------------------------------
    {
      variant->record_padded_fields(regions, physical_regions); 
    }

    //--------------------------------------------------------------------------
    LegionErrorType TaskContext::check_privilege_internal(
        const RegionRequirement &req, const RegionRequirement &our_req,
        std::set<FieldID>& privilege_fields, FieldID &bad_field, 
        int local_index, int &bad_index, bool skip_privilege) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(our_req.handle_type == LEGION_SINGULAR_PROJECTION);
#endif
      // Check to see if we found the requirement in the parent
      if (our_req.region == req.parent)
      {
        // If we make it in here then we know we have at least found
        // the parent name so we can set the bad index
        bad_index = local_index;
        bad_field = LEGION_AUTO_GENERATE_ID; // set it to an invalid field
        if ((req.handle_type == LEGION_SINGULAR_PROJECTION) || 
            (req.handle_type == LEGION_REGION_PROJECTION))
        {
          std::vector<LegionColor> path;
          if (!runtime->forest->compute_index_path(req.parent.index_space,
                                            req.region.index_space, path))
            return ERROR_BAD_REGION_PATH;
        }
        else
        {
          std::vector<LegionColor> path;
          if (!runtime->forest->compute_partition_path(req.parent.index_space,
                                        req.partition.index_partition, path))
            return ERROR_BAD_PARTITION_PATH;
        }
        // Now check that the types are subset of the fields
        // Note we can use the parent since all the regions/partitions
        // in the same region tree have the same field space
        for (std::set<FieldID>::iterator fit = privilege_fields.begin();
              fit != privilege_fields.end(); /*nothing*/)
        {
          if (our_req.privilege_fields.find(*fit) != 
              our_req.privilege_fields.end())
          {
            // Only need to do this check if there were overlapping fields
            if (!skip_privilege && (PRIV_ONLY(req) & (~(our_req.privilege))))
            {
              if ((req.handle_type == LEGION_SINGULAR_PROJECTION) || 
                  (req.handle_type == LEGION_REGION_PROJECTION))
                return ERROR_BAD_REGION_PRIVILEGES;
              else
                return ERROR_BAD_PARTITION_PRIVILEGES;
            }
            std::set<FieldID>::iterator to_delete = fit++;
            privilege_fields.erase(to_delete);
          }
          else
            fit++;
        }
      }

      if (!privilege_fields.empty()) 
      {
        bad_field = *(privilege_fields.begin());
        return ERROR_BAD_PARENT_REGION;
      }
        // If we make it here then we are good
      return LEGION_NO_ERROR;
    }

    //--------------------------------------------------------------------------
    bool TaskContext::check_region_dependence(RegionTreeID our_tid,
                                              IndexSpace our_space,
                                              const RegionRequirement &our_req,
                                              const RegionUsage &our_usage,
                                              const RegionRequirement &req,
                                              bool check_privileges) const
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, CHECK_REGION_DEPENDENCE_CALL);
      if ((req.handle_type == LEGION_SINGULAR_PROJECTION) || 
          (req.handle_type == LEGION_REGION_PROJECTION))
      {
        // If the trees are different we're done 
        if (our_tid != req.region.get_tree_id())
          return false;
        // Check to see if there is a path between
        // the index spaces
        std::vector<LegionColor> path;
        if (!runtime->forest->compute_index_path(our_space,
                         req.region.get_index_space(),path))
          return false;
      }
      else
      {
        // Check if the trees are different
        if (our_tid != req.partition.get_tree_id())
          return false;
        std::vector<LegionColor> path;
        if (!runtime->forest->compute_partition_path(our_space,
                     req.partition.get_index_partition(), path))
          return false;
      }
      // Check to see if any privilege fields overlap
      std::vector<FieldID> intersection(our_req.privilege_fields.size());
      std::vector<FieldID>::iterator intersect_it = 
        std::set_intersection(our_req.privilege_fields.begin(),
                              our_req.privilege_fields.end(),
                              req.privilege_fields.begin(),
                              req.privilege_fields.end(),
                              intersection.begin());
      intersection.resize(intersect_it - intersection.begin());
      if (intersection.empty())
        return false;
      // If we aren't supposed to check privileges then we're done
      if (!check_privileges)
        return true;
      // Finally if everything has overlapped, do a dependence analysis
      // on the privileges and coherence
      RegionUsage usage(req);
      switch (check_dependence_type<true>(our_usage,usage))
      {
        // Only allow no-dependence, or simultaneous dependence through
        case LEGION_NO_DEPENDENCE:
        case LEGION_SIMULTANEOUS_DEPENDENCE:
          {
            return false;
          }
        default:
          break;
      }
      return true;
    }

    //--------------------------------------------------------------------------
    const std::vector<PhysicalRegion>& TaskContext::begin_task(Processor proc)
    //--------------------------------------------------------------------------
    {
      if (implicit_runtime == NULL)
        implicit_runtime = this->runtime;
      implicit_context = this;
      implicit_provenance = owner_task->get_unique_op_id();
      if (overhead_profiler != NULL)
        overhead_profiler->previous_profiling_time = 
          Realm::Clock::current_time_in_nanoseconds();
      if (implicit_profiler != NULL)
        implicit_profiler->start_time = 
          Realm::Clock::current_time_in_nanoseconds();
      // Switch over the executing processor to the one
      // that has actually been assigned to run this task.
      executing_processor = proc; 
      owner_task->current_proc = executing_processor;
      if (runtime->legion_spy_enabled)
        LegionSpy::log_task_processor(get_unique_id(), executing_processor.id);
#ifdef DEBUG_LEGION
      log_task.debug("Task %s (ID %lld) starting on processor " IDFMT "",
                    get_task_name(), get_unique_id(), executing_processor.id);
      assert(regions.size() == physical_regions.size());
#endif
      // Issue a utility task to decrement the number of outstanding
      // tasks now that this task has started running
      if (!inline_task)
        find_parent_context()->decrement_pending(owner_task);
      return physical_regions;
    }

    //--------------------------------------------------------------------------
    PhysicalInstance TaskContext::create_task_local_instance(Memory memory, 
                                           Realm::InstanceLayoutGeneric *layout)
    //--------------------------------------------------------------------------
    {
      PhysicalInstance instance;
      MemoryManager *manager = runtime->find_memory_manager(memory);
#ifdef LEGION_MALLOC_INSTANCES
      const Realm::ProfilingRequestSet no_requests;
      const ApEvent wait_on(manager->allocate_legion_instance(layout->clone(),
                                                      no_requests, instance));
#else
      LgEvent unique_event;
      if (runtime->profiler != NULL)
      {
        // If we're profiling then each of these needs a unique event
        const RtUserEvent unique = Runtime::create_rt_user_event();
        Runtime::trigger_event(unique);
        unique_event = unique;
      }
      const ApEvent wait_on(manager->create_eager_instance(instance, 
                                              unique_event, layout));
      if (!instance.exists())
      {
        const char *mem_names[] = {
#define MEM_NAMES(name, desc) desc,
          REALM_MEMORY_KINDS(MEM_NAMES) 
#undef MEM_NAMES
        };
        REPORT_LEGION_ERROR(ERROR_DEFERRED_ALLOCATION_FAILURE,
            "Failed to allocate DeferredBuffer/Value/Reduction in task %s "
            "(UID %lld) because %s memory " IDFMT " is full. This is an eager "
            "allocation so you must adjust the percentage of this memory "
            "dedicated for eager allocations with '-lg:eager_alloc_percentage' "
            "flag on the command line.", get_task_name(), get_unique_id(), 
            mem_names[memory.kind()], memory.id)
      }
#endif
      task_local_instances[instance] = unique_event;
      if (wait_on.exists())
      {
        bool poisoned = false;
        if (!wait_on.has_triggered_faultaware(poisoned))
          wait_on.wait_faultaware(poisoned);
        if (poisoned)
          raise_poison_exception();
      }
      return instance;
    }

    //--------------------------------------------------------------------------
    void TaskContext::destroy_task_local_instance(PhysicalInstance instance)
    //--------------------------------------------------------------------------
    {
      std::map<PhysicalInstance,LgEvent>::iterator finder =
        task_local_instances.find(instance);
#ifdef DEBUG_LEGION
      assert(finder != task_local_instances.end());
#endif
      task_local_instances.erase(finder);
      MemoryManager *manager = 
        runtime->find_memory_manager(instance.get_location());
#ifdef LEGION_MALLOC_INSTANCES
      manager->free_legion_instance(RtEvent::NO_RT_EVENT, instance);
#else
      manager->free_eager_instance(instance, RtEvent::NO_RT_EVENT);
#endif
    }

    //--------------------------------------------------------------------------
    void TaskContext::end_task(const void *res, size_t res_size, bool owned,
                               PhysicalInstance deferred_result_instance,
                               FutureFunctor *callback_functor,
                               const Realm::ExternalInstanceResource *resource,
                      void (*freefunc)(const Realm::ExternalInstanceResource&),
                  const void *metadataptr, size_t metadatasize, ApEvent effects)
    //--------------------------------------------------------------------------
    {
      // Finalize output regions by setting realm instances created during
      // task execution to the output regions' physical managers
      if (!output_regions.empty())
        finalize_output_regions(); 
      // See if we need to pull the data in from a callback in the case
      // where we are going to be doing a reduction immediately, if we
      // are then we're going to overwrite 'owned' so save it to callback_owned
      bool callback_owned = false;
      bool eager_callback = false;
      if (callback_functor != NULL)
      {
#ifdef DEBUG_LEGION
        assert(res == NULL);
        assert(metadataptr == NULL);
        assert(metadatasize == 0);
#endif
        if (owner_task->is_reducing_future())
        {
          eager_callback = true;
          callback_owned = owned;
          res = callback_functor->callback_get_future(res_size,
              owned, resource, freefunc, metadataptr, metadatasize);
        }
      }
      // If we have a deferred result instance we need to escape that too
      FutureInstance *instance = NULL;
      if (deferred_result_instance.exists())
      {
#ifdef DEBUG_LEGION
        assert(res != NULL);
        assert(freefunc == NULL);
#endif
        // escape this task local instance
        LgEvent unique = escape_task_local_instance(deferred_result_instance);
        instance = new FutureInstance(res, res_size, true/*eager*/,
            false/*external*/, true/*own alloc*/,
            unique, deferred_result_instance);
      }
      else if (resource != NULL)
      {
        if (!owned)
        {
          void *buffer = malloc(res_size);
          instance = new FutureInstance(buffer, res_size, false/*eager*/,
              true/*external*/, true/*own allocation*/);
          if (!FutureInstance::check_meta_visible(resource->suggested_memory()))
          {
            FutureInstance source(res, res_size, false/*own allocation*/,
                resource->clone(), freefunc, executing_processor);
            effects = instance->copy_from(&source, owner_task, effects);
            // Need to wait for the copy to be done before returning
            if (effects.exists())
              effects.wait_faultignorant();
          }
          else
          {
            // We can do a simple memory copy here but we need to wait
            // for the results to be ready first before returning
            if (effects.exists())
              effects.wait_faultignorant();
            memcpy(buffer, res, res_size);
          }
        }
        else
          instance = new FutureInstance(res, res_size, true/*own allocation*/,
              resource->clone(), freefunc, executing_processor);
      }
      else if (res_size > 0)
      {
#ifdef DEBUG_LEGION
        assert(res != NULL);
#endif
        if (owned)
        {
          const Realm::ExternalMemoryResource resource(
              reinterpret_cast<uintptr_t>(res), res_size, true/*read only*/);
          instance = new FutureInstance(res, res_size, true/*own allocation*/,
              resource.clone(), FutureInstance::free_host_memory,
              executing_processor);
        }
        else
        {
          // Wait for any effects for immediate values this is
          // not a Realm task (e.g. an implicit task) because
          // we can't track sub-effects on them so we need to 
          // trust that the effects the user is giving us are correct
          if (effects.exists() && 
              !Processor::get_executing_processor().exists())
            effects.wait_faultignorant();
          instance = copy_to_future_inst(res, res_size);
        }
      }
      // If we did an eager callback, restore whether we own it now
      bool release_callback = false;
      if (eager_callback)
      {
        // Release the callback here if we do not own the output and
        // therefore going to make a copy of it
        release_callback = !owned;
        owned = callback_owned;
      }
      // Once there are no more escaping instances we can release the rest
      if (!task_local_instances.empty())
      {
        RtEvent done;
        if (effects.exists())
          done = Runtime::protect_event(effects);
        for (std::map<PhysicalInstance,LgEvent>::iterator it =
             task_local_instances.begin(); it !=
             task_local_instances.end(); ++it)
        {
          PhysicalInstance inst = it->first;
          MemoryManager *manager =
            runtime->find_memory_manager(inst.get_location());
#ifdef LEGION_MALLOC_INSTANCES
          manager->free_legion_instance(done, inst);
#else
          manager->free_eager_instance(inst, done);
#endif
        }
        task_local_instances.clear();
      }
      // Grab some information before doing the next step in case it
      // results in the deletion of 'this'
#ifdef DEBUG_LEGION
      assert(owner_task != NULL);
      const TaskID owner_task_id = owner_task->task_id;
#endif
      Runtime *runtime_ptr = runtime;
      // Tell the parent context that we are ready for post-end
      InnerContext *parent_ctx = owner_task->get_context();
      if (inline_task)
        parent_ctx->decrement_inlined();
      if (release_callback)
        parent_ctx->add_to_post_task_queue(this, RtEvent::NO_RT_EVENT,
          instance, NULL/*no functor here*/, owned, metadataptr, metadatasize);
      else
        parent_ctx->add_to_post_task_queue(this, RtEvent::NO_RT_EVENT,
            instance, callback_functor, owned, metadataptr, metadatasize); 
#ifdef DEBUG_LEGION
      runtime_ptr->decrement_total_outstanding_tasks(owner_task_id, 
                                                     false/*meta*/);
#else
      runtime_ptr->decrement_total_outstanding_tasks();
#endif
      // See if we can release our callback down
      if (release_callback)
      {
        callback_functor->callback_release_future();
        if (callback_owned)
          delete callback_functor;
      }
    }

    //--------------------------------------------------------------------------
    FutureInstance* TaskContext::copy_to_future_inst(const void *value,
                                                     size_t size)
    //--------------------------------------------------------------------------
    {
      // Make a simple memory copy here now
      if (size > LEGION_MAX_RETURN_SIZE)
      {
        MemoryManager *manager = 
          runtime->find_memory_manager(runtime->runtime_system_memory);
        FutureInstance *instance = 
          manager->create_future_instance(owner_task,
            owner_task->get_unique_op_id(), size, true/*eager*/);
        memcpy(const_cast<void*>(instance->get_data()), value, size);
        return instance;
      }
      else
      {
        void *buffer = malloc(size);
        memcpy(buffer, value, size);
        return new FutureInstance(buffer, size, false/*eager*/,
            true/*external*/, true/*own allocation*/);
      }
    }

    //--------------------------------------------------------------------------
    bool TaskContext::is_task_local_instance(PhysicalInstance instance)
    //--------------------------------------------------------------------------
    {
      return task_local_instances.find(instance) != task_local_instances.end();
    }

    //--------------------------------------------------------------------------
    LgEvent TaskContext::escape_task_local_instance(PhysicalInstance instance)
    //--------------------------------------------------------------------------
    {
      std::map<PhysicalInstance,LgEvent>::iterator finder =
        task_local_instances.find(instance);
#ifdef DEBUG_LEGION
      assert(finder != task_local_instances.end());
#endif
      const LgEvent result = finder->second;
      // Remove the instance from the set of task local instances
      task_local_instances.erase(finder);
      return result;
    }

    //--------------------------------------------------------------------------
    void TaskContext::handle_mispredication(void)
    //--------------------------------------------------------------------------
    {
      // Issue a utility task to decrement the number of outstanding
      // tasks now that this task has started running
      owner_task->get_context()->decrement_pending(owner_task);
#ifdef DEBUG_LEGION
      assert(owner_task != NULL);
      runtime->decrement_total_outstanding_tasks(owner_task->task_id, 
                                                     false/*meta*/);
#else
      runtime->decrement_total_outstanding_tasks();
#endif
      owner_task->complete_execution();
      owner_task->trigger_children_complete();
      owner_task->trigger_children_committed();
    }

    //--------------------------------------------------------------------------
    Lock TaskContext::create_lock(void)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      return Lock(Reservation::create_reservation());
    }

    //--------------------------------------------------------------------------
    PhaseBarrier TaskContext::create_phase_barrier(unsigned arrivals)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      return PhaseBarrier(ApBarrier(Realm::Barrier::create_barrier(arrivals)));
    }

    //--------------------------------------------------------------------------
    PhaseBarrier TaskContext::advance_phase_barrier(PhaseBarrier pb)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      PhaseBarrier result = pb;
      Runtime::advance_barrier(result);
#ifdef LEGION_SPY
      LegionSpy::log_event_dependence(pb.phase_barrier, result.phase_barrier);
#endif
      return result;
    }

    //--------------------------------------------------------------------------
    void TaskContext::initialize_overhead_profiler(void)
    //--------------------------------------------------------------------------
    {
      // Make an overhead tracker
#ifdef DEBUG_LEGION
      assert(overhead_profiler == NULL);
#endif
      overhead_profiler = new OverheadProfiler();
    } 

    //--------------------------------------------------------------------------
    void TaskContext::remap_unmapped_regions(LogicalTrace *trace,
                            const std::vector<PhysicalRegion> &unmapped_regions,
                            Provenance *provenance)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!unmapped_regions.empty());
#endif
      if (trace != NULL)
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_RUNTIME_REMAPPING,
                    "Illegal runtime remapping in trace %d inside of "
                    "task %s (UID %lld). Traces must perfectly "
                    "manage their physical mappings with no runtime help.",
                    trace->tid, get_task_name(), get_unique_id())
      std::set<ApEvent> mapped_events;
      for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
      {
        const ApEvent ready = 
          remap_region(unmapped_regions[idx], provenance, true/*internal*/);
        if (ready.exists())
          mapped_events.insert(ready);
      }
      // Wait for all the re-mapping operations to complete
      const ApEvent mapped_event = Runtime::merge_events(NULL, mapped_events);
      bool poisoned = false;
      if (mapped_event.has_triggered_faultaware(poisoned))
      {
        if (poisoned)
          raise_poison_exception();
        return;
      }
      mapped_event.wait_faultaware(poisoned);
      if (poisoned)
        raise_poison_exception();
    }

    //--------------------------------------------------------------------------
    void* TaskContext::get_local_task_variable(LocalVariableID id)
    //--------------------------------------------------------------------------
    {
      std::map<LocalVariableID,std::pair<void*,void (*)(void*)> >::
        const_iterator finder = task_local_variables.find(id);
      if (finder == task_local_variables.end())
        REPORT_LEGION_ERROR(ERROR_UNABLE_FIND_TASK_LOCAL,
          "Unable to find task local variable %d in task %s "
                      "(UID %lld)", id, get_task_name(), get_unique_id())  
      return finder->second.first;
    }

    //--------------------------------------------------------------------------
    void TaskContext::set_local_task_variable(LocalVariableID id,
                                              const void *value,
                                              void (*destructor)(void*))
    //--------------------------------------------------------------------------
    {
      std::map<LocalVariableID,std::pair<void*,void (*)(void*)> >::iterator
        finder = task_local_variables.find(id);
      if (finder != task_local_variables.end())
      {
        // See if we need to clean things up first
        if (finder->second.second != NULL)
          (*finder->second.second)(finder->second.first);
        finder->second = 
          std::pair<void*,void (*)(void*)>(const_cast<void*>(value),destructor);
      }
      else
        task_local_variables[id] = 
          std::pair<void*,void (*)(void*)>(const_cast<void*>(value),destructor);
    }

    //--------------------------------------------------------------------------
    void TaskContext::yield(void)
    //--------------------------------------------------------------------------
    {
      const Processor proc = Processor::get_executing_processor();
      if (proc.exists())
      {
        // Normal realm task
        YieldArgs args(owner_task->get_unique_id());
        // Run this as the lowest possible priority task on the same processor
        // which will give all other ready tasks an opportunity to run. Once
        // the meta-task does run though then we know we can wake-up.
        const RtEvent wait_for = 
          runtime->issue_application_processor_task(args, LG_MIN_PRIORITY,proc);
        wait_for.wait();
      }
      else // external implicit top-level task
        std::this_thread::yield();
    }

    //--------------------------------------------------------------------------
    size_t TaskContext::query_available_memory(Memory target)
    //--------------------------------------------------------------------------
    {
      if (target.address_space() != runtime->address_space)
        return 0;
      MemoryManager *manager = runtime->find_memory_manager(target); 
      return manager->query_available_eager_memory();
    }

    //--------------------------------------------------------------------------
    void TaskContext::increment_inlined(void)
    //--------------------------------------------------------------------------
    {
      AutoLock i_lock(inline_lock);
      inlined_tasks++;
    }

    //--------------------------------------------------------------------------
    void TaskContext::decrement_inlined(void)
    //--------------------------------------------------------------------------
    {
      AutoLock i_lock(inline_lock);
#ifdef DEBUG_LEGION
      assert(inlined_tasks > 0);
#endif
      if ((--inlined_tasks == 0) && inlining_done.exists())
      {
        Runtime::trigger_event(inlining_done);
        inlining_done = RtUserEvent::NO_RT_USER_EVENT;
      }
    }

    //--------------------------------------------------------------------------
    void TaskContext::wait_for_inlined(void)
    //--------------------------------------------------------------------------
    {
      RtEvent wait_on;
      {
        AutoLock i_lock(inline_lock);
        if (inlined_tasks > 0)
        {
#ifdef DEBUG_LEGION
          assert(!inlining_done.exists());
#endif
          inlining_done = Runtime::create_rt_user_event();
          wait_on = inlining_done;
        }
      }
      if (wait_on.exists())
        wait_on.wait();
    }

    //--------------------------------------------------------------------------
    Future TaskContext::predicate_task_false(const TaskLauncher &launcher,
                                             Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      if (launcher.elide_future_return)
        return Future();
      if (launcher.predicate_false_future.impl != NULL)
        return launcher.predicate_false_future;
      // Otherwise check to see if we have a value
      FutureImpl *result = new FutureImpl(this, runtime, true/*register*/,
        runtime->get_available_distributed_id(), provenance);
      const size_t future_size = launcher.predicate_false_result.get_size(); 
      if (future_size > 0)
        result->set_local(launcher.predicate_false_result.get_ptr(),
            future_size, false/*own*/);
      else
        result->set_result(ApEvent::NO_AP_EVENT, NULL);
      return Future(result);
    }

    //--------------------------------------------------------------------------
    FutureMap TaskContext::predicate_index_task_false(size_t context_index,
        IndexSpace launch_space, const IndexTaskLauncher &launcher,
        Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      if (launcher.elide_future_return)
        return FutureMap();
      Domain launch_domain = launcher.launch_domain;
      if (!launch_domain.exists())
        runtime->forest->find_domain(launch_space, launch_domain);
      IndexSpaceNode *launch_node = runtime->forest->get_node(launch_space);
      FutureMapImpl *result = new FutureMapImpl(this, runtime,
          launch_node, runtime->get_available_distributed_id(), context_index,
          ApEvent::NO_AP_EVENT, provenance);
      if (launcher.predicate_false_future.impl != NULL)
      {
        for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)
        {
          Future f = result->get_future(itr.p, true/*internal*/);
          f.impl->set_result(launcher.predicate_false_future.impl, owner_task);
        }
      }
      else if (launcher.predicate_false_result.get_size() == 0)
      {
        // Just initialize all the futures
        for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)
        {
          Future f = result->get_future(itr.p, true/*internal*/);
          f.impl->set_result(ApEvent::NO_AP_EVENT, NULL);
        }
      }
      else
      {
        const void *ptr = launcher.predicate_false_result.get_ptr();
        size_t ptr_size = launcher.predicate_false_result.get_size();
        for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)
        {
          Future f = result->get_future(itr.p, true/*internal*/);
          f.impl->set_local(ptr, ptr_size, false/*own*/);
        }
      }
      return FutureMap(result);
    }

    //--------------------------------------------------------------------------
    Future TaskContext::predicate_index_task_reduce_false(
        const IndexTaskLauncher &launcher, IndexSpace launch_space,
        ReductionOpID redop, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      if (launcher.elide_future_return)
        return Future();
      // If there is an initial value for the reduction use that
      if (launcher.initial_value.impl != NULL)
        return launcher.initial_value;
      // Otherwise set it to the identity value of the reduction operator
      FutureImpl *result = new FutureImpl(this, runtime, true/*register*/, 
        runtime->get_available_distributed_id(), provenance);
      const ReductionOp *reduction_op = runtime->get_reduction(redop);
      result->set_local(&reduction_op->identity, reduction_op->sizeof_rhs);
      return Future(result);
    }

    //--------------------------------------------------------------------------
    VariantImpl* TaskContext::select_inline_variant(TaskOp *child,
                              const std::vector<PhysicalRegion> &parent_regions,
                              std::deque<InstanceSet> &physical_instances)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, SELECT_INLINE_VARIANT_CALL);
      Mapper::SelectVariantInput input;
      Mapper::SelectVariantOutput output;
      input.processor = executing_processor;
      input.chosen_instances.resize(parent_regions.size());
      // Extract the specific field instances for each region requirement
      for (unsigned idx1 = 0; idx1 < parent_regions.size(); idx1++)
      {
        const RegionRequirement &child_req = child->regions[idx1];
        FieldSpaceNode *space = 
          runtime->forest->get_node(child_req.parent.get_field_space());
        FieldMask mask = space->get_field_mask(child_req.privilege_fields);
        InstanceSet instances;
        parent_regions[idx1].impl->get_references(instances);
        for (unsigned idx2 = 0; idx2 < instances.size(); idx2++)
        {
          const InstanceRef &ref = instances[idx2];
          const FieldMask overlap = mask & ref.get_valid_fields();
          if (!overlap)
            continue;
          physical_instances[idx1].add_instance(
              InstanceRef(ref.get_manager(), overlap));
          input.chosen_instances[idx1].push_back(
              MappingInstance(ref.get_manager()));
          mask -= overlap;
          if (!mask)
            break;
        }
#ifdef DEBUG_LEGION
        assert(!mask);
#endif
      }
      output.chosen_variant = 0;
      // Always do this with the child mapper
      MapperManager *child_mapper = 
        runtime->find_mapper(executing_processor, child->map_id);
      child_mapper->invoke_select_task_variant(child, input, output);
      VariantImpl *variant_impl = runtime->find_variant_impl(child->task_id,
                                   output.chosen_variant, true/*can fail*/);
      if (variant_impl == NULL)
        REPORT_LEGION_ERROR(ERROR_INVALID_MAPPER_OUTPUT,
                      "Invalid mapper output from invoction of "
                      "'select_task_variant' on mapper %s. Mapper selected "
                      "an invalid variant ID %d for inlining of task %s "
                      "(UID %lld).", child_mapper->get_mapper_name(),
                      output.chosen_variant, child->get_task_name(), 
                      child->get_unique_id())
      if (!runtime->unsafe_mapper)
        child->validate_variant_selection(child_mapper, variant_impl,
         executing_processor.kind(), physical_instances, "select_task_variant");
      return variant_impl;
    }

    //--------------------------------------------------------------------------
    /*static*/ void TaskContext::help_complete_future(Future &f,
                               const void *result, size_t result_size, bool own)
    //--------------------------------------------------------------------------
    {
      f.impl->set_local(result, result_size, own);
    }

    /////////////////////////////////////////////////////////////
    // Inner Context 
    /////////////////////////////////////////////////////////////
    
    //--------------------------------------------------------------------------
    InnerContext::InnerContext(Runtime *rt, SingleTask *owner,int d,bool finner,
                               const std::vector<RegionRequirement> &reqs,
                               const std::vector<OutputRequirement> &out_reqs,
                               const std::vector<unsigned> &parent_indexes,
                               const std::vector<bool> &virt_mapped,
                               ApEvent exec_fence, 
                               DistributedID id, bool inline_task, 
                               bool implicit_task, bool concurrent,
                               CollectiveMapping *mapping)
      : TaskContext(rt, owner, d, reqs, out_reqs, 
          LEGION_DISTRIBUTED_HELP_ENCODE((id > 0) ? id : 
          rt->get_available_distributed_id(), INNER_CONTEXT_DC),
          (id == 0)/*register if not remote*/, 
          inline_task, implicit_task, mapping),
        tree_context(rt->allocate_region_tree_context()),
        full_inner_context(finner), concurrent_context(concurrent), 
        finished_execution(false), has_inline_accessor(false),
        next_created_index(reqs.size()), parent_req_indexes(parent_indexes),
        virtual_mapped(virt_mapped), total_children_count(0),
        executing_children_count(0), executed_children_count(0),
        total_summary_count(0), total_tunable_count(0), 
        outstanding_children_count(0),
        ready_comp_queue(CompletionQueue::NO_QUEUE),
        enqueue_task_comp_queue(CompletionQueue::NO_QUEUE),
        distribute_task_comp_queue(CompletionQueue::NO_QUEUE),
        launch_task_comp_queue(CompletionQueue::NO_QUEUE),
        resolution_comp_queue(CompletionQueue::NO_QUEUE),
        trigger_execution_comp_queue(CompletionQueue::NO_QUEUE),
        deferred_execution_comp_queue(CompletionQueue::NO_QUEUE),
        trigger_completion_comp_queue(CompletionQueue::NO_QUEUE),
        deferred_completion_comp_queue(CompletionQueue::NO_QUEUE),
        trigger_commit_comp_queue(CompletionQueue::NO_QUEUE),
        deferred_commit_comp_queue(CompletionQueue::NO_QUEUE),
        post_task_comp_queue(CompletionQueue::NO_QUEUE), 
        current_trace(NULL), previous_trace(NULL),
        physical_trace_replay_status(0), valid_wait_event(false), 
        outstanding_subtasks(0), pending_subtasks(0), pending_frames(0),
        currently_active_context(false), current_mapping_fence_index(0), 
        current_execution_fence_event(exec_fence),
        current_execution_fence_index(0), last_implicit_creation(NULL),
        last_implicit_creation_gen(0)
    //--------------------------------------------------------------------------
    {
      // Set some of the default values for a context
      context_configuration.max_window_size = 
        runtime->initial_task_window_size;
      context_configuration.hysteresis_percentage = 
        runtime->initial_task_window_hysteresis;
      context_configuration.max_outstanding_frames = 0;
      context_configuration.min_tasks_to_schedule = 
        runtime->initial_tasks_to_schedule;
      context_configuration.min_frames_to_schedule = 0;
      context_configuration.meta_task_vector_width = 
        runtime->initial_meta_task_vector_width;
      context_configuration.max_templates_per_trace =
        LEGION_DEFAULT_MAX_TEMPLATES_PER_TRACE;
      context_configuration.mutable_priority = false;
      // If we have an owner, clone our local fields from its context
      // and also compute the coordinates for this context in the task tree
      if (owner != NULL)
      {
        TaskContext *owner_ctx = owner_task->get_context();
#ifdef DEBUG_LEGION
        InnerContext *parent_ctx = dynamic_cast<InnerContext*>(owner_ctx);
        assert(parent_ctx != NULL);
#else
        InnerContext *parent_ctx = static_cast<InnerContext*>(owner_ctx);
#endif
        parent_ctx->clone_local_fields(local_field_infos);
        // Get the coordinates for the parent task
        parent_ctx->compute_task_tree_coordinates(context_coordinates);
        // Then add our coordinates for our task
        context_coordinates.push_back(ContextCoordinate(
              owner_task->get_context_index(), owner_task->index_point));
      }
#ifdef LEGION_GC
      log_garbage.info("GC Inner Context %lld %d", 
          LEGION_DISTRIBUTED_ID_FILTER(this->did), local_space); 
#endif
#ifdef LEGION_SPY
      current_fence_uid = 0;
      current_mapping_fence_gen = 0;
#endif
    }

    //--------------------------------------------------------------------------
    InnerContext::~InnerContext(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(deletion_counts.empty());
#endif
      // At this point we can free our region tree context
      runtime->free_region_tree_context(tree_context);
      if (ready_comp_queue.exists())
        ready_comp_queue.destroy();
      if (enqueue_task_comp_queue.exists())
        enqueue_task_comp_queue.destroy();
      if (distribute_task_comp_queue.exists())
        distribute_task_comp_queue.destroy();
      if (launch_task_comp_queue.exists())
        launch_task_comp_queue.destroy();
      if (resolution_comp_queue.exists())
        resolution_comp_queue.destroy();
      if (trigger_execution_comp_queue.exists())
        trigger_execution_comp_queue.destroy();
      if (deferred_execution_comp_queue.exists())
        deferred_execution_comp_queue.destroy();
      if (trigger_completion_comp_queue.exists())
        trigger_completion_comp_queue.destroy();
      if (deferred_completion_comp_queue.exists())
        deferred_completion_comp_queue.destroy();
      if (trigger_commit_comp_queue.exists())
        trigger_commit_comp_queue.destroy();
      if (deferred_commit_comp_queue.exists())
        deferred_commit_comp_queue.destroy();
      if (post_task_comp_queue.exists())
        post_task_comp_queue.destroy();
      for (std::map<TraceID,LogicalTrace*>::const_iterator it = 
            traces.begin(); it != traces.end(); it++)
        if (it->second->remove_reference())
          delete (it->second);
      traces.clear();
      // Clean up any locks and barriers that the user
      // asked us to destroy
      while (!context_locks.empty())
      {
        context_locks.back().destroy_reservation();
        context_locks.pop_back();
      }
      while (!context_barriers.empty())
      {
        Realm::Barrier bar = context_barriers.back();
        bar.destroy_barrier();
        context_barriers.pop_back();
      }
      if (valid_wait_event)
      {
        valid_wait_event = false;
        Runtime::trigger_event(window_wait);
      }
      // No need for the lock here since we're being cleaned up
      if (!local_field_infos.empty())
        local_field_infos.clear(); 
      if (!attach_functions.empty())
      {
        for (std::map<IndexTreeNode*,
                std::vector<AttachProjectionFunctor*> >::const_iterator fit =
              attach_functions.begin(); fit != attach_functions.end(); fit++)
        {
          for (std::vector<AttachProjectionFunctor*>::const_iterator it =
                fit->second.begin(); it != fit->second.end(); it++)
          {
            // Unregister it with the runtime if it is not the identity
            // The runtime will delete the functor for us
            if ((*it)->pid > 0)
              runtime->unregister_projection_functor((*it)->pid);
            else // This is the identity so we can just delete it ourself
              delete (*it);
          }
        }
        attach_functions.clear();
      }
#ifdef DEBUG_LEGION
      assert(pending_top_views.empty());
      assert(outstanding_subtasks == 0);
      assert(pending_subtasks == 0);
      assert(pending_frames == 0);
#endif
    }

    //--------------------------------------------------------------------------
    void InnerContext::notify_local(void)
    //--------------------------------------------------------------------------
    {
      // Remove any references that we are holding on instance top views
      std::map<PhysicalManager*,IndividualView*> to_unregister;
      {
        AutoLock inst_lock(instance_view_lock);
        to_unregister.swap(instance_top_views);
      }
      for (std::map<PhysicalManager*,IndividualView*>::const_iterator it = 
            to_unregister.begin(); it != to_unregister.end(); it++)
      {
        it->first->unregister_active_context(this);
        if (it->second->remove_nested_gc_ref(did))
          delete (it->second);
      }
      // Remove any global references that we are holding on collective views
      std::map<RegionTreeID,std::vector<CollectiveResult*> > to_release;
      {
        AutoLock c_lock(collective_lock);
        to_release.swap(collective_results);
      }
      for (std::map<RegionTreeID,
            std::vector<CollectiveResult*> >::const_iterator rit =
            to_release.begin(); rit != to_release.end(); rit++)
      {
        for (std::vector<CollectiveResult*>::const_iterator it =
              rit->second.begin(); it != rit->second.end(); it++)
        {
          release_collective_view(runtime, did, (*it)->collective_did);
          delete (*it);
        }
      }
      // Shouldn't need any lock for these as the context is not longer
      // valid and there shouldn't be any races
      while (!value_fill_view_cache.empty())
      {
        FillView* next = value_fill_view_cache.front();
        value_fill_view_cache.pop_front();
        if (next->remove_nested_valid_ref(did))
          delete next;
      }
      while (!future_fill_view_cache.empty())
      {
        FillView *next = future_fill_view_cache.front().first;
        future_fill_view_cache.pop_front();
        if (next->remove_nested_valid_ref(did))
          delete next;
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::receive_resources(size_t return_index,
              std::map<LogicalRegion,unsigned> &created_regs,
              std::vector<DeletedRegion> &deleted_regs,
              std::set<std::pair<FieldSpace,FieldID> > &created_fids,
              std::vector<DeletedField> &deleted_fids,
              std::map<FieldSpace,unsigned> &created_fs,
              std::map<FieldSpace,std::set<LogicalRegion> > &latent_fs,
              std::vector<DeletedFieldSpace> &deleted_fs,
              std::map<IndexSpace,unsigned> &created_is,
              std::vector<DeletedIndexSpace> &deleted_is,
              std::map<IndexPartition,unsigned> &created_partitions,
              std::vector<DeletedPartition> &deleted_partitions,
              std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      bool need_deletion_dependences = true;
      ApEvent precondition;
      std::map<Operation*,GenerationID> dependences;
      if (!created_regs.empty())
        register_region_creations(created_regs);
      if (!deleted_regs.empty())
      {
        precondition = 
          compute_return_deletion_dependences(return_index, dependences);
        need_deletion_dependences = false;
        register_region_deletions(precondition, dependences, 
                                  deleted_regs, preconditions);
      }
      if (!created_fids.empty())
        register_field_creations(created_fids);
      if (!deleted_fids.empty())
      {
        if (need_deletion_dependences)
        {
          precondition = 
            compute_return_deletion_dependences(return_index, dependences);
          need_deletion_dependences = false;
        }
        register_field_deletions(precondition, dependences, 
                                 deleted_fids, preconditions);
      }
      if (!created_fs.empty())
        register_field_space_creations(created_fs);
      if (!latent_fs.empty())
        register_latent_field_spaces(latent_fs);
      if (!deleted_fs.empty())
      {
        if (need_deletion_dependences)
        {
          precondition = 
            compute_return_deletion_dependences(return_index, dependences);
          need_deletion_dependences = false;
        }
        register_field_space_deletions(precondition, dependences,
                                       deleted_fs, preconditions);
      }
      if (!created_is.empty())
        register_index_space_creations(created_is);
      if (!deleted_is.empty())
      {
        if (need_deletion_dependences)
        {
          precondition = 
            compute_return_deletion_dependences(return_index, dependences);
          need_deletion_dependences = false;
        }
        register_index_space_deletions(precondition, dependences,
                                       deleted_is, preconditions);
      }
      if (!created_partitions.empty())
        register_index_partition_creations(created_partitions);
      if (!deleted_partitions.empty())
      {
        if (need_deletion_dependences)
        {
          precondition = 
            compute_return_deletion_dependences(return_index, dependences);
          need_deletion_dependences = false;
        }
        register_index_partition_deletions(precondition, dependences,
                                           deleted_partitions, preconditions);
      }
    }

    //--------------------------------------------------------------------------
    bool InnerContext::verify_hash(const uint64_t hash[2],
                    const char *description, Provenance *provenance, bool every)
    //--------------------------------------------------------------------------
    {
      // Nothing to do for now, but this is where trace checking code
      // should go once we start checking that on replays
      return true;
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_region_creations(
                                      std::map<LogicalRegion,unsigned> &regions)
    //--------------------------------------------------------------------------
    {
      AutoLock priv_lock(privilege_lock);
      if (!latent_field_spaces.empty())
      {
        for (std::map<LogicalRegion,unsigned>::const_iterator it = 
              regions.begin(); it != regions.end(); it++)
        {
          std::map<FieldSpace,std::set<LogicalRegion> >::iterator finder =
            latent_field_spaces.find(it->first.get_field_space());
          if (finder != latent_field_spaces.end())
            finder->second.insert(it->first);
        }
      }
      if (!created_regions.empty())
      {
        for (std::map<LogicalRegion,unsigned>::const_iterator it = 
              regions.begin(); it != regions.end(); it++)
        {
          std::map<LogicalRegion,unsigned>::iterator finder = 
            created_regions.find(it->first);
          if (finder == created_regions.end())
          {
            created_regions.insert(*it);
            add_created_region(it->first, false/*task local*/);
          }
          else
            finder->second += it->second;
        }
      }
      else
      {
        created_regions.swap(regions);
        for (std::map<LogicalRegion,unsigned>::const_iterator it = 
              created_regions.begin(); it != created_regions.end(); it++)
          add_created_region(it->first, false/*task local*/);
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_region_deletions(ApEvent precondition,
                           const std::map<Operation*,GenerationID> &dependences,
                                            std::vector<DeletedRegion> &regions,
                                            std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      std::vector<DeletedRegion> delete_now;
      {
        AutoLock priv_lock(privilege_lock);
        for (std::vector<DeletedRegion>::const_iterator rit =
              regions.begin(); rit != regions.end(); rit++)
        {
          std::map<LogicalRegion,unsigned>::iterator region_finder = 
            created_regions.find(rit->region);
          if (region_finder == created_regions.end())
          {
            if (local_regions.find(rit->region) != local_regions.end())
              REPORT_LEGION_ERROR(ERROR_ILLEGAL_RESOURCE_DESTRUCTION,
                  "Local logical region (%x,%x,%x) in task %s (UID %lld) was "
                  "not deleted by this task. Local regions can only be deleted "
                  "by the task that made them.", rit->region.index_space.id,
                  rit->region.field_space.id, rit->region.tree_id, 
                  get_task_name(), get_unique_id())
            // Deletion keeps going up
            deleted_regions.push_back(*rit);
          }
          else
          {
            // One of ours to delete
#ifdef DEBUG_LEGION
            assert(region_finder->second > 0);
#endif
            if (--region_finder->second == 0)
            {
              // No need to delete this here, it will be deleted by the op
              // Check to see if we have any latent field spaces to clean up
              if (!latent_field_spaces.empty())
              {
                std::map<FieldSpace,std::set<LogicalRegion> >::iterator finder =
                  latent_field_spaces.find(rit->region.get_field_space());
                if (finder != latent_field_spaces.end())
                {
                  std::set<LogicalRegion>::iterator latent_finder = 
                    finder->second.find(rit->region);
#ifdef DEBUG_LEGION
                  assert(latent_finder != finder->second.end());
#endif
                  finder->second.erase(latent_finder);
                  if (finder->second.empty())
                  {
                    // Now that all the regions using this field space have
                    // been deleted we can clean up all the created_fields
                    for (std::set<std::pair<FieldSpace,FieldID> >::iterator it =
                          created_fields.begin(); it != 
                          created_fields.end(); /*nothing*/)
                    {
                      if (it->first == finder->first)
                      {
                        std::set<std::pair<FieldSpace,FieldID> >::iterator 
                          to_delete = it++;
                        created_fields.erase(to_delete);
                      }
                      else
                        it++;
                    }
                    latent_field_spaces.erase(finder);
                  }
                }
              }
              delete_now.push_back(*rit);
            }
          }
        }
      }
      if (!delete_now.empty())
      {
        for (std::vector<DeletedRegion>::const_iterator it = 
              delete_now.begin(); it != delete_now.end(); it++)
        {
          DeletionOp *op = runtime->get_available_deletion_op();
          op->initialize_logical_region_deletion(this, it->region,
              true/*unordered*/, it->provenance,
              true/*skip dependence analysis*/);
          op->set_deletion_preconditions(precondition, dependences);
          if (!add_to_dependence_queue(op, true/*unordered*/))
          {
            // We're past the execution of the parent task so we need
            // to run this manually and capture its effects ourselves
            preconditions.insert(
                Runtime::protect_event(op->get_completion_event()));
            op->execute_dependence_analysis();
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_field_creations(
                               std::set<std::pair<FieldSpace,FieldID> > &fields)
    //--------------------------------------------------------------------------
    {
      AutoLock priv_lock(privilege_lock);
      if (!created_fields.empty())
      {
#ifdef DEBUG_LEGION
        for (std::set<std::pair<FieldSpace,FieldID> >::const_iterator it = 
              fields.begin(); it != fields.end(); it++)
        {
          assert(created_fields.find(*it) == created_fields.end());
          created_fields.insert(*it);
        }
#else
        created_fields.insert(fields.begin(), fields.end());
#endif
      }
      else
        created_fields.swap(fields);
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_field_deletions(ApEvent precondition,
                           const std::map<Operation*,GenerationID> &dependences,
                           std::vector<DeletedField> &fields,
                           std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      std::map<std::pair<FieldSpace,Provenance*>,std::set<FieldID> > delete_now;
      {
        AutoLock priv_lock(privilege_lock);
        for (std::vector<DeletedField>::const_iterator fit =
              fields.begin(); fit != fields.end(); fit++)
        {
          const std::pair<FieldSpace,FieldID> key(fit->space, fit->fid);
          std::set<std::pair<FieldSpace,FieldID> >::const_iterator 
            field_finder = created_fields.find(key);
          if (field_finder == created_fields.end())
          {
            std::map<std::pair<FieldSpace,FieldID>,bool>::iterator 
              local_finder = local_fields.find(key);
            if (local_finder != local_fields.end())
              REPORT_LEGION_ERROR(ERROR_ILLEGAL_RESOURCE_DESTRUCTION,
                  "Local field %d in field space %x in task %s (UID %lld) was "
                  "not deleted by this task. Local fields can only be deleted "
                  "by the task that made them.", fit->fid, fit->space.id,
                  get_task_name(), get_unique_id())
            deleted_fields.push_back(*fit);
          }
          else
          {
            // One of ours to delete
            std::pair<FieldSpace,Provenance*> now_key(fit->space,
                                                      fit->provenance);
            delete_now[now_key].insert(fit->fid);
            // No need to delete this now, it will be deleted
            // when the deletion op makes its region requirements
          }
        }
      }
      if (!delete_now.empty())
      {
        for (std::map<std::pair<FieldSpace,Provenance*>,
                      std::set<FieldID> >::const_iterator it = 
              delete_now.begin(); it != delete_now.end(); it++)
        {
          DeletionOp *op = runtime->get_available_deletion_op();
          FieldAllocatorImpl *allocator = 
            create_field_allocator(it->first.first, true/*unordered*/);
          op->initialize_field_deletions(this, it->first.first, it->second, 
             true/*unordered*/, allocator, it->first.second,
             false/*non owner shard*/, true/*skip dependence analysis*/);
          op->set_deletion_preconditions(precondition, dependences);
          if (!add_to_dependence_queue(op, true/*unordered*/))
          {
            // We're past the execution of the parent task so we need
            // to run this manually and capture its effects ourselves
            preconditions.insert(
                Runtime::protect_event(op->get_completion_event()));
            op->execute_dependence_analysis();
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_field_space_creations(
                                          std::map<FieldSpace,unsigned> &spaces)
    //--------------------------------------------------------------------------
    {
      AutoLock priv_lock(privilege_lock);
      if (!latent_field_spaces.empty())
      {
        // Remove any latent field spaces we have ownership for
        for (std::map<FieldSpace,unsigned>::const_iterator it =
              spaces.begin(); it != spaces.end(); it++)
        {
          std::map<FieldSpace,std::set<LogicalRegion> >::iterator finder = 
            latent_field_spaces.find(it->first);
          if (finder != latent_field_spaces.end())
            latent_field_spaces.erase(finder);
        }
      }
      if (!created_field_spaces.empty())
      {
        for (std::map<FieldSpace,unsigned>::const_iterator it = 
              spaces.begin(); it != spaces.end(); it++)
        {
          std::map<FieldSpace,unsigned>::iterator finder = 
            created_field_spaces.find(it->first);
          if (finder == created_field_spaces.end())
            created_field_spaces.insert(*it);
          else
            finder->second += it->second;
        }
      }
      else
        created_field_spaces.swap(spaces);
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_latent_field_spaces(
                          std::map<FieldSpace,std::set<LogicalRegion> > &spaces)
    //--------------------------------------------------------------------------
    {
      AutoLock p_lock(privilege_lock);
      if (!created_field_spaces.empty())
      {
        // Remote any latent field spaces we already have ownership on
        for (std::map<FieldSpace,std::set<LogicalRegion> >::iterator it =
              spaces.begin(); it != spaces.end(); /*nothing*/)
        {
          if (created_field_spaces.find(it->first) != 
                created_field_spaces.end())
          {
            std::map<FieldSpace,std::set<LogicalRegion> >::iterator 
              to_delete = it++;
            spaces.erase(to_delete);
          }
          else
            it++;
        }
        if (spaces.empty())
          return;
      }
      if (!created_regions.empty())
      {
        // See if any of these regions are copies of our latent spaces
        for (std::map<LogicalRegion,unsigned>::const_iterator it = 
              created_regions.begin(); it != created_regions.end(); it++)
        {
          std::map<FieldSpace,std::set<LogicalRegion> >::iterator finder = 
            spaces.find(it->first.get_field_space());
          if (finder != spaces.end())
            finder->second.insert(it->first);
        }
      }
      // Now we can do the merge
      if (!latent_field_spaces.empty())
      {
        for (std::map<FieldSpace,std::set<LogicalRegion> >::const_iterator it =
              spaces.begin(); it != spaces.end(); it++)
        {
          std::map<FieldSpace,std::set<LogicalRegion> >::iterator finder = 
            latent_field_spaces.find(it->first);
          if (finder != latent_field_spaces.end())
            finder->second.insert(it->second.begin(), it->second.end());
          else
            latent_field_spaces.insert(*it);
        }
      }
      else
        latent_field_spaces.swap(spaces);
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_field_space_deletions(ApEvent precondition,
                           const std::map<Operation*,GenerationID> &dependences,
                                         std::vector<DeletedFieldSpace> &spaces,
                                               std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      std::vector<DeletedFieldSpace> delete_now;
      {
        AutoLock priv_lock(privilege_lock);
        for (std::vector<DeletedFieldSpace>::const_iterator fit = 
              spaces.begin(); fit != spaces.end(); fit++)
        {
          std::map<FieldSpace,unsigned>::iterator finder = 
            created_field_spaces.find(fit->space);
          if (finder != created_field_spaces.end())
          {
#ifdef DEBUG_LEGION
            assert(finder->second > 0);
#endif
            if (--finder->second == 0)
            {
              delete_now.push_back(*fit);
              created_field_spaces.erase(finder);
              // Count how many regions are still using this field space
              // that still need to be deleted before we can remove the
              // list of created fields
              std::set<LogicalRegion> remaining_regions;
              for (std::map<LogicalRegion,unsigned>::const_iterator it = 
                    created_regions.begin(); it != created_regions.end(); it++)
                if (it->first.get_field_space() == fit->space)
                  remaining_regions.insert(it->first);
              for (std::map<LogicalRegion,bool>::const_iterator it = 
                    local_regions.begin(); it != local_regions.end(); it++)
                if (it->first.get_field_space() == fit->space)
                  remaining_regions.insert(it->first);
              if (remaining_regions.empty())
              {
                // No remaining regions so we can remove any created fields now
                for (std::set<std::pair<FieldSpace,FieldID> >::iterator it = 
                      created_fields.begin(); it != 
                      created_fields.end(); /*nothing*/)
                {
                  if (it->first == fit->space)
                  {
                    std::set<std::pair<FieldSpace,FieldID> >::iterator 
                      to_delete = it++;
                    created_fields.erase(to_delete);
                  }
                  else
                    it++;
                }
              }
              else
                latent_field_spaces[fit->space] = remaining_regions;
            }
          }
          else
            // If we didn't make this field space, record the deletion
            // and keep going. It will be handled by the context that
            // made the field space
            deleted_field_spaces.push_back(*fit);
        }
      }
      if (!delete_now.empty())
      {
        for (std::vector<DeletedFieldSpace>::const_iterator it = 
              delete_now.begin(); it != delete_now.end(); it++)
        {
          DeletionOp *op = runtime->get_available_deletion_op();
          op->initialize_field_space_deletion(this, it->space,
                            true/*unordered*/, it->provenance);
          op->set_deletion_preconditions(precondition, dependences);
          if (!add_to_dependence_queue(op, true/*unordered*/))
          {
            // We're past the execution of the parent task so we need
            // to run this manually and capture its effects ourselves
            preconditions.insert(
                Runtime::protect_event(op->get_completion_event()));
            op->execute_dependence_analysis();
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_index_space_creations(
                                          std::map<IndexSpace,unsigned> &spaces)
    //--------------------------------------------------------------------------
    {
      AutoLock priv_lock(privilege_lock);
      if (!created_index_spaces.empty())
      {
        for (std::map<IndexSpace,unsigned>::const_iterator it = 
              spaces.begin(); it != spaces.end(); it++)
        {
          std::map<IndexSpace,unsigned>::iterator finder = 
            created_index_spaces.find(it->first);
          if (finder == created_index_spaces.end())
            created_index_spaces.insert(*it);
          else
            finder->second += it->second;
        }
      }
      else
        created_index_spaces.swap(spaces);
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_index_space_deletions(ApEvent precondition,
                           const std::map<Operation*,GenerationID> &dependences,
                                         std::vector<DeletedIndexSpace> &spaces,
                                               std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      std::vector<DeletedIndexSpace> delete_now;
      std::vector<std::vector<IndexPartition> > sub_partitions;
      {
        AutoLock priv_lock(privilege_lock);
        for (std::vector<DeletedIndexSpace>::const_iterator sit =
              spaces.begin(); sit != spaces.end(); sit++)
        {
          std::map<IndexSpace,unsigned>::iterator finder = 
            created_index_spaces.find(sit->space);
          if (finder != created_index_spaces.end())
          {
#ifdef DEBUG_LEGION
            assert(finder->second > 0);
#endif
            if (--finder->second == 0)
            {
              delete_now.push_back(*sit);
              sub_partitions.resize(sub_partitions.size() + 1);
              created_index_spaces.erase(finder);
              if (sit->recurse)
              {
                std::vector<IndexPartition> &subs = sub_partitions.back();
                // Also remove any index partitions for this index space tree
                for (std::map<IndexPartition,unsigned>::iterator it = 
                      created_index_partitions.begin(); it !=
                      created_index_partitions.end(); /*nothing*/)
                {
                  if (it->first.get_tree_id() == sit->space.get_tree_id()) 
                  {
#ifdef DEBUG_LEGION
                    assert(it->second > 0);
#endif
                    if (--it->second == 0)
                    {
                      subs.push_back(it->first);
                      std::map<IndexPartition,unsigned>::iterator 
                        to_delete = it++;
                      created_index_partitions.erase(to_delete);
                    }
                    else
                      it++;
                  }
                  else
                    it++;
                }
              }
            }
          }
          else
            // If we didn't make the index space in this context, just
            // record it and keep going, it will get handled later
            deleted_index_spaces.push_back(*sit);
        }
      }
      if (!delete_now.empty())
      {
#ifdef DEBUG_LEGION
        assert(delete_now.size() == sub_partitions.size());
#endif
        for (unsigned idx = 0; idx < delete_now.size(); idx++)
        {
          DeletionOp *op = runtime->get_available_deletion_op();
          op->initialize_index_space_deletion(this, delete_now[idx].space,
            sub_partitions[idx], true/*unordered*/, delete_now[idx].provenance);
          op->set_deletion_preconditions(precondition, dependences);
          if (!add_to_dependence_queue(op, true/*unordered*/))
          {
            // We're past the execution of the parent task so we need
            // to run this manually and capture its effects ourselves
            preconditions.insert(
                Runtime::protect_event(op->get_completion_event()));
            op->execute_dependence_analysis();
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_index_partition_creations(
                                       std::map<IndexPartition,unsigned> &parts)
    //--------------------------------------------------------------------------
    {
      AutoLock priv_lock(privilege_lock);
      if (!created_index_partitions.empty())
      {
        for (std::map<IndexPartition,unsigned>::const_iterator it = 
              parts.begin(); it != parts.end(); it++)
        {
          std::map<IndexPartition,unsigned>::iterator finder = 
            created_index_partitions.find(it->first);
          if (finder == created_index_partitions.end())
            created_index_partitions.insert(*it);
          else
            finder->second += it->second;
        }
      }
      else
        created_index_partitions.swap(parts);
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_index_partition_deletions(ApEvent precondition,
                           const std::map<Operation*,GenerationID> &dependences,
                                           std::vector<DeletedPartition> &parts, 
                                               std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      std::vector<DeletedPartition> delete_now;
      std::vector<std::vector<IndexPartition> > sub_partitions;
      {
        AutoLock priv_lock(privilege_lock);
        for (std::vector<DeletedPartition>::const_iterator pit =
              parts.begin(); pit != parts.end(); pit++)
        {
          std::map<IndexPartition,unsigned>::iterator finder = 
            created_index_partitions.find(pit->partition);
          if (finder != created_index_partitions.end())
          {
#ifdef DEBUG_LEGION
            assert(finder->second > 0);
#endif
            if (--finder->second == 0)
            {
              delete_now.push_back(*pit);
              sub_partitions.resize(sub_partitions.size() + 1);
              created_index_partitions.erase(finder);
              if (pit->recurse)
              {
                std::vector<IndexPartition> &subs = sub_partitions.back();
                // Remove any other partitions that this partition dominates
                for (std::map<IndexPartition,unsigned>::iterator it = 
                      created_index_partitions.begin(); it !=
                      created_index_partitions.end(); /*nothing*/)
                {
                  if ((pit->partition.get_tree_id() == it->first.get_tree_id()) 
                        && runtime->forest->is_dominated_tree_only(it->first, 
                                                                pit->partition))
                  {
#ifdef DEBUG_LEGION
                    assert(it->second > 0);
#endif
                    if (--it->second == 0)
                    {
                      subs.push_back(it->first);
                      std::map<IndexPartition,unsigned>::iterator 
                        to_delete = it++;
                      created_index_partitions.erase(to_delete);
                    }
                    else
                      it++;
                  }
                  else
                    it++;
                }
              }
            }
          }
          else
            // If we didn't make the partition, record it and keep going
            deleted_index_partitions.push_back(*pit);
        }
      }
      if (!delete_now.empty())
      {
#ifdef DEBUG_LEGION
        assert(delete_now.size() == sub_partitions.size());
#endif
        for (unsigned idx = 0; idx < delete_now.size(); idx++)
        {
          DeletionOp *op = runtime->get_available_deletion_op();
          op->initialize_index_part_deletion(this, delete_now[idx].partition,
            sub_partitions[idx], true/*unordered*/, delete_now[idx].provenance);
          op->set_deletion_preconditions(precondition, dependences);
          if (!add_to_dependence_queue(op, true/*unordered*/))
          {
            // We're past the execution of the parent task so we need
            // to run this manually and capture its effects ourselves
            preconditions.insert(
                Runtime::protect_event(op->get_completion_event()));
            op->execute_dependence_analysis();
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    ApEvent InnerContext::compute_return_deletion_dependences(
            size_t return_index, std::map<Operation*,GenerationID> &dependences)
    //--------------------------------------------------------------------------
    {
      // This is a mixed mapping and execution fence analysis 
      std::set<ApEvent> previous_events;
      {
        AutoLock child_lock(child_op_lock,1,false/*exclusive*/); 
        for (std::deque<ReorderBufferEntry>::const_iterator it =
              reorder_buffer.begin(); it != reorder_buffer.end(); it++)
        {
          if ((it->stage == COMPLETED_STAGE) || (it->stage == COMMITTED_STAGE))
            continue;
          // If it's younger than our deletion we don't care
          if (it->operation_index >= return_index)
            continue;
          dependences[it->operation] = it->operation->get_generation();
          it->operation->find_completion_effects(previous_events);
        }
      }
      // Do not check the current execution fence as it may have come after us
      if (!previous_events.empty())
        return Runtime::merge_events(NULL, previous_events);
      return ApEvent::NO_AP_EVENT;
    }

    //--------------------------------------------------------------------------
    ContextID InnerContext::get_logical_tree_context(void) const
    //--------------------------------------------------------------------------
    {
      return tree_context;
    }

    //--------------------------------------------------------------------------
    ContextID InnerContext::get_physical_tree_context(void) const
    //--------------------------------------------------------------------------
    {
      return tree_context;
    }

    //--------------------------------------------------------------------------
    bool InnerContext::is_inner_context(void) const
    //--------------------------------------------------------------------------
    {
      return full_inner_context;
    }

    //--------------------------------------------------------------------------
    RtEvent InnerContext::compute_equivalence_sets(unsigned req_index,
                             const std::vector<EqSetTracker*> &targets,
                             const std::vector<AddressSpaceID> &target_spaces,
                             AddressSpaceID creation_target_space, 
                             IndexSpaceExpression *expr, const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(targets.size() == target_spaces.size());
      assert(std::is_sorted(target_spaces.begin(), target_spaces.end()));
      assert(std::binary_search(target_spaces.begin(), target_spaces.end(),
                                creation_target_space));
#endif
      // Find the equivalence set tree for this region requirement
      LocalLock *tree_lock = NULL;
      EqKDTree *tree = find_equivalence_set_kd_tree(req_index, tree_lock);
      // Then ask the index space expression to traverse the tree for
      // all of its rectangles and find the equivalence sets that are needed
      FieldMaskSet<EqKDTree> to_create, new_subscriptions;
      FieldMaskSet<EquivalenceSet> eq_sets;
      std::vector<RtEvent> pending_sets;
      std::map<EqKDTree*,Domain> creation_rects;
      std::map<EquivalenceSet*,LegionMap<Domain,FieldMask> > creation_srcs;
      std::map<ShardID,LegionMap<Domain,FieldMask> > remote_shard_rects;
      std::vector<unsigned> new_target_references(targets.size(), 0);
      expr->compute_equivalence_sets(tree, tree_lock, mask, targets,
          target_spaces, new_target_references, eq_sets, pending_sets,
          new_subscriptions, to_create, creation_rects, creation_srcs,
          remote_shard_rects);
#ifdef DEBUG_LEGION
      assert(remote_shard_rects.empty());
#endif
      const CollectiveMapping target_mapping(target_spaces, 
                          runtime->legion_collective_radix); 
      return report_equivalence_sets(target_mapping, targets, 
          creation_target_space, mask, new_target_references, eq_sets, 
          new_subscriptions, to_create, creation_rects, creation_srcs,
          1/*expected responses*/, pending_sets);
    }

    //--------------------------------------------------------------------------
    RtEvent InnerContext::report_equivalence_sets(
          const CollectiveMapping &target_mapping, 
          const std::vector<EqSetTracker*> &targets,
          const AddressSpaceID creation_target_space, const FieldMask &mask,
          std::vector<unsigned> &new_target_references,
          FieldMaskSet<EquivalenceSet> &eq_sets,
          FieldMaskSet<EqKDTree> &new_subscriptions,
          FieldMaskSet<EqKDTree> &to_create,
          std::map<EqKDTree*,Domain> &creation_rects,
          std::map<EquivalenceSet*,LegionMap<Domain,FieldMask> > &creation_srcs,
          size_t expected_responses, std::vector<RtEvent> &ready_events)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(targets.size() == target_mapping.size());
      assert(targets.size() == new_target_references.size());
      assert(to_create.size() == creation_rects.size());
#endif
      // Figure out where the origin of the target mapping should be
      const AddressSpaceID local_space = runtime->address_space;
      const AddressSpaceID origin_space = target_mapping.contains(local_space) ?
        local_space : target_mapping.find_nearest(local_space);
      std::vector<AddressSpaceID> children;
      if (origin_space == local_space)
        target_mapping.get_children(origin_space, local_space, children);
      else
        children.push_back(origin_space);
      AddressSpaceID creation_child = origin_space;
      if (!to_create.empty() && !children.empty() && 
          (creation_target_space != origin_space))
      {
        std::sort(children.begin(), children.end());
        creation_child = creation_target_space;
        while (!std::binary_search(children.begin(), 
                    children.end(), creation_child))
        {
#ifdef DEBUG_LEGION
          assert(creation_child != origin_space);
#endif
          creation_child =
            target_mapping.get_parent(origin_space, creation_child);
        }
      }
      for (std::vector<AddressSpaceID>::const_iterator cit =
            children.begin(); cit != children.end(); cit++)
      {
        // Send a message back to the child node with the results
        const RtUserEvent ready_event = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          pack_inner_context(rez);
          rez.serialize(runtime->address_space);
          target_mapping.pack(rez);
          for (unsigned idx = 0; idx < targets.size(); idx++)
          {
            rez.serialize(targets[idx]);
            rez.serialize(new_target_references[idx]);
          }
          rez.serialize(creation_target_space);
          rez.serialize(mask);
          rez.serialize<size_t>(eq_sets.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                eq_sets.begin(); it != eq_sets.end(); it++)
          {
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
          rez.serialize<size_t>(new_subscriptions.size());
          for (FieldMaskSet<EqKDTree>::const_iterator it =
                new_subscriptions.begin(); it != new_subscriptions.end(); it++)
          {
            rez.serialize(it->first);
            rez.serialize(it->second);
          }
          // We only need to send the creation sets along the path to 
          // creation_target_space
          if ((*cit) == creation_child)
          {
            rez.serialize<size_t>(to_create.size());
            for (FieldMaskSet<EqKDTree>::const_iterator it =
                  to_create.begin(); it != to_create.end(); it++)
            {
#ifdef DEBUG_LEGION
              assert(creation_rects.find(it->first) != creation_rects.end());
#endif
              rez.serialize(it->first);
              rez.serialize(it->second);
              rez.serialize(creation_rects[it->first]);
            }
            rez.serialize<size_t>(creation_srcs.size());
            for (std::map<EquivalenceSet*,
                          LegionMap<Domain,FieldMask> >::const_iterator sit =
                  creation_srcs.begin(); sit != creation_srcs.end(); sit++)
            {
              // No need to pack a reference since we know that that 
              // EqKDTree is still holding references to the creation_srcs
              // until we're done making the new equivalence sets
              rez.serialize(sit->first->did);
              rez.serialize<size_t>(sit->second.size());
              for (LegionMap<Domain,FieldMask>::const_iterator it =
                    sit->second.begin(); it != sit->second.end(); it++)
              {
                rez.serialize(it->first);
                rez.serialize(it->second);
              }
            }
          }
          else
          {
            rez.serialize<size_t>(0);
            rez.serialize<size_t>(0);
          }
          rez.serialize(expected_responses);
          rez.serialize(ready_event);
        }
        runtime->send_compute_equivalence_sets_response(*cit, rez);
        ready_events.push_back(ready_event);
      }
      if (origin_space == local_space)
      {
        // We can report the results back immediately
        const unsigned target_index = target_mapping.find_index(local_space);
#ifdef DEBUG_LEGION
        assert(target_index < targets.size());
#endif
        targets[target_index]->record_equivalence_sets(this,
            mask, eq_sets, to_create, creation_rects, creation_srcs,
            new_subscriptions, new_target_references[target_index], local_space,
            expected_responses, ready_events, target_mapping, targets, 
            creation_target_space);
      }
      if (!ready_events.empty())
        return Runtime::merge_events(ready_events);
      else
        return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_compute_equivalence_sets_response(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      InnerContext *context = unpack_inner_context(derez, runtime);
      AddressSpaceID source_space;
      derez.deserialize(source_space);
      size_t total_spaces;
      derez.deserialize(total_spaces);
      const CollectiveMapping target_mapping(derez, total_spaces);
      const AddressSpaceID origin_space = target_mapping.contains(source_space)
        ? source_space : target_mapping.find_nearest(source_space);
      // Check to see if there are any children to continue sending this to
      std::vector<AddressSpaceID> children;
      target_mapping.get_children(origin_space,runtime->address_space,children);
      std::vector<EqSetTracker*> targets(target_mapping.size());
      std::vector<unsigned> new_target_references(target_mapping.size());
      for (unsigned idx = 0; idx < targets.size(); idx++)
      {
        derez.deserialize(targets[idx]);
        derez.deserialize(new_target_references[idx]);
      }
      AddressSpaceID creation_target_space;
      derez.deserialize(creation_target_space);
      FieldMask mask;
      derez.deserialize(mask);
      size_t num_sets;
      derez.deserialize(num_sets);
      FieldMaskSet<EquivalenceSet> eq_sets;
      std::map<EquivalenceSet*,DistributedID> did_map;
      std::vector<RtEvent> done_events;
      for (unsigned idx = 0; idx < num_sets; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        RtEvent ready;
        EquivalenceSet *set = 
          runtime->find_or_request_equivalence_set(did, ready);
        if (ready.exists())
          done_events.push_back(ready);
        FieldMask set_mask;
        derez.deserialize(set_mask);
        eq_sets.insert(set, set_mask);
        if (!children.empty())
          did_map.emplace(std::make_pair(set, did));
      }
      size_t num_subscriptions;
      derez.deserialize(num_subscriptions);
      FieldMaskSet<EqKDTree> new_subscriptions;
      for (unsigned idx = 0; idx < num_subscriptions; idx++)
      {
        EqKDTree *tree;
        derez.deserialize(tree);
        FieldMask mask;
        derez.deserialize(mask);
        new_subscriptions.insert(tree, mask);
      }
      size_t num_creations;
      derez.deserialize(num_creations);
      FieldMaskSet<EqKDTree> to_create;
      std::map<EqKDTree*,Domain> creation_rects;
      for (unsigned idx = 0; idx < num_creations; idx++)
      {
        EqKDTree *tree;
        derez.deserialize(tree);
        FieldMask tree_mask;
        derez.deserialize(tree_mask);
        to_create.insert(tree, tree_mask);
        derez.deserialize(creation_rects[tree]);
      }
      std::map<DistributedID,LegionMap<Domain,FieldMask> > temporary_srcs;
      std::map<EquivalenceSet*,LegionMap<Domain,FieldMask> > creation_srcs;
      size_t num_sources;
      derez.deserialize(num_sources);
      std::vector<RtEvent> ready_events;
      if (creation_target_space == runtime->address_space)
      {
        for (unsigned idx1 = 0; idx1 < num_sources; idx1++)
        {
          DistributedID did;
          derez.deserialize(did);
          RtEvent ready;
          EquivalenceSet *set = 
            runtime->find_or_request_equivalence_set(did, ready);
          if (ready.exists())
            ready_events.push_back(ready);
          LegionMap<Domain,FieldMask> &rects = creation_srcs[set];
          size_t num_rects;
          derez.deserialize(num_rects);
          for (unsigned idx2 = 0; idx2 < num_rects; idx2++)
          {
            Domain rect;
            derez.deserialize(rect);
            derez.deserialize(rects[rect]);
          }
        }
      }
      else
      {
        for (unsigned idx1 = 0; idx1 < num_sources; idx1++)
        {
          DistributedID did;
          derez.deserialize(did);
          LegionMap<Domain,FieldMask> &rects = temporary_srcs[did];
          size_t num_rects;
          derez.deserialize(num_rects);
          for (unsigned idx2 = 0; idx2 < num_rects; idx2++)
          {
            Domain rect;
            derez.deserialize(rect);
            derez.deserialize(rects[rect]);
          }
        }
      }
      size_t expected_responses;
      derez.deserialize(expected_responses);
      RtUserEvent done_event;
      derez.deserialize(done_event);

      AddressSpaceID creation_child = origin_space;
      if (!to_create.empty() && !children.empty() &&
          (creation_target_space != runtime->address_space))
      {
        std::sort(children.begin(), children.end());
        creation_child = creation_target_space;
        while (!std::binary_search(children.begin(), 
                    children.end(), creation_child))
        {
#ifdef DEBUG_LEGION
          assert(creation_child != origin_space);
#endif
          creation_child =
            target_mapping.get_parent(origin_space, creation_child);
        }
      }
      // Send off any messages to children 
      for (std::vector<AddressSpaceID>::const_iterator cit =
            children.begin(); cit != children.end(); cit++)
      {
        // Send a message back to the child node with the results
        const RtUserEvent child_event = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          context->pack_inner_context(rez);
          rez.serialize(source_space);
          target_mapping.pack(rez);
          for (unsigned idx = 0; idx < targets.size(); idx++)
          {
            rez.serialize(targets[idx]);
            rez.serialize(new_target_references[idx]);
          }
          rez.serialize(creation_target_space);
          rez.serialize(mask);
          rez.serialize<size_t>(eq_sets.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                eq_sets.begin(); it != eq_sets.end(); it++)
          {
#ifdef DEBUG_LEGION
            assert(did_map.find(it->first) != did_map.end());
#endif
            rez.serialize(did_map[it->first]);
            rez.serialize(it->second);
          }
          rez.serialize<size_t>(new_subscriptions.size());
          for (FieldMaskSet<EqKDTree>::const_iterator it =
                new_subscriptions.begin(); it != new_subscriptions.end(); it++)
          {
            rez.serialize(it->first);
            rez.serialize(it->second);
          }
          // We only need to send the creation sets along the path to 
          // creation_target_space
          if ((*cit) == creation_child)
          {
#ifdef DEBUG_LEGION
            assert(creation_srcs.empty());
#endif
            rez.serialize<size_t>(to_create.size());
            for (FieldMaskSet<EqKDTree>::const_iterator it =
                  to_create.begin(); it != to_create.end(); it++)
            {
#ifdef DEBUG_LEGION
              assert(creation_rects.find(it->first) != creation_rects.end());
#endif
              rez.serialize(it->first);
              rez.serialize(it->second);
              rez.serialize(creation_rects[it->first]);
            }
            rez.serialize<size_t>(temporary_srcs.size());
            for (std::map<DistributedID,
                          LegionMap<Domain,FieldMask> >::const_iterator sit =
                  temporary_srcs.begin(); sit != temporary_srcs.end(); sit++)
            {
              // No need to pack a reference since we know that that 
              // EqKDTree is still holding references to the creation_srcs
              // until we're done making the new equivalence sets
              rez.serialize(sit->first);
              rez.serialize<size_t>(sit->second.size());
              for (LegionMap<Domain,FieldMask>::const_iterator it =
                    sit->second.begin(); it != sit->second.end(); it++)
              {
                rez.serialize(it->first);
                rez.serialize(it->second);
              }
            }
          }
          else
          {
            rez.serialize<size_t>(0);
            rez.serialize<size_t>(0);
          }
          rez.serialize(expected_responses);
          rez.serialize(child_event);
        }
        runtime->send_compute_equivalence_sets_response(*cit, rez);
        done_events.push_back(child_event);
      }
      // Wait for any ready events to be complete
      if (!ready_events.empty())
      {
        const RtEvent wait_on = Runtime::merge_events(ready_events);
        if (wait_on.exists() && !wait_on.has_triggered())
          wait_on.wait();
      }
      // Find the local target
#ifdef DEBUG_LEGION
      assert(target_mapping.contains(runtime->address_space));
#endif
      unsigned target_index = target_mapping.find_index(runtime->address_space);
#ifdef DEBUG_LEGION
      assert(target_index < targets.size());
#endif
      targets[target_index]->record_equivalence_sets(context,
          mask, eq_sets, to_create, creation_rects, creation_srcs, 
          new_subscriptions, new_target_references[target_index], source_space,
          expected_responses, done_events, target_mapping, targets,
          creation_target_space);
      if (!done_events.empty())
        Runtime::trigger_event(done_event, Runtime::merge_events(done_events));
      else
        Runtime::trigger_event(done_event);
    }

    //--------------------------------------------------------------------------
    RtEvent InnerContext::record_output_equivalence_set(EqSetTracker *source,
                              AddressSpaceID source_space, unsigned req_index,
                              EquivalenceSet *set, const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      // Be very careful, you can't use find_equivalence_set_kd_tree here
      // because the tree will not be marked ready until after all the 
      // output equivalence sets have registered themselves, so it's up
      // to us to make an equivalence set tree if one doesn't already
      // exist and then register ourselves with it
      LocalLock *tree_lock = NULL;
      EqKDTree *tree = find_or_create_output_set_kd_tree(req_index, tree_lock);
      FieldMaskSet<EqKDTree> new_subscriptions;
      std::map<ShardID,LegionMap<Domain,FieldMask> > remote_shard_rects;
      unsigned references = set->set_expr->record_output_equivalence_set(tree,
          tree_lock, set, mask, source, source_space, new_subscriptions,
          remote_shard_rects);
#ifdef DEBUG_LEGION
      assert(remote_shard_rects.empty());
#endif
      return report_output_registrations(source, source_space, references,
                                         new_subscriptions);
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_output_equivalence_set_request(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      InnerContext *context = unpack_inner_context(derez, runtime);
      EqSetTracker *source;
      derez.deserialize(source);
      AddressSpaceID source_space;
      derez.deserialize(source_space);
      unsigned req_index;
      derez.deserialize(req_index);
      DistributedID set_did;
      derez.deserialize(set_did);
      RtEvent set_ready;
      EquivalenceSet *set = 
        runtime->find_or_request_equivalence_set(set_did, set_ready);
      FieldMask mask;
      derez.deserialize(mask);
      RtUserEvent recorded;
      derez.deserialize(recorded);
      if (set_ready.exists() && !set_ready.has_triggered())
        set_ready.wait();
      Runtime::trigger_event(recorded,
          context->record_output_equivalence_set(source, source_space,
            req_index, set, mask));
      set->unpack_global_ref();
    }

    //--------------------------------------------------------------------------
    RtEvent InnerContext::report_output_registrations(EqSetTracker *target,
        AddressSpaceID target_space, unsigned references,
        FieldMaskSet<EqKDTree> &new_subscriptions)
    //--------------------------------------------------------------------------
    {
      if (new_subscriptions.empty())
      {
#ifdef DEBUG_LEGION
        assert(references == 0);
#endif
        return RtEvent::NO_RT_EVENT;
      }
      if (target_space != runtime->address_space)
      {
        const RtUserEvent reported = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(target);
          rez.serialize(references);
          rez.serialize<size_t>(new_subscriptions.size());
          for (FieldMaskSet<EqKDTree>::const_iterator it =
                new_subscriptions.begin(); it != new_subscriptions.end(); it++)
          {
            rez.serialize(it->first);
            rez.serialize(it->second);
          }
          rez.serialize(reported);
        }
        runtime->send_output_equivalence_set_response(target_space, rez);
        return reported;
      }
      else
      {
        if (references > 0)
          target->add_subscription_reference(references);
        target->record_output_subscriptions(runtime->address_space,
                                            new_subscriptions);
        return RtEvent::NO_RT_EVENT;
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_output_equivalence_set_response(
                   Deserializer &derez, Runtime *runtime, AddressSpaceID source) 
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      EqSetTracker *tracker;
      derez.deserialize(tracker);
      unsigned references;
      derez.deserialize(references);
      if (references > 0)
        tracker->add_subscription_reference(references);
      size_t num_subscriptions;
      derez.deserialize(num_subscriptions);
      FieldMaskSet<EqKDTree> new_subscriptions;
      for (unsigned idx = 0; idx < num_subscriptions; idx++)
      {
        EqKDTree *tree;
        derez.deserialize(tree);
        FieldMask mask;
        derez.deserialize(mask);
        new_subscriptions.insert(tree, mask);
      }
      tracker->record_output_subscriptions(source, new_subscriptions);
      RtUserEvent reported;
      derez.deserialize(reported);
      Runtime::trigger_event(reported);
    }

    //--------------------------------------------------------------------------
    InnerContext::EqKDRoot::EqKDRoot(void)
      : tree(NULL), lock(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    InnerContext::EqKDRoot::EqKDRoot(EqKDTree *t)
      : tree(t), lock(new LocalLock())
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(tree != NULL);
#endif
      tree->add_reference();
    }

    //--------------------------------------------------------------------------
    InnerContext::EqKDRoot::EqKDRoot(EqKDRoot &&rhs)
      : tree(rhs.tree), lock(rhs.lock)
    //--------------------------------------------------------------------------
    {
      rhs.tree = NULL;
      rhs.lock = NULL;
    }

    //--------------------------------------------------------------------------
    InnerContext::EqKDRoot::~EqKDRoot(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      // Either both NULL or both not NULL
      assert((tree == NULL) == (lock == NULL));
#endif
      if (tree != NULL)
      {
        if (tree->remove_reference())
          delete tree;
        delete lock;
      }
    }

    //--------------------------------------------------------------------------
    InnerContext::EqKDRoot& InnerContext::EqKDRoot::operator=(EqKDRoot &&rhs)
    //--------------------------------------------------------------------------
    {
      if (tree == NULL)
      {
#ifdef DEBUG_LEGION
        assert(lock == NULL);
#endif
        tree = rhs.tree;
        lock = rhs.lock;
        rhs.tree = NULL;
        rhs.lock = NULL;
      }
      else
      {
#ifdef DEBUG_LEGION
        assert(lock != NULL);
        // Should never be overwriting one tree with another
        assert(rhs.tree == NULL);
        assert(rhs.lock == NULL);
#endif
        if (tree->remove_reference())
          delete tree;
        delete lock;
        tree = NULL;
        lock = NULL;
      }
      return *this;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::find_root_index_space(unsigned req_index)
    //--------------------------------------------------------------------------
    {
      // Already holding the privilege lock from the caller
      if (req_index < regions.size())
      {
#ifdef DEBUG_LEGION
        assert(regions[req_index].handle_type == LEGION_SINGULAR_PROJECTION);
#endif
        return regions[req_index].region.get_index_space();
      }
      else
      {
        std::map<unsigned,RegionRequirement>::const_iterator finder =
          created_requirements.find(req_index);
#ifdef DEBUG_LEGION
        assert(finder != created_requirements.end());
        assert(finder->second.handle_type == LEGION_SINGULAR_PROJECTION);
#endif
        return finder->second.region.get_index_space();
      }
    }

    //--------------------------------------------------------------------------
    EqKDTree* InnerContext::find_equivalence_set_kd_tree(unsigned req_index,
                        LocalLock *&tree_lock, bool return_null_if_doesnt_exist)
    //--------------------------------------------------------------------------
    {
      // Use the privilege lock since we also need to access the created
      // requirements data structure as well in this routine
      RtEvent wait_on;
      {
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
        // If there is a guard we always need to wait on it
        std::map<unsigned,RtUserEvent>::const_iterator finder =
          pending_equivalence_set_trees.find(req_index);
        if (finder != pending_equivalence_set_trees.end())
          wait_on = finder->second;
        else
        {
          std::map<unsigned,EqKDRoot>::const_iterator finder =
            equivalence_set_trees.find(req_index);
          if (finder != equivalence_set_trees.end())
          {
            tree_lock = finder->second.lock;
            return finder->second.tree;
          }
          else if (return_null_if_doesnt_exist)
            return NULL;
        }
      }
      IndexSpace root_space = IndexSpace::NO_SPACE;
      if (!wait_on.exists())
      {
        // Make sure we didn't lose the race
        AutoLock priv_lock(privilege_lock);
        std::map<unsigned,RtUserEvent>::iterator finder =
          pending_equivalence_set_trees.find(req_index);
        // If there's a guard always make sure we wait on it
        if (finder != pending_equivalence_set_trees.end())
        {
          // There's already a guard so someone else is making it
          if (!finder->second.exists())
            finder->second = Runtime::create_rt_user_event();
          wait_on = finder->second;
        }
        else 
        {
          std::map<unsigned,EqKDRoot>::const_iterator finder =
            equivalence_set_trees.find(req_index);
          if (finder != equivalence_set_trees.end())
          {
            tree_lock = finder->second.lock;
            return finder->second.tree;
          }
          // save a guard that we're making this
          pending_equivalence_set_trees[req_index] = 
            RtUserEvent::NO_RT_USER_EVENT;
          root_space = find_root_index_space(req_index);
        }
      }
      if (!wait_on.exists())
      {
#ifdef DEBUG_LEGION
        assert(root_space.exists());
#endif
        // Create the equivalence set tree
        IndexSpaceNode *root = runtime->forest->get_node(root_space);
        // Normal contexts and control replication contexts will do 
        // different things here while creating the equivalence set tree
        EqKDTree *tree = create_equivalence_set_kd_tree(root);
        // Now we can save it and wake up anyone looking for it
        AutoLock priv_lock(privilege_lock);
        std::map<unsigned,EqKDRoot>::const_iterator it =
          equivalence_set_trees.emplace(req_index, EqKDRoot(tree)).first;
        tree_lock = it->second.lock;
        std::map<unsigned,RtUserEvent>::iterator finder = 
          pending_equivalence_set_trees.find(req_index);
#ifdef DEBUG_LEGION
        assert(finder != pending_equivalence_set_trees.end());
#endif
        if (finder->second.exists())
          Runtime::trigger_event(finder->second);
        pending_equivalence_set_trees.erase(finder);
        return tree;
      }
      else
      {
        wait_on.wait();
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
        std::map<unsigned,EqKDRoot>::const_iterator finder =
          equivalence_set_trees.find(req_index);
#ifdef DEBUG_LEGION
        assert(finder != equivalence_set_trees.end());
#endif
        tree_lock = finder->second.lock;
        return finder->second.tree;
      }
    }

    //--------------------------------------------------------------------------
    EqKDTree* InnerContext::find_or_create_output_set_kd_tree(
                                      unsigned req_index, LocalLock *&tree_lock)
    //--------------------------------------------------------------------------
    {
      IndexSpace root_space = IndexSpace::NO_SPACE;
      {
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
#ifdef DEBUG_LEGION
        // Should always have a guard here
        assert(pending_equivalence_set_trees.find(req_index) !=
            pending_equivalence_set_trees.end());
#endif
        std::map<unsigned,EqKDRoot>::const_iterator finder =
          equivalence_set_trees.find(req_index);
        if (finder != equivalence_set_trees.end())
        {
          tree_lock = finder->second.lock;
          return finder->second.tree;
        }
        root_space = find_root_index_space(req_index);
      }
#ifdef DEBUG_LEGION
      assert(root_space.exists());
#endif
      IndexSpaceNode *root = runtime->forest->get_node(root_space);
      EqKDTree *tree = create_equivalence_set_kd_tree(root);
      AutoLock priv_lock(privilege_lock);
      std::map<unsigned,EqKDRoot>::const_iterator finder =
        equivalence_set_trees.find(req_index);
      if (finder == equivalence_set_trees.end())
        finder = equivalence_set_trees.emplace(req_index, EqKDRoot(tree)).first;
      else
        delete tree;
      tree_lock = finder->second.lock;
      return finder->second.tree;
    }

    //--------------------------------------------------------------------------
    void InnerContext::finalize_output_eqkd_tree(unsigned req_index)
    //--------------------------------------------------------------------------
    {
      IndexSpace root_space = IndexSpace::NO_SPACE;
      {
        AutoLock priv_lock(privilege_lock);
        std::map<unsigned,RtUserEvent>::iterator finder =
            pending_equivalence_set_trees.find(req_index);
#ifdef DEBUG_LEGION
        // Should always find an existing guard for outputs
        assert(finder != pending_equivalence_set_trees.end());
#endif
        // If there are no waiters or the equivalence set tree has
        // already been made then we are just done
        if (equivalence_set_trees.find(req_index) != 
            equivalence_set_trees.end())
        {
          if (finder->second.exists())
            Runtime::trigger_event(finder->second);
          pending_equivalence_set_trees.erase(finder);
          return;
        }
        // If we get here there is someone waiting for us to make
        // the tree and it hasn't been made yet, so do that now
        root_space = find_root_index_space(req_index);
      }
      // If we get here then we need to make the new tree
      IndexSpaceNode *root = runtime->forest->get_node(root_space);
      // Normal contexts and control replication contexts will do 
      // different things here while creating the equivalence set tree
      EqKDTree *tree = create_equivalence_set_kd_tree(root);
      AutoLock priv_lock(privilege_lock);
#ifdef DEBUG_LEGION
      // No one else should have made it in the interim
      assert(equivalence_set_trees.find(req_index) == 
              equivalence_set_trees.end());
#endif
      equivalence_set_trees.emplace(req_index, EqKDRoot(tree));
      std::map<unsigned,RtUserEvent>::iterator finder = 
        pending_equivalence_set_trees.find(req_index);
#ifdef DEBUG_LEGION
      assert(finder != pending_equivalence_set_trees.end());
#endif
      if (finder->second.exists())
        Runtime::trigger_event(finder->second);
      pending_equivalence_set_trees.erase(finder);
    }

    //--------------------------------------------------------------------------
    EqKDTree* InnerContext::create_equivalence_set_kd_tree(IndexSpaceNode *node)
    //--------------------------------------------------------------------------
    {
      // We can just construct this like normal
      return node->create_equivalence_set_kd_tree();
    }

    //--------------------------------------------------------------------------
    void InnerContext::refine_equivalence_sets(unsigned req_index,
                        IndexSpaceNode *node, const FieldMask &refinement_mask,
                        std::vector<RtEvent> &applied_events, bool sharded)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!sharded);
#endif
      LocalLock *tree_lock = NULL;
      EqKDTree *tree = find_equivalence_set_kd_tree(req_index, tree_lock);
      node->invalidate_equivalence_set_kd_tree(tree, tree_lock, refinement_mask,
                            applied_events, true/*move to previous*/);
    }

    //--------------------------------------------------------------------------
    int InnerContext::find_parent_region_req(const RegionRequirement &req,
                                             bool check_privilege /*= true*/)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, FIND_PARENT_REGION_REQ_CALL);
      // We can check most of our region requirements without the lock
      for (unsigned idx = 0; idx < regions.size(); idx++)
      {
        const RegionRequirement &our_req = regions[idx];
        // First check that the regions match
        if (our_req.region != req.parent)
          continue;
        // Next check the privileges
        if (check_privilege && 
            ((PRIV_ONLY(req) & our_req.privilege) != PRIV_ONLY(req)))
          continue;
        // Finally check that all the fields are contained
        bool dominated = true;
        for (std::set<FieldID>::const_iterator it = 
              req.privilege_fields.begin(); it !=
              req.privilege_fields.end(); it++)
        {
          if (our_req.privilege_fields.find(*it) ==
              our_req.privilege_fields.end())
          {
            dominated = false;
            break;
          }
        }
        if (!dominated)
          continue;
        return int(idx);
      }
      const FieldSpace fs = req.parent.get_field_space(); 
      // The created region requirements have to be checked while holding
      // the lock since they are subject to mutation by the application
      // We might also mutate it so we take the lock in exclusive mode
      AutoLock priv_lock(privilege_lock);
      for (std::map<unsigned,RegionRequirement>::iterator it = 
           created_requirements.begin(); it != created_requirements.end(); it++)
      {
        RegionRequirement &our_req = it->second;
        // First check that the regions match
        if (our_req.region != req.parent)
          continue;
        // Next check the privileges
        if (check_privilege && 
            ((PRIV_ONLY(req) & our_req.privilege) != PRIV_ONLY(req)))
          continue;
#ifdef DEBUG_LEGION
        assert(returnable_privileges.find(it->first) != 
                returnable_privileges.end());
#endif
        // If this is a returnable privilege requiremnt that means
        // that we made this region so we always have privileges
        // on any fields for that region, just add them and be done
        if (returnable_privileges[it->first])
        {
          our_req.privilege_fields.insert(req.privilege_fields.begin(),
                                          req.privilege_fields.end());
          return it->first;
        }
        // Finally check that all the fields are contained
        bool dominated = true;
        for (std::set<FieldID>::const_iterator fit = 
              req.privilege_fields.begin(); fit !=
              req.privilege_fields.end(); fit++)
        {
          if (our_req.privilege_fields.find(*fit) ==
              our_req.privilege_fields.end())
          {
            // Check to see if this is a field we made
            // and haven't destroyed yet
            std::pair<FieldSpace,FieldID> key(fs, *fit);
            if (created_fields.find(key) != created_fields.end())
            {
              // We made it so we can add it to the requirement
              // and continue on our way
              our_req.privilege_fields.insert(*fit);
              continue;
            }
            if (local_fields.find(key) != local_fields.end())
            {
              // We made it so we can add it to the requirement
              // and continue on our way
              our_req.privilege_fields.insert(*fit);
              continue;
            }
            // Otherwise we don't have privileges
            dominated = false;
            break;
          }
        }
        if (!dominated)
          continue;
        // Include the offset by the number of base requirements
        return it->first;
      }
      // Method of last resort, check to see if we made all the fields
      // if we did, then we can make a new requirement for all the fields
      for (std::set<FieldID>::const_iterator it = req.privilege_fields.begin();
            it != req.privilege_fields.end(); it++)
      {
        std::pair<FieldSpace,FieldID> key(fs, *it);
        // Didn't make it so we don't have privileges anywhere
        if ((created_fields.find(key) == created_fields.end()) &&
            (local_fields.find(key) == local_fields.end()))
          return -1;
      }
      // If we get here then we can make a new requirement
      // which has non-returnable privileges
      // Get the top level region for the region tree
      RegionNode *top = runtime->forest->get_tree(req.parent.get_tree_id());
      const unsigned index = next_created_index++;
      RegionRequirement &new_req = created_requirements[index];
      new_req = RegionRequirement(top->handle, LEGION_READ_WRITE, 
                                  LEGION_EXCLUSIVE, top->handle);
      if (runtime->legion_spy_enabled)
        TaskOp::log_requirement(get_unique_id(), index, new_req);
      // Add our fields
      new_req.privilege_fields.insert(
          req.privilege_fields.begin(), req.privilege_fields.end());
      // This is not a returnable privilege requirement
      returnable_privileges[index] = false;
      return index;
    }

    //--------------------------------------------------------------------------
    LegionErrorType InnerContext::check_privilege(
                                         const IndexSpaceRequirement &req) const
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, CHECK_PRIVILEGE_CALL);
      if (req.verified)
        return LEGION_NO_ERROR;
      // Find the parent index space
      for (std::vector<IndexSpaceRequirement>::const_iterator it = 
            owner_task->indexes.begin(); it != owner_task->indexes.end(); it++)
      {
        // Check to see if we found the requirement in the parent 
        if (it->handle == req.parent)
        {
          // Check that there is a path between the parent and the child
          std::vector<LegionColor> path;
          if (!runtime->forest->compute_index_path(req.parent, 
                                                   req.handle, path))
            return ERROR_BAD_INDEX_PATH;
          // Now check that the privileges are less than or equal
          if (req.privilege & (~(it->privilege)))
          {
            return ERROR_BAD_INDEX_PRIVILEGES;  
          }
          return LEGION_NO_ERROR;
        }
      }
      // If we didn't find it here, we have to check the added 
      // index spaces that we have
      if (has_created_index_space(req.parent))
      {
        // Still need to check that there is a path between the two
        std::vector<LegionColor> path;
        if (!runtime->forest->compute_index_path(req.parent, req.handle, path))
          return ERROR_BAD_INDEX_PATH;
        // No need to check privileges here since it is a created space
        // which means that the parent has all privileges.
        return LEGION_NO_ERROR;
      }
      return ERROR_BAD_PARENT_INDEX;
    }

    //--------------------------------------------------------------------------
    LegionErrorType InnerContext::check_privilege(const RegionRequirement &req,
                                                  FieldID &bad_field,
                                                  int &bad_index,
                                                  bool skip_privilege) const
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, CHECK_PRIVILEGE_CALL);
#ifdef DEBUG_LEGION
      assert(bad_index < 0);
#endif
      if (req.flags & LEGION_VERIFIED_FLAG)
        return LEGION_NO_ERROR;
      // Copy privilege fields for check
      std::set<FieldID> privilege_fields(req.privilege_fields);
      // Try our original region requirements first
      for (unsigned idx = 0; idx < regions.size(); idx++)
      {
        LegionErrorType et = 
          check_privilege_internal(req, regions[idx], privilege_fields, 
                                   bad_field, idx, bad_index, skip_privilege);
        // No error so we are done
        if (et == LEGION_NO_ERROR)
          return et;
        // Something other than bad parent region is a real error
        if (et != ERROR_BAD_PARENT_REGION)
          return et;
        // Otherwise we just keep going
      }
      // If none of that worked, we now get to try the created requirements
      AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
      for (std::map<unsigned,RegionRequirement>::const_iterator it = 
            created_requirements.begin(); it != 
            created_requirements.end(); it++)
      {
        const RegionRequirement &created_req = it->second;
        LegionErrorType et = 
          check_privilege_internal(req, created_req, privilege_fields, 
                      bad_field, it->first, bad_index, skip_privilege);
        // No error so we are done
        if (et == LEGION_NO_ERROR)
          return et;
        // Something other than bad parent region is a real error
        if (et != ERROR_BAD_PARENT_REGION)
          return et;
        // If we got a BAD_PARENT_REGION, see if this a returnable
        // privilege in which case we know we have privileges on all fields
        if (created_req.privilege_fields.empty())
        {
          // Still have to check the parent region is right
          if (req.parent == created_req.region)
            return LEGION_NO_ERROR;
        }
        // Otherwise we just keep going
      }
      // Finally see if we created all the fields in which case we know
      // we have privileges on all their regions
      const FieldSpace sp = req.parent.get_field_space();
      for (std::set<FieldID>::const_iterator it = req.privilege_fields.begin();
            it != req.privilege_fields.end(); it++)
      {
        std::pair<FieldSpace,FieldID> key(sp, *it);
        // If we don't find the field, then we are done
        if ((created_fields.find(key) == created_fields.end()) &&
            (local_fields.find(key) == local_fields.end()))
          return ERROR_BAD_PARENT_REGION;
      }
      // Check that the parent is the root of the tree, if not it is bad
      RegionNode *parent_region = runtime->forest->get_node(req.parent);
      if (parent_region->parent != NULL)
        return ERROR_BAD_PARENT_REGION;
      // Otherwise we have privileges on these fields for all regions
      // so we are good on privileges
      return LEGION_NO_ERROR;
    }  

    //--------------------------------------------------------------------------
    LogicalRegion InnerContext::find_logical_region(unsigned index)
    //--------------------------------------------------------------------------
    {
      if (index < regions.size())
        return regions[index].region;
      AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
      std::map<unsigned,RegionRequirement>::const_iterator finder = 
        created_requirements.find(index);
#ifdef DEBUG_LEGION
      assert(finder != created_requirements.end());
#endif
      return finder->second.region;
    }

    //--------------------------------------------------------------------------
    bool InnerContext::nonexclusive_virtual_mapping(unsigned index)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(regions.size() == virtual_mapped.size());
      assert(regions.size() == parent_req_indexes.size());
#endif
      // There is a very strange case here for virtual mappings
      // More comments on this when we initialize the region tree contexts
      // With read-write privileges on a virtual mapping, we can do
      // copy-in/copy-out of the equivalence set meta-data so we can make
      // our own local equivalence sets and do refinements on them, but for
      // other privileges we can't so we need to share equivalence sets with
      // other contexts and not make our own refinements. We detect this case
      // here and prevent the logical analysis from ever making a refinement
      return ((index < virtual_mapped.size()) && virtual_mapped[index] && 
              !IS_WRITE(regions[index]));
    }

    //--------------------------------------------------------------------------
    InnerContext* InnerContext::find_parent_physical_context(unsigned index)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(regions.size() == virtual_mapped.size());
      assert(regions.size() == parent_req_indexes.size());
#endif     
      if (index < virtual_mapped.size())
      {
        // See if it is virtual mapped
        if (virtual_mapped[index])
          return find_parent_context()->find_parent_physical_context(
                                            parent_req_indexes[index]);
        else // We mapped a physical instance so we're it
          return this;
      }
      else // We created it
      {
        // Check to see if this has returnable privileges or not
        // If they are not returnable, then we can just be the 
        // context for the handling the meta-data management, 
        // otherwise if they are returnable then the top-level
        // context has to provide global guidance about which
        // node manages the meta-data.
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
        std::map<unsigned,bool>::const_iterator finder = 
          returnable_privileges.find(index);
        if ((finder != returnable_privileges.end()) && !finder->second)
          return this;
      }
      return find_top_context();
    }

    //--------------------------------------------------------------------------
    InnerContext* InnerContext::find_top_context(InnerContext *previous)
    //--------------------------------------------------------------------------
    {
      TaskContext *parent = find_parent_context();
      if (parent != NULL)
        return parent->find_top_context(this);
#ifdef DEBUG_LEGION
      assert(previous != NULL);
#endif
      return previous;
    }

    //--------------------------------------------------------------------------
    void InnerContext::pack_remote_context(Serializer &rez, 
                                           AddressSpaceID target,bool replicate)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, PACK_REMOTE_CONTEXT_CALL);
#ifdef DEBUG_LEGION
      assert(owner_task != NULL);
#endif
      rez.serialize(depth);
      // See if we need to pack up base task information
      owner_task->pack_external_task(rez, target);
#ifdef DEBUG_LEGION
      assert(regions.size() == parent_req_indexes.size());
#endif
      for (unsigned idx = 0; idx < regions.size(); idx++)
        rez.serialize(parent_req_indexes[idx]);
      // Pack up our virtual mapping information
      std::vector<unsigned> virtual_indexes;
      for (unsigned idx = 0; idx < regions.size(); idx++)
      {
        if (virtual_mapped[idx])
          virtual_indexes.push_back(idx);
      }
      rez.serialize<size_t>(virtual_indexes.size());
      for (unsigned idx = 0; idx < virtual_indexes.size(); idx++)
        rez.serialize(virtual_indexes[idx]);
      rez.serialize(find_parent_context()->did);
      rez.serialize<size_t>(context_coordinates.size());
      for (TaskTreeCoordinates::const_iterator it =
            context_coordinates.begin(); it != context_coordinates.end(); it++)
        it->serialize(rez);
      Provenance *provenance = owner_task->get_provenance();
      if (provenance != NULL)
        provenance->serialize(rez);
      else
        Provenance::serialize_null(rez);
      rez.serialize(get_unique_id());
      // Finally pack the local field infos
      AutoLock local_lock(local_field_lock,1,false/*exclusive*/);
      rez.serialize<size_t>(local_field_infos.size());
      for (std::map<FieldSpace,std::vector<LocalFieldInfo> >::const_iterator 
            it = local_field_infos.begin(); 
            it != local_field_infos.end(); it++)
      {
        rez.serialize(it->first);
        rez.serialize<size_t>(it->second.size());
        for (unsigned idx = 0; idx < it->second.size(); idx++)
          rez.serialize(it->second[idx]);
      }
      rez.serialize<bool>(concurrent_context);
      rez.serialize<bool>(replicate);
    }

    //--------------------------------------------------------------------------
    void InnerContext::compute_task_tree_coordinates(
                                         TaskTreeCoordinates &coordinates) const
    //--------------------------------------------------------------------------
    {
      // Reserve an extra level for the common case
      coordinates.reserve(context_coordinates.size() + 1);
      coordinates = context_coordinates;
    } 

    //--------------------------------------------------------------------------
    void InnerContext::return_resources(ResourceTracker *target, 
                          size_t return_index, std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      if (created_regions.empty() && deleted_regions.empty() && 
          created_fields.empty() && deleted_fields.empty() &&
          created_field_spaces.empty() && latent_field_spaces.empty() &&
          deleted_field_spaces.empty() && created_index_spaces.empty() &&
          deleted_index_spaces.empty() && created_index_partitions.empty() &&
          deleted_index_partitions.empty())
        return;
      target->receive_resources(return_index, created_regions, deleted_regions,
          created_fields, deleted_fields, created_field_spaces, 
          latent_field_spaces, deleted_field_spaces, created_index_spaces,
          deleted_index_spaces, created_index_partitions, 
          deleted_index_partitions, preconditions); 
      created_regions.clear();
      deleted_regions.clear();
      created_fields.clear();
      deleted_fields.clear();
      created_field_spaces.clear();
      latent_field_spaces.clear();
      deleted_field_spaces.clear();
      created_index_spaces.clear();
      deleted_index_spaces.clear();
      created_index_partitions.clear();
      deleted_index_partitions.clear();
    }

    //--------------------------------------------------------------------------
    void InnerContext::pack_return_resources(Serializer &rez,
                                             size_t return_index)
    //--------------------------------------------------------------------------
    {
      pack_resources_return(rez, return_index);
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::create_index_space(const Domain &bounds, 
                                       TypeTag type_tag, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this); 
      return create_index_space_internal(&bounds, type_tag, provenance);
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::create_index_space(
                 const std::vector<DomainPoint> &points, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      switch (points[0].get_dim())
      {
#define DIMFUNC(DIM) \
        case DIM: \
          { \
            std::vector<Realm::Point<DIM,coord_t> > \
              realm_points(points.size()); \
            for (unsigned idx = 0; idx < points.size(); idx++) \
              realm_points[idx] = Point<DIM,coord_t>(points[idx]); \
            const DomainT<DIM,coord_t> realm_is( \
                (Realm::IndexSpace<DIM,coord_t>(realm_points))); \
            const Domain bounds(realm_is); \
            return create_index_space_internal(&bounds, \
                NT_TemplateHelper::encode_tag<DIM,coord_t>(), provenance); \
          }
        LEGION_FOREACH_N(DIMFUNC)
#undef DIMFUNC
        default:
          assert(false);
      }
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::create_index_space(
                       const std::vector<Domain> &rects, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      switch (rects[0].get_dim())
      {
#define DIMFUNC(DIM) \
        case DIM: \
          { \
            std::vector<Realm::Rect<DIM,coord_t> > realm_rects(rects.size()); \
            for (unsigned idx = 0; idx < rects.size(); idx++) \
              realm_rects[idx] = Rect<DIM,coord_t>(rects[idx]); \
            const DomainT<DIM,coord_t> realm_is( \
                (Realm::IndexSpace<DIM,coord_t>(realm_rects))); \
            const Domain bounds(realm_is); \
            return create_index_space_internal(&bounds, \
                NT_TemplateHelper::encode_tag<DIM,coord_t>(), provenance); \
          }
        LEGION_FOREACH_N(DIMFUNC)
#undef DIMFUNC
        default:
          assert(false);
      }
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::create_index_space_internal(const Domain *bounds,
                                       TypeTag type_tag, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      IndexSpace handle(runtime->get_unique_index_space_id(),
                        runtime->get_unique_index_tree_id(), type_tag);
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating index space %x in task%s (ID %lld)", 
                      handle.id, get_task_name(), get_unique_id()); 
#endif
      if (runtime->legion_spy_enabled)
        LegionSpy::log_top_index_space(handle.id, runtime->address_space,
            (provenance == NULL) ? NULL : provenance->human_str());
      // Will take ownership of provenance if not NULL
      runtime->forest->create_index_space(handle, bounds, did, provenance); 
      register_index_space_creation(handle);
      return handle;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::find_index_launch_space(const Domain &domain,
                                                     Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      std::map<Domain,IndexSpace>::const_iterator finder =
        index_launch_spaces.find(domain);
      if (finder != index_launch_spaces.end())
        return finder->second;
      IndexSpace result;
      switch (domain.get_dim())
      {
#define DIMFUNC(DIM) \
        case DIM: \
          { \
            result = create_index_space_internal(&domain, \
              NT_TemplateHelper::encode_tag<DIM,coord_t>(), provenance); \
            break; \
          }
        LEGION_FOREACH_N(DIMFUNC)
#undef DIMFUNC
        default:
          assert(false);
      }
      index_launch_spaces[domain] = result;
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::create_unbound_index_space(TypeTag type_tag,
                                                       Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      return create_index_space_internal(NULL, type_tag, provenance);
    }

    //--------------------------------------------------------------------------
    void InnerContext::create_shared_ownership(IndexSpace handle)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (!handle.exists())
        return;
      // Check to see if this is a top-level index space, if not then
      // we shouldn't even be destroying it
      if (!runtime->forest->is_top_level_index_space(handle))
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_SHARED_OWNERSHIP,
            "Illegal call to create shared ownership for index space %x in " 
            "task %s (UID %lld) which is not a top-level index space. Legion "
            "only permits top-level index spaces to have shared ownership.", 
            handle.get_id(), get_task_name(), get_unique_id())
      runtime->create_shared_ownership(handle);
      AutoLock priv_lock(privilege_lock);
      std::map<IndexSpace,unsigned>::iterator finder = 
        created_index_spaces.find(handle);
      if (finder != created_index_spaces.end())
        finder->second++;
      else
        created_index_spaces[handle] = 1;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::union_index_spaces(
                  const std::vector<IndexSpace> &spaces, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      if (spaces.empty())
        return IndexSpace::NO_SPACE;
      AutoRuntimeCall call(this); 
      bool none_exists = true;
      for (std::vector<IndexSpace>::const_iterator it = 
            spaces.begin(); it != spaces.end(); it++)
      {
        if (none_exists && it->exists())
          none_exists = false;
        if (spaces[0].get_type_tag() != it->get_type_tag())
          REPORT_LEGION_ERROR(ERROR_DYNAMIC_TYPE_MISMATCH,
                        "Dynamic type mismatch in 'union_index_spaces' "
                        "performed in task %s (UID %lld)",
                        get_task_name(), get_unique_id())
      }
      if (none_exists)
        return IndexSpace::NO_SPACE;
      const IndexSpace handle(runtime->get_unique_index_space_id(),
          runtime->get_unique_index_tree_id(), spaces[0].get_type_tag());
      const DistributedID did = runtime->get_available_distributed_id();
      runtime->forest->create_union_space(handle, did, provenance, spaces);
      register_index_space_creation(handle);
      if (runtime->legion_spy_enabled)
        LegionSpy::log_top_index_space(handle.get_id(), runtime->address_space,
            (provenance == NULL) ? NULL : provenance->human_str());
      return handle;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::intersect_index_spaces(
                  const std::vector<IndexSpace> &spaces, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      if (spaces.empty())
        return IndexSpace::NO_SPACE;
      AutoRuntimeCall call(this); 
      bool none_exists = true;
      for (std::vector<IndexSpace>::const_iterator it = 
            spaces.begin(); it != spaces.end(); it++)
      {
        if (none_exists && it->exists())
          none_exists = false;
        if (spaces[0].get_type_tag() != it->get_type_tag())
          REPORT_LEGION_ERROR(ERROR_DYNAMIC_TYPE_MISMATCH,
                        "Dynamic type mismatch in 'intersect_index_spaces' "
                        "performed in task %s (UID %lld)",
                        get_task_name(), get_unique_id())
      }
      if (none_exists)
        return IndexSpace::NO_SPACE;
      const IndexSpace handle(runtime->get_unique_index_space_id(),
          runtime->get_unique_index_tree_id(), spaces[0].get_type_tag());
      const DistributedID did = runtime->get_available_distributed_id();
      runtime->forest->create_intersection_space(handle,did,provenance,spaces);
      register_index_space_creation(handle);
      if (runtime->legion_spy_enabled)
        LegionSpy::log_top_index_space(handle.get_id(), runtime->address_space,
            (provenance == NULL) ? NULL : provenance->human_str());
      return handle;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::subtract_index_spaces(
                      IndexSpace left, IndexSpace right, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this); 
      if (!left.exists())
        return IndexSpace::NO_SPACE;
      if (right.exists() && left.get_type_tag() != right.get_type_tag())
        REPORT_LEGION_ERROR(ERROR_DYNAMIC_TYPE_MISMATCH,
                        "Dynamic type mismatch in 'create_difference_spaces' "
                        "performed in task %s (UID %lld)",
                        get_task_name(), get_unique_id())
      const IndexSpace handle(runtime->get_unique_index_space_id(),
          runtime->get_unique_index_tree_id(), left.get_type_tag());
      const DistributedID did = runtime->get_available_distributed_id();
      runtime->forest->create_difference_space(handle, did, provenance,
                                               left, right); 
      register_index_space_creation(handle);
      if (runtime->legion_spy_enabled)
        LegionSpy::log_top_index_space(handle.get_id(), runtime->address_space,
            (provenance == NULL) ? NULL : provenance->human_str());
      return handle;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::create_index_space(const Future &future, 
                                       TypeTag type_tag, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      IndexSpace handle(runtime->get_unique_index_space_id(),
                        runtime->get_unique_index_tree_id(), type_tag);
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating index space %x in task%s (ID %lld)", 
                      handle.id, get_task_name(), get_unique_id()); 
#endif
      if (runtime->legion_spy_enabled)
        LegionSpy::log_top_index_space(handle.id, runtime->address_space,
            (provenance == NULL) ? NULL : provenance->human_str());
      // Get a new creation operation
      CreationOp *creator_op = runtime->get_available_creation_op();
      const ApEvent ready = creator_op->get_completion_event();
      IndexSpaceNode *node = runtime->forest->create_index_space(handle,
          NULL/*domain*/, did, provenance, NULL/*collective map*/,
          0/*expr id*/, ready);
      creator_op->initialize_index_space(this, node, future, provenance);
      register_index_space_creation(handle);
      add_to_dependence_queue(creator_op);
      return handle;
    } 

    //--------------------------------------------------------------------------
    void InnerContext::destroy_index_space(IndexSpace handle, 
               const bool unordered, const bool recurse, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (!handle.exists())
        return;
#ifdef DEBUG_LEGION
      log_index.debug("Destroying index space %x in task %s (ID %lld)", 
                      handle.id, get_task_name(), get_unique_id());
#endif
      // Check to see if this is a top-level index space, if not then
      // we shouldn't even be destroying it
      if (!runtime->forest->is_top_level_index_space(handle))
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_RESOURCE_DESTRUCTION,
            "Illegal call to destroy index space %x in task %s (UID %lld) "
            "which is not a top-level index space. Legion only permits "
            "top-level index spaces to be destroyed.", handle.get_id(),
            get_task_name(), get_unique_id())
      // Check to see if this is one that we should be allowed to destory
      std::vector<IndexPartition> sub_partitions;
      {
        AutoLock priv_lock(privilege_lock);
        std::map<IndexSpace,unsigned>::iterator finder = 
          created_index_spaces.find(handle);
        if (finder == created_index_spaces.end())
        {
          // If we didn't make the index space in this context, just
          // record it and keep going, it will get handled later
          deleted_index_spaces.emplace_back(
              DeletedIndexSpace(handle, recurse, provenance));
          return;
        }
        else
        {
#ifdef DEBUG_LEGION
          assert(finder->second > 0);
#endif
          if (--finder->second == 0)
            created_index_spaces.erase(finder);
          else
            return;
        }
        if (recurse)
        {
          // Also remove any index partitions for this index space tree
          for (std::map<IndexPartition,unsigned>::iterator it = 
                created_index_partitions.begin(); it !=
                created_index_partitions.end(); /*nothing*/)
          {
            if (it->first.get_tree_id() == handle.get_tree_id()) 
            {
              sub_partitions.push_back(it->first);
#ifdef DEBUG_LEGION
              assert(it->second > 0);
#endif
              if (--it->second == 0)
              {
                std::map<IndexPartition,unsigned>::iterator to_delete = it++;
                created_index_partitions.erase(to_delete);
              }
              else
                it++;
            }
            else
              it++;
          }
        }
      }
      DeletionOp *op = runtime->get_available_deletion_op();
      op->initialize_index_space_deletion(this, handle, sub_partitions,
                                          unordered, provenance);
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered index space deletion performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::create_shared_ownership(IndexPartition handle)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (!handle.exists())
        return;
      runtime->create_shared_ownership(handle);
      AutoLock priv_lock(privilege_lock);
      std::map<IndexPartition,unsigned>::iterator finder = 
        created_index_partitions.find(handle);
      if (finder != created_index_partitions.end())
        finder->second++;
      else
        created_index_partitions[handle] = 1;
    }

    //--------------------------------------------------------------------------
    void InnerContext::destroy_index_partition(IndexPartition handle,
               const bool unordered, const bool recurse, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (!handle.exists())
        return;
#ifdef DEBUG_LEGION
      log_index.debug("Destroying index partition %x in task %s (ID %lld)",
                      handle.id, get_task_name(), get_unique_id());
#endif
      std::vector<IndexPartition> sub_partitions;
      {
        AutoLock priv_lock(privilege_lock);
        std::map<IndexPartition,unsigned>::iterator finder = 
          created_index_partitions.find(handle);
        if (finder != created_index_partitions.end())
        {
#ifdef DEBUG_LEGION
          assert(finder->second > 0);
#endif
          if (--finder->second == 0)
            created_index_partitions.erase(finder);
          else
            return;
          if (recurse)
          {
            // Remove any other partitions that this partition dominates
            for (std::map<IndexPartition,unsigned>::iterator it = 
                  created_index_partitions.begin(); it !=
                  created_index_partitions.end(); /*nothing*/)
            {
              if ((handle.get_tree_id() == it->first.get_tree_id()) &&
                  runtime->forest->is_dominated_tree_only(it->first, handle))
              {
                sub_partitions.push_back(it->first);
#ifdef DEBUG_LEGION
                assert(it->second > 0);
#endif
                if (--it->second == 0)
                {
                  std::map<IndexPartition,unsigned>::iterator to_delete = it++;
                  created_index_partitions.erase(to_delete);
                }
                else
                  it++;
              }
              else
                it++;
            }
          }
        }
        else
        {
          // If we didn't make the partition, record it and keep going
          deleted_index_partitions.push_back(
              DeletedPartition(handle, recurse, provenance));
          return;
        }
      }
      DeletionOp *op = runtime->get_available_deletion_op();
      op->initialize_index_part_deletion(this, handle, 
                                         sub_partitions, unordered, provenance);
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered index partition deletion performed after task %s"
            " (UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
    }
    
    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_equal_partition(
                                                      IndexSpace parent,
                                                      IndexSpace color_space,
                                                      size_t granularity,
                                                      Color color,
                                                      Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);  
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         parent.get_tree_id(), parent.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating equal partition %d with parent index space %x "
                      "in task %s (ID %lld)", pid.id, parent.id,
                      get_task_name(), get_unique_id());
#endif
      LegionColor partition_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      part_op->initialize_equal_partition(this, pid, granularity, provenance);
      // Tell the region tree forest about this partition
      RtEvent safe = runtime->forest->create_pending_partition(this,pid,parent,
                    color_space, partition_color, LEGION_DISJOINT_COMPLETE_KIND,
                    did, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_partition_by_weights(IndexSpace parent,
                                                const FutureMap &weights, 
                                                IndexSpace color_space,
                                                size_t granularity, Color color,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);  
      const IndexPartition pid(runtime->get_unique_index_partition_id(), 
                               parent.get_tree_id(), parent.get_type_tag());
      const DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating partition %d by weights with parent index "
                      "space %x in task %s (ID %lld)", pid.id, parent.id,
                      get_task_name(), get_unique_id());
#endif
      LegionColor partition_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      part_op->initialize_weight_partition(this, pid, weights, 
                                           granularity, provenance);
      // Tell the region tree forest about this partition
      RegionTreeForest *forest = runtime->forest;
      const RtEvent safe = forest->create_pending_partition(this, pid, parent,
                  color_space, partition_color, LEGION_DISJOINT_COMPLETE_KIND,
                  did, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_partition_by_union(
                                          IndexSpace parent,
                                          IndexPartition handle1,
                                          IndexPartition handle2,
                                          IndexSpace color_space,
                                          PartitionKind kind, Color color,
                                          Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, kind)
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         parent.get_tree_id(), parent.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating union partition %d with parent index "
                      "space %x in task %s (ID %lld)", pid.id, parent.id,
                      get_task_name(), get_unique_id());
      if (parent.get_tree_id() != handle1.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEXPARTITION_NOT_SAME_INDEX_TREE,
          "IndexPartition %d is not part of the same "
                        "index tree as IndexSpace %d in create "
                        "partition by union!", handle1.id, parent.id)
      if (parent.get_tree_id() != handle2.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEXPARTITION_NOT_SAME_INDEX_TREE,
          "IndexPartition %d is not part of the same "
                        "index tree as IndexSpace %d in create "
                        "partition by union!", handle2.id, parent.id)
#endif
      LegionColor partition_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      part_op->initialize_union_partition(this, pid, handle1, 
                                          handle2, provenance);
      // If either partition is aliased the result is aliased
      if ((kind == LEGION_COMPUTE_KIND) || 
          (kind == LEGION_COMPUTE_COMPLETE_KIND) ||
          (kind == LEGION_COMPUTE_INCOMPLETE_KIND))
      {
        // If one of these partitions is aliased then the result is aliased
        IndexPartNode *p1 = runtime->forest->get_node(handle1);
        if (p1->is_disjoint(true/*from app*/))
        {
          IndexPartNode *p2 = runtime->forest->get_node(handle2);
          if (!p2->is_disjoint(true/*from app*/))
          {
            if (kind == LEGION_COMPUTE_KIND)
              kind = LEGION_ALIASED_KIND;
            else if (kind == LEGION_COMPUTE_COMPLETE_KIND)
              kind = LEGION_ALIASED_COMPLETE_KIND;
            else
              kind = LEGION_ALIASED_INCOMPLETE_KIND;
          }
        }
        else
        {
          if (kind == LEGION_COMPUTE_KIND)
            kind = LEGION_ALIASED_KIND;
          else if (kind == LEGION_COMPUTE_COMPLETE_KIND)
            kind = LEGION_ALIASED_COMPLETE_KIND;
          else
            kind = LEGION_ALIASED_INCOMPLETE_KIND;
        }
      }
      // Tell the region tree forest about this partition
      RtEvent safe = runtime->forest->create_pending_partition(this, pid,
          parent, color_space, partition_color, kind, did, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__); 
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_partition_by_intersection(
                                              IndexSpace parent,
                                              IndexPartition handle1,
                                              IndexPartition handle2,
                                              IndexSpace color_space,
                                              PartitionKind kind, Color color,
                                              Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, kind)
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         parent.get_tree_id(), parent.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating intersection partition %d with parent "
                      "index space %x in task %s (ID %lld)", pid.id, parent.id,
                      get_task_name(), get_unique_id());
      if (parent.get_tree_id() != handle1.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEXPARTITION_NOT_SAME_INDEX_TREE,
          "IndexPartition %d is not part of the same "
                        "index tree as IndexSpace %d in create partition by "
                        "intersection!", handle1.id, parent.id)
      if (parent.get_tree_id() != handle2.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEXPARTITION_NOT_SAME_INDEX_TREE,
          "IndexPartition %d is not part of the same "
                        "index tree as IndexSpace %d in create partition by "
                        "intersection!", handle2.id, parent.id)
#endif
      LegionColor partition_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      part_op->initialize_intersection_partition(this, pid, handle1, 
                                                 handle2, provenance);
      // If either partition is disjoint then the result is disjoint
      if ((kind == LEGION_COMPUTE_KIND) || 
          (kind == LEGION_COMPUTE_COMPLETE_KIND) ||
          (kind == LEGION_COMPUTE_INCOMPLETE_KIND))
      {
        IndexPartNode *p1 = runtime->forest->get_node(handle1);
        if (!p1->is_disjoint(true/*from app*/))
        {
          IndexPartNode *p2 = runtime->forest->get_node(handle2);
          if (p2->is_disjoint(true/*from app*/))
          {
            if (kind == LEGION_COMPUTE_KIND)
              kind = LEGION_DISJOINT_KIND;
            else if (kind == LEGION_COMPUTE_COMPLETE_KIND)
              kind = LEGION_DISJOINT_COMPLETE_KIND;
            else
              kind = LEGION_DISJOINT_INCOMPLETE_KIND;
          }
        }
        else
        {
          if (kind == LEGION_COMPUTE_KIND)
            kind = LEGION_DISJOINT_KIND;
          else if (kind == LEGION_COMPUTE_COMPLETE_KIND)
            kind = LEGION_DISJOINT_COMPLETE_KIND;
          else
            kind = LEGION_DISJOINT_INCOMPLETE_KIND;
        }
      }
      // Tell the region tree forest about this partition
      RtEvent safe = runtime->forest->create_pending_partition(this, pid,
          parent, color_space, partition_color, kind, did, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_partition_by_intersection(
                                              IndexSpace parent,
                                              IndexPartition partition,
                                              PartitionKind kind, Color color,
                                              bool dominates,
                                              Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, kind)
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         parent.get_tree_id(), parent.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating intersection partition %d with parent "
                      "index space %x in task %s (ID %lld)", pid.id, parent.id,
                      get_task_name(), get_unique_id());
      if (parent.get_type_tag() != partition.get_type_tag())
        REPORT_LEGION_ERROR(ERROR_INDEXPARTITION_NOT_SAME_INDEX_TREE,
            "IndexPartition %d does not have the same type as the "
            "parent index space %x in task %s (UID %lld)", partition.id,
            parent.id, get_task_name(), get_unique_id())
#endif
      LegionColor partition_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      part_op->initialize_intersection_partition(this, pid, partition,
                                                 dominates, provenance);
      IndexPartNode *part_node = runtime->forest->get_node(partition);
      // See if we can determine disjointness if we weren't told
      if ((kind == LEGION_COMPUTE_KIND) || 
          (kind == LEGION_COMPUTE_COMPLETE_KIND) ||
          (kind == LEGION_COMPUTE_INCOMPLETE_KIND))
      {
        if (part_node->is_disjoint(true/*from app*/))
        {
          if (kind == LEGION_COMPUTE_KIND)
            kind = LEGION_DISJOINT_KIND;
          else if (kind == LEGION_COMPUTE_COMPLETE_KIND)
            kind = LEGION_DISJOINT_COMPLETE_KIND;
          else
            kind = LEGION_DISJOINT_INCOMPLETE_KIND;
        }
      }
      // Tell the region tree forest about this partition
      RtEvent safe = runtime->forest->create_pending_partition(this, pid,parent,
                     part_node->color_space->handle, partition_color, kind, did,
                     provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_partition_by_difference(
                                                  IndexSpace parent,
                                                  IndexPartition handle1,
                                                  IndexPartition handle2,
                                                  IndexSpace color_space,
                                                  PartitionKind kind, 
                                                  Color color,
                                                  Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this); 
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, kind)
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         parent.get_tree_id(), parent.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating difference partition %d with parent "
                      "index space %x in task %s (ID %lld)", pid.id, parent.id,
                      get_task_name(), get_unique_id());
      if (parent.get_tree_id() != handle1.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEXPARTITION_NOT_SAME_INDEX_TREE,
          "IndexPartition %d is not part of the same "
                              "index tree as IndexSpace %d in create "
                              "partition by difference!",
                              handle1.id, parent.id)
      if (parent.get_tree_id() != handle2.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEXPARTITION_NOT_SAME_INDEX_TREE,
          "IndexPartition %d is not part of the same "
                              "index tree as IndexSpace %d in create "
                              "partition by difference!",
                              handle2.id, parent.id)
#endif
      LegionColor partition_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      part_op->initialize_difference_partition(this, pid, handle1, 
                                               handle2, provenance);
      // If the left-hand-side is disjoint the result is disjoint
      if ((kind == LEGION_COMPUTE_KIND) || 
          (kind == LEGION_COMPUTE_COMPLETE_KIND) ||
          (kind == LEGION_COMPUTE_INCOMPLETE_KIND))
      {
        IndexPartNode *p1 = runtime->forest->get_node(handle1);
        if (p1->is_disjoint(true/*from app*/))
        {
          if (kind == LEGION_COMPUTE_KIND)
            kind = LEGION_DISJOINT_KIND;
          else if (kind == LEGION_COMPUTE_COMPLETE_KIND)
            kind = LEGION_DISJOINT_COMPLETE_KIND;
          else
            kind = LEGION_DISJOINT_INCOMPLETE_KIND;
        }
      }
      // Tell the region tree forest about this partition
      RtEvent safe = runtime->forest->create_pending_partition(this, pid, 
                         parent, color_space, partition_color, kind, did,
                         provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    Color InnerContext::create_cross_product_partitions(
                                                      IndexPartition handle1,
                                                      IndexPartition handle2,
                                   std::map<IndexSpace,IndexPartition> &handles,
                                                      PartitionKind kind,
                                                      Color color,
                                                      Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
#ifdef DEBUG_LEGION
      log_index.debug("Creating cross product partitions in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
      if (handle1.get_tree_id() != handle2.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEXPARTITION_NOT_SAME_INDEX_TREE,
          "IndexPartition %d is not part of the same "
                              "index tree as IndexPartition %d in create "
                              "cross product partitions!",
                              handle1.id, handle2.id)
#endif
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, kind)
      LegionColor partition_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      // Tell the region tree forest about this partition
      std::set<RtEvent> safe_events;
      runtime->forest->create_pending_cross_product(this, handle1, handle2, 
           handles, kind, provenance, partition_color, safe_events);
      part_op->initialize_cross_product(this, handle1, handle2,
                                        partition_color, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      if (!safe_events.empty())
      {
        const RtEvent wait_on = Runtime::merge_events(safe_events);
        if (wait_on.exists() && !wait_on.has_triggered())
          wait_on.wait();
      }
      if (runtime->verify_partitions)
      {
        Domain color_space = runtime->get_index_partition_color_space(handle1);
        // This code will only work if the color space has type coord_t
        TypeTag type_tag;
        switch (color_space.get_dim())
        {
#define DIMFUNC(DIM) \
          case DIM: \
            { \
              type_tag = NT_TemplateHelper::encode_tag<DIM,coord_t>(); \
              break; \
            }
          LEGION_FOREACH_N(DIMFUNC)
#undef DIMFUNC
          default:
            assert(false);
        }
        for (Domain::DomainPointIterator itr(color_space); itr; itr++)
        {
          IndexSpace subspace;
          switch (color_space.get_dim())
          {
#define DIMFUNC(DIM) \
            case DIM: \
              { \
                const Point<DIM,coord_t> p(itr.p); \
                subspace = runtime->get_index_subspace(handle1, &p, type_tag); \
                break; \
              }
            LEGION_FOREACH_N(DIMFUNC)
#undef DIMFUNC
            default:
              assert(false);
          }
          IndexPartition part = 
            runtime->get_index_partition(subspace, partition_color);
          verify_partition(part, verify_kind, __func__);
        }
      }
      return partition_color;
    }

    //--------------------------------------------------------------------------
    void InnerContext::create_association(LogicalRegion domain,
                                          LogicalRegion domain_parent,
                                          FieldID domain_fid,
                                          IndexSpace range,
                                          MapperID id, MappingTagID tag,
                                          const UntypedBuffer &marg,
                                          Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
#ifdef DEBUG_LEGION
      log_index.debug("Creating association in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      DependentPartitionOp *part_op = 
        runtime->get_available_dependent_partition_op();
      part_op->initialize_by_association(this, domain, domain_parent, 
                              domain_fid, range, id, tag, marg, provenance);
      // Now figure out if we need to unmap and re-map any inline mappings
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(part_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around create_association call "
              "in task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(part_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_restricted_partition(
                                              IndexSpace parent,
                                              IndexSpace color_space,
                                              const void *transform,
                                              size_t transform_size,
                                              const void *extent,
                                              size_t extent_size,
                                              PartitionKind part_kind,
                                              Color color,
                                              Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         parent.get_tree_id(), parent.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating restricted partition in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      LegionColor part_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color; 
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      part_op->initialize_restricted_partition(this, pid, transform, 
                          transform_size, extent, extent_size, provenance);
      // Tell the region tree forest about this partition
      RtEvent safe = runtime->forest->create_pending_partition(this, pid,
          parent, color_space, part_color, part_kind, did, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_partition_by_domain(
                                                IndexSpace parent,
                                    const std::map<DomainPoint,Domain> &domains,
                                                IndexSpace color_space,
                                                bool perform_intersections,
                                                PartitionKind part_kind,
                                                Color color,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      ArgumentMap argmap;
      for (std::map<DomainPoint,Domain>::const_iterator it = 
            domains.begin(); it != domains.end(); it++)
        argmap.set_point(it->first,
            UntypedBuffer(&it->second, sizeof(it->second)));
      FutureMap future_map(argmap.impl->freeze(this, provenance));
      return create_partition_by_domain(parent, future_map, color_space,
          perform_intersections, part_kind, color, provenance);
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_partition_by_domain(
                                                IndexSpace parent,
                                                const FutureMap &domains,
                                                IndexSpace color_space,
                                                bool perform_intersections,
                                                PartitionKind part_kind,
                                                Color color, 
                                                Provenance *provenance,
                                                bool skip_check)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         parent.get_tree_id(), parent.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating partition by domain in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      LegionColor part_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color; 
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      part_op->initialize_by_domain(this, pid, domains, 
                          perform_intersections, provenance);
      // Tell the region tree forest about this partition
      RtEvent safe = runtime->forest->create_pending_partition(this, pid,
          parent, color_space, part_color, part_kind, did, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_partition_by_field(
                                              LogicalRegion handle,
                                              LogicalRegion parent_priv,
                                              FieldID fid,
                                              IndexSpace color_space,
                                              Color color,
                                              MapperID id, MappingTagID tag,
                                              PartitionKind part_kind,
                                              const UntypedBuffer &marg,
                                              Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      // Partition by field is disjoint by construction
      PartitionKind verify_kind = LEGION_DISJOINT_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      IndexSpace parent = handle.get_index_space(); 
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         parent.get_tree_id(), parent.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating partition by field in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      LegionColor part_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color;
      // Allocate the partition operation
      DependentPartitionOp *part_op = 
        runtime->get_available_dependent_partition_op();
      // Tell the region tree forest about this partition 
      RtEvent safe = runtime->forest->create_pending_partition(this, pid,
          parent, color_space, part_color, part_kind, did, provenance);
      // Do this after creating the pending partition so the node exists
      // in case we need to look at it during initialization
      part_op->initialize_by_field(this, pid, handle, parent_priv, 
                                   color_space, fid, id, tag, marg, provenance);
      // Now figure out if we need to unmap and re-map any inline mappings
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(part_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around create_partition_by_field call "
              "in task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(part_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_partition_by_image(
                                                    IndexSpace handle,
                                                    LogicalPartition projection,
                                                    LogicalRegion parent,
                                                    FieldID fid,
                                                    IndexSpace color_space,
                                                    PartitionKind part_kind,
                                                    Color color,
                                                    MapperID id, 
                                                    MappingTagID tag,
                                                    const UntypedBuffer &marg,
                                                    Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this); 
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         handle.get_tree_id(), handle.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating partition by image in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      LegionColor part_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color;
      // Allocate the partition operation
      DependentPartitionOp *part_op = 
        runtime->get_available_dependent_partition_op();
      // Tell the region tree forest about this partition
      RtEvent safe = runtime->forest->create_pending_partition(this, pid,
          handle, color_space, part_color, part_kind, did, provenance);
      // Do this after creating the pending partition so the node exists
      // in case we need to look at it during initialization
      part_op->initialize_by_image(this, pid, handle, projection, parent,
                                   fid, id, tag, marg, provenance);
      // Now figure out if we need to unmap and re-map any inline mappings
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(part_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around create_partition_by_image call "
              "in task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(part_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_partition_by_image_range(
                                                    IndexSpace handle,
                                                    LogicalPartition projection,
                                                    LogicalRegion parent,
                                                    FieldID fid,
                                                    IndexSpace color_space,
                                                    PartitionKind part_kind,
                                                    Color color,
                                                    MapperID id, 
                                                    MappingTagID tag,
                                                    const UntypedBuffer &marg,
                                                    Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this); 
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         handle.get_tree_id(), handle.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating partition by image range in task %s (ID %lld)",
                      get_task_name(), get_unique_id());
#endif
      LegionColor part_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color;
      // Allocate the partition operation
      DependentPartitionOp *part_op = 
        runtime->get_available_dependent_partition_op();
      // Tell the region tree forest about this partition
      RtEvent safe = runtime->forest->create_pending_partition(this, pid,
          handle, color_space, part_color, part_kind, did, provenance);
      // Do this after creating the pending partition so the node exists
      // in case we need to look at it during initialization
      part_op->initialize_by_image_range(this, pid, handle, projection, parent,
                                         fid, id, tag, marg, provenance);
      // Now figure out if we need to unmap and re-map any inline mappings
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(part_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around create_partition_by_image_range call "
              "in task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(part_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_partition_by_preimage(
                                                  IndexPartition projection,
                                                  LogicalRegion handle,
                                                  LogicalRegion parent,
                                                  FieldID fid,
                                                  IndexSpace color_space,
                                                  PartitionKind part_kind,
                                                  Color color,
                                                  MapperID id, MappingTagID tag,
                                                  const UntypedBuffer &marg,
                                                  Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this); 
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         handle.get_index_space().get_tree_id(),
                         handle.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating partition by preimage in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      LegionColor part_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color;
      // Allocate the partition operation
      DependentPartitionOp *part_op = 
        runtime->get_available_dependent_partition_op(); 
      // If the source of the preimage is disjoint then the result is disjoint
      // Note this only applies here and not to range
      if ((part_kind == LEGION_COMPUTE_KIND) || 
          (part_kind == LEGION_COMPUTE_COMPLETE_KIND) ||
          (part_kind == LEGION_COMPUTE_INCOMPLETE_KIND))
      {
        IndexPartNode *p = runtime->forest->get_node(projection);
        if (p->is_disjoint(true/*from app*/))
        {
          if (part_kind == LEGION_COMPUTE_KIND)
            part_kind = LEGION_DISJOINT_KIND;
          else if (part_kind == LEGION_COMPUTE_COMPLETE_KIND)
            part_kind = LEGION_DISJOINT_COMPLETE_KIND;
          else
            part_kind = LEGION_DISJOINT_INCOMPLETE_KIND;
        }
      }
      // Tell the region tree forest about this partition
      RtEvent safe = runtime->forest->create_pending_partition(this, pid, 
                           handle.get_index_space(), color_space, 
                           part_color, part_kind, did, provenance);
      // Do this after creating the pending partition so the node exists
      // in case we need to look at it during initialization
      part_op->initialize_by_preimage(this, pid, projection, handle, 
                                      parent, fid, id, tag, marg, provenance);
      // Now figure out if we need to unmap and re-map any inline mappings
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(part_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around create_partition_by_preimage call "
              "in task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(part_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_partition_by_preimage_range(
                                                  IndexPartition projection,
                                                  LogicalRegion handle,
                                                  LogicalRegion parent,
                                                  FieldID fid,
                                                  IndexSpace color_space,
                                                  PartitionKind part_kind,
                                                  Color color,
                                                  MapperID id, MappingTagID tag,
                                                  const UntypedBuffer &marg,
                                                  Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this); 
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         handle.get_index_space().get_tree_id(),
                         handle.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating partition by preimage range in task %s "
                      "(ID %lld)", get_task_name(), get_unique_id());
#endif
      LegionColor part_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color;
      // Allocate the partition operation
      DependentPartitionOp *part_op = 
        runtime->get_available_dependent_partition_op(); 
      // Tell the region tree forest about this partition
      RtEvent safe = runtime->forest->create_pending_partition(this, pid,
                       handle.get_index_space(), color_space,
                       part_color, part_kind, did, provenance);
      // Do this after creating the pending partition so the node exists
      // in case we need to look at it during initialization
      part_op->initialize_by_preimage_range(this, pid, projection, handle,
                                  parent, fid, id, tag, marg, provenance);
      // Now figure out if we need to unmap and re-map any inline mappings
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(part_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around create_partition_by_preimage_range call "
              "in task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(part_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition InnerContext::create_pending_partition(
                                                IndexSpace parent,
                                                IndexSpace color_space, 
                                                PartitionKind part_kind,
                                                Color color, 
                                                Provenance *provenance,
                                                bool trust)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions && !trust)
        SWAP_PART_KINDS(verify_kind, part_kind)
      IndexPartition pid(runtime->get_unique_index_partition_id(), 
                         parent.get_tree_id(), parent.get_type_tag());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_index.debug("Creating pending partition in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      LegionColor part_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color;
      RtEvent safe = runtime->forest->create_pending_partition(this, pid,
                            parent, color_space, part_color, part_kind,
                            did, provenance);
      // Wait for any notifications to occur before returning
      if (safe.exists())
        safe.wait();
      if (runtime->verify_partitions && !trust)
      {
        // We can't block to check this here because the user needs 
        // control back in order to fill in the pieces of the partitions
        // so just launch a meta-task to check it when we can
        VerifyPartitionArgs args(this, pid, verify_kind, __func__);
        runtime->issue_runtime_meta_task(args, LG_LOW_PRIORITY); 
      }
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::create_index_space_union(IndexPartition parent,
                                                      const void *realm_color,
                                                      size_t color_size,
                                                      TypeTag type_tag,
                                        const std::vector<IndexSpace> &handles,
                                                      Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
#ifdef DEBUG_LEGION
      log_index.debug("Creating index space union in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      IndexSpace result = 
        runtime->forest->instantiate_subspace(parent, realm_color, type_tag);
      part_op->initialize_index_space_union(this, result, handles, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::create_index_space_union(IndexPartition parent,
                                                      const void *realm_color,
                                                      size_t color_size,
                                                      TypeTag type_tag,
                                                      IndexPartition handle,
                                                      Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
#ifdef DEBUG_LEGION
      log_index.debug("Creating index space union in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      IndexSpace result = 
        runtime->forest->instantiate_subspace(parent, realm_color, type_tag);
      part_op->initialize_index_space_union(this, result, handle, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::create_index_space_intersection(
                                                      IndexPartition parent,
                                                      const void *realm_color,
                                                      size_t color_size,
                                                      TypeTag type_tag,
                                        const std::vector<IndexSpace> &handles,
                                                      Provenance *prov)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
#ifdef DEBUG_LEGION
      log_index.debug("Creating index space intersection in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      IndexSpace result = 
        runtime->forest->instantiate_subspace(parent, realm_color, type_tag); 
      part_op->initialize_index_space_intersection(this, result, handles, prov);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::create_index_space_intersection(
                                                      IndexPartition parent,
                                                      const void *realm_color,
                                                      size_t color_size,
                                                      TypeTag type_tag,
                                                      IndexPartition handle,
                                                      Provenance *prov)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
#ifdef DEBUG_LEGION
      log_index.debug("Creating index space intersection in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      IndexSpace result = 
        runtime->forest->instantiate_subspace(parent, realm_color, type_tag); 
      part_op->initialize_index_space_intersection(this, result, handle, prov);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpace InnerContext::create_index_space_difference(
                                                    IndexPartition parent,
                                                    const void *realm_color,
                                                    size_t color_size,
                                                    TypeTag type_tag,
                                                    IndexSpace initial,
                                        const std::vector<IndexSpace> &handles,
                                                    Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
#ifdef DEBUG_LEGION
      log_index.debug("Creating index space difference in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      PendingPartitionOp *part_op = 
        runtime->get_available_pending_partition_op();
      IndexSpace result = 
        runtime->forest->instantiate_subspace(parent, realm_color, type_tag); 
      part_op->initialize_index_space_difference(this, result, initial,
                                                 handles, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      return result;
    } 

    //--------------------------------------------------------------------------
    void InnerContext::verify_partition(IndexPartition pid, PartitionKind kind,
                                        const char *function_name)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *node = runtime->forest->get_node(pid);
      // Check containment first because our implementation of the algorithms
      // for disjointnss and completeness rely upon it.
      for (ColorSpaceIterator itr(node); itr; itr++)
      {
        IndexSpaceNode *child_node = node->get_child(*itr);
        IndexSpaceExpression *diff = 
          runtime->forest->subtract_index_spaces(child_node, node->parent);
        if (!diff->is_empty())
        {
          const DomainPoint bad = 
            node->color_space->delinearize_color_to_point(*itr);
          switch (bad.get_dim())
          {
            case 1:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0])
            case 2:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1])
            case 3:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2])
            case 4:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,"
                  "%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2], bad[3])
            case 5:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,"
                  "%lld,%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2], bad[3], bad[4])
            case 6:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,"
                  "%lld,%lld,%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2], bad[3], bad[4], bad[5])
            case 7:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,"
                  "%lld,%lld,%lld,%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2], bad[3], bad[4], bad[5], bad[6])
            case 8:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,"
                  "%lld,%lld,%lld,%lld,%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2], bad[3], bad[4], bad[5], bad[6],
                  bad[7])
            case 9:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,"
                  "%lld,%lld,%lld,%lld,%lld,%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2], bad[3], bad[4], bad[5], bad[6],
                  bad[7], bad[8])
            default:
              assert(false);
          }
        }
      }
      // Check disjointness
      if ((kind == LEGION_DISJOINT_KIND) || 
          (kind == LEGION_DISJOINT_COMPLETE_KIND) ||
          (kind == LEGION_DISJOINT_INCOMPLETE_KIND))
      {
        if (!node->is_disjoint(true/*from application*/))
          REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
              "Call to partitioning function %s in %s (UID %lld) specified "
              "partition was %s but the partition is aliased.",
              function_name, get_task_name(), get_unique_id(),
              (kind == LEGION_DISJOINT_KIND) ? "DISJOINT_KIND" :
              (kind == LEGION_DISJOINT_COMPLETE_KIND) ? "DISJOINT_COMPLETE_KIND"
              : "DISJOINT_INCOMPLETE_KIND")
      }
      else if ((kind == LEGION_ALIASED_KIND) || 
               (kind == LEGION_ALIASED_COMPLETE_KIND) ||
               (kind == LEGION_ALIASED_INCOMPLETE_KIND))
      {
        if (node->is_disjoint(true/*from application*/))
          REPORT_LEGION_WARNING(LEGION_WARNING_PARTITION_VERIFICATION,
              "Call to partitioning function %s in %s (UID %lld) specified "
              "partition was %s but the partition is disjoint. This could "
              "lead to a performance bug.",
              function_name, get_task_name(), get_unique_id(),
              (kind == LEGION_ALIASED_KIND) ? "ALIASED_KIND" :
              (kind == LEGION_ALIASED_COMPLETE_KIND) ? "ALIASED_COMPLETE_KIND" :
              "ALIASED_INCOMPLETE_KIND")
      }
      // Check completeness
      if ((kind == LEGION_DISJOINT_COMPLETE_KIND) || 
          (kind == LEGION_ALIASED_COMPLETE_KIND) ||
          (kind == LEGION_COMPUTE_COMPLETE_KIND))
      {
        if (!node->is_complete(true/*from application*/))
          REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
              "Call to partitioning function %s in %s (UID %lld) specified "
              "partition was %s but the partition is incomplete.",
              function_name, get_task_name(), get_unique_id(),
              (kind == LEGION_DISJOINT_COMPLETE_KIND) ? "DISJOINT_COMPLETE_KIND" 
            : (kind == LEGION_ALIASED_COMPLETE_KIND) ? "ALIASED_COMPLETE_KIND" :
              "COMPUTE_COMPLETE_KIND")
      }
      else if ((kind == LEGION_DISJOINT_INCOMPLETE_KIND) || 
               (kind == LEGION_ALIASED_INCOMPLETE_KIND) || 
               (kind == LEGION_COMPUTE_INCOMPLETE_KIND))
      {
        if (node->is_complete(true/*from application*/))
          REPORT_LEGION_WARNING(LEGION_WARNING_PARTITION_VERIFICATION,
              "Call to partitioning function %s in %s (UID %lld) specified "
              "partition was %s but the partition is complete. This could "
              "lead to a performance bug.",
              function_name, get_task_name(), get_unique_id(),
              (kind == LEGION_DISJOINT_INCOMPLETE_KIND) ? 
                "DISJOINT_INCOMPLETE_KIND" :
              (kind == LEGION_ALIASED_INCOMPLETE_KIND) ? 
              "ALIASED_INCOMPLETE_KIND" : "COMPUTE_INCOMPLETE_KIND")
      }
    }

    //--------------------------------------------------------------------------
    /*static*/void InnerContext::handle_partition_verification(const void *args)
    //--------------------------------------------------------------------------
    {
      const VerifyPartitionArgs *vargs = (const VerifyPartitionArgs*)args;
      vargs->proxy_this->verify_partition(vargs->pid, vargs->kind, vargs->func);
    }

    //--------------------------------------------------------------------------
    FieldSpace InnerContext::create_field_space(Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      FieldSpace space(runtime->get_unique_field_space_id());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_field.debug("Creating field space %x in task %s (ID %lld)", 
                      space.id, get_task_name(), get_unique_id());
#endif
      if (runtime->legion_spy_enabled)
        LegionSpy::log_field_space(space.id, runtime->address_space,
            (provenance == NULL) ? NULL : provenance->human_str());

      runtime->forest->create_field_space(space, did, provenance);
      register_field_space_creation(space);
      return space;
    }

    //--------------------------------------------------------------------------
    FieldSpace InnerContext::create_field_space(
                                         const std::vector<size_t> &sizes,
                                         std::vector<FieldID> &resulting_fields,
                                         CustomSerdezID serdez_id,
                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      FieldSpace space(runtime->get_unique_field_space_id());
      DistributedID did = runtime->get_available_distributed_id();
#ifdef DEBUG_LEGION
      log_field.debug("Creating field space %x in task %s (ID %lld)", 
                      space.id, get_task_name(), get_unique_id());
#endif
      if (runtime->legion_spy_enabled)
        LegionSpy::log_field_space(space.id, runtime->address_space,
            (provenance == NULL) ? NULL : provenance->human_str());

      FieldSpaceNode *node =
        runtime->forest->create_field_space(space, did, provenance);
      register_field_space_creation(space);
      if (resulting_fields.size() < sizes.size())
        resulting_fields.resize(sizes.size(), LEGION_AUTO_GENERATE_ID);
      for (unsigned idx = 0; idx < resulting_fields.size(); idx++)
      {
        if (resulting_fields[idx] == LEGION_AUTO_GENERATE_ID)
          resulting_fields[idx] = runtime->get_unique_field_id();
#ifdef DEBUG_LEGION
        else if (resulting_fields[idx] >= LEGION_MAX_APPLICATION_FIELD_ID)
          REPORT_LEGION_ERROR(ERROR_TASK_ATTEMPTED_ALLOCATE_FIELD,
            "Task %s (ID %lld) attempted to allocate a field with "
            "ID %d which exceeds the LEGION_MAX_APPLICATION_FIELD_ID "
            "bound set in legion_config.h", get_task_name(),
            get_unique_id(), resulting_fields[idx])
#endif
        if (runtime->legion_spy_enabled)
          LegionSpy::log_field_creation(space.id, resulting_fields[idx],
             sizes[idx], (provenance == NULL) ? NULL : provenance->human_str());
      }
      node->initialize_fields(sizes, resulting_fields, serdez_id, provenance);
      register_all_field_creations(space, false/*local*/, resulting_fields);
      return space;
    }

    //--------------------------------------------------------------------------
    FieldSpace InnerContext::create_field_space(
                                         const std::vector<Future> &sizes,
                                         std::vector<FieldID> &resulting_fields,
                                         CustomSerdezID serdez_id,
                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      const FieldSpace space = create_field_space(provenance);
      AutoRuntimeCall call(this);
      FieldSpaceNode *node = runtime->forest->get_node(space);
      if (resulting_fields.size() < sizes.size())
        resulting_fields.resize(sizes.size(), LEGION_AUTO_GENERATE_ID);
      for (unsigned idx = 0; idx < resulting_fields.size(); idx++)
      {
        if (resulting_fields[idx] == LEGION_AUTO_GENERATE_ID)
          resulting_fields[idx] = runtime->get_unique_field_id();
#ifdef DEBUG_LEGION
        else if (resulting_fields[idx] >= LEGION_MAX_APPLICATION_FIELD_ID)
          REPORT_LEGION_ERROR(ERROR_TASK_ATTEMPTED_ALLOCATE_FIELD,
            "Task %s (ID %lld) attempted to allocate a field with "
            "ID %d which exceeds the LEGION_MAX_APPLICATION_FIELD_ID "
            "bound set in legion_config.h", get_task_name(),
            get_unique_id(), resulting_fields[idx])
#endif
      }
      for (unsigned idx = 0; idx < sizes.size(); idx++)
        if (sizes[idx].impl == NULL)
          REPORT_LEGION_ERROR(ERROR_REQUEST_FOR_EMPTY_FUTURE,
              "Invalid empty future passed to field allocation for field %d "
              "in task %s (UID %lld)", resulting_fields[idx],
              get_task_name(), get_unique_id())
      // Get a new creation operation
      CreationOp *creator_op = runtime->get_available_creation_op();  
      const ApEvent ready = creator_op->get_completion_event();
      creator_op->initialize_fields(this, node, resulting_fields,
                                    sizes, provenance);
      node->initialize_fields(ready, resulting_fields, serdez_id,
                              creator_op->get_provenance());
      register_all_field_creations(space, false/*local*/, resulting_fields);
      add_to_dependence_queue(creator_op);
      return space;
    }

    //--------------------------------------------------------------------------
    void InnerContext::create_shared_ownership(FieldSpace handle)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (!handle.exists())
        return;
      runtime->create_shared_ownership(handle);
      AutoLock priv_lock(privilege_lock);
      std::map<FieldSpace,unsigned>::iterator finder = 
        created_field_spaces.find(handle);
      if (finder != created_field_spaces.end())
        finder->second++;
      else
        created_field_spaces[handle] = 1;
    }

    //--------------------------------------------------------------------------
    void InnerContext::destroy_field_space(FieldSpace handle,
                                   const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (!handle.exists())
        return;
#ifdef DEBUG_LEGION
      log_field.debug("Destroying field space %x in task %s (ID %lld)", 
                      handle.id, get_task_name(), get_unique_id());
#endif
      // Check to see if this is one that we should be allowed to destory
      {
        AutoLock priv_lock(privilege_lock);
        std::map<FieldSpace,unsigned>::iterator finder = 
          created_field_spaces.find(handle);
        if (finder != created_field_spaces.end())
        {
#ifdef DEBUG_LEGION
          assert(finder->second > 0);
#endif
          if (--finder->second == 0)
            created_field_spaces.erase(finder);
          else
            return;
          // Count how many regions are still using this field space
          // that still need to be deleted before we can remove the
          // list of created fields
          std::set<LogicalRegion> latent_regions;
          for (std::map<LogicalRegion,unsigned>::const_iterator it = 
                created_regions.begin(); it != created_regions.end(); it++)
            if (it->first.get_field_space() == handle)
              latent_regions.insert(it->first);
          for (std::map<LogicalRegion,bool>::const_iterator it = 
                local_regions.begin(); it != local_regions.end(); it++)
            if (it->first.get_field_space() == handle)
              latent_regions.insert(it->first);
          if (latent_regions.empty())
          {
            // No remaining regions so we can remove any created fields now
            for (std::set<std::pair<FieldSpace,FieldID> >::iterator it = 
                  created_fields.begin(); it != 
                  created_fields.end(); /*nothing*/)
            {
              if (it->first == handle)
              {
                std::set<std::pair<FieldSpace,FieldID> >::iterator 
                  to_delete = it++;
                created_fields.erase(to_delete);
              }
              else
                it++;
            }
          }
          else
            latent_field_spaces[handle] = latent_regions;
        }
        else
        {
          // If we didn't make this field space, record the deletion
          // and keep going. It will be handled by the context that
          // made the field space
          deleted_field_spaces.emplace_back(
              DeletedFieldSpace(handle, provenance));
          return;
        }
      }
      DeletionOp *op = runtime->get_available_deletion_op();
      op->initialize_field_space_deletion(this, handle, unordered, provenance);
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered field space deletion performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
    } 

    //--------------------------------------------------------------------------
    FieldAllocatorImpl* InnerContext::create_field_allocator(FieldSpace handle,
                                                             bool unordered)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      {
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
        std::map<FieldSpace,FieldAllocatorImpl*>::const_iterator finder = 
          field_allocators.find(handle);
        if (finder != field_allocators.end())
          return finder->second;
      }
      // Didn't find it, so have to make, retake the lock in exclusive mode
      FieldSpaceNode *node = runtime->forest->get_node(handle);
      AutoLock priv_lock(privilege_lock);
      // Check to see if we lost the race
      std::map<FieldSpace,FieldAllocatorImpl*>::const_iterator finder = 
        field_allocators.find(handle);
      if (finder != field_allocators.end())
        return finder->second;
      // Don't have one so make a new one
      const RtEvent ready = node->create_allocator(runtime->address_space);
      FieldAllocatorImpl *result = new FieldAllocatorImpl(node, this, ready);
      // Save it for later
      field_allocators[handle] = result;
      return result;
    }

    //--------------------------------------------------------------------------
    void InnerContext::destroy_field_allocator(FieldSpaceNode *node,
                                               bool from_application)
    //--------------------------------------------------------------------------
    {
      if (from_application)
      {
        AutoRuntimeCall call(this);
        destroy_field_allocator(node, false/*from application*/);
        return;
      }
      const RtEvent ready = node->destroy_allocator(runtime->address_space);
      if (ready.exists() && !ready.has_triggered())
        ready.wait();
      AutoLock priv_lock(privilege_lock);
      std::map<FieldSpace,FieldAllocatorImpl*>::iterator finder = 
        field_allocators.find(node->handle);
#ifdef DEBUG_LEGION
      assert(finder != field_allocators.end());
#endif
      field_allocators.erase(finder);
    }

    //--------------------------------------------------------------------------
    FieldID InnerContext::allocate_field(FieldSpace space, size_t field_size,
                                        FieldID fid, bool local,
                                        CustomSerdezID serdez_id,
                                        Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (fid == LEGION_AUTO_GENERATE_ID)
        fid = runtime->get_unique_field_id();
#ifdef DEBUG_LEGION
      else if (fid >= LEGION_MAX_APPLICATION_FIELD_ID)
        REPORT_LEGION_ERROR(ERROR_TASK_ATTEMPTED_ALLOCATE_FIELD,
          "Task %s (ID %lld) attempted to allocate a field with "
          "ID %d which exceeds the LEGION_MAX_APPLICATION_FIELD_ID "
          "bound set in legion_config.h", get_task_name(), get_unique_id(), fid)
#endif
      if (runtime->legion_spy_enabled)
        LegionSpy::log_field_creation(space.id, fid, field_size,
            (provenance == NULL) ? NULL : provenance->human_str());

      std::set<RtEvent> done_events;
      if (local)
        allocate_local_field(space, field_size, fid, 
                             serdez_id, done_events, provenance);
      else
        runtime->forest->allocate_field(space, field_size, fid,
                                        serdez_id, provenance);
      register_field_creation(space, fid, local);
      if (!done_events.empty())
      {
        RtEvent wait_on = Runtime::merge_events(done_events);
        wait_on.wait();
      }
      return fid;
    }

    //--------------------------------------------------------------------------
    void InnerContext::allocate_fields(FieldSpace space,
                                       const std::vector<size_t> &sizes,
                                       std::vector<FieldID> &resulting_fields,
                                       bool local, CustomSerdezID serdez_id,
                                       Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (resulting_fields.size() < sizes.size())
        resulting_fields.resize(sizes.size(), LEGION_AUTO_GENERATE_ID);
      for (unsigned idx = 0; idx < resulting_fields.size(); idx++)
      {
        if (resulting_fields[idx] == LEGION_AUTO_GENERATE_ID)
          resulting_fields[idx] = runtime->get_unique_field_id();
#ifdef DEBUG_LEGION
        else if (resulting_fields[idx] >= LEGION_MAX_APPLICATION_FIELD_ID)
          REPORT_LEGION_ERROR(ERROR_TASK_ATTEMPTED_ALLOCATE_FIELD,
            "Task %s (ID %lld) attempted to allocate a field with "
            "ID %d which exceeds the LEGION_MAX_APPLICATION_FIELD_ID "
            "bound set in legion_config.h", get_task_name(),
            get_unique_id(), resulting_fields[idx])
#endif
        if (runtime->legion_spy_enabled)
          LegionSpy::log_field_creation(space.id, resulting_fields[idx],
             sizes[idx], (provenance == NULL) ? NULL : provenance->human_str());
      }
      std::set<RtEvent> done_events;
      if (local)
        allocate_local_fields(space, sizes, resulting_fields,
                              serdez_id, done_events, provenance);
      else
        runtime->forest->allocate_fields(space, sizes, resulting_fields,
                                         serdez_id, provenance);
      register_all_field_creations(space, local, resulting_fields);
      if (!done_events.empty())
      {
        RtEvent wait_on = Runtime::merge_events(done_events);
        wait_on.wait();
      }
    }

    //--------------------------------------------------------------------------
    FieldID InnerContext::allocate_field(FieldSpace space, 
                                         const Future &field_size,
                                         FieldID fid, bool local,
                                         CustomSerdezID serdez_id,
                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (local)
        REPORT_LEGION_FATAL(LEGION_FATAL_UNIMPLEMENTED_FEATURE,
            "Local fields do no support allocation with future sizes yet.")
      if (fid == LEGION_AUTO_GENERATE_ID)
        fid = runtime->get_unique_field_id();
#ifdef DEBUG_LEGION
      else if (fid >= LEGION_MAX_APPLICATION_FIELD_ID)
        REPORT_LEGION_ERROR(ERROR_TASK_ATTEMPTED_ALLOCATE_FIELD,
          "Task %s (ID %lld) attempted to allocate a field with "
          "ID %d which exceeds the LEGION_MAX_APPLICATION_FIELD_ID "
          "bound set in legion_config.h", get_task_name(), get_unique_id(), fid)
#endif
      if (field_size.impl == NULL)
        REPORT_LEGION_ERROR(ERROR_REQUEST_FOR_EMPTY_FUTURE,
            "Invalid empty future passed to field allocation for field %d "
            "in task %s (UID %lld)", fid, get_task_name(), get_unique_id())
      // Get a new creation operation
      CreationOp *creator_op = runtime->get_available_creation_op();  
      const ApEvent ready = creator_op->get_completion_event();
      // Tell the node that we're allocating a field of size zero
      // which will indicate that we'll fill in the size later
      RtEvent precondition;
      FieldSpaceNode *node = runtime->forest->allocate_field(space, ready, fid, 
                                          serdez_id, provenance, precondition);
      creator_op->initialize_field(this, node, fid, field_size, provenance);
      register_field_creation(space, fid, local);
      // Make sure the IDs are valid for the user 
      if (precondition.exists() && !precondition.has_triggered())
        precondition.wait();
      add_to_dependence_queue(creator_op);
      return fid;
    }

    //--------------------------------------------------------------------------
    void InnerContext::allocate_local_field(FieldSpace space, size_t field_size,
                                          FieldID fid, CustomSerdezID serdez_id,
                                          std::set<RtEvent> &done_events,
                                          Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      // See if we've exceeded our local field allocations 
      // for this field space
      AutoLock local_lock(local_field_lock);
      std::vector<LocalFieldInfo> &infos = local_field_infos[space];
      if (infos.size() == runtime->max_local_fields)
        REPORT_LEGION_ERROR(ERROR_EXCEEDED_MAXIMUM_NUMBER_LOCAL_FIELDS,
          "Exceeded maximum number of local fields in "
                      "context of task %s (UID %lld). The maximum "
                      "is currently set to %d, but can be modified "
                      "with the -lg:local flag.", get_task_name(),
                      get_unique_id(), runtime->max_local_fields)
      std::set<unsigned> current_indexes;
      for (std::vector<LocalFieldInfo>::const_iterator it = 
            infos.begin(); it != infos.end(); it++)
        current_indexes.insert(it->index);
      std::vector<FieldID> fields(1, fid);
      std::vector<size_t> sizes(1, field_size);
      std::vector<unsigned> new_indexes;
      if (!runtime->forest->allocate_local_fields(space, fields, sizes, 
                  serdez_id, current_indexes, new_indexes, provenance))
        REPORT_LEGION_ERROR(ERROR_UNABLE_ALLOCATE_LOCAL_FIELD,
          "Unable to allocate local field in context of "
                      "task %s (UID %lld) due to local field size "
                      "fragmentation. This situation can be improved "
                      "by increasing the maximum number of permitted "
                      "local fields in a context with the -lg:local "
                      "flag.", get_task_name(), get_unique_id())
#ifdef DEBUG_LEGION
      assert(new_indexes.size() == 1);
#endif
      // Only need the lock here when modifying since all writes
      // to this data structure are serialized
      infos.push_back(LocalFieldInfo(fid, field_size, serdez_id, 
                                     new_indexes[0], false));
      struct Functor {
      public:
        Functor(DistributedID id, FieldSpace sp, Runtime *rt,
                Provenance *prov, const LocalFieldInfo &in, 
                std::set<RtEvent> &done)
          : did(id), space(sp), runtime(rt), provenance(prov),
            info(in), done_events(done), count(0) { }
        void apply(AddressSpaceID target)
        {
          RtUserEvent done_event = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize<size_t>(1); // field space count
            rez.serialize(space);
            if (provenance != NULL)
              provenance->serialize(rez);
            else
              Provenance::serialize_null(rez);
            rez.serialize<size_t>(1); // field count
            rez.serialize(info);
            rez.serialize(done_event);
          }
          runtime->send_local_field_update(target, rez);
          done_events.insert(done_event);
          count++;
        };
      public:
        DistributedID did;
        FieldSpace space;
        Runtime *runtime;
        Provenance *provenance;
        const LocalFieldInfo &info;
        std::set<RtEvent> &done_events;
        unsigned count;
      };
      Functor functor(did, space, runtime, 
          provenance, infos.back(), done_events);
      map_over_remote_instances(functor);
      if (functor.count > 0)
        pack_global_ref(functor.count);
    }

    //--------------------------------------------------------------------------
    void InnerContext::allocate_fields(FieldSpace space,
                                       const std::vector<Future> &sizes,
                                       std::vector<FieldID> &resulting_fields,
                                       bool local, CustomSerdezID serdez_id,
                                       Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (local)
        REPORT_LEGION_FATAL(LEGION_FATAL_UNIMPLEMENTED_FEATURE,
            "Local fields do no support allocation with future sizes yet.") 
      if (resulting_fields.size() < sizes.size())
        resulting_fields.resize(sizes.size(), LEGION_AUTO_GENERATE_ID);
      for (unsigned idx = 0; idx < resulting_fields.size(); idx++)
      {
        if (resulting_fields[idx] == LEGION_AUTO_GENERATE_ID)
          resulting_fields[idx] = runtime->get_unique_field_id();
#ifdef DEBUG_LEGION
        else if (resulting_fields[idx] >= LEGION_MAX_APPLICATION_FIELD_ID)
          REPORT_LEGION_ERROR(ERROR_TASK_ATTEMPTED_ALLOCATE_FIELD,
            "Task %s (ID %lld) attempted to allocate a field with "
            "ID %d which exceeds the LEGION_MAX_APPLICATION_FIELD_ID "
            "bound set in legion_config.h", get_task_name(),
            get_unique_id(), resulting_fields[idx])
#endif
      }
      for (unsigned idx = 0; idx < sizes.size(); idx++)
        if (sizes[idx].impl == NULL)
          REPORT_LEGION_ERROR(ERROR_REQUEST_FOR_EMPTY_FUTURE,
              "Invalid empty future passed to field allocation for field %d "
              "in task %s (UID %lld)", resulting_fields[idx],
              get_task_name(), get_unique_id())
      // Get a new creation operation
      CreationOp *creator_op = runtime->get_available_creation_op();  
      const ApEvent ready = creator_op->get_completion_event();
      // Tell the node that we're allocating a field of size zero
      // which will indicate that we'll fill in the size later
      RtEvent precondition;
      FieldSpaceNode *node = runtime->forest->allocate_fields(space, ready, 
                    resulting_fields, serdez_id, provenance, precondition);
      creator_op->initialize_fields(this, node, resulting_fields, 
                                    sizes, provenance);
      register_all_field_creations(space, local, resulting_fields);
      // Need to make sure that field IDs are valid for users
      if (precondition.exists() && !precondition.has_triggered())
        precondition.wait();
      add_to_dependence_queue(creator_op);
    }

    //--------------------------------------------------------------------------
    void InnerContext::allocate_local_fields(FieldSpace space,
                                   const std::vector<size_t> &sizes,
                                   const std::vector<FieldID> &resulting_fields,
                                   CustomSerdezID serdez_id,
                                   std::set<RtEvent> &done_events,
                                   Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      // See if we've exceeded our local field allocations 
      // for this field space
      AutoLock local_lock(local_field_lock);
      std::vector<LocalFieldInfo> &infos = local_field_infos[space];
      if ((infos.size() + sizes.size()) > runtime->max_local_fields)
        REPORT_LEGION_ERROR(ERROR_EXCEEDED_MAXIMUM_NUMBER_LOCAL_FIELDS,
          "Exceeded maximum number of local fields in "
                      "context of task %s (UID %lld). The maximum "
                      "is currently set to %d, but can be modified "
                      "with the -lg:local flag.", get_task_name(),
                      get_unique_id(), runtime->max_local_fields)
      std::set<unsigned> current_indexes;
      for (std::vector<LocalFieldInfo>::const_iterator it = 
            infos.begin(); it != infos.end(); it++)
        current_indexes.insert(it->index);
      std::vector<unsigned> new_indexes;
      if (!runtime->forest->allocate_local_fields(space, resulting_fields, 
              sizes, serdez_id, current_indexes, new_indexes, provenance))
        REPORT_LEGION_ERROR(ERROR_UNABLE_ALLOCATE_LOCAL_FIELD,
          "Unable to allocate local field in context of "
                      "task %s (UID %lld) due to local field size "
                      "fragmentation. This situation can be improved "
                      "by increasing the maximum number of permitted "
                      "local fields in a context with the -lg:local "
                      "flag.", get_task_name(), get_unique_id())
#ifdef DEBUG_LEGION
      assert(new_indexes.size() == resulting_fields.size());
#endif
      // Only need the lock here when writing since we know all writes
      // are serialized and we only need to worry about interfering readers
      const unsigned offset = infos.size();
      for (unsigned idx = 0; idx < resulting_fields.size(); idx++)
        infos.push_back(LocalFieldInfo(resulting_fields[idx], 
                   sizes[idx], serdez_id, new_indexes[idx], false));
      // Have to send notifications to any remote nodes 
      struct Functor {
      public:
        Functor(DistributedID id, FieldSpace sp, Runtime *rt,
                Provenance *prov, size_t s, unsigned off,
                const std::vector<LocalFieldInfo> &in, std::set<RtEvent> &done)
          : did(id), space(sp), runtime(rt), provenance(prov), size(s),
            offset(off), infos(in), done_events(done), count(0) { }
        void apply(AddressSpaceID target)
        {
          RtUserEvent done_event = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize<size_t>(1); // field space count
            rez.serialize(space);
            if (provenance != NULL)
              provenance->serialize(rez);
            else
              Provenance::serialize_null(rez);
            rez.serialize<size_t>(size); // field count
            for (unsigned idx = 0; idx < size; idx++)
              rez.serialize(infos[offset+idx]);
            rez.serialize(done_event);
          }
          runtime->send_local_field_update(target, rez);
          done_events.insert(done_event);
          count++;
        }
      public:
        DistributedID did;
        FieldSpace space;
        Runtime *runtime;
        Provenance *provenance;
        size_t size;
        unsigned offset;
        const std::vector<LocalFieldInfo> &infos;
        std::set<RtEvent> &done_events;
        unsigned count;
      };
      Functor functor(did, space, runtime, provenance, resulting_fields.size(),
                      offset, infos, done_events);
      map_over_remote_instances(functor);
      if (functor.count > 0)
        pack_global_ref(functor.count);
    }

    //--------------------------------------------------------------------------
    void InnerContext::free_field(FieldAllocatorImpl *allocator, 
    FieldSpace space, FieldID fid, const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      {
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
        const std::pair<FieldSpace,FieldID> key(space, fid);
        // This field will actually be removed in analyze_destroy_fields
        std::set<std::pair<FieldSpace,FieldID> >::const_iterator 
          finder = created_fields.find(key);
        if (finder == created_fields.end())
        {
          std::map<std::pair<FieldSpace,FieldID>,bool>::iterator 
            local_finder = local_fields.find(key);
          if (local_finder == local_fields.end())
          {
            // If we didn't make this field, record the deletion and
            // then have a later context handle it
            deleted_fields.emplace_back(DeletedField(space, fid, provenance));
            return;
          }
          else
            local_finder->second = true;
        }
        // Don't remove anything from created fields yet, we still might
        // need it as part of the logical dependence analysis for earlier ops
      }
      // Launch off the deletion operation
      DeletionOp *op = runtime->get_available_deletion_op();
      op->initialize_field_deletion(this, space, fid, unordered, allocator,
                                    provenance, false/*non owner shard*/);
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered field free performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
    } 

    //--------------------------------------------------------------------------
    void InnerContext::free_fields(FieldAllocatorImpl *allocator, 
                                   FieldSpace space,
                                   const std::set<FieldID> &to_free,
                                   const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      std::set<FieldID> free_now;
      {
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
        // These fields will actually be removed in analyze_destroy_fields
        for (std::set<FieldID>::const_iterator it = 
              to_free.begin(); it != to_free.end(); it++)
        {
          const std::pair<FieldSpace,FieldID> key(space, *it);
          std::set<std::pair<FieldSpace,FieldID> >::const_iterator 
            finder = created_fields.find(key);
          if (finder == created_fields.end())
          {
            std::map<std::pair<FieldSpace,FieldID>,bool>::iterator 
              local_finder = local_fields.find(key);
            if (local_finder != local_fields.end())
            {
              local_finder->second = true;
              free_now.insert(*it);
            }
            else
              deleted_fields.emplace_back(DeletedField(space, *it, provenance));
          }
          else
          {
            // Don't remove anything from created fields yet, 
            // we still might need need it as part of the logical 
            // dependence analysis for earlier ops
            free_now.insert(*it);
          }
        }
      }
      if (free_now.empty())
        return;
      DeletionOp *op = runtime->get_available_deletion_op();
      op->initialize_field_deletions(this, space, free_now, unordered, 
                     allocator, provenance, false/*non owner shard*/);
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered free fields performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
    }

    //--------------------------------------------------------------------------
    LogicalRegion InnerContext::create_logical_region(IndexSpace index_space,
                                                      FieldSpace field_space,
                                                      const bool task_local,
                                                      Provenance *provenance,
                                                      const bool output_region)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      RegionTreeID tid = runtime->get_unique_region_tree_id();
      LogicalRegion region(tid, index_space, field_space);
#ifdef DEBUG_LEGION
      log_region.debug("Creating logical region in task %s (ID %lld) with "
                       "index space %x and field space %x in new tree %d",
                       get_task_name(), get_unique_id(), 
                       index_space.id, field_space.id, tid);
#endif
      if (runtime->legion_spy_enabled)
        LegionSpy::log_top_region(index_space.id, field_space.id, tid,
            runtime->address_space, (provenance == NULL) ? 
            NULL : provenance->human_str());
      const DistributedID did = runtime->get_available_distributed_id();
      runtime->forest->create_logical_region(region, did, provenance);
      // Register the creation of a top-level region with the context
      const unsigned created_index =
        register_region_creation(region, task_local, output_region);
      if (output_region)
      {
        // If this is an output region make sure nobody tries to compute
        // the equivalence sets for it until we know it is ready
        AutoLock priv_lock(privilege_lock);
#ifdef DEBUG_LEGION
        assert(equivalence_set_trees.find(created_index) == 
                equivalence_set_trees.end());
        assert(pending_equivalence_set_trees.find(created_index) ==
            pending_equivalence_set_trees.end());
#endif
        // Put in a guard so that nobody else tries to make it
        pending_equivalence_set_trees[created_index] = 
          RtUserEvent::NO_RT_USER_EVENT; 
      }
      return region;
    }

    //--------------------------------------------------------------------------
    void InnerContext::create_shared_ownership(LogicalRegion handle)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (!handle.exists())
        return;
      if (!runtime->forest->is_top_level_region(handle))
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_SHARED_OWNERSHIP,
            "Illegal call to create shared ownership for logical region "
            "(%x,%x,%x in task %s (UID %lld) which is not a top-level logical "
            "region. Legion only permits top-level logical regions to have "
            "shared ownerships.", handle.index_space.id, handle.field_space.id,
            handle.tree_id, get_task_name(), get_unique_id())
      runtime->create_shared_ownership(handle);
      AutoLock priv_lock(privilege_lock);
      std::map<LogicalRegion,unsigned>::iterator finder = 
        created_regions.find(handle);
      if (finder != created_regions.end())
        finder->second++;
      else
        created_regions[handle] = 1;
    }

    //--------------------------------------------------------------------------
    void InnerContext::destroy_logical_region(LogicalRegion handle,
                                   const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (!handle.exists())
        return;
#ifdef DEBUG_LEGION
      log_region.debug("Deleting logical region (%x,%x) in task %s (ID %lld)",
                       handle.index_space.id, handle.field_space.id, 
                       get_task_name(), get_unique_id());
#endif
      // Check to see if this is a top-level logical region, if not then
      // we shouldn't even be destroying it
      if (!runtime->forest->is_top_level_region(handle))
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_RESOURCE_DESTRUCTION,
            "Illegal call to destroy logical region (%x,%x,%x in task %s "
            "(UID %lld) which is not a top-level logical region. Legion only "
            "permits top-level logical regions to be destroyed.", 
            handle.index_space.id, handle.field_space.id, handle.tree_id,
            get_task_name(), get_unique_id())
      // Check to see if this is one that we should be allowed to destory
      {
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
        std::map<LogicalRegion,unsigned>::iterator finder = 
          created_regions.find(handle);
        if (finder == created_regions.end())
        {
          // Check to see if it is a local region
          std::map<LogicalRegion,bool>::iterator local_finder = 
            local_regions.find(handle);
          // Mark that this region is deleted, safe even though this
          // is a read-only lock because we're not changing the structure
          // of the map
          if (local_finder == local_regions.end())
          {
            // Record the deletion for later and propagate it up
            deleted_regions.emplace_back(DeletedRegion(handle, provenance));
            return;
          }
          else
            local_finder->second = true;
        }
        else
        {
          if (finder->second == 0)
          {
            REPORT_LEGION_WARNING(LEGION_WARNING_DUPLICATE_DELETION,
                "Duplicate deletions were performed for region (%x,%x,%x) "
                "in task tree rooted by %s", handle.index_space.id, 
                handle.field_space.id, handle.tree_id, get_task_name())
            return;
          }
          if (--finder->second > 0)
            return;
          // Don't remove anything from created regions yet, we still might
          // need it as part of the logical dependence analysis for earlier
          // operations, but the reference count is zero so we're protected
        }
      }
      DeletionOp *op = runtime->get_available_deletion_op(); 
      op->initialize_logical_region_deletion(this, handle,unordered,provenance);
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered logical region deletion performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::reset_equivalence_sets(LogicalRegion parent,
                                              LogicalRegion region,
                                              const std::set<FieldID> &fields)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      // Ignore reset calls inside of traces
      if ((current_trace != NULL) && current_trace->is_fixed())
      {
        REPORT_LEGION_WARNING(
            LEGION_WARNING_IGNORING_EQUIVALENCE_SETS_RESET,
            "Ignoring equivalence sets reset in %s (UID %lld) because "
            "it was made inside of a trace.",
            get_task_name(), get_unique_id())
        return;
      }
      if (fields.empty())
      {
        REPORT_LEGION_WARNING(
            LEGION_WARNING_IGNORING_EQUIVALENCE_SETS_RESET,
            "Ignoring equivalence sets reset in %s (UID %lld) because "
            "it contains no fields.",
            get_task_name(), get_unique_id())
        return;
      }
      ResetOp *reset = runtime->get_available_reset_op();
      reset->initialize(this, parent, region, fields);
      add_to_dependence_queue(reset);
    }

    //--------------------------------------------------------------------------
    void InnerContext::get_local_field_set(const FieldSpace handle,
                                           const std::set<unsigned> &indexes,
                                           std::set<FieldID> &to_set) const
    //--------------------------------------------------------------------------
    {
      AutoLock lf_lock(local_field_lock, 1, false/*exclusive*/);
      std::map<FieldSpace,std::vector<LocalFieldInfo> >::const_iterator
        finder = local_field_infos.find(handle);
#ifdef DEBUG_LEGION
      assert(finder != local_field_infos.end());
      unsigned found = 0;
#endif
      for (std::vector<LocalFieldInfo>::const_iterator it = 
            finder->second.begin(); it != finder->second.end(); it++)
      {
        if (indexes.find(it->index) != indexes.end())
        {
#ifdef DEBUG_LEGION
          found++;
#endif
          to_set.insert(it->fid);
        }
      }
#ifdef DEBUG_LEGION
      assert(found == indexes.size());
#endif
    }

    //--------------------------------------------------------------------------
    void InnerContext::get_local_field_set(const FieldSpace handle,
                                           const std::set<unsigned> &indexes,
                                           std::vector<FieldID> &to_set) const
    //--------------------------------------------------------------------------
    {
      AutoLock lf_lock(local_field_lock, 1, false/*exclusive*/);
      std::map<FieldSpace,std::vector<LocalFieldInfo> >::const_iterator
        finder = local_field_infos.find(handle);
#ifdef DEBUG_LEGION
      assert(finder != local_field_infos.end());
      unsigned found = 0;
#endif
      for (std::vector<LocalFieldInfo>::const_iterator it = 
            finder->second.begin(); it != finder->second.end(); it++)
      {
        if (indexes.find(it->index) != indexes.end())
        {
#ifdef DEBUG_LEGION
          found++;
#endif
          to_set.push_back(it->fid);
        }
      }
#ifdef DEBUG_LEGION
      assert(found == indexes.size());
#endif
    }

    //--------------------------------------------------------------------------
    unsigned InnerContext::add_created_region(LogicalRegion handle,
                                          bool task_local, bool output_region)
    //--------------------------------------------------------------------------
    {
      // Already hold the lock from the caller
      if (!task_local && !output_region)
      {
        // There's a race here with created region tree contexts coming back
        // and making these requirements for themselves so we check for
        // duplications here in that case
        for (std::map<unsigned,RegionRequirement>::const_iterator it =
              created_requirements.begin(); it != 
              created_requirements.end(); it++)
        {
          if (it->second.parent == handle)
            return it->first;
#ifdef DEBUG_LEGION
          // shouldn't have anything from the same region tree here
          assert(it->second.parent.get_tree_id() != handle.get_tree_id());
#endif
        }
      }
      RegionRequirement new_req(handle, LEGION_READ_WRITE, 
                                LEGION_EXCLUSIVE, handle);
      if (output_region)
        new_req.flags |= LEGION_CREATED_OUTPUT_REQUIREMENT_FLAG;
      if (runtime->legion_spy_enabled)
        TaskOp::log_requirement(get_unique_id(), next_created_index, new_req);
      // Put a region requirement with no fields in the list of
      // created requirements, we know we can add any fields for
      // this field space in the future since we own all privileges
      created_requirements[next_created_index] = new_req;
      // Created regions always return privileges that they make
      returnable_privileges[next_created_index] = !task_local;
      return next_created_index++;
    }

    //--------------------------------------------------------------------------
    void InnerContext::log_created_requirements(void)
    //--------------------------------------------------------------------------
    {
      for (std::map<unsigned,RegionRequirement>::const_iterator it = 
           created_requirements.begin(); it != created_requirements.end(); it++)
      {
        // We already logged the requirement when we made it
        // Skip it if there are no privilege fields
        if (it->second.privilege_fields.empty())
          continue;
        owner_task->log_virtual_mapping(it->first, it->second);
      }
    } 

    //--------------------------------------------------------------------------
    unsigned InnerContext::register_region_creation(LogicalRegion handle,
                                                    bool task_local,
                                                    bool output_region)
    //--------------------------------------------------------------------------
    {
      // Create a new logical region 
      // Hold the operation lock when doing this since children could
      // be returning values from the utility processor
      AutoLock priv_lock(privilege_lock);
#ifdef DEBUG_LEGION
      assert(local_regions.find(handle) == local_regions.end());
      assert(created_regions.find(handle) == created_regions.end());
#endif
      if (task_local)
      {
        if (is_leaf_context())
          REPORT_LEGION_ERROR(ERROR_ILLEGAL_REGION_CREATION,
              "Illegal task-local region creation performed in leaf task %s "
                           "(ID %lld)", get_task_name(), get_unique_id())
        local_regions[handle] = false/*not deleted*/;
      }
      else
      {
#ifdef DEBUG_LEGION
        assert(created_regions.find(handle) == created_regions.end());
#endif
        created_regions[handle] = 1;
      }
      return add_created_region(handle, task_local, output_region);
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_field_creation(FieldSpace handle, 
                                              FieldID fid, bool local)
    //--------------------------------------------------------------------------
    {
      AutoLock priv_lock(privilege_lock);
      std::pair<FieldSpace,FieldID> key(handle,fid);
#ifdef DEBUG_LEGION
      assert(local_fields.find(key) == local_fields.end());
      assert(created_fields.find(key) == created_fields.end());
#endif
      if (!local)
      {
#ifdef DEBUG_LEGION
        assert(created_fields.find(key) == created_fields.end());
#endif
        created_fields.insert(key);
      }
      else
        local_fields[key] = false/*deleted*/;
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_all_field_creations(FieldSpace handle,
                                 bool local, const std::vector<FieldID> &fields)
    //--------------------------------------------------------------------------
    {
      AutoLock priv_lock(privilege_lock);
      if (local)
      {
        for (unsigned idx = 0; idx < fields.size(); idx++)
        {
          std::pair<FieldSpace,FieldID> key(handle,fields[idx]);
#ifdef DEBUG_LEGION
          assert(local_fields.find(key) == local_fields.end());
#endif
          local_fields[key] = false/*deleted*/;
        }
      }
      else
      {
        for (unsigned idx = 0; idx < fields.size(); idx++)
        {
          std::pair<FieldSpace,FieldID> key(handle,fields[idx]);
#ifdef DEBUG_LEGION
          assert(created_fields.find(key) == created_fields.end());
#endif
          created_fields.insert(key);
        }
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_field_space_creation(FieldSpace space)
    //--------------------------------------------------------------------------
    {
      AutoLock priv_lock(privilege_lock);
#ifdef DEBUG_LEGION
      assert(created_field_spaces.find(space) == created_field_spaces.end());
#endif
      created_field_spaces[space] = 1;
    }

    //--------------------------------------------------------------------------
    bool InnerContext::has_created_index_space(IndexSpace space) const
    //--------------------------------------------------------------------------
    {
      AutoLock priv_lock(privilege_lock);
      return (created_index_spaces.find(space) != created_index_spaces.end());
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_index_space_creation(IndexSpace space)
    //--------------------------------------------------------------------------
    {
      AutoLock priv_lock(privilege_lock);
#ifdef DEBUG_LEGION
      assert(created_index_spaces.find(space) == created_index_spaces.end());
#endif
      created_index_spaces[space] = 1;
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_index_partition_creation(IndexPartition handle)
    //--------------------------------------------------------------------------
    {
      AutoLock priv_lock(privilege_lock);
#ifdef DEBUG_LEGION
      assert(created_index_partitions.find(handle) == 
             created_index_partitions.end());
#endif
      created_index_partitions[handle] = 1;
    }

    //--------------------------------------------------------------------------
    void InnerContext::report_leaks_and_duplicates(
                                               std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      if (!deleted_regions.empty())
      {
        for (std::vector<DeletedRegion>::const_iterator it = 
              deleted_regions.begin(); it != deleted_regions.end(); it++)
          REPORT_LEGION_WARNING(LEGION_WARNING_DUPLICATE_DELETION,
              "Duplicate deletions were performed for region (%x,%x,%x) "
              "in task tree rooted by %s (provenance %s)", 
              it->region.index_space.id, it->region.field_space.id, 
              it->region.tree_id, get_task_name(), (it->provenance != NULL) ?
              it->provenance->human.c_str() : "unknown")
        deleted_regions.clear();
      }
      if (!deleted_fields.empty())
      {
        for (std::vector<DeletedField>::const_iterator it =
              deleted_fields.begin(); it != deleted_fields.end(); it++)
          REPORT_LEGION_WARNING(LEGION_WARNING_DUPLICATE_DELETION,
              "Duplicate deletions were performed on field %d of "
              "field space %x in task tree rooted by %s (provenance %s)", 
              it->fid, it->space.id, get_task_name(), 
              (it->provenance != NULL) ? it->provenance->human.c_str() :
              "unknown")
        deleted_fields.clear();
      }
      if (!deleted_field_spaces.empty())
      {
        for (std::vector<DeletedFieldSpace>::const_iterator it = 
              deleted_field_spaces.begin(); it != 
              deleted_field_spaces.end(); it++)
          REPORT_LEGION_WARNING(LEGION_WARNING_DUPLICATE_DELETION,
              "Duplicate deletions were performed on field space %x "
              "in task tree rooted by %s (provenance %s)", it->space.id,
              get_task_name(), (it->provenance != NULL) ?
              it->provenance->human.c_str() : "unknown")
        deleted_field_spaces.clear();
      }
      if (!deleted_index_spaces.empty())
      {
        for (std::vector<DeletedIndexSpace>::const_iterator it =
              deleted_index_spaces.begin(); it != 
              deleted_index_spaces.end(); it++)
          REPORT_LEGION_WARNING(LEGION_WARNING_DUPLICATE_DELETION,
              "Duplicate deletions were performed on index space %x "
              "in task tree rooted by %s (provenance %s)", it->space.id,
              get_task_name(), (it->provenance != NULL) ?
              it->provenance->human.c_str() : "unknown")
        deleted_index_spaces.clear();
      }
      if (!deleted_index_partitions.empty())
      {
        for (std::vector<DeletedPartition>::const_iterator it =
              deleted_index_partitions.begin(); it !=
              deleted_index_partitions.end(); it++)
          REPORT_LEGION_WARNING(LEGION_WARNING_DUPLICATE_DELETION,
              "Duplicate deletions were performed on index partition %x "
              "in task tree rooted by %s (provenance %s)", it->partition.id,
              get_task_name(), (it->provenance != NULL) ?
              it->provenance->human.c_str() : "unknown")
        deleted_index_partitions.clear();
      }
      // Now we go through and delete anything that the user leaked
      if (!created_regions.empty())
      {
        for (std::map<LogicalRegion,unsigned>::const_iterator rit = 
              created_regions.begin(); rit != created_regions.end(); rit++)
        {
          if (runtime->report_leaks)
            REPORT_LEGION_WARNING(LEGION_WARNING_LEAKED_RESOURCE,
                "Logical region (%x,%x,%x) was leaked out of task tree rooted "
                "by task %s", rit->first.index_space.id, 
                rit->first.field_space.id, rit->first.tree_id, get_task_name())
          runtime->forest->destroy_logical_region(rit->first, preconditions);
          // Remove any latent field spaces and therefore any created fields
          // since they might not be able to be cleaned up after this since
          // this region might be holding the last reference to the field space
          if (!latent_field_spaces.empty())
          {
            std::map<FieldSpace,std::set<LogicalRegion> >::iterator finder =
              latent_field_spaces.find(rit->first.get_field_space());
            if (finder != latent_field_spaces.end())
            {
              std::set<LogicalRegion>::iterator latent_finder = 
                finder->second.find(rit->first);
#ifdef DEBUG_LEGION
              assert(latent_finder != finder->second.end());
#endif
              finder->second.erase(latent_finder);
              if (finder->second.empty())
              {
                // Now that all the regions using this field space have
                // been deleted we can clean up all the created_fields
                for (std::set<std::pair<FieldSpace,FieldID> >::iterator it =
                      created_fields.begin(); it != 
                      created_fields.end(); /*nothing*/)
                {
                  if (it->first == finder->first)
                  {
                    std::set<std::pair<FieldSpace,FieldID> >::iterator 
                      to_delete = it++;
                    created_fields.erase(to_delete);
                  }
                  else
                    it++;
                }
                latent_field_spaces.erase(finder);
              }
            }
          }
        }
        created_regions.clear();
      }
      if (!created_fields.empty())
      {
        std::map<FieldSpace,FieldAllocatorImpl*> leak_allocators;
        for (std::set<std::pair<FieldSpace,FieldID> >::const_iterator 
              it = created_fields.begin(); it != created_fields.end(); it++)
        {
          if (runtime->report_leaks)
            REPORT_LEGION_WARNING(LEGION_WARNING_LEAKED_RESOURCE,
                "Field %d of field space %x was leaked out of task tree rooted "
                "by task %s", it->second, it->first.id, get_task_name())
          std::map<FieldSpace,FieldAllocatorImpl*>::const_iterator finder =
              leak_allocators.find(it->first);
          if (finder == leak_allocators.end())
          {
            FieldAllocatorImpl *allocator =
              create_field_allocator(it->first, true/*unordered*/);
            allocator->add_reference();
            leak_allocators[it->first] = allocator;
            allocator->ready_event.wait();
          }
          else
            finder->second->ready_event.wait();
          runtime->forest->free_field(it->first, it->second, preconditions);
        }
        for (std::map<FieldSpace,FieldAllocatorImpl*>::const_iterator it =
              leak_allocators.begin(); it != leak_allocators.end(); it++)
          if (it->second->remove_reference())
            delete it->second;
        created_fields.clear();
      }
      if (!created_field_spaces.empty())
      {
        for (std::map<FieldSpace,unsigned>::const_iterator it = 
              created_field_spaces.begin(); it != 
              created_field_spaces.end(); it++)
        {
          if (runtime->report_leaks)
            REPORT_LEGION_WARNING(LEGION_WARNING_LEAKED_RESOURCE,
                "Field space %x was leaked out of task tree rooted by task %s",
                it->first.id, get_task_name())
          runtime->forest->destroy_field_space(it->first, preconditions);
        }
        created_field_spaces.clear();
      }
      if (!created_index_partitions.empty())
      {
        for (std::map<IndexPartition,unsigned>::const_iterator it =
              created_index_partitions.begin(); it != 
              created_index_partitions.end(); it++)
        {
          if (runtime->report_leaks)
            REPORT_LEGION_WARNING(LEGION_WARNING_LEAKED_RESOURCE,
                "Index partition %x was leaked out of task tree rooted by "
                "task %s", it->first.id, get_task_name())
          runtime->forest->destroy_index_partition(it->first, preconditions);
        }
        created_index_partitions.clear();
      }
      if (!created_index_spaces.empty())
      {
        for (std::map<IndexSpace,unsigned>::const_iterator it = 
              created_index_spaces.begin(); it !=
              created_index_spaces.end(); it++)
        {
          if (runtime->report_leaks)
            REPORT_LEGION_WARNING(LEGION_WARNING_LEAKED_RESOURCE,
                "Index space %x was leaked out of task tree rooted by task %s",
                it->first.id, get_task_name())
          runtime->forest->destroy_index_space(it->first, 
                  runtime->address_space, preconditions);
        }
        created_index_spaces.clear();
      } 
    }

    //--------------------------------------------------------------------------
    void InnerContext::analyze_destroy_fields(FieldSpace handle,
                                             const std::set<FieldID> &to_delete,
                                    std::vector<RegionRequirement> &delete_reqs,
                                    std::vector<unsigned> &parent_req_indexes,
                                    std::vector<FieldID> &global_to_free,
                                    std::vector<FieldID> &local_to_free,
                                    std::vector<unsigned> &local_field_indexes,
                                    std::vector<unsigned> &deletion_indexes)
    //--------------------------------------------------------------------------
    {
      {
        // We can't destroy any fields from our original regions because we
        // were not the ones that made them.
        AutoLock priv_lock(privilege_lock);
        // We can actually remove the fields from the data structure now 
        for (std::set<FieldID>::const_iterator it =
              to_delete.begin(); it != to_delete.end(); it++)
        {
          const std::pair<FieldSpace,FieldID> key(handle, *it);
          std::set<std::pair<FieldSpace,FieldID> >::iterator finder = 
            created_fields.find(key);
          if (finder == created_fields.end())
          {
            std::map<std::pair<FieldSpace,FieldID>,bool>::iterator 
              local_finder = local_fields.find(key);
#ifdef DEBUG_LEGION
            assert(local_finder != local_fields.end());
            assert(local_finder->second);
#endif
            local_fields.erase(local_finder);
            local_to_free.push_back(*it);
          }
          else
          {
            created_fields.erase(finder);
            global_to_free.push_back(*it);
          }
        }
        // Now figure out which region requirements can be destroyed
        for (std::map<unsigned,RegionRequirement>::iterator it = 
              created_requirements.begin(); it != 
              created_requirements.end(); it++)
        {
          if (it->second.region.get_field_space() != handle)
            continue;
          std::set<FieldID> overlapping_fields;
          for (std::set<FieldID>::const_iterator fit = to_delete.begin();
                fit != to_delete.end(); fit++)
          {
            std::set<FieldID>::iterator finder = 
              it->second.privilege_fields.find(*fit);
            if (finder != it->second.privilege_fields.end())
            {
              overlapping_fields.insert(*fit);
              // Remove this from the created requirements fields
              it->second.privilege_fields.erase(finder);
            }
          }
          if (overlapping_fields.empty())
            continue;
          delete_reqs.resize(delete_reqs.size()+1);
          RegionRequirement &req = delete_reqs.back();
          req.region = it->second.region;
          req.parent = it->second.region;
          req.privilege = LEGION_READ_WRITE;
          req.prop = LEGION_EXCLUSIVE;
          req.privilege_fields.swap(overlapping_fields);
          req.handle_type = LEGION_SINGULAR_PROJECTION;
          parent_req_indexes.push_back(it->first);
          // We need some extra logging for legion spy
          if (runtime->legion_spy_enabled)
          {
            LegionSpy::log_requirement_fields(get_unique_id(),
                                              it->first, req.privilege_fields);
            owner_task->log_virtual_mapping(it->first, req);
          }
        }
      }
      if (!local_to_free.empty())
        analyze_free_local_fields(handle, local_to_free, local_field_indexes);
    }

    //--------------------------------------------------------------------------
    void InnerContext::analyze_destroy_logical_region(LogicalRegion handle,
                                    std::vector<RegionRequirement> &delete_reqs,
                                    std::vector<unsigned> &parent_req_indexes,
                                    std::vector<bool> &returnable)
    //--------------------------------------------------------------------------
    {
      // If we're deleting a field space then we can't be deleting any of the 
      // original requirements, only requirements that we created
      if (runtime->legion_spy_enabled)
      {
        // We need some extra logging for legion spy
        std::vector<MappingInstance> instances(1, 
              Mapping::PhysicalInstance::get_virtual_instance());
        AutoLock priv_lock(privilege_lock);
        for (std::map<unsigned,RegionRequirement>::iterator it = 
              created_requirements.begin(); it != 
              created_requirements.end(); it++)
        {
          // Has to match precisely
          if (handle.get_tree_id() == it->second.region.get_tree_id())
          {
#ifdef DEBUG_LEGION
            // Should be the same region
            assert(handle == it->second.region);
            assert(returnable_privileges.find(it->first) !=
                    returnable_privileges.end());
#endif
            // Do extra logging for legion spy
            owner_task->log_virtual_mapping(it->first, it->second);
            // Then do the result of the normal operations
            delete_reqs.resize(delete_reqs.size()+1);
            RegionRequirement &req = delete_reqs.back();
            req.region = it->second.region;
            req.parent = it->second.region;
            req.privilege = LEGION_READ_WRITE;
            req.prop = LEGION_EXCLUSIVE;
            // Swap the privilege fields so that nothing else tries
            // to delete those particular fields
            req.privilege_fields.swap(it->second.privilege_fields);
            req.handle_type = LEGION_SINGULAR_PROJECTION;
            req.flags = it->second.flags;
            parent_req_indexes.push_back(it->first);
            returnable.push_back(returnable_privileges[it->first]);
          }
        }
        // Remove the region from the created set
        {
          std::map<LogicalRegion,unsigned>::iterator finder = 
            created_regions.find(handle);
          if (finder == created_regions.end())
          {
            std::map<LogicalRegion,bool>::iterator local_finder = 
              local_regions.find(handle);
#ifdef DEBUG_LEGION
            assert(local_finder != local_regions.end());
            assert(local_finder->second);
#endif
            local_regions.erase(local_finder);
          }
          else
          {
#ifdef DEBUG_LEGION
            assert(finder->second == 0);
#endif
            created_regions.erase(finder);
          }
        }
        // Check to see if we have any latent field spaces to clean up
        if (!latent_field_spaces.empty())
        {
          std::map<FieldSpace,std::set<LogicalRegion> >::iterator finder =
            latent_field_spaces.find(handle.get_field_space());
          if (finder != latent_field_spaces.end())
          {
            std::set<LogicalRegion>::iterator region_finder = 
              finder->second.find(handle);
#ifdef DEBUG_LEGION
            assert(region_finder != finder->second.end());
#endif
            finder->second.erase(region_finder);
            if (finder->second.empty())
            {
              // Now that all the regions using this field space have
              // been deleted we can clean up all the created_fields
              for (std::set<std::pair<FieldSpace,FieldID> >::iterator it =
                    created_fields.begin(); it != 
                    created_fields.end(); /*nothing*/)
              {
                if (it->first == finder->first)
                {
                  std::set<std::pair<FieldSpace,FieldID> >::iterator 
                    to_delete = it++;
                  created_fields.erase(to_delete);
                }
                else
                  it++;
              }
              latent_field_spaces.erase(finder);
            }
          }
        }
      }
      else
      {
        AutoLock priv_lock(privilege_lock);
        for (std::map<unsigned,RegionRequirement>::iterator it = 
              created_requirements.begin(); it != 
              created_requirements.end(); it++)
        {
          // Has to match precisely
          if (handle.get_tree_id() == it->second.region.get_tree_id())
          {
#ifdef DEBUG_LEGION
            // Should be the same region
            assert(handle == it->second.region);
            assert(returnable_privileges.find(it->first) !=
                    returnable_privileges.end());
#endif
            delete_reqs.resize(delete_reqs.size()+1);
            RegionRequirement &req = delete_reqs.back();
            req.region = it->second.region;
            req.parent = it->second.region;
            req.privilege = LEGION_READ_WRITE;
            req.prop = LEGION_EXCLUSIVE;
            // Swap the privilege fields so that nothing else tries
            // to delete those particular fields
            req.privilege_fields.swap(it->second.privilege_fields);
            req.handle_type = LEGION_SINGULAR_PROJECTION;
            parent_req_indexes.push_back(it->first);
            returnable.push_back(returnable_privileges[it->first]);
          }
        }
        // Remove the region from the created set
        {
          std::map<LogicalRegion,unsigned>::iterator finder = 
            created_regions.find(handle);
          if (finder == created_regions.end())
          {
            std::map<LogicalRegion,bool>::iterator local_finder = 
              local_regions.find(handle);
#ifdef DEBUG_LEGION
            assert(local_finder != local_regions.end());
            assert(local_finder->second);
#endif
            local_regions.erase(local_finder);
          }
          else
          {
#ifdef DEBUG_LEGION
            assert(finder->second == 0);
#endif
            created_regions.erase(finder);
          }
        }
        // Check to see if we have any latent field spaces to clean up
        if (!latent_field_spaces.empty())
        {
          std::map<FieldSpace,std::set<LogicalRegion> >::iterator finder =
            latent_field_spaces.find(handle.get_field_space());
          if (finder != latent_field_spaces.end())
          {
            std::set<LogicalRegion>::iterator region_finder = 
              finder->second.find(handle);
#ifdef DEBUG_LEGION
            assert(region_finder != finder->second.end());
#endif
            finder->second.erase(region_finder);
            if (finder->second.empty())
            {
              // Now that all the regions using this field space have
              // been deleted we can clean up all the created_fields
              for (std::set<std::pair<FieldSpace,FieldID> >::iterator it =
                    created_fields.begin(); it != 
                    created_fields.end(); /*nothing*/)
              {
                if (it->first == finder->first)
                {
                  std::set<std::pair<FieldSpace,FieldID> >::iterator 
                    to_delete = it++;
                  created_fields.erase(to_delete);
                }
                else
                  it++;
              }
              latent_field_spaces.erase(finder);
            }
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_physical_region(const RegionRequirement &req,
          bool mapped, MapperID mid, MappingTagID tag, ApUserEvent &unmap_event,
          bool virtual_mapped, const InstanceSet &physical_instances)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!unmap_event.exists());
#endif
      if (!virtual_mapped)
        unmap_event = Runtime::create_ap_user_event(NULL);
      PhysicalRegionImpl *impl = new PhysicalRegionImpl(req,
          RtEvent::NO_RT_EVENT, ApEvent::NO_AP_EVENT,
          mapped ? unmap_event : ApUserEvent::NO_AP_USER_EVENT, mapped, this,
          mid, tag, false/*leaf region*/, virtual_mapped, 
          false/*never collective*/, runtime);
      physical_regions.emplace_back(PhysicalRegion(impl));
      if (!virtual_mapped)
      {
#ifdef DEBUG_LEGION
        if (owner_task->is_remote())
        {
          // If the owner task is remote, then we need to acquire the 
          // valid references first since the valid references are held
          // on the owner node and the checking code wants to see that 
          // these instances are already valid when adding the references
          if (!physical_instances.acquire_valid_references(CONTEXT_REF))
            REPORT_LEGION_FATAL(LEGION_FATAL_GARBAGE_COLLECTION_RACE,
                "Found an internal garbage collection race. Please "
                "run with -lg:safe_mapper and see if it reports any "
                "errors. If not, then please report this as a bug.")
          impl->set_references(physical_instances, true/*safe*/);
          // Remove the references we acquired after they've been added
          // by the physical region
          physical_instances.remove_valid_references(CONTEXT_REF);
        }
        else
#endif
        impl->set_references(physical_instances, true/*safe*/);
      }
    }

    //--------------------------------------------------------------------------
    Future InnerContext::execute_task(const TaskLauncher &launcher,
                                      std::vector<OutputRequirement> *outputs)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      // Quick out for predicate false
      if (launcher.predicate == Predicate::FALSE_PRED)
        return predicate_task_false(launcher, provenance);
      IndividualTask *task = runtime->get_available_individual_task();
      Future result = task->initialize_task(this, launcher, provenance,
                                            true/*track parent*/,
                                            false/*top level*/,
                                            false/*must epoch*/,
                                            outputs);
#ifdef DEBUG_LEGION
      log_task.debug("Registering new single task with unique id %lld "
                      "and task %s (ID %lld) with high level runtime in "
                      "addresss space %d",
                      task->get_unique_id(), task->get_task_name(), 
                      task->get_unique_id(), runtime->address_space);
#endif
      execute_task_launch(task, false/*index*/, current_trace, provenance, 
                          launcher.silence_warnings, launcher.enable_inlining);
      return result;
    }

    //--------------------------------------------------------------------------
    FutureMap InnerContext::execute_index_space(
                                        const IndexTaskLauncher &launcher,
                                        std::vector<OutputRequirement> *outputs)
    //--------------------------------------------------------------------------
    {
      if (launcher.must_parallelism)
      {
        // Turn around and use a must epoch launcher
        MustEpochLauncher epoch_launcher(launcher.map_id, launcher.tag);
        epoch_launcher.add_index_task(launcher);
        epoch_launcher.provenance = launcher.provenance;
        FutureMap result = execute_must_epoch(epoch_launcher);
        return result;
      }
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      if (launcher.launch_domain.exists() && 
          (launcher.launch_domain.get_volume() == 0))
      {
        REPORT_LEGION_WARNING(LEGION_WARNING_IGNORING_EMPTY_INDEX_TASK_LAUNCH,
          "Ignoring empty index task launch in task %s (ID %lld)",
                        get_task_name(), get_unique_id());
        return FutureMap();
      }
      IndexSpace launch_space = launcher.launch_space;
      if (!launch_space.exists())
        launch_space = 
          find_index_launch_space(launcher.launch_domain, provenance);
      // Quick out for predicate false
      if (launcher.predicate == Predicate::FALSE_PRED)
        return predicate_index_task_false(total_children_count++, launch_space,
                                          launcher, provenance);
      IndexTask *task = runtime->get_available_index_task();
      FutureMap result = task->initialize_task(this,
                                               launcher,
                                               launch_space,
                                               provenance,
                                               true /*track*/,
                                               outputs);
#ifdef DEBUG_LEGION
      log_task.debug("Registering new index space task with unique id "
                     "%lld and task %s (ID %lld) with high level runtime in "
                     "address space %d",
                     task->get_unique_id(), task->get_task_name(), 
                     task->get_unique_id(), runtime->address_space);
#endif
      execute_task_launch(task, true/*index*/, current_trace, provenance,
                          launcher.silence_warnings, launcher.enable_inlining);
      return result;
    }

    //--------------------------------------------------------------------------
    Future InnerContext::execute_index_space(const IndexTaskLauncher &launcher,
                                        ReductionOpID redop, bool deterministic,
                                        std::vector<OutputRequirement> *outputs)
    //--------------------------------------------------------------------------
    {
      AutoProvenance provenance(launcher.provenance);
      if (launcher.must_parallelism)
      {
        // Turn around and use a must epoch launcher
        MustEpochLauncher epoch_launcher(launcher.map_id, launcher.tag);
        epoch_launcher.add_index_task(launcher);
        epoch_launcher.provenance = launcher.provenance;
        FutureMap result = execute_must_epoch(epoch_launcher);
        return reduce_future_map(result, redop, deterministic,
                                 launcher.map_id, launcher.tag, provenance,
                                 launcher.initial_value);
      }
      AutoRuntimeCall call(this);
      if (launcher.launch_domain.exists() &&
          (launcher.launch_domain.get_volume() == 0))
      {
        REPORT_LEGION_WARNING(LEGION_WARNING_IGNORING_EMPTY_INDEX_TASK_LAUNCH,
          "Ignoring empty index task launch in task %s (ID %lld)",
                        get_task_name(), get_unique_id());

        if (!launcher.initial_value.is_empty())
          return launcher.initial_value;

        // Else return the reduction operation's identity value
        const ReductionOp *reduction_op = runtime->get_reduction(redop);
        FutureImpl *result = new FutureImpl(this, runtime, true/*register*/,
          runtime->get_available_distributed_id(), provenance);
        result->set_local(reduction_op->identity,
                          reduction_op->sizeof_rhs, false/*own*/);
        return Future(result);
      }
      IndexSpace launch_space = launcher.launch_space;
      if (!launch_space.exists())
        launch_space = find_index_launch_space(launcher.launch_domain,
                                               provenance);
      // Quick out for predicate false
      if (launcher.predicate == Predicate::FALSE_PRED)
        return predicate_index_task_reduce_false(launcher, launch_space,
                                                 redop, provenance);
      IndexTask *task = runtime->get_available_index_task();
      Future result = task->initialize_task(this, launcher, launch_space, 
                                            provenance, redop, deterministic,
                                            true /*track*/, outputs);
#ifdef DEBUG_LEGION
      log_task.debug("Registering new index space task with unique id "
                     "%lld and task %s (ID %lld) with high level runtime in "
                     "address space %d",
                     task->get_unique_id(), task->get_task_name(), 
                     task->get_unique_id(), runtime->address_space);
#endif
      execute_task_launch(task, true/*index*/, current_trace, provenance,
                          launcher.silence_warnings, launcher.enable_inlining);
      return result;
    }

    //--------------------------------------------------------------------------
    Future InnerContext::reduce_future_map(const FutureMap &future_map,
                                        ReductionOpID redop, bool deterministic,
                                        MapperID mapper_id, MappingTagID tag,
                                        Provenance *prov,
                                        Future initial_value)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this); 
      if (future_map.impl == NULL)
      {
        const ReductionOp *reduction_op = runtime->get_reduction(redop);
        FutureImpl *result = new FutureImpl(this, runtime, true/*register*/,
          runtime->get_available_distributed_id(), prov);
        result->set_local(reduction_op->identity,
                          reduction_op->sizeof_rhs, false/*own*/);
        return Future(result);
      }
      AllReduceOp *all_reduce_op = runtime->get_available_all_reduce_op();
      Future result = all_reduce_op->initialize(this, future_map, redop, 
                                    deterministic, mapper_id, tag, prov,
                                    initial_value);
      add_to_dependence_queue(all_reduce_op);
      return result;
    }

    //--------------------------------------------------------------------------
    FutureMap InnerContext::construct_future_map(IndexSpace space,
                                const std::map<DomainPoint,UntypedBuffer> &data,
                                Provenance *provenance, bool collective,
                                ShardingID sid, bool implicit, 
                                bool internal, bool check_space)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      Domain domain;
      runtime->forest->find_domain(space, domain);
      if (data.size() != domain.get_volume())
        REPORT_LEGION_ERROR(ERROR_FUTURE_MAP_COUNT_MISMATCH,
          "The number of buffers passed into a future map construction (%zd) "
          "does not match the volume of the domain (%zd) for the future map "
          "in task %s (UID %lld)", data.size(), domain.get_volume(),
          get_task_name(), get_unique_id())
      const DistributedID did = runtime->get_available_distributed_id();
      IndexSpaceNode *launch_node = runtime->forest->get_node(space);
      FutureMapImpl *impl = new FutureMapImpl(this, runtime, launch_node, did,
          total_children_count++, ApEvent::NO_AP_EVENT, provenance);
      for (std::map<DomainPoint,UntypedBuffer>::const_iterator it =
            data.begin(); it != data.end(); it++)
      {
        if (!domain.contains(it->first))
          REPORT_LEGION_ERROR(ERROR_FUTURE_MAP_COUNT_MISMATCH,
            "Point passed into future map construction is not contained "
            "within the bounds of the domain in task %s (UID %lld)",
            get_task_name(), get_unique_id())
        const size_t future_size = it->second.get_size();
        FutureImpl *future = new FutureImpl(this, runtime, true/*register*/,
            runtime->get_available_distributed_id(), provenance);
        future->set_local(it->second.get_ptr(), future_size);
        impl->set_future(it->first, future);
      }
      return FutureMap(impl);
    }

    //--------------------------------------------------------------------------
    FutureMap InnerContext::construct_future_map(const Domain &domain,
                                const std::map<DomainPoint,UntypedBuffer> &data,
                                bool collective, ShardingID sid, bool implicit)
    //--------------------------------------------------------------------------
    {
      // this method is deprecated so don't care about provenance
      // make sure we don't do any control replication checks on the
      // space since we can't guarantee it is the same across the shards
      return construct_future_map(find_index_launch_space(domain, NULL/*prov*/),
          data, NULL/*deprecated so no provenance*/, collective, sid, 
          implicit, false/*internal*/, false/*check space*/);
    }

    //--------------------------------------------------------------------------
    FutureMap InnerContext::construct_future_map(IndexSpace space,
                                    const std::map<DomainPoint,Future> &futures,
                                    Provenance *provenance,
                                    bool internal, bool collective,
                                    ShardingID sid, bool implicit,
                                    bool check_space)
    //--------------------------------------------------------------------------
    {
      if (!internal)
      {
        AutoRuntimeCall call(this);
        return construct_future_map(space, futures, provenance,true/*internal*/,
                                    collective, sid, implicit, check_space);
      }
      CreationOp *creation_op = runtime->get_available_creation_op();
      creation_op->initialize_map(this, provenance, futures);
      const DistributedID did = runtime->get_available_distributed_id();
      IndexSpaceNode *launch_node = runtime->forest->get_node(space);
      if (futures.size() != launch_node->get_volume())
        REPORT_LEGION_ERROR(ERROR_FUTURE_MAP_COUNT_MISMATCH,
            "The number of futures passed into a future map construction (%zd) "
            "does not match the volume of the domain (%zd) for the future map "
            "in task %s (UID %lld)", futures.size(), launch_node->get_volume(),
            get_task_name(), get_unique_id())
      FutureMapImpl *impl = new FutureMapImpl(this, creation_op, launch_node,
                            runtime, did, provenance);
      add_to_dependence_queue(creation_op);
      impl->set_all_futures(futures);
      return FutureMap(impl);
    }

    //--------------------------------------------------------------------------
    FutureMap InnerContext::construct_future_map(const Domain &domain,
                                 const std::map<DomainPoint,Future> &futures,
                                 bool internal, bool collective,
                                 ShardingID sid, bool implicit) 
    //--------------------------------------------------------------------------
    {
      // Make sure we don't do any control replication checks on the 
      // space here since it might not be the same across the shards
      return construct_future_map(find_index_launch_space(domain, NULL),
              futures, NULL/*deprecated so no provenance*/, true/*internal*/,
              collective, sid, implicit, false/*check space*/);
    }

    //--------------------------------------------------------------------------
    FutureMap InnerContext::transform_future_map(const FutureMap &fm,
       IndexSpace new_domain, TransformFutureMapImpl::PointTransformFnptr fnptr,
       Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (fm.impl == NULL)
        return fm;
      IndexSpaceNode *new_node = runtime->forest->get_node(new_domain);
      return FutureMap(
          new TransformFutureMapImpl(fm.impl, new_node, fnptr, provenance));
    }

    //--------------------------------------------------------------------------
    FutureMap InnerContext::transform_future_map(const FutureMap &fm,
           IndexSpace new_domain, PointTransformFunctor *functor,
           bool own_func, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (fm.impl == NULL)
        return fm;
      IndexSpaceNode *new_node = runtime->forest->get_node(new_domain);
      return FutureMap(new TransformFutureMapImpl(fm.impl, new_node, 
                                      functor, own_func, provenance));
    }

    //--------------------------------------------------------------------------
    PhysicalRegion InnerContext::map_region(const InlineLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (IS_NO_ACCESS(launcher.requirement))
        return PhysicalRegion();
      AutoProvenance provenance(launcher.provenance);
      MapOp *map_op = runtime->get_available_map_op();
      PhysicalRegion result = map_op->initialize(this, launcher, provenance);
#ifdef DEBUG_LEGION
      log_run.debug("Registering a map operation for region "
                    "(%x,%x,%x) in task %s (ID %lld)",
                    launcher.requirement.region.index_space.id, 
                    launcher.requirement.region.field_space.id, 
                    launcher.requirement.region.tree_id, 
                    get_task_name(), get_unique_id());
#endif
      if (current_trace != NULL)
        REPORT_LEGION_ERROR(ERROR_ATTEMPTED_INLINE_MAPPING_REGION,
                      "Attempted an inline mapping of region "
                      "(%x,%x,%x) inside of trace %d of parent task %s "
                      "(ID %lld). It is illegal to perform inline mapping "
                      "operations inside of traces.",
                      launcher.requirement.region.index_space.id, 
                      launcher.requirement.region.field_space.id, 
                      launcher.requirement.region.tree_id, 
                      current_trace->tid, get_task_name(), get_unique_id())
      bool parent_conflict = false, inline_conflict = false;  
      const int index = 
        has_conflicting_regions(map_op, parent_conflict, inline_conflict);
      if (parent_conflict)
        REPORT_LEGION_ERROR(ERROR_ATTEMPTED_INLINE_MAPPING_REGION,
                      "Attempted an inline mapping of region "
                      "(%x,%x,%x) that conflicts with mapped region " 
                      "(%x,%x,%x) at index %d of parent task %s "
                      "(ID %lld) that would ultimately result in "
                      "deadlock. Instead you receive this error message.",
                      launcher.requirement.region.index_space.id,
                      launcher.requirement.region.field_space.id,
                      launcher.requirement.region.tree_id,
                      regions[index].region.index_space.id,
                      regions[index].region.field_space.id,
                      regions[index].region.tree_id,
                      index, get_task_name(), get_unique_id())
      if (inline_conflict)
        REPORT_LEGION_ERROR(ERROR_ATTEMPTED_INLINE_MAPPING_REGION,
                      "Attempted an inline mapping of region (%x,%x,%x) "
                      "that conflicts with previous inline mapping in "
                      "task %s (ID %lld) that would ultimately result in "
                      "deadlock.  Instead you receive this error message.",
                      launcher.requirement.region.index_space.id,
                      launcher.requirement.region.field_space.id,
                      launcher.requirement.region.tree_id,
                      get_task_name(), get_unique_id())
      register_inline_mapped_region(result);
      add_to_dependence_queue(map_op);
      return result;
    }

    //--------------------------------------------------------------------------
    ApEvent InnerContext::remap_region(const PhysicalRegion &region,
                                       Provenance *provenance, bool internal)
    //--------------------------------------------------------------------------
    {
      if (!internal)
      {
        AutoRuntimeCall call(this);
        return remap_region(region, provenance, true/*internal*/);
      }
      // Check to see if the region is already mapped,
      // if it is then we are done
      if (region.is_mapped())
        return ApEvent::NO_AP_EVENT;
      if (current_trace != NULL)
      {
        const RegionRequirement &req = region.impl->get_requirement();
        REPORT_LEGION_ERROR(ERROR_ATTEMPTED_INLINE_MAPPING_REGION,
                      "Attempted an inline mapping of region "
                      "(%x,%x,%x) inside of trace %d of parent task %s "
                      "(ID %lld). It is illegal to perform inline mapping "
                      "operations inside of traces.", req.region.index_space.id,
                      req.region.field_space.id, req.region.tree_id, 
                      current_trace->tid, get_task_name(), get_unique_id())
      }
      MapOp *map_op = runtime->get_available_map_op();
      map_op->initialize(this, region, provenance);
      register_inline_mapped_region(region);
      const ApEvent result = map_op->get_completion_event();
      add_to_dependence_queue(map_op);
      return result;
    }

    //--------------------------------------------------------------------------
    void InnerContext::unmap_region(PhysicalRegion region)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (!region.is_mapped())
        return;
      region.impl->unmap_region();
      unregister_inline_mapped_region(region);
    }

    //--------------------------------------------------------------------------
    void InnerContext::unmap_all_regions(bool external)
    //--------------------------------------------------------------------------
    {
      if (external)
      {
        AutoRuntimeCall call(this);
        unmap_all_regions(false);
        return;
      }
      for (std::vector<PhysicalRegion>::const_iterator it = 
            physical_regions.begin(); it != physical_regions.end(); it++)
      {
        if (it->is_mapped())
          it->impl->unmap_region();
      }
      // Also unmap any of our inline mapped physical regions
      AutoLock i_lock(inline_lock);
      for (LegionList<PhysicalRegion,TASK_INLINE_REGION_ALLOC>::const_iterator
            it = inline_regions.begin(); it != inline_regions.end(); it++)
      {
        if (it->is_mapped())
          it->impl->unmap_region();
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::fill_fields(const FillLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (launcher.fields.empty())
      {
        REPORT_LEGION_WARNING(LEGION_WARNING_EMPTY_FILL_FIELDS,
            "Ignoring fill request with no fields in task %s (UID %lld)",
            get_task_name(), get_unique_id())
        return;
      }
      FillOp *fill_op = runtime->get_available_fill_op();
      AutoProvenance provenance(launcher.provenance);
      fill_op->initialize(this, launcher, provenance);
#ifdef DEBUG_LEGION
      log_run.debug("Registering a fill operation in task %s (ID %lld)",
                     get_task_name(), get_unique_id());
#endif
      // Check to see if we need to do any unmappings and remappings
      // before we can issue this copy operation
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(fill_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "WARNING: Runtime is unmapping and remapping "
              "physical regions around fill_fields call in task %s (UID %lld).",
              get_task_name(), get_unique_id());
        }
        // Unmap any regions which are conflicting
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(fill_op);
      // Remap any regions which we unmapped
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    void InnerContext::fill_fields(const IndexFillLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (launcher.fields.empty())
      {
        REPORT_LEGION_WARNING(LEGION_WARNING_EMPTY_FILL_FIELDS,
            "Ignoring index fill request with no fields in task %s (UID %lld)",
            get_task_name(), get_unique_id())
        return;
      }
      if (launcher.launch_domain.exists() && 
          (launcher.launch_domain.get_volume() == 0))
      {
        REPORT_LEGION_WARNING(LEGION_WARNING_IGNORING_EMPTY_INDEX_SPACE_FILL,
          "Ignoring empty index space fill in task %s (ID %lld)",
                        get_task_name(), get_unique_id());
        return;
      }
      AutoProvenance provenance(launcher.provenance);
      IndexSpace launch_space = launcher.launch_space;
      if (!launch_space.exists())
        launch_space = find_index_launch_space(launcher.launch_domain,
                                               provenance);
      IndexFillOp *fill_op = runtime->get_available_index_fill_op();
      fill_op->initialize(this, launcher, launch_space, provenance); 
#ifdef DEBUG_LEGION
      log_run.debug("Registering an index fill operation in task %s (ID %lld)",
                     get_task_name(), get_unique_id());
#endif
      // Check to see if we need to do any unmappings and remappings
      // before we can issue this copy operation
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(fill_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around fill_fields call in task %s (UID %lld).",
              get_task_name(), get_unique_id());
        }
        // Unmap any regions which are conflicting
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(fill_op);
      // Remap any regions which we unmapped
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    void InnerContext::discard_fields(const DiscardLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (launcher.fields.empty())
      {
        REPORT_LEGION_WARNING(LEGION_WARNING_EMPTY_FILL_FIELDS,
            "Ignoring discard request with no fields in task %s (UID %lld)",
            get_task_name(), get_unique_id())
        return;
      }
      AutoProvenance provenance(launcher.provenance);
      DiscardOp *discard_op = runtime->get_available_discard_op();
      discard_op->initialize(this, launcher, provenance);
      // We still unamp conflicting regions for discard, but we wil never
      // remap them afterwards since this is invalidating the data
      if (!runtime->unsafe_launch)
      {
        std::vector<PhysicalRegion> unmapped_regions;
        find_conflicting_regions(discard_op, unmapped_regions);
        if (!unmapped_regions.empty())
        {
          if (runtime->runtime_warnings && !launcher.silence_warnings)
          {
            REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
              "Runtime is unmapping and remapping "
                "physical regions around discard_fields call in "
                "task %s (UID %lld).", get_task_name(), get_unique_id());
          }
          // Unmap any regions which are conflicting
          for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
            unmapped_regions[idx].impl->unmap_region();
        }
      }
      add_to_dependence_queue(discard_op);
      // Do not remap the previously mapped regions, they are uninitialized
    }

    //--------------------------------------------------------------------------
    void InnerContext::issue_copy(const CopyLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      CopyOp *copy_op = runtime->get_available_copy_op();
      copy_op->initialize(this, launcher, provenance);
#ifdef DEBUG_LEGION
      log_run.debug("Registering a copy operation in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      // Check to see if we need to do any unmappings and remappings
      // before we can issue this copy operation
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(copy_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around issue_copy_operation call in "
              "task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        // Unmap any regions which are conflicting
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(copy_op);
      // Remap any regions which we unmapped
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    void InnerContext::issue_copy(const IndexCopyLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (launcher.launch_domain.exists() &&
          (launcher.launch_domain.get_volume() == 0))
      {
        REPORT_LEGION_WARNING(LEGION_WARNING_IGNORING_EMPTY_INDEX_SPACE_COPY,
          "Ignoring empty index space copy in task %s "
                        "(ID %lld)", get_task_name(), get_unique_id());
        return;
      }
      AutoProvenance provenance(launcher.provenance);
      IndexSpace launch_space = launcher.launch_space;
      if (!launch_space.exists())
        launch_space = find_index_launch_space(launcher.launch_domain,
                                               provenance);
      IndexCopyOp *copy_op = runtime->get_available_index_copy_op();
      copy_op->initialize(this, launcher, launch_space, provenance);
#ifdef DEBUG_LEGION
      log_run.debug("Registering an index copy operation in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      // Check to see if we need to do any unmappings and remappings
      // before we can issue this copy operation
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(copy_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around issue_copy_operation call in "
              "task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        // Unmap any regions which are conflicting
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(copy_op);
      // Remap any regions which we unmapped
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    void InnerContext::issue_acquire(const AcquireLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      AcquireOp *acquire_op = runtime->get_available_acquire_op();
      acquire_op->initialize(this, launcher, provenance);
#ifdef DEBUG_LEGION
      log_run.debug("Issuing an acquire operation in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      // Check to see if we need to do any unmappings and remappings
      // before we can issue this acquire operation.
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(acquire_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around issue_acquire call in "
              "task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the acquire operation
      add_to_dependence_queue(acquire_op);
      // Remap any regions which we unmapped
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    void InnerContext::issue_release(const ReleaseLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      ReleaseOp *release_op = runtime->get_available_release_op();
      release_op->initialize(this, launcher, provenance);
#ifdef DEBUG_LEGION
      log_run.debug("Issuing a release operation in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      // Check to see if we need to do any unmappings and remappings
      // before we can issue the release operation
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(release_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around issue_release call in "
              "task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the release operation
      add_to_dependence_queue(release_op);
      // Remap any regions which we unmapped
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    PhysicalRegion InnerContext::attach_resource(const AttachLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      AttachOp *attach_op = runtime->get_available_attach_op();
      PhysicalRegion result = attach_op->initialize(this, launcher, provenance);
      bool parent_conflict = false, inline_conflict = false;
      int index = has_conflicting_regions(attach_op, 
                                          parent_conflict, inline_conflict);
      if (parent_conflict)
        REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
                      "Attempted an external attach operation on region "
                      "(%x,%x,%x) that conflicts with mapped region " 
                      "(%x,%x,%x) at index %d of parent task %s (ID %lld) "
                      "that would ultimately result in deadlock. Instead you "
                      "receive this error message. Try unmapping the region "
                      "before invoking 'attach_external_resource'.",
                      launcher.handle.index_space.id, 
                      launcher.handle.field_space.id, 
                      launcher.handle.tree_id, 
                      regions[index].region.index_space.id,
                      regions[index].region.field_space.id,
                      regions[index].region.tree_id, index, 
                      get_task_name(), get_unique_id())
      if (inline_conflict)
        REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
                      "Attempted an external attach operation on region "
                      "(%x,%x,%x) that conflicts with previous inline "
                      "mapping in task %s (ID %lld) "
                      "that would ultimately result in deadlock. Instead you "
                      "receive this error message. Try unmapping the region "
                      "before invoking 'attach_external_resource'.",
                      launcher.handle.index_space.id, 
                      launcher.handle.field_space.id, launcher.handle.tree_id,
                      get_task_name(), get_unique_id())
      // Add this region to the list of inline mapped regions if it is mapped
      if (result.is_mapped())
        register_inline_mapped_region(result);
      add_to_dependence_queue(attach_op);
      return result;
    }

    //--------------------------------------------------------------------------
    ExternalResources InnerContext::attach_resources(
                                            const IndexAttachLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (launcher.handles.empty())
        return ExternalResources();
      AutoProvenance provenance(launcher.provenance);
      // This is not control replicated so no need to deduplicate anything
      std::vector<unsigned> indexes(launcher.handles.size());
      for (unsigned idx = 0; idx < indexes.size(); idx++)
        indexes[idx] = idx;
      // Compute the upper bound partition node from this launcher
      RegionTreeNode *node = compute_index_attach_upper_bound(launcher,indexes);
      IndexSpaceNode *launch_space = runtime->forest->get_node(
       find_index_launch_space(Domain(Point<1>(0),
           Point<1>(indexes.size()-1)), provenance));
      IndexAttachOp *attach_op = runtime->get_available_index_attach_op();
      ExternalResources result = attach_op->initialize(this, node, launch_space,
                            launcher, indexes, provenance, false/*replicated*/);
      const RegionRequirement &req = attach_op->get_requirement();
      bool parent_conflict = false, inline_conflict = false;
      int index = has_conflicting_internal(req,parent_conflict,inline_conflict);
      if (parent_conflict)
      {
        if (req.handle_type == LEGION_PARTITION_PROJECTION)
          REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
                        "Attempted an index attach operation with upper bound "
                        "partition (%x,%x,%x) that conflicts with mapped region"
                        " (%x,%x,%x) at index %d of parent task %s (ID %lld) "
                        "that would ultimately result in deadlock. Instead you "
                        "receive this error message. Try unmapping the region "
                        "before invoking 'attach_external_resources'.",
                        req.partition.index_partition.id, 
                        req.partition.field_space.id, 
                        req.partition.tree_id, 
                        regions[index].region.index_space.id,
                        regions[index].region.field_space.id,
                        regions[index].region.tree_id, index, 
                        get_task_name(), get_unique_id())
        else
          REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
                        "Attempted an index attach operation with upper bound "
                        "region (%x,%x,%x) that conflicts with mapped region "
                        "(%x,%x,%x) at index %d of parent task %s (ID %lld) "
                        "that would ultimately result in deadlock. Instead you "
                        "receive this error message. Try unmapping the region "
                        "before invoking 'attach_external_resources'.",
                        req.region.index_space.id, 
                        req.region.field_space.id, 
                        req.region.tree_id, 
                        regions[index].region.index_space.id,
                        regions[index].region.field_space.id,
                        regions[index].region.tree_id, index, 
                        get_task_name(), get_unique_id())
      }
      if (inline_conflict)
      {
        if (req.handle_type == LEGION_PARTITION_PROJECTION)
          REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
                        "Attempted an index attach operation with upper bound "
                        "partition (%x,%x,%x) that conflicts with previous "
                        "inline mapping in task %s (ID %lld) "
                        "that would ultimately result in deadlock. Instead you "
                        "receive this error message. Try unmapping the region "
                        "before invoking 'attach_external_resources'.",
                        req.partition.index_partition.id, 
                        req.partition.field_space.id, req.partition.tree_id,
                        get_task_name(), get_unique_id())
        else
          REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
                        "Attempted an index attach operation with upper bound "
                        "region (%x,%x,%x) that conflicts with previous inline "
                        "mapping in task %s (ID %lld) "
                        "that would ultimately result in deadlock. Instead you "
                        "receive this error message. Try unmapping the region "
                        "before invoking 'attach_external_resources'.",
                        req.region.index_space.id, 
                        req.region.field_space.id, req.region.tree_id,
                        get_task_name(), get_unique_id())
      }
      add_to_dependence_queue(attach_op);
      return result;
    }

    //--------------------------------------------------------------------------
    RegionTreeNode* InnerContext::compute_index_attach_upper_bound(
      const IndexAttachLauncher &launcher, const std::vector<unsigned> &indexes)
    //--------------------------------------------------------------------------
    {
      std::vector<RegionTreeNode*> previous_nodes(indexes.size());
      std::vector<unsigned> depths(indexes.size());
      unsigned max_depth = 0;
      for (unsigned idx = 0; idx < indexes.size(); idx++)
      {
        const unsigned index = indexes[idx];
        LogicalRegion handle = launcher.handles[index];
        if (handle.get_tree_id() != launcher.parent.get_tree_id())
          REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
              "Handle (%d,%d,%d) of index attach operation in parent task %s "
              "(UID %lld) does not come from the same region tree as the "
              "parent region (%d,%d,%d). All regions for an index space "
              "attach must be from the same tree.", handle.index_space.id,
              handle.field_space.id, handle.tree_id, get_task_name(),
              get_unique_id(), launcher.parent.index_space.id,
              launcher.parent.field_space.id, launcher.parent.tree_id)
        previous_nodes[idx] = runtime->forest->get_node(handle);
        depths[idx] = previous_nodes[idx]->get_depth();
        if (max_depth < depths[idx])
          max_depth = depths[idx];
      }
      // Walk all the nodes up from the bottom until they arrive at a 
      // common ancestor, along the way check to make sure that any nodes
      // that arrive at a common join point from two different paths do
      // so at a disjoint partition
      std::vector<RegionTreeNode*> next_nodes(indexes.size());
      while (max_depth > 0)
      {
        std::map<RegionTreeNode*,std::vector<unsigned> > next_to_previous;
        bool all_same = true;
        for (unsigned idx = 0; idx < indexes.size(); idx++)
        {
          if (depths[idx] == max_depth)
          {
            depths[idx]--;
            next_nodes[idx] = previous_nodes[idx]->get_parent();
            next_to_previous[next_nodes[idx]].push_back(idx);
            if (all_same && (idx > 0) && (next_nodes[idx-1] != next_nodes[idx]))
              all_same = false;
          }
          else
          {
            next_nodes[idx] = previous_nodes[idx];
            all_same = false;
          }
        }
        // check to see if all the next to previous cases play by the rules
        for (std::map<RegionTreeNode*,std::vector<unsigned> >::const_iterator
              it = next_to_previous.begin(); it != next_to_previous.end(); it++)
        {
          if (it->second.size() == 1)
            continue;
          // Can skip any disjoint partitions since it doesn't matter where
          // their children came from
          if (!it->first->is_region() &&
              it->first->as_partition_node()->row_source->is_disjoint())
            continue;
          // Otherwise check to see that they all came from the same child
          // If they didn't, then we can't prove tree disjointness
          RegionTreeNode *previous = previous_nodes[it->second.front()];
          for (unsigned idx = 1; idx < it->second.size(); idx++)
          {
            if (previous == previous_nodes[it->second[idx]])
              continue;
            const LogicalRegion h1 = launcher.handles[it->second.front()];
            const LogicalRegion h2 = launcher.handles[it->second[idx]];
            REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
              "Logical region handle (%d,%d,%d) from index %d of index attach "
              "operation in parent task %s (UID %lld) is not region-tree "
              "disjoint with logical region handle (%d,%d,%d) from index %d. "
              "All regions in index space attach operations must be "
              "region-tree disjoint.", h1.index_space.id,
              h1.field_space.id, h1.tree_id, it->second.front(),
              get_task_name(), get_unique_id(), h2.index_space.id,
              h2.field_space.id, h2.tree_id, it->second[idx])
          }
        }
        previous_nodes.swap(next_nodes);
        if (all_same)
          break;
        max_depth--;
      }
      // At this point all the previous nodes should be the same
      return previous_nodes.back();
    }

    //--------------------------------------------------------------------------
    ProjectionID InnerContext::compute_index_attach_projection(
                                  IndexTreeNode *upper_bound, IndexAttachOp *op,
                                  unsigned local_start, size_t local_size, 
                                  std::vector<IndexSpace> &spaces,
                                  const bool can_use_identity)
    //--------------------------------------------------------------------------
    {
      std::map<IndexTreeNode*,std::vector<AttachProjectionFunctor*> >::iterator
        finder = attach_functions.find(upper_bound);
      if (finder != attach_functions.end())
      {
        for (std::vector<AttachProjectionFunctor*>::const_iterator it =
              finder->second.begin(); it != finder->second.end(); it++)
        {
          if ((*it)->handles.size() != spaces.size())
            continue;
          bool equal = true;
          for (unsigned idx = 0; idx < spaces.size(); idx++)
          {
            if ((*it)->handles[idx] == spaces[idx])
              continue;
            equal = false;
            break;
          }
          if (equal)
            return (*it)->pid;
        }
      }
      else // instantiate the entry in the map
        finder = attach_functions.insert(std::make_pair(upper_bound,
              std::vector<AttachProjectionFunctor*>())).first;
      // If the upper bound is a partition, do a quick check to see if
      // all the spaces are immediate children of the upper bound, if
      // so then we can use projection function 0
      if (!upper_bound->is_index_space_node() && can_use_identity)
      {
        bool all_children = true;
        IndexPartNode *parent = upper_bound->as_index_part_node();
        for (unsigned idx = 0; idx < local_size; idx++)
        {
          IndexSpaceNode *child = 
            runtime->forest->get_node(spaces[local_start+idx]);
          if (child->parent == parent)
            continue;
          all_children = false;
          break;
        }
        // Bounce this off the operation in case we are control replicated
        all_children = op->are_all_direct_children(all_children);
        if (all_children)
        {
          // We can use the identity projection in this case
          // so just make it, but no need to register it with the runtime
          finder->second.push_back(new AttachProjectionFunctor(runtime,
                                      0/*identity*/, std::move(spaces)));
          return 0;
        }
      }
      // If we get here then we need to make it
      // Generate a fresh dynamic ID and store it
      const ProjectionID result =
        runtime->generate_dynamic_projection_id(false/*check context*/);
      AttachProjectionFunctor *functor =
        new AttachProjectionFunctor(runtime, result, std::move(spaces));
      runtime->register_projection_functor(result, functor, false/*check*/,
                                           true/*silence warnings*/);
      finder->second.push_back(functor);
      if (runtime->legion_spy_enabled)
        LegionSpy::log_projection_function(result, functor->get_depth(),
                                           functor->is_invertible());
      return result;
    }

    //--------------------------------------------------------------------------
    InnerContext::AttachProjectionFunctor::AttachProjectionFunctor(Runtime *rt,
                               ProjectionID p, std::vector<IndexSpace> &&spaces)
      : ProjectionFunctor(rt->external), handles(spaces), pid(p)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalRegion InnerContext::AttachProjectionFunctor::project(
      LogicalRegion upper_bound, const DomainPoint &point, const Domain &launch)
    //--------------------------------------------------------------------------
    {
      const unsigned offset = compute_offset(point, launch);
#ifdef DEBUG_LEGION
      assert(offset < handles.size());
#endif
      return runtime->get_logical_subregion_by_tree(handles[offset],
          upper_bound.get_field_space(), upper_bound.get_tree_id());
    }

    //--------------------------------------------------------------------------
    LogicalRegion InnerContext::AttachProjectionFunctor::project(
         LogicalPartition upper, const DomainPoint &point, const Domain &launch)
    //--------------------------------------------------------------------------
    {
      const unsigned offset = compute_offset(point, launch);
#ifdef DEBUG_LEGION
      assert(offset < handles.size());
#endif
      return runtime->get_logical_subregion_by_tree(handles[offset],
                    upper.get_field_space(), upper.get_tree_id());
    }

    //--------------------------------------------------------------------------
    /*static*/ unsigned InnerContext::AttachProjectionFunctor::compute_offset(
                                 const DomainPoint &point, const Domain &launch)
    //--------------------------------------------------------------------------
    {
      if (point.get_dim() == 2)
      {
        const Point<2> p = point;
        // Control replication case, see if we're compacted or not
        if (launch.dense() && (launch.lo()[0] == 0))
        {
          // Dense means that all the shards had the same number of points
          // so we can compute where our offset is based on that
          const Rect<2> bounds = launch;
          return p[0] * (bounds.hi[1] - bounds.lo[1] + 1) + p[1];
        }
        else
        {
          // We computed prefix sums for the non-dense case so whatever
          // the second dimension is is the one that says which space we are
          return p[1];
        }
      }
      else
      {
        const Point<1> p = point;
        return p[0];
      }
    }

    //--------------------------------------------------------------------------
    Future InnerContext::detach_resource(PhysicalRegion region,
                 const bool flush, const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      // Unmap the region here so that it is safe for re-use
      if (region.impl->is_mapped())
      {
        region.impl->unmap_region();
        // Remove this region from the list of inline regions if it is mapped
        unregister_inline_mapped_region(region);
      }
      DetachOp *op = runtime->get_available_detach_op();
      Future result =
        op->initialize_detach(this, region, flush, unordered, provenance);
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered detach operation performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
      return result;
    }

    //--------------------------------------------------------------------------
    Future InnerContext::detach_resources(ExternalResources resources,
                 const bool flush, const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (resources.impl == NULL)
        return Future();
      IndexDetachOp *op = runtime->get_available_index_detach_op();
      Future result =
        resources.impl->detach(this, op, flush, unordered, provenance);
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered index detach operation performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
      return result;
    }

    //--------------------------------------------------------------------------
    void InnerContext::progress_unordered_operations(bool end_task)
    //--------------------------------------------------------------------------
    {
      AutoLock d_lock(dependence_lock);
      if (end_task)
      {
        // This is the end of this parent task so mark that we're done
#ifdef DEBUG_LEGION
        assert(!finished_execution);
        assert(current_trace == NULL);
#endif
        finished_execution = true;
      }
      // No progress can occur inside of a trace
      else if (current_trace != NULL)
        return;
      insert_unordered_ops(d_lock);
    }

    //--------------------------------------------------------------------------
    FutureMap InnerContext::execute_must_epoch(
                                              const MustEpochLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      MustEpochOp *epoch_op = runtime->get_available_epoch_op();
#ifdef DEBUG_LEGION
      log_run.debug("Executing a must epoch in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      AutoProvenance provenance(launcher.provenance);
      FutureMap result = epoch_op->initialize(this, launcher, provenance);
      // Now find all the parent task regions we need to invalidate
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        epoch_op->find_conflicted_regions(unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around issue_release call in "
              "task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Now we can issue the must epoch
      add_to_dependence_queue(epoch_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      return result;
    }

    //--------------------------------------------------------------------------
    Future InnerContext::issue_timing_measurement(const TimingLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
#ifdef DEBUG_LEGION
      log_run.debug("Issuing a timing measurement in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      AutoProvenance provenance(launcher.provenance);
      TimingOp *timing_op = runtime->get_available_timing_op();
      Future result = timing_op->initialize(this, launcher, provenance);
      add_to_dependence_queue(timing_op);
      return result;
    }

    //--------------------------------------------------------------------------
    Future InnerContext::select_tunable_value(const TunableLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
#ifdef DEBUG_LEGION
      log_run.debug("Issuing a tunable request in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      AutoProvenance provenance(launcher.provenance);
      TunableOp *tunable_op = runtime->get_available_tunable_op();
      Future result = tunable_op->initialize(this, launcher, provenance);
      add_to_dependence_queue(tunable_op);
      return result;
    }

    //--------------------------------------------------------------------------
    Future InnerContext::issue_mapping_fence(Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      FenceOp *fence_op = runtime->get_available_fence_op();
#ifdef DEBUG_LEGION
      log_run.debug("Issuing a mapping fence in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      Future f = fence_op->initialize(this, FenceOp::MAPPING_FENCE,
                                      true/*return future*/, provenance);
      add_to_dependence_queue(fence_op);
      return f;
    }

    //--------------------------------------------------------------------------
    Future InnerContext::issue_execution_fence(Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      FenceOp *fence_op = runtime->get_available_fence_op();
#ifdef DEBUG_LEGION
      log_run.debug("Issuing an execution fence in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      Future f = fence_op->initialize(this, FenceOp::EXECUTION_FENCE,
                                      true/*return future*/, provenance);
      add_to_dependence_queue(fence_op);
      return f; 
    }

    //--------------------------------------------------------------------------
    void InnerContext::complete_frame(Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      FrameOp *frame_op = runtime->get_available_frame_op();
#ifdef DEBUG_LEGION
      log_run.debug("Issuing a frame in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      frame_op->initialize(this, provenance);
      add_to_dependence_queue(frame_op);
    }

    //--------------------------------------------------------------------------
    Predicate InnerContext::create_predicate(const Future &f,
                                             Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (f.impl == NULL)
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_PREDICATE_CREATION,
          "Illegal predicate creation performed on "
                      "empty future inside of task %s (ID %lld).",
                      get_task_name(), get_unique_id())
      FuturePredOp *pred_op = runtime->get_available_future_pred_op();
      // Hold a reference before initialization
      Predicate result = pred_op->initialize(this, f, provenance);
      add_to_dependence_queue(pred_op);
      return result;
    }

    //--------------------------------------------------------------------------
    Predicate InnerContext::predicate_not(const Predicate &p,
                                          Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (p == Predicate::TRUE_PRED)
        return Predicate::FALSE_PRED;
      if (p == Predicate::FALSE_PRED)
        return Predicate::TRUE_PRED;
      NotPredOp *pred_op = runtime->get_available_not_pred_op();
      // Hold a reference before initialization
      Predicate result = pred_op->initialize(this, p, provenance);
      add_to_dependence_queue(pred_op);
      return result;
    }

    //--------------------------------------------------------------------------
    Predicate InnerContext::create_predicate(const PredicateLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (launcher.predicates.empty())
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_PREDICATE_CREATION,
          "Illegal predicate creation performed on a "
                      "set of empty previous predicates in task %s (ID %lld).",
                      get_task_name(), get_unique_id())
      else if (launcher.predicates.size() == 1)
        return launcher.predicates[0];
      AutoProvenance provenance(launcher.provenance);
      if (launcher.and_op)
      {
        // Check for short circuit cases
        std::vector<Predicate> actual_predicates;
        for (std::vector<Predicate>::const_iterator it = 
              launcher.predicates.begin(); it != 
              launcher.predicates.end(); it++)
        {
          if ((*it) == Predicate::FALSE_PRED)
            return Predicate::FALSE_PRED;
          else if ((*it) == Predicate::TRUE_PRED)
            continue;
          actual_predicates.push_back(*it);
        }
        if (actual_predicates.empty()) // they were all true
          return Predicate::TRUE_PRED;
        else if (actual_predicates.size() == 1)
          return actual_predicates[0];
        AndPredOp *pred_op = runtime->get_available_and_pred_op();
        // Hold a reference before initialization
        Predicate result = 
          pred_op->initialize(this, actual_predicates, provenance);
        add_to_dependence_queue(pred_op);
        return result;
      }
      else
      {
        // Check for short circuit cases
        std::vector<Predicate> actual_predicates;
        for (std::vector<Predicate>::const_iterator it = 
              launcher.predicates.begin(); it != 
              launcher.predicates.end(); it++)
        {
          if ((*it) == Predicate::TRUE_PRED)
            return Predicate::TRUE_PRED;
          else if ((*it) == Predicate::FALSE_PRED)
            continue;
          actual_predicates.push_back(*it);
        }
        if (actual_predicates.empty()) // they were all false
          return Predicate::FALSE_PRED;
        else if (actual_predicates.size() == 1)
          return actual_predicates[0];
        OrPredOp *pred_op = runtime->get_available_or_pred_op();
        // Hold a reference before initialization
        Predicate result =
          pred_op->initialize(this, actual_predicates, provenance);
        add_to_dependence_queue(pred_op);
        return result;
      }
    }

    //--------------------------------------------------------------------------
    Future InnerContext::get_predicate_future(const Predicate &p,
                                              Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this); 
      if (p == Predicate::TRUE_PRED)
      {
        Future result(new FutureImpl(this, runtime, true/*register*/,
              runtime->get_available_distributed_id(), provenance));
        const bool value = true;
        result.impl->set_local(&value, sizeof(value));
        return result;
      }
      else if (p == Predicate::FALSE_PRED)
      {
        Future result(new FutureImpl(this, runtime, true/*register*/,
              runtime->get_available_distributed_id(), provenance));
        const bool value = false;
        result.impl->set_local(&value, sizeof(value));
        return result;
      }
      else
      {
#ifdef DEBUG_LEGION
        assert(p.impl != NULL);
#endif
        FuturePredOp *pred_op = runtime->get_available_future_pred_op();
        // Hold a reference before initialization
        Future result = pred_op->initialize(this, p, provenance);
        add_to_dependence_queue(pred_op);
        return result;
      }
    }

    //--------------------------------------------------------------------------
    PredicateImpl* InnerContext::create_predicate_impl(Operation *op)
    //--------------------------------------------------------------------------
    {
      return new PredicateImpl(op);
    }

    //--------------------------------------------------------------------------
    void InnerContext::perform_window_wait(void)
    //--------------------------------------------------------------------------
    {
      RtEvent wait_event;
      // Take the context lock in exclusive mode
      {
        AutoLock child_lock(child_op_lock);
        // We already hold our lock from the callsite above
        // Outstanding children count has already been incremented for the
        // operation being launched so decrement it in case we wait and then
        // re-increment it when we wake up again
        const int outstanding_count = outstanding_children_count.fetch_sub(1);
        // We already decided to wait, so we need to wait for any hysteresis
        // to play a role here
        if (outstanding_count >
            int((100 - context_configuration.hysteresis_percentage) *
                context_configuration.max_window_size / 100))
        {
#ifdef DEBUG_LEGION
          assert(!valid_wait_event);
#endif
          window_wait = Runtime::create_rt_user_event();
          valid_wait_event = true;
          wait_event = window_wait;
        }
      }
      wait_event.wait();
      // Re-increment the count once we are awake again
      outstanding_children_count.fetch_add(1);
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_prepipeline_queue(Operation *op)
    //--------------------------------------------------------------------------
    {
      bool issue_task = false;
      const GenerationID gen = op->get_generation();
      {
        AutoLock p_lock(prepipeline_lock);
        issue_task = prepipeline_queue.empty();
        prepipeline_queue.push_back(std::pair<Operation*,GenerationID>(op,gen));
      }
      if (issue_task)
      {
        add_base_resource_ref(META_TASK_REF);
        PrepipelineArgs args(op, this);
        runtime->issue_runtime_meta_task(args, LG_THROUGHPUT_WORK_PRIORITY);
      }
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_prepipeline_stage(void)
    //--------------------------------------------------------------------------
    {
      std::vector<std::pair<Operation*,GenerationID> > to_perform;
      to_perform.reserve(context_configuration.meta_task_vector_width);
      Operation *launch_next_op = NULL;
      {
        AutoLock p_lock(prepipeline_lock);
        for (unsigned idx = 0; idx < 
              context_configuration.meta_task_vector_width; idx++)
        {
          if (prepipeline_queue.empty())
            break;
          to_perform.push_back(prepipeline_queue.front());
          prepipeline_queue.pop_front();
        }
        if (!prepipeline_queue.empty())
          launch_next_op = prepipeline_queue.back().first;
      }
      // Perform our prepipeline tasks
      for (std::vector<std::pair<Operation*,GenerationID> >::const_iterator it =
            to_perform.begin(); it != to_perform.end(); it++)
        it->first->execute_prepipeline_stage(it->second, false/*need wait*/);
      if (launch_next_op != NULL)
      {
        // This could maybe give a bad op ID for profiling, but it
        // will not impact the correctness of the code
        PrepipelineArgs args(launch_next_op, this);
        runtime->issue_runtime_meta_task(args, LG_THROUGHPUT_WORK_PRIORITY);
        // Reference keeps flowing with the continuation
        return false;
      }
      else
        return true;
    }

    //--------------------------------------------------------------------------
    bool InnerContext::add_to_dependence_queue(Operation *op, bool unordered, 
                                               bool outermost)
    //--------------------------------------------------------------------------
    {
      LgPriority priority = LG_THROUGHPUT_WORK_PRIORITY;
      // If this is tracking, add it to our data structure first
      if (op->is_tracking_parent())
      {
        AutoLock child_lock(child_op_lock);
#ifdef DEBUG_LEGION
        assert(reorder_buffer.empty() ||
            (reorder_buffer.back().operation_index < op->get_ctx_index()));
#endif
        // Pad the reorder buffer for missing entries if necessary
        while (!reorder_buffer.empty() &&
            ((reorder_buffer.back().operation_index+1) < op->get_ctx_index()))
          reorder_buffer.emplace_back(
              ReorderBufferEntry(reorder_buffer.back().operation_index+1));
        reorder_buffer.emplace_back(ReorderBufferEntry(op));
        executing_children_count++;
        // Bump our priority if the context is not active as it means
        // that the runtime is currently not ahead of execution
        if (!currently_active_context)
          priority = LG_THROUGHPUT_DEFERRED_PRIORITY;
      }
      // Launch the task to perform the prepipeline stage for the operation
      if (op->has_prepipeline_stage())
        add_to_prepipeline_queue(op);
      RtEvent precondition;
      ApEvent term_event;
      bool issue_task = false;
      // We disable program order execution when we are replaying a
      // physical trace since it might not be sound to block
      if (runtime->program_order_execution && !unordered && 
          outermost && !is_replaying_physical_trace())
        term_event = op->get_completion_event();
      {
        AutoLock d_lock(dependence_lock);
        if (unordered)
        {
          if (finished_execution)
            return false;
          // If this is unordered, stick it on the list of 
          // unordered ops to be added later and then we're done
          unordered_ops.push_back(op);
          return true;
        }
        if (dependence_queue.empty())
        {
          issue_task = true;
          precondition = dependence_precondition;
        }
        dependence_queue.push_back(op);
        // Insert any unordered operations now as long as we aren't
        // doing program order execution, if we're doing program order
        // execution then we'll do that after running this operation
        if (!term_event.exists())
          insert_unordered_ops(d_lock);
      }
      if (issue_task)
      {
        DependenceArgs args(op, this);
        runtime->issue_runtime_meta_task(args, priority, precondition); 
      }
      if (term_event.exists())
      {
        bool poisoned = false;
        term_event.wait_faultaware(poisoned);
        if (poisoned)
          raise_poison_exception();
        // Now do our insertion of any unordered operations
        AutoLock d_lock(dependence_lock);
        insert_unordered_ops(d_lock);
      }
      return true;
    }

    //--------------------------------------------------------------------------
    void InnerContext::process_dependence_stage(void)
    //--------------------------------------------------------------------------
    {
      std::vector<Operation*> to_perform;
      to_perform.reserve(context_configuration.meta_task_vector_width);
      Operation *launch_next_op = NULL;
      {
        AutoLock d_lock(dependence_lock);
        for (unsigned idx = 0; idx < 
              context_configuration.meta_task_vector_width; idx++)
        {
          if (dependence_queue.empty())
            break;
          to_perform.push_back(dependence_queue.front());
          dependence_queue.pop_front();
        }
        if (dependence_queue.empty())
          // Guard ourselves against tasks running after us
          dependence_precondition = 
            RtEvent(Processor::get_current_finish_event());
        else
          launch_next_op = dependence_queue.front();
      }
      // Perform our operations
      for (std::vector<Operation*>::const_iterator it = 
            to_perform.begin(); it != to_perform.end(); it++)
        (*it)->execute_dependence_analysis();
      // Then launch the next task if needed
      if (launch_next_op != NULL)
      {
        DependenceArgs args(launch_next_op, this);
        // Sample currently_active without the lock to try to get our priority
        runtime->issue_runtime_meta_task(args, !currently_active_context ? 
              LG_THROUGHPUT_DEFERRED_PRIORITY : LG_THROUGHPUT_WORK_PRIORITY);
      }
    }

    //--------------------------------------------------------------------------
    template<typename T, typename ARGS, bool HAS_BOUND>
    void InnerContext::add_to_queue(QueueEntry<T> entry, LocalLock &lock,
                  std::list<QueueEntry<T> > &queue, CompletionQueue &comp_queue)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(entry.ready.exists());
#endif
      bool issue_task = false;
      RtEvent precondition;
      {
        AutoLock l_lock(lock);
        // Issue a task if there isn't one running right now
        if (queue.empty())
        {
          issue_task = true;
          // Add a reference to the context the first time we defer this
          add_base_resource_ref(META_TASK_REF);
          // Make the queue the first time if necessary
          if (!comp_queue.exists())
            // We can put an upper bound on the number of operations as long
            // as we aren't using frames to runahead, if we're using frames
            // to run ahead then we can't know the maximum run ahead size
            comp_queue = CompletionQueue::create_completion_queue((HAS_BOUND &&
                (context_configuration.min_frames_to_schedule == 0)) ?
                  context_configuration.max_window_size : 0);
        }
        queue.push_back(entry);
        comp_queue.add_event(entry.ready);
        if (issue_task)
          precondition = RtEvent(comp_queue.get_nonempty_event());
      }
      if (issue_task)
      {
        ARGS args(entry.op, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_WORK_PRIORITY, precondition);
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_ready_queue(Operation *op, RtEvent ready)
    //--------------------------------------------------------------------------
    {
      add_to_queue<Operation*,TriggerReadyArgs,true/*has bounds*/>(
          QueueEntry<Operation*>(op, ready), ready_lock,
          ready_queue, ready_comp_queue);
    }

    //--------------------------------------------------------------------------
    template<typename T>
    T InnerContext::process_queue(LocalLock &lock, RtEvent &next_ready, 
                                  std::list<QueueEntry<T> > &queue,
                                  CompletionQueue &comp_queue,
                                  std::vector<T> &to_perform) const
    //--------------------------------------------------------------------------
    {
      T next{};
      const size_t vector_width = context_configuration.meta_task_vector_width;
      to_perform.reserve(vector_width);
      AutoLock l_lock(lock);
      std::vector<RtEvent> ready_events(vector_width);
      size_t num_ready =
        comp_queue.pop_events(&ready_events.front(), vector_width);
      // Realm permits spurious wake-ups sometimes on completion queues where
      // no events are actually ready. The number of times this can happen is
      // bounded by the number of events that are added into the queue so we
      // don't need to worry about indefinite starvation.
      if (num_ready > 0)
      {
        ready_events.resize(num_ready);
        std::sort(ready_events.begin(), ready_events.end());
        // Find the entries
        for (typename std::list<QueueEntry<T> >::iterator it =
              queue.begin(); it != queue.end(); /*nothing*/)
        {
          std::vector<RtEvent>::iterator finder = 
            std::lower_bound(ready_events.begin(),ready_events.end(),it->ready);
          if ((finder != ready_events.end()) && (*finder == it->ready))
          {
            to_perform.push_back(it->op);
            it = queue.erase(it);
            ready_events.erase(finder);
            if (ready_events.empty())
              break;
          }
          else
            it++;
        }
#ifdef DEBUG_LEGION
        assert(ready_events.empty());
#endif
      }
      if (!queue.empty())
      {
        next_ready = RtEvent(comp_queue.get_nonempty_event());
        next = queue.front().op;
      }
      return next;
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_ready_queue(void)
    //--------------------------------------------------------------------------
    {
      RtEvent precondition;
      std::vector<Operation*> to_perform;
      Operation *next = process_queue<Operation*>(ready_lock, precondition,
                                 ready_queue, ready_comp_queue, to_perform);
      if (next != NULL)
      {
        TriggerReadyArgs args(next, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_WORK_PRIORITY, precondition);
      }
      for (std::vector<Operation*>::const_iterator it =
            to_perform.begin(); it != to_perform.end(); it++)
      {
        implicit_provenance = (*it)->get_unique_op_id();
        (*it)->trigger_ready();
      }
      return (next == NULL);
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_task_queue(TaskOp *task, RtEvent ready)
    //--------------------------------------------------------------------------
    {
      add_to_queue<TaskOp*,DeferredEnqueueTaskArgs,false/*has bounds*/>(
          QueueEntry<TaskOp*>(task, ready), enqueue_task_lock,
          enqueue_task_queue, enqueue_task_comp_queue);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_enqueue_task_queue(void)
    //--------------------------------------------------------------------------
    {
      RtEvent precondition;
      std::vector<TaskOp*> to_perform;
      TaskOp *next = process_queue<TaskOp*>(enqueue_task_lock, precondition,
                    enqueue_task_queue, enqueue_task_comp_queue, to_perform);
      if (next != NULL)
      {
        DeferredEnqueueTaskArgs args(next, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_WORK_PRIORITY, precondition);
      }
      for (std::vector<TaskOp*>::const_iterator it =
            to_perform.begin(); it != to_perform.end(); it++)
      {
        implicit_provenance = (*it)->get_unique_op_id();
        (*it)->enqueue_ready_task(false/*use target*/);
      }
      return (next == NULL);
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_distribute_task_queue(TaskOp *task, RtEvent ready)
    //--------------------------------------------------------------------------
    {
      add_to_queue<TaskOp*,DeferredDistributeTaskArgs,false/*has bounds*/>(
          QueueEntry<TaskOp*>(task, ready), distribute_task_lock,
          distribute_task_queue, distribute_task_comp_queue);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_distribute_task_queue(void)
    //--------------------------------------------------------------------------
    {
      RtEvent precondition;
      std::vector<TaskOp*> to_perform;
      TaskOp *next = process_queue<TaskOp*>(distribute_task_lock, precondition,
                distribute_task_queue, distribute_task_comp_queue, to_perform);
      if (next != NULL)
      {
        DeferredDistributeTaskArgs args(next, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_WORK_PRIORITY, precondition);
      }
      for (std::vector<TaskOp*>::const_iterator it =
            to_perform.begin(); it != to_perform.end(); it++)
      {
        implicit_provenance = (*it)->get_unique_op_id();
        if ((*it)->distribute_task())
          (*it)->launch_task();
      }
      return (next == NULL);
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_launch_task_queue(TaskOp *task, RtEvent ready)
    //--------------------------------------------------------------------------
    {
      add_to_queue<TaskOp*,DeferredLaunchTaskArgs,false/*has bounds*/>(
          QueueEntry<TaskOp*>(task, ready), launch_task_lock,
          launch_task_queue, launch_task_comp_queue);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_launch_task_queue(void)
    //--------------------------------------------------------------------------
    {
      RtEvent precondition;
      std::vector<TaskOp*> to_perform;
      TaskOp *next = process_queue<TaskOp*>(launch_task_lock, precondition,
                    launch_task_queue, launch_task_comp_queue, to_perform);
      if (next != NULL)
      {
        DeferredLaunchTaskArgs args(next, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_WORK_PRIORITY, precondition);
      }
      for (std::vector<TaskOp*>::const_iterator it =
            to_perform.begin(); it != to_perform.end(); it++)
      {
        implicit_provenance = (*it)->get_unique_op_id();
        (*it)->launch_task();
      }
      return (next == NULL);
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_resolution_queue(Operation *op, RtEvent ready)
    //--------------------------------------------------------------------------
    {
      add_to_queue<Operation*,TriggerResolutionArgs,true/*has bounds*/>(
          QueueEntry<Operation*>(op, ready), resolution_lock,
          resolution_queue, resolution_comp_queue);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_resolution_queue(void)
    //--------------------------------------------------------------------------
    {
      RtEvent precondition;
      std::vector<Operation*> to_perform;
      Operation *next = process_queue<Operation*>(resolution_lock, precondition,
                           resolution_queue, resolution_comp_queue, to_perform);
      if (next != NULL)
      {
        TriggerResolutionArgs args(next, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_WORK_PRIORITY, precondition);
      }
      for (std::vector<Operation*>::const_iterator it =
            to_perform.begin(); it != to_perform.end(); it++)
      {
        implicit_provenance = (*it)->get_unique_op_id();
        (*it)->trigger_resolution();
      }
      return (next == NULL);
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_trigger_execution_queue(Operation *op, 
                                                      RtEvent ready)
    //--------------------------------------------------------------------------
    {
      add_to_queue<Operation*,TriggerExecutionArgs,false/*has bounds*/>(
          QueueEntry<Operation*>(op, ready), trigger_execution_lock,
          trigger_execution_queue, trigger_execution_comp_queue);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_trigger_execution_queue(void)
    //--------------------------------------------------------------------------
    {
      RtEvent precondition;
      std::vector<Operation*> to_perform;
      Operation *next = process_queue<Operation*>(trigger_execution_lock,
          precondition, trigger_execution_queue, 
          trigger_execution_comp_queue, to_perform);
      if (next != NULL)
      {
        TriggerExecutionArgs args(next, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_WORK_PRIORITY, precondition);
      }
      for (std::vector<Operation*>::const_iterator it =
            to_perform.begin(); it != to_perform.end(); it++)
      {
        implicit_provenance = (*it)->get_unique_op_id();
        (*it)->trigger_execution();
      }
      return (next == NULL);
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_deferred_execution_queue(Operation *op,
                                                       RtEvent ready)
    //--------------------------------------------------------------------------
    {
      add_to_queue<Operation*,DeferredExecutionArgs,false/*has bounds*/>(
          QueueEntry<Operation*>(op, ready), deferred_execution_lock,
          deferred_execution_queue, deferred_execution_comp_queue);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_deferred_execution_queue(void)
    //--------------------------------------------------------------------------
    {
      RtEvent precondition;
      std::vector<Operation*> to_perform;
      Operation *next = process_queue<Operation*>(deferred_execution_lock,
          precondition, deferred_execution_queue, 
          deferred_execution_comp_queue, to_perform);
      if (next != NULL)
      {
        DeferredExecutionArgs args(next, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_WORK_PRIORITY, precondition);
      }
      for (std::vector<Operation*>::const_iterator it =
            to_perform.begin(); it != to_perform.end(); it++)
      {
        implicit_provenance = (*it)->get_unique_op_id();
        (*it)->complete_execution();
      }
      return (next == NULL);
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_trigger_completion_queue(Operation *op,
                                                       RtEvent ready)
    //--------------------------------------------------------------------------
    {
      add_to_queue<Operation*,TriggerCompletionArgs,false/*has bounds*/>(
          QueueEntry<Operation*>(op, ready),
          trigger_completion_lock, trigger_completion_queue,
          trigger_completion_comp_queue);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_trigger_completion_queue(void)
    //--------------------------------------------------------------------------
    {
      RtEvent precondition;
      std::vector<Operation*> to_perform;
      Operation *next = process_queue<Operation*>(trigger_completion_lock,
          precondition, trigger_completion_queue,
          trigger_completion_comp_queue, to_perform);
      if (next != NULL)
      {
        TriggerCompletionArgs args(next, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_WORK_PRIORITY, precondition);
      }
      for (std::vector<Operation*>::const_iterator it =
            to_perform.begin(); it != to_perform.end(); it++)
      {
        implicit_provenance = (*it)->get_unique_op_id();
        (*it)->trigger_complete();
      }
      return (next == NULL);
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_deferred_completion_queue(Operation *op,
                                                        RtEvent ready)
    //--------------------------------------------------------------------------
    {
      add_to_queue<Operation*,DeferredCompletionArgs,false/*has bounds*/>(
          QueueEntry<Operation*>(op, ready), deferred_completion_lock,
          deferred_completion_queue, deferred_completion_comp_queue);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_deferred_completion_queue(void)
    //--------------------------------------------------------------------------
    {
      RtEvent precondition;
      std::vector<Operation*> to_perform;
      Operation *next = process_queue<Operation*>(deferred_completion_lock,
          precondition, deferred_completion_queue,
          deferred_completion_comp_queue, to_perform);
      if (next != NULL)
      {
        DeferredCompletionArgs args(next, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_WORK_PRIORITY, precondition);
      }
      for (std::vector<Operation*>::const_iterator it =
            to_perform.begin(); it != to_perform.end(); it++)
      {
        implicit_provenance = (*it)->get_unique_op_id();
        (*it)->complete_operation(RtEvent::NO_RT_EVENT, false/*first invoke*/);
      }
      return (next == NULL);
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_trigger_commit_queue(Operation *op, RtEvent ready)
    //--------------------------------------------------------------------------
    {
      add_to_queue<Operation*,TriggerCommitArgs,false/*has bounds*/>(
          QueueEntry<Operation*>(op, ready), trigger_commit_lock,
          trigger_commit_queue, trigger_commit_comp_queue);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_trigger_commit_queue(void)
    //--------------------------------------------------------------------------
    {
      RtEvent precondition;
      std::vector<Operation*> to_perform;
      Operation *next = process_queue<Operation*>(trigger_commit_lock,
          precondition, trigger_commit_queue,
          trigger_commit_comp_queue, to_perform);
      if (next != NULL)
      {
        TriggerCommitArgs args(next, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_WORK_PRIORITY, precondition);
      }
      for (std::vector<Operation*>::const_iterator it =
            to_perform.begin(); it != to_perform.end(); it++)
      {
        implicit_provenance = (*it)->get_unique_op_id();
        (*it)->trigger_commit();
      }
      return (next == NULL);
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_deferred_commit_queue(Operation *op,
                                                 RtEvent ready, bool deactivate)
    //--------------------------------------------------------------------------
    {
      add_to_queue<std::pair<Operation*,bool>,
          DeferredCommitArgs,false/*has bounds*/>(
            QueueEntry<std::pair<Operation*,bool> >(std::pair<Operation*,bool>(
              op, deactivate), ready), deferred_commit_lock,
          deferred_commit_queue, deferred_commit_comp_queue);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_deferred_commit_queue(void)
    //--------------------------------------------------------------------------
    {
      RtEvent precondition;
      std::vector<std::pair<Operation*,bool> > to_perform;
      std::pair<Operation*,bool> next =
        process_queue<std::pair<Operation*,bool> >(deferred_commit_lock,
          precondition, deferred_commit_queue, 
          deferred_commit_comp_queue, to_perform);
      if (next.first != NULL)
      {
        DeferredCommitArgs args(next, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_WORK_PRIORITY, precondition);
      }
      for (std::vector<std::pair<Operation*,bool> >::const_iterator it =
            to_perform.begin(); it != to_perform.end(); it++)
      {
        implicit_provenance = it->first->get_unique_op_id();
        it->first->commit_operation(it->second);
      }
      return (next.first == NULL);
    }

    //--------------------------------------------------------------------------
    void InnerContext::add_to_post_task_queue(TaskContext *ctx, RtEvent wait_on,
                                              FutureInstance *instance,
                                              FutureFunctor *callback_functor,
                                              bool own_callback_functor,
                                              const void *metadataptr,
                                              size_t metadatasize)
    //--------------------------------------------------------------------------
    {
      void *metadatacopy = NULL;
      if (metadataptr != NULL)
      {
        metadatacopy = malloc(metadatasize);
        memcpy(metadatacopy, metadataptr, metadatasize);
      }
      bool issue_task = false;
      RtEvent precondition;
      const size_t task_index = ctx->get_owner_task()->get_context_index();
      {
        AutoLock p_lock(post_task_lock);
        // Issue a task if there isn't one running right now
        if (post_task_queue.empty())
        {
          issue_task = true;
          // Add a reference to the context the first time we defer this
          add_base_resource_ref(META_TASK_REF);
        }
        post_task_queue.push_back(PostTaskArgs(ctx, task_index, wait_on, 
              instance, metadatacopy, metadatasize, callback_functor,
              own_callback_functor));
        // If we've already got a completion queue then use it
        if (post_task_comp_queue.exists())
          post_task_comp_queue.add_event(wait_on);
        if (issue_task)
        {
          if (!post_task_comp_queue.exists())
          {
            // Find the one with the minimum index
            size_t min_index = 0;
            for (std::list<PostTaskArgs>::const_iterator it = 
                  post_task_queue.begin(); it != post_task_queue.end(); it++)
            {
              if (!precondition.exists() || (it->index < min_index))
              {
                precondition = it->wait_on;
                min_index = it->index;
              }
            }
          }
          else
            precondition = RtEvent(post_task_comp_queue.get_nonempty_event());
        }
      }
      if (issue_task)
      {
        // Other things could be added to the queue by the time we're here
        PostEndArgs args(ctx->owner_task, this);
        runtime->issue_runtime_meta_task(args, 
            LG_THROUGHPUT_DEFERRED_PRIORITY, precondition);
      }
    }

    //--------------------------------------------------------------------------
    bool InnerContext::process_post_end_tasks(void)
    //--------------------------------------------------------------------------
    {
      RtEvent precondition;
      TaskContext *next_ctx = NULL;
      std::vector<PostTaskArgs> to_perform;
      to_perform.reserve(context_configuration.meta_task_vector_width);
      {
        std::vector<RtEvent> ready_events(
                          context_configuration.meta_task_vector_width);
        AutoLock p_lock(post_task_lock);
        // Ask the completion queue for the ready events
        size_t num_ready = 0;
        if (!post_task_comp_queue.exists())
        {
          // No completion queue so go through and do this manually
          for (std::list<PostTaskArgs>::const_iterator it =
                post_task_queue.begin(); it != post_task_queue.end(); it++)
          {
            if (it->wait_on.has_triggered())
            {
              ready_events[num_ready++] = it->wait_on;
              if (num_ready == ready_events.size())
                break;
            }
          }
        }
        else // We can just use the comp queue to get the ready events
          num_ready = post_task_comp_queue.pop_events(
            &ready_events.front(), ready_events.size());
        // Realm permits spurious wake-ups sometimes on completion queues where
        // no events are actually ready. The number of times this can happen is
        // bounded by the number of events that are added into the queue so we
        // don't need to worry about indefinite starvation.
        if (num_ready > 0)
        {
          // Find all the entries for all the ready events
          for (std::list<PostTaskArgs>::iterator it = post_task_queue.begin();
                it != post_task_queue.end(); /*nothing*/)
          {
            bool found = false;
            for (unsigned idx = 0; idx < num_ready; idx++)
            {
              if (it->wait_on == ready_events[idx])
              {
                found = true;
                break;
              }
            }
            if (found)
            {
              to_perform.push_back(*it);
              it = post_task_queue.erase(it);
              // Check to see if we're done early
              if (to_perform.size() == num_ready)
                break;
            }
            else
              it++;
          }
        }
        if (!post_task_queue.empty())
        {
          if (!post_task_comp_queue.exists())
          {
            // See if we want to switch over to using a completion queue
            if (post_task_queue.size() < 
                  context_configuration.meta_task_vector_width)
            {
              // Find the one with the minimum index
              size_t min_index = 0;
              for (std::list<PostTaskArgs>::const_iterator it = 
                    post_task_queue.begin(); it != post_task_queue.end(); it++)
              {
                if (!precondition.exists() || (it->index < min_index))
                {
                  precondition = it->wait_on;
                  min_index = it->index;
                  next_ctx = it->context;
                }
              }
            }
            else
            {
              // Switch over to using a completion queue
              post_task_comp_queue =
                CompletionQueue::create_completion_queue(0);
              // Fill in the completion queue with events
              for (std::list<PostTaskArgs>::const_iterator it = 
                    post_task_queue.begin(); it != post_task_queue.end(); it++)
                post_task_comp_queue.add_event(it->wait_on);
              precondition = RtEvent(post_task_comp_queue.get_nonempty_event());
              next_ctx = post_task_queue.front().context;
            }
          }
          else
          {
            precondition = RtEvent(post_task_comp_queue.get_nonempty_event());
            next_ctx = post_task_queue.front().context;
          }
#ifdef DEBUG_LEGION
          assert(next_ctx != NULL);
#endif
        }
      }
      // Launch this first to get it in flight so it can run when ready
      if (next_ctx != NULL)
      {
        PostEndArgs args(next_ctx->owner_task, this);
        runtime->issue_runtime_meta_task(args,
            LG_THROUGHPUT_DEFERRED_PRIORITY, precondition);
      }
      // Now perform our operations
      if (!to_perform.empty())
      {
        // Sort these into order by their index before we perform them
        // so we do them in order or we could risk a hang
        std::sort(to_perform.begin(), to_perform.end());
        for (std::vector<PostTaskArgs>::const_iterator it =
              to_perform.begin(); it != to_perform.end(); it++)
        {
          implicit_provenance = it->context->get_unique_id();
          it->context->post_end_task(it->instance, it->metadata,
                                     it->metasize, it->functor,it->own_functor);
        }
      }
      // If we didn't launch a next op, then we can remove the reference
      return (next_ctx == NULL);
    }

    //--------------------------------------------------------------------------
    size_t InnerContext::register_new_child_operation(Operation *op,
        RtUserEvent &resolved, const std::vector<StaticDependence> *dependences)
    //--------------------------------------------------------------------------
    {
      // TODO: set the resolved event for any speculative executions occurring
      // If we are performing a trace mark that the child has a trace
      if (current_trace != NULL)
        op->set_trace(current_trace, dependences);
      size_t result = total_children_count++;
      const size_t outstanding_count =
        outstanding_children_count.fetch_add(1) + 1;
      // Only need to check if we are not tracing by frames
      if ((context_configuration.min_frames_to_schedule == 0) &&
          (context_configuration.max_window_size > 0) &&
            (outstanding_count > context_configuration.max_window_size) &&
            !is_replaying_physical_trace())
        perform_window_wait();
      if (runtime->legion_spy_enabled)
        LegionSpy::log_child_operation_index(get_unique_id(), result,
                                             op->get_unique_op_id());
      return result;
    }

    //--------------------------------------------------------------------------
    void InnerContext::insert_unordered_ops(AutoLock &d_lock)
    //--------------------------------------------------------------------------
    {
      // No progressing of unordered operations inside a trace
      if (current_trace != NULL)
        return;
      // If we don't have any unordered operations then we are done
      if (unordered_ops.empty())
        return;
      issue_unordered_operations(d_lock, unordered_ops); 
    }

    //--------------------------------------------------------------------------
    void InnerContext::issue_unordered_operations(AutoLock &d_lock,
                                      std::vector<Operation*> &ready_operations)
    //--------------------------------------------------------------------------
    {
      if (runtime->program_order_execution)
      {
        while (!ready_operations.empty())
        {
          Operation *op = ready_operations.back();
          ready_operations.pop_back();
          // Record it in the reorder buffer
          {
            AutoLock child_lock(child_op_lock);
            const size_t op_index = total_children_count++;
            op->set_tracking_parent(op_index);
#ifdef DEBUG_LEGION
            assert(reorder_buffer.empty() || 
                (reorder_buffer.back().operation_index < op_index));
#endif       
            // Pad the reorder buffer for missing entries if necessary
            while (!reorder_buffer.empty() &&
                ((reorder_buffer.back().operation_index+1) < op_index))
              reorder_buffer.emplace_back(
                 ReorderBufferEntry(reorder_buffer.back().operation_index+1));
            reorder_buffer.emplace_back(ReorderBufferEntry(op));
            executing_children_count++;
          }
#ifdef DEBUG_LEGION
          assert(dependence_queue.empty());
#endif
          dependence_queue.push_back(op);
          outstanding_children_count.fetch_add(1);
          RtEvent precondition = dependence_precondition;
          // Release the lock and launch the meta-task
          d_lock.release();
          const ApEvent term_event = op->get_completion_event();
          DependenceArgs args(op, this);
          const LgPriority priority = LG_THROUGHPUT_WORK_PRIORITY;
          runtime->issue_runtime_meta_task(args, priority, precondition);
          bool poisoned = false;
          term_event.wait_faultaware(poisoned);
          if (poisoned)
            raise_poison_exception();
          // Reacquire the lock before doing the next operation
          d_lock.reacquire();
        }
      }
      else
      {
        // Common path for normal execution where we don't need to
        // execute each of these things in program order
        // We need the child op lock here so we can add these to this
        // list of executing children as well
        AutoLock child_lock(child_op_lock);
        for (std::vector<Operation*>::const_iterator it = 
              ready_operations.begin(); it != ready_operations.end(); it++)
        {
          const size_t op_index = total_children_count++;
          (*it)->set_tracking_parent(op_index);
#ifdef DEBUG_LEGION
          assert(reorder_buffer.empty() || 
              (reorder_buffer.back().operation_index < op_index));
#endif       
          // Pad the reorder buffer for missing entries if necessary
          while (!reorder_buffer.empty() &&
              ((reorder_buffer.back().operation_index+1) < op_index))
            reorder_buffer.emplace_back(
                ReorderBufferEntry(reorder_buffer.back().operation_index+1));
          reorder_buffer.emplace_back(ReorderBufferEntry(*it));
          executing_children_count++;
          if (dependence_queue.empty())
          {
            DependenceArgs args(*it, this);
            const LgPriority priority = LG_THROUGHPUT_WORK_PRIORITY;
            runtime->issue_runtime_meta_task(args, priority, 
                                    dependence_precondition);
          }
          dependence_queue.push_back(*it);
        }
        outstanding_children_count.fetch_add(ready_operations.size());
        ready_operations.clear();
      }
    }

    //--------------------------------------------------------------------------
    size_t InnerContext::register_new_summary_operation(TraceSummaryOp *op)
    //--------------------------------------------------------------------------
    {
      // For now we just bump our counter
      size_t result = total_summary_count++;
      const size_t outstanding_count =
        outstanding_children_count.fetch_add(1) + 1; 
      // Only need to check if we are not tracing by frames
      if ((context_configuration.min_frames_to_schedule == 0) && 
          (context_configuration.max_window_size > 0) && 
            (outstanding_count > context_configuration.max_window_size) &&
            !is_replaying_physical_trace())
        perform_window_wait();
      if (runtime->legion_spy_enabled)
        LegionSpy::log_child_operation_index(get_unique_id(), result, 
                                             op->get_unique_op_id()); 
      return result;
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_executing_child(Operation *op)
    //--------------------------------------------------------------------------
    {
      AutoLock child_lock(child_op_lock);
#ifdef DEBUG_LEGION
      assert(reorder_buffer.empty() ||
          (reorder_buffer.back().operation_index < op->get_ctx_index()));
#endif
      // Pad the reorder buffer for missing entries if necessary
      while (!reorder_buffer.empty() &&
          ((reorder_buffer.back().operation_index+1) < op->get_ctx_index()))
        reorder_buffer.emplace_back(
            ReorderBufferEntry(reorder_buffer.back().operation_index+1));
      reorder_buffer.emplace_back(ReorderBufferEntry(op));
      executing_children_count++;
    }

    //--------------------------------------------------------------------------
    InnerContext::ReorderBufferEntry& InnerContext::find_rob_entry(
                                                                  Operation *op)
    //--------------------------------------------------------------------------
    {
      ReorderBufferEntry &head = reorder_buffer.front();
#ifdef DEBUG_LEGION
      assert(head.operation_index <= op->get_ctx_index());
#endif
      size_t offset = op->get_ctx_index() - head.operation_index;
#ifdef DEBUG_LEGION
      assert(offset < reorder_buffer.size());
#endif
      ReorderBufferEntry &entry = reorder_buffer[offset];
#ifdef DEBUG_LEGION
      assert(entry.operation == op);
#endif
      return entry;
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_child_executed(Operation *op)
    //--------------------------------------------------------------------------
    {
      RtUserEvent to_trigger;
      {
        AutoLock child_lock(child_op_lock);
        ReorderBufferEntry &entry = find_rob_entry(op);
#ifdef DEBUG_LEGION
        assert(entry.stage == EXECUTING_STAGE);
        assert(executing_children_count > 0);
#endif
        entry.stage = EXECUTED_STAGE;
        executing_children_count--;
        executed_children_count++;
        // Add some hysteresis here so that we have some runway for when
        // the paused task resumes it can run for a little while.
        int outstanding_count = outstanding_children_count.fetch_sub(1) - 1;
#ifdef DEBUG_LEGION
        assert(outstanding_count >= 0);
#endif
        if (valid_wait_event && (context_configuration.max_window_size > 0) &&
            (outstanding_count <=
             int((100 - context_configuration.hysteresis_percentage) * 
                 context_configuration.max_window_size / 100)))
        {
          to_trigger = window_wait;
          valid_wait_event = false;
        }
      }
      if (to_trigger.exists())
        Runtime::trigger_event(to_trigger);
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_child_complete(Operation *op)
    //--------------------------------------------------------------------------
    {
      bool needs_trigger = false;
      std::vector<ApEvent> child_completion_events;
      {
        AutoLock child_lock(child_op_lock);
        ReorderBufferEntry &entry = find_rob_entry(op);
#ifdef DEBUG_LEGION
        assert(entry.stage == EXECUTED_STAGE);
        assert(executed_children_count > 0);
#endif
        entry.stage = COMPLETED_STAGE;
        executed_children_count--;
        // See if we need to trigger the all children complete call
        if (task_executed && (owner_task != NULL) && 
            (executing_children_count == 0) && (executed_children_count == 0) &&
            !children_complete_invoked)
        {
          needs_trigger = true;
          children_complete_invoked = true;
#ifdef LEGION_SPY
          child_completion_events.insert(child_completion_events.end(),
              cummulative_child_completion_events.begin(),
              cummulative_child_completion_events.end());
          cummulative_child_completion_events.clear();
#endif
          for (std::deque<ReorderBufferEntry>::const_iterator it =
                reorder_buffer.begin(); it != reorder_buffer.end(); it++)
          {
#ifdef DEBUG_LEGION
            assert(it->stage != EXECUTING_STAGE);
            assert(it->stage != EXECUTED_STAGE);
#endif
            if (it->stage == COMPLETED_STAGE)
              it->operation->find_completion_effects(child_completion_events);
          }
        }
#ifdef LEGION_SPY
        else
        {
          op->find_completion_effects(cummulative_child_completion_events);
          // Make sure this vector doesn't grow too large for long-running tasks
          constexpr size_t MAX_SIZE = 32;
          if (cummulative_child_completion_events.size() == MAX_SIZE)
          {
            const ApEvent merged = 
              Runtime::merge_events(NULL, cummulative_child_completion_events);
            cummulative_child_completion_events.clear();
            cummulative_child_completion_events.push_back(merged);
          }
        }
#endif
      }
      if (needs_trigger)
      {
        if (!child_completion_events.empty())
        {
          if (realm_done_event.exists())
            child_completion_events.push_back(realm_done_event);
          owner_task->record_inner_termination(
            Runtime::merge_events(NULL, child_completion_events));
        }
        else
          owner_task->record_inner_termination(realm_done_event);
        owner_task->trigger_children_complete();
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_child_commit(Operation *op)
    //--------------------------------------------------------------------------
    {
      bool needs_trigger = false;
      {
        AutoLock child_lock(child_op_lock);
        ReorderBufferEntry &entry = find_rob_entry(op);
#ifdef DEBUG_LEGION
        assert(entry.stage == COMPLETED_STAGE);
#endif
        entry.stage = COMMITTED_STAGE;
        while (!reorder_buffer.empty() && 
            (reorder_buffer.front().stage == COMMITTED_STAGE))
          reorder_buffer.pop_front();
        // See if we need to trigger the all children commited call
        if (task_executed && reorder_buffer.empty() && !children_commit_invoked)
        {
          needs_trigger = true;
          children_commit_invoked = true;
        }
      }
      if (needs_trigger && (owner_task != NULL))
        owner_task->trigger_children_committed();
    }

    //--------------------------------------------------------------------------
    int InnerContext::has_conflicting_regions(MapOp *op, bool &parent_conflict,
                                              bool &inline_conflict)
    //--------------------------------------------------------------------------
    {
      const RegionRequirement &req = op->get_requirement(); 
      return has_conflicting_internal(req, parent_conflict, inline_conflict);
    }

    //--------------------------------------------------------------------------
    int InnerContext::has_conflicting_regions(AttachOp *attach,
                                              bool &parent_conflict,
                                              bool &inline_conflict)
    //--------------------------------------------------------------------------
    {
      const RegionRequirement &req = attach->get_requirement();
      return has_conflicting_internal(req, parent_conflict, inline_conflict);
    }

    //--------------------------------------------------------------------------
    int InnerContext::has_conflicting_internal(const RegionRequirement &req,
                                               bool &parent_conflict,
                                               bool &inline_conflict)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, HAS_CONFLICTING_INTERNAL_CALL);
      parent_conflict = false;
      inline_conflict = false;
      // No need to hold our lock here because we are the only ones who
      // could possibly be doing any mutating of the physical_regions data 
      // structure but we are here so we aren't mutating
      for (unsigned our_idx = 0; our_idx < physical_regions.size(); our_idx++)
      {
        // skip any regions which are not mapped
        if (!physical_regions[our_idx].is_mapped())
          continue;
        const RegionRequirement &our_req = 
          physical_regions[our_idx].impl->get_requirement();
#ifdef DEBUG_LEGION
        // This better be true for a single task
        assert(our_req.handle_type == LEGION_SINGULAR_PROJECTION);
#endif
        RegionTreeID our_tid = our_req.region.get_tree_id();
        IndexSpace our_space = our_req.region.get_index_space();
        RegionUsage our_usage(our_req);
        if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
        {
          parent_conflict = true;
          return our_idx;
        }
      }
      // Need lock here because of unordered detach operations
      AutoLock i_lock(inline_lock,1,false/*exclusive*/);
      for (std::list<PhysicalRegion>::const_iterator it = 
            inline_regions.begin(); it != inline_regions.end(); it++)
      {
        if (!it->is_mapped())
          continue;
        const RegionRequirement &our_req = it->impl->get_requirement();
#ifdef DEBUG_LEGION
        // This better be true for a single task
        assert(our_req.handle_type == LEGION_SINGULAR_PROJECTION);
#endif
        RegionTreeID our_tid = our_req.region.get_tree_id();
        IndexSpace our_space = our_req.region.get_index_space();
        RegionUsage our_usage(our_req);
        if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
        {
          inline_conflict = true;
          // No index for inline conflicts
          return -1;
        }
      }
      return -1;
    }

    //--------------------------------------------------------------------------
    void InnerContext::find_conflicting_regions(TaskOp *task,
                                       std::vector<PhysicalRegion> &conflicting)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, FIND_CONFLICTING_CALL);
      // No need to hold our lock here because we are the only ones who
      // could possibly be doing any mutating of the physical_regions data 
      // structure but we are here so we aren't mutating
      for (unsigned our_idx = 0; our_idx < physical_regions.size(); our_idx++)
      {
        // Skip any regions which are not mapped
        if (!physical_regions[our_idx].is_mapped())
          continue;
        const RegionRequirement &our_req = 
          physical_regions[our_idx].impl->get_requirement();
#ifdef DEBUG_LEGION
        // This better be true for a single task
        assert(our_req.handle_type == LEGION_SINGULAR_PROJECTION);
#endif
        RegionTreeID our_tid = our_req.region.get_tree_id();
        IndexSpace our_space = our_req.region.get_index_space();
        RegionUsage our_usage(our_req);
        // Check to see if any region requirements from the child have
        // a dependence on our region at location our_idx
        for (unsigned idx = 0; idx < task->regions.size(); idx++)
        {
          const RegionRequirement &req = task->regions[idx];  
          if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
          {
            conflicting.push_back(physical_regions[our_idx]);
            // Once we find a conflict, we don't need to check
            // against it anymore, so go onto our next region
            break;
          }
        }
      }
      // Need lock here because of unordered detach operations
      AutoLock i_lock(inline_lock,1,false/*exclusive*/);
      for (std::list<PhysicalRegion>::const_iterator it = 
            inline_regions.begin(); it != inline_regions.end(); it++)
      {
        if (!it->is_mapped())
          continue;
        const RegionRequirement &our_req = it->impl->get_requirement();
#ifdef DEBUG_LEGION
        // This better be true for a single task
        assert(our_req.handle_type == LEGION_SINGULAR_PROJECTION);
#endif
        RegionTreeID our_tid = our_req.region.get_tree_id();
        IndexSpace our_space = our_req.region.get_index_space();
        RegionUsage our_usage(our_req);
        // Check to see if any region requirements from the child have
        // a dependence on our region at location our_idx
        for (unsigned idx = 0; idx < task->regions.size(); idx++)
        {
          const RegionRequirement &req = task->regions[idx];  
          if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
          {
            conflicting.push_back(*it);
            // Once we find a conflict, we don't need to check
            // against it anymore, so go onto our next region
            break;
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::find_conflicting_regions(CopyOp *copy,
                                       std::vector<PhysicalRegion> &conflicting)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, FIND_CONFLICTING_CALL);
      // No need to hold our lock here because we are the only ones who
      // could possibly be doing any mutating of the physical_regions data 
      // structure but we are here so we aren't mutating
      for (unsigned our_idx = 0; our_idx < physical_regions.size(); our_idx++)
      {
        // skip any regions which are not mapped
        if (!physical_regions[our_idx].is_mapped())
          continue;
        const RegionRequirement &our_req = 
          physical_regions[our_idx].impl->get_requirement();
#ifdef DEBUG_LEGION
        // This better be true for a single task
        assert(our_req.handle_type == LEGION_SINGULAR_PROJECTION);
#endif
        RegionTreeID our_tid = our_req.region.get_tree_id();
        IndexSpace our_space = our_req.region.get_index_space();
        RegionUsage our_usage(our_req);
        bool has_conflict = false;
        for (unsigned idx = 0; !has_conflict &&
              (idx < copy->src_requirements.size()); idx++)
        {
          const RegionRequirement &req = copy->src_requirements[idx];
          if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
            has_conflict = true;
        }
        for (unsigned idx = 0; !has_conflict &&
              (idx < copy->dst_requirements.size()); idx++)
        {
          const RegionRequirement &req = copy->dst_requirements[idx];
          if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
            has_conflict = true;
        }
        for (unsigned idx = 0; !has_conflict &&
              (idx < copy->src_indirect_requirements.size()); idx++)
        {
          const RegionRequirement &req = copy->src_indirect_requirements[idx];
          if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
            has_conflict = true;
        }
        for (unsigned idx = 0; !has_conflict &&
              (idx < copy->dst_indirect_requirements.size()); idx++)
        {
          const RegionRequirement &req = copy->dst_indirect_requirements[idx];
          if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
            has_conflict = true;
        }
        if (has_conflict)
          conflicting.push_back(physical_regions[our_idx]);
      }
      // Need lock here because of unordered detach operations
      AutoLock i_lock(inline_lock,1,false/*exclusive*/);
      for (std::list<PhysicalRegion>::const_iterator it = 
            inline_regions.begin(); it != inline_regions.end(); it++)
      {
        if (!it->is_mapped())
          continue;
        const RegionRequirement &our_req = it->impl->get_requirement();
#ifdef DEBUG_LEGION
        // This better be true for a single task
        assert(our_req.handle_type == LEGION_SINGULAR_PROJECTION);
#endif
        RegionTreeID our_tid = our_req.region.get_tree_id();
        IndexSpace our_space = our_req.region.get_index_space();
        RegionUsage our_usage(our_req);
        bool has_conflict = false;
        for (unsigned idx = 0; !has_conflict &&
              (idx < copy->src_requirements.size()); idx++)
        {
          const RegionRequirement &req = copy->src_requirements[idx];
          if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
            has_conflict = true;
        }
        for (unsigned idx = 0; !has_conflict &&
              (idx < copy->dst_requirements.size()); idx++)
        {
          const RegionRequirement &req = copy->dst_requirements[idx];
          if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
            has_conflict = true;
        }
        for (unsigned idx = 0; !has_conflict &&
              (idx < copy->src_indirect_requirements.size()); idx++)
        {
          const RegionRequirement &req = copy->src_indirect_requirements[idx];
          if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
            has_conflict = true;
        }
        for (unsigned idx = 0; !has_conflict &&
              (idx < copy->dst_indirect_requirements.size()); idx++)
        {
          const RegionRequirement &req = copy->dst_indirect_requirements[idx];
          if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
            has_conflict = true;
        }
        if (has_conflict)
          conflicting.push_back(*it);
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::find_conflicting_regions(AcquireOp *acquire,
                                       std::vector<PhysicalRegion> &conflicting)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, FIND_CONFLICTING_CALL);
      const RegionRequirement &req = acquire->get_requirement();
      find_conflicting_internal(req, conflicting); 
    }

    //--------------------------------------------------------------------------
    void InnerContext::find_conflicting_regions(ReleaseOp *release,
                                       std::vector<PhysicalRegion> &conflicting)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, FIND_CONFLICTING_CALL);
      const RegionRequirement &req = release->get_requirement();
      find_conflicting_internal(req, conflicting);      
    }

    //--------------------------------------------------------------------------
    void InnerContext::find_conflicting_regions(DependentPartitionOp *partition,
                                       std::vector<PhysicalRegion> &conflicting)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, FIND_CONFLICTING_CALL);
      const RegionRequirement &req = partition->get_requirement();
      find_conflicting_internal(req, conflicting);
    }

    //--------------------------------------------------------------------------
    void InnerContext::find_conflicting_internal(const RegionRequirement &req,
                                       std::vector<PhysicalRegion> &conflicting)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, FIND_CONFLICTING_CALL);
      // No need to hold our lock here because we are the only ones who
      // could possibly be doing any mutating of the physical_regions data 
      // structure but we are here so we aren't mutating
      for (unsigned our_idx = 0; our_idx < physical_regions.size(); our_idx++)
      {
        // skip any regions which are not mapped
        if (!physical_regions[our_idx].is_mapped())
          continue;
        const RegionRequirement &our_req = 
          physical_regions[our_idx].impl->get_requirement();
#ifdef DEBUG_LEGION
        // This better be true for a single task
        assert(our_req.handle_type == LEGION_SINGULAR_PROJECTION);
#endif
        RegionTreeID our_tid = our_req.region.get_tree_id();
        IndexSpace our_space = our_req.region.get_index_space();
        RegionUsage our_usage(our_req);
        if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
          conflicting.push_back(physical_regions[our_idx]);
      }
      // Need lock here because of unordered detach operations
      AutoLock i_lock(inline_lock,1,false/*exclusive*/);
      for (std::list<PhysicalRegion>::const_iterator it = 
            inline_regions.begin(); it != inline_regions.end(); it++)
      {
        if (!it->is_mapped())
          continue;
        const RegionRequirement &our_req = it->impl->get_requirement();
#ifdef DEBUG_LEGION
        // This better be true for a single task
        assert(our_req.handle_type == LEGION_SINGULAR_PROJECTION);
#endif
        RegionTreeID our_tid = our_req.region.get_tree_id();
        IndexSpace our_space = our_req.region.get_index_space();
        RegionUsage our_usage(our_req);
        if (check_region_dependence(our_tid,our_space,our_req,our_usage,req))
          conflicting.push_back(*it);
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::find_conflicting_regions(FillOp *fill,
                                       std::vector<PhysicalRegion> &conflicting)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, FIND_CONFLICTING_CALL);
      const RegionRequirement &req = fill->get_requirement();
      find_conflicting_internal(req, conflicting);
    } 

    //--------------------------------------------------------------------------
    void InnerContext::find_conflicting_regions(DiscardOp *discard,
                                       std::vector<PhysicalRegion> &conflicting)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, FIND_CONFLICTING_CALL);
      const RegionRequirement &req = discard->get_requirement();
      find_conflicting_internal(req, conflicting);
    }

    //--------------------------------------------------------------------------
    void InnerContext::register_inline_mapped_region(
                                                   const PhysicalRegion &region)
    //--------------------------------------------------------------------------
    {
      // Because of 'remap_region', this method can be called
      // both for inline regions as well as regions which were
      // initally mapped for the task.  Do a quick check to see
      // if it was an original region.  If it was then we're done.
      for (unsigned idx = 0; idx < physical_regions.size(); idx++)
      {
        if (physical_regions[idx].impl == region.impl)
          return;
      }
      // Need lock because of unordered detach operations
      AutoLock i_lock(inline_lock);
      inline_regions.push_back(region);
    }

    //--------------------------------------------------------------------------
    void InnerContext::unregister_inline_mapped_region(
                                                   const PhysicalRegion &region)
    //--------------------------------------------------------------------------
    {
      // Need lock because of unordered detach operations
      AutoLock i_lock(inline_lock);
      for (std::list<PhysicalRegion>::iterator it = 
            inline_regions.begin(); it != inline_regions.end(); it++)
      {
        if (it->impl == region.impl)
        {
          if (runtime->runtime_warnings && !has_inline_accessor)
            has_inline_accessor = it->impl->created_accessor();
          inline_regions.erase(it);
          return;
        }
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::print_children(void)
    //--------------------------------------------------------------------------
    {
      // Don't both taking the lock since this is for debugging
      // and isn't actually called anywhere
      for (std::deque<ReorderBufferEntry>::const_iterator it =
            reorder_buffer.begin(); it != reorder_buffer.end(); it++)
      {
        switch (it->stage)
        {
          case EXECUTING_STAGE:
            {
              printf("Executing Child %p\n", it->operation);
              break;
            }
          case EXECUTED_STAGE:
            {
              printf("Executed Child %p\n", it->operation);
              break;
            }
          case COMPLETED_STAGE:
            {
              printf("Completed Child %p\n", it->operation);
              break;
            }
          case COMMITTED_STAGE:
            {
              printf("Committed Child %p\n", it->operation);
              break;
            }
          default:
            assert(false);
        }
      }
    }

    //--------------------------------------------------------------------------
    ApEvent InnerContext::register_implicit_dependences(Operation *op,
                                                   RtEvent &mapping_fence_event)
    //--------------------------------------------------------------------------
    {
      // If there are any outstanding unmapped dependent partition operations
      // outstanding then we might have an implicit dependence on its execution
      // so we always record a dependence on it
      if (last_implicit_creation != NULL)
      {
#ifdef LEGION_SPY
        // Can't prune when doing legion spy
        op->register_dependence(last_implicit_creation, 
                                last_implicit_creation_gen);
#else
        if (op->register_dependence(last_implicit_creation,
                                    last_implicit_creation_gen))
          last_implicit_creation = NULL;
#endif
      }
      if (current_mapping_fence_event.exists())
      {
        mapping_fence_event = current_mapping_fence_event;
#ifdef LEGION_SPY
        if (current_fence_uid > 0)
        {
          unsigned num_regions = op->get_region_count();
          if (num_regions > 0)
          {
            for (unsigned idx = 0; idx < num_regions; idx++)
            {
              LegionSpy::log_mapping_dependence(
                  get_unique_id(), current_fence_uid, 0,
                  op->get_unique_op_id(), idx, TRUE_DEPENDENCE);
            }
          }
          else
            LegionSpy::log_mapping_dependence(
                get_unique_id(), current_fence_uid, 0,
                op->get_unique_op_id(), 0, TRUE_DEPENDENCE);
        }
#endif
      }
#ifdef LEGION_SPY
      op->find_completion_effects(previous_completion_events);
      // Periodically merge these to keep this data structure from exploding
      // when we have a long-running task, although don't do this for fence
      // operations in case we have to prune ourselves out of the set
      if (previous_completion_events.size() >= LEGION_DEFAULT_MAX_TASK_WINDOW)
      {
        // Only merge ones that we know are completed
        std::vector<ApEvent> triggered;
        for (std::set<ApEvent>::const_iterator it = 
              previous_completion_events.begin(); it !=
              previous_completion_events.end(); /*nothing*/)
        {
          if (it->has_triggered_faultignorant())
          {
            triggered.push_back(*it);
            std::set<ApEvent>::const_iterator delete_it = it++;
            previous_completion_events.erase(delete_it);
          }
          else
            it++;
        }
        if (!triggered.empty())
          previous_completion_events.insert(
              Runtime::merge_events(NULL, triggered));
      }
      // Have to record this operation in case there is a fence later
      ops_since_last_fence.push_back(op->get_unique_op_id());
      return current_execution_fence_event;
#else
      if (current_execution_fence_event.exists())
      {
        // We can't have internal operations pruning out fences
        // because we can't test if they are memoizing or not
        // Their 'get_memoizable' method will always return NULL
        bool poisoned = false;
        if (current_execution_fence_event.has_triggered_faultaware(poisoned))
        {
          if (poisoned)
            raise_poison_exception();
          if (!op->is_internal_op())
          {
            // We can only do this optimization safely if we're not 
            // recording a physical trace, otherwise the physical
            // trace needs to see this dependence
            MemoizableOp *memo = op->get_memoizable();
            if ((memo == NULL) || !memo->is_recording())
              current_execution_fence_event = ApEvent::NO_AP_EVENT;
          }
        }
        return current_execution_fence_event;
      }
      return ApEvent::NO_AP_EVENT;
#endif
    }

    //--------------------------------------------------------------------------
    void InnerContext::perform_fence_analysis(Operation *op, 
               std::set<ApEvent> &previous_events, bool mapping, bool execution)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      {
        const Operation::OpKind op_kind = op->get_operation_kind();
        // It's alright if you hit this assertion for a new operation kind
        // Just add the new operation kind here and then update the check
        // in register_implicit_dependences that looks for all these kinds too
        // so that we do not run into trouble when running with Legion Spy.
        assert((op_kind == Operation::FENCE_OP_KIND) || 
               (op_kind == Operation::FRAME_OP_KIND) || 
               (op_kind == Operation::DELETION_OP_KIND) ||
               (op_kind == Operation::TRACE_BEGIN_OP_KIND) ||
               (op_kind == Operation::TRACE_COMPLETE_OP_KIND) ||
               (op_kind == Operation::TRACE_CAPTURE_OP_KIND) ||
               (op_kind == Operation::TRACE_REPLAY_OP_KIND) ||
               (op_kind == Operation::TRACE_SUMMARY_OP_KIND));
      }
#endif
      std::vector<std::pair<Operation*,GenerationID> > previous_operations;
      // Take the lock and iterate through our current pending
      // operations and find all the ones with a context index
      // that is less than the index for the fence operation
      const size_t next_fence_index = op->get_ctx_index();
      // We only need the list of previous operations if we are recording
      // mapping dependences for this fence
      if (!execution)
      {
        // Mapping analysis only
        AutoLock child_lock(child_op_lock,1,false/*exclusive*/);
        for (std::deque<ReorderBufferEntry>::const_reverse_iterator it =
              reorder_buffer.crbegin(); it != reorder_buffer.crend(); it++)
        {
          if (it->operation_index < current_mapping_fence_index)
            break;
          // If it came after this fence we skip it
          if (next_fence_index <= it->operation_index)
            continue;
          if (it->stage == COMMITTED_STAGE)
            continue;
          previous_operations.emplace_back(
              std::make_pair(it->operation, it->operation->get_generation()));
        }
        // We can update the current mapping fence index since it only
        // needs to be referred to here as the previous "upward" facing fence
        current_mapping_fence_index = next_fence_index;
      }
      else if (!mapping)
      {
        // Execution analysis only
        AutoLock child_lock(child_op_lock,1,false/*exclusive*/);
        for (std::deque<ReorderBufferEntry>::const_reverse_iterator it =
              reorder_buffer.crbegin(); it != reorder_buffer.crend(); it++)
        {
          if (it->operation_index < current_execution_fence_index)
            break;
          if (next_fence_index <= it->operation_index)
            continue;
          if (it->stage == COMMITTED_STAGE)
            continue;
          it->operation->find_completion_effects(previous_events);
        }
        // We can update the current execution fence index since it only
        // needs to be referred to here as the previous "upward" facing fence
        current_execution_fence_index = next_fence_index;
      }
      else
      {
        // Both mapping and execution analysis at the same time
        AutoLock child_lock(child_op_lock,1,false/*exclusive*/);
        for (std::deque<ReorderBufferEntry>::const_reverse_iterator it =
              reorder_buffer.crbegin(); it != reorder_buffer.crend(); it++)
        {
          if ((it->operation_index < current_mapping_fence_index) &&
              (it->operation_index < current_execution_fence_index))
            break;
          if (next_fence_index <= it->operation_index)
            continue;
          if (it->stage == COMMITTED_STAGE)
            continue;
          if (current_mapping_fence_index <= it->operation_index)
            previous_operations.emplace_back(
                std::make_pair(it->operation, it->operation->get_generation()));
          if (current_execution_fence_index <= it->operation_index)
            it->operation->find_completion_effects(previous_events);
        }
        // We can update the current mapping and execution fence indexes since
        // they only need to be referred to here as the previous "upward" 
        // facing fences
        current_mapping_fence_index = next_fence_index;
        current_execution_fence_index = next_fence_index;
      }

      // Now record the dependences
      if (!previous_operations.empty())
      {
        for (std::vector<std::pair<Operation*,GenerationID> >::const_iterator
              it = previous_operations.begin(); 
              it != previous_operations.end(); it++)
          op->register_dependence(it->first, it->second);
      }

#ifdef LEGION_SPY
      // Record a dependence on the previous fence
      if (mapping)
      {
        if (current_fence_uid > 0)
          LegionSpy::log_mapping_dependence(get_unique_id(), current_fence_uid,
              0/*index*/, op->get_unique_op_id(), 0/*index*/, TRUE_DEPENDENCE);
        for (std::deque<UniqueID>::const_iterator it = 
              ops_since_last_fence.begin(); it != 
              ops_since_last_fence.end(); it++)
        {
          // Skip ourselves if we are here
          if ((*it) == op->get_unique_op_id())
            continue;
          LegionSpy::log_mapping_dependence(get_unique_id(), *it, 0/*index*/,
              op->get_unique_op_id(), 0/*index*/, TRUE_DEPENDENCE); 
        }
      }
      // If we're doing execution record dependence on all previous operations
      if (execution)
      {
        previous_events.insert(previous_completion_events.begin(),
                               previous_completion_events.end());
        // Don't include ourselves though
        previous_events.erase(op->get_completion_event());
      }
#endif
      // Also include the current execution fence in case the operation
      // already completed and wasn't in the set, make sure to do this
      // before we update the current fence
      if (execution && current_execution_fence_event.exists())
        previous_events.insert(current_execution_fence_event);
    }

    //--------------------------------------------------------------------------
#ifdef DEBUG_LEGION_COLLECTIVES
    RefinementOp* InnerContext::get_refinement_op(Operation *op,
                                                  RegionTreeNode *node)
#else
    RefinementOp* InnerContext::get_refinement_op(void)
#endif
    //--------------------------------------------------------------------------
    {
      return runtime->get_available_refinement_op();
    }

    //--------------------------------------------------------------------------
    void InnerContext::update_current_fence(FenceOp *op, 
                                            bool mapping, bool execution)
    //--------------------------------------------------------------------------
    {
      if (mapping)
      {
        current_mapping_fence_index = op->get_ctx_index();
        current_mapping_fence_event = op->get_mapped_event();
#ifdef LEGION_SPY
        current_fence_uid = op->get_unique_op_id();
        current_mapping_fence_gen = op->get_generation();
        ops_since_last_fence.clear();
#endif
      }
      if (execution)
      {
        // Only update the current fence event if we're actually an
        // execution fence, otherwise by definition we need the previous event
        current_execution_fence_event = op->get_completion_event();
        current_execution_fence_index = op->get_ctx_index();
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::update_current_implicit_creation(Operation *op)
    //--------------------------------------------------------------------------
    {
      // Just overwrite since we know we already recorded a dependence
      // between this operation and the previous last deppart op
      last_implicit_creation = op;
      last_implicit_creation_gen = op->get_generation();
    }

    //--------------------------------------------------------------------------
    RtEvent InnerContext::get_current_mapping_fence_event(void)
    //--------------------------------------------------------------------------
    {
      return current_mapping_fence_event;
    }

    //--------------------------------------------------------------------------
    ApEvent InnerContext::get_current_execution_fence_event(void)
    //--------------------------------------------------------------------------
    {
      return current_execution_fence_event;
    }

    //--------------------------------------------------------------------------
    void InnerContext::begin_trace(TraceID tid, bool logical_only,
        bool static_trace, const std::set<RegionTreeID> *trees,
        bool deprecated, Provenance *provenance, bool from_application)
    //--------------------------------------------------------------------------
    {
      if (runtime->no_tracing) return;
      if (runtime->no_physical_tracing) logical_only = true;

      if (from_application) {
        AutoRuntimeCall call(this);
        this->begin_trace(tid, logical_only, static_trace, trees, deprecated, provenance, false /* from application */);
        return;
      }
#ifdef DEBUG_LEGION
      log_run.debug("Beginning a trace in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      // No need to hold the lock here, this is only ever called
      // by the one thread that is running the task.
      if (current_trace != NULL)
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_NESTED_TRACE,
          "Illegal nested trace with ID %d attempted in task %s (ID %lld)", 
          tid, get_task_name(), get_unique_id())

      std::map<TraceID,LogicalTrace*>::const_iterator finder = traces.find(tid);
      LogicalTrace *trace = NULL;
      if (finder == traces.end())
      {
        // Trace does not exist yet, so make one and record it
        trace = new LogicalTrace(this, tid, logical_only, 
                                 static_trace, provenance, trees);
        if (!deprecated)
          traces[tid] = trace;
        trace->add_reference();
      }
      else
        trace = finder->second;

#ifdef DEBUG_LEGION
      assert(trace != NULL);
#endif
      trace->clear_blocking_call();

      // Issue a begin op
      TraceBeginOp *begin = runtime->get_available_begin_op();
      begin->initialize_begin(this, trace, provenance);
      add_to_dependence_queue(begin);

      if (!logical_only)
      {
        // Issue a replay op
        TraceReplayOp *replay = runtime->get_available_replay_op();
        replay->initialize_replay(this, trace, provenance);
        // Record the event for when the trace replay is ready
        physical_trace_replay_status.store(replay->get_mapped_event().id);
        add_to_dependence_queue(replay);
      }

      // Now mark that we are starting a trace
      current_trace = trace;
    }

    //--------------------------------------------------------------------------
    void InnerContext::record_physical_trace_replay(RtEvent ready, bool replay)
    //--------------------------------------------------------------------------
    {
      physical_trace_replay_status.compare_exchange_strong(ready.id, 
                                                           replay ? 1 : 0);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::is_replaying_physical_trace(void)
    //--------------------------------------------------------------------------
    {
      if (current_trace == NULL)
        return false;
      if (!current_trace->is_fixed())
        return false;
      realm_id_t status = physical_trace_replay_status.load();
      if (status > 1)
      {
        // Result is not ready yet so wait until it is
        RtEvent ready;
        ready.id = status;
        if (!ready.has_triggered())
          ready.wait();
        status = physical_trace_replay_status.load();
        // No need to spin again because there won't be anymore outstanding
        // trace capture ops to be setting this
#ifdef DEBUG_LEGION
        assert((status == 0) || (status == 1));
#endif
      }
      return (status == 1);
    }

    //--------------------------------------------------------------------------
    void InnerContext::end_trace(TraceID tid, bool deprecated,
                                 Provenance *provenance, bool from_application)
    //--------------------------------------------------------------------------
    {
      if (runtime->no_tracing) return;

      if (from_application) {
        AutoRuntimeCall call(this);
        this->end_trace(tid, deprecated, provenance, false /* from_application */);
        return;
      }
#ifdef DEBUG_LEGION
      log_run.debug("Ending a trace in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      if (current_trace == NULL)
        REPORT_LEGION_ERROR(ERROR_UMATCHED_END_TRACE,
          "Unmatched end trace for ID %d in task %s "
                       "(ID %lld)", tid, get_task_name(),
                       get_unique_id())
      else if (!deprecated && (current_trace->tid != tid))
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_END_TRACE_CALL,
          "Illegal end trace call on trace ID %d that does not match "
          "the current trace ID %d in task %s (UID %lld)", tid,
          current_trace->tid, get_task_name(), get_unique_id())
      bool has_blocking_call = current_trace->has_blocking_call();
      if (current_trace->is_fixed())
      {
        // Already fixed, dump a complete trace op into the stream
        TraceCompleteOp *complete_op = runtime->get_available_trace_op();
        complete_op->initialize_complete(this, has_blocking_call, provenance);
        // Remove the current trace now so we block at the end of the
        // trace in the case of program order execution
        current_trace = NULL;
        add_to_dependence_queue(complete_op);
      }
      else
      {
        // Not fixed yet, dump a capture trace op into the stream
        TraceCaptureOp *capture_op = runtime->get_available_capture_op(); 
        capture_op->initialize_capture(this, has_blocking_call,
                                       deprecated, provenance);
        // Mark that the current trace is now fixed
        current_trace->fix_trace(provenance);
        // Remove the current trace now so we block at the end of the
        // trace in the case of program order execution
        current_trace = NULL;
        add_to_dependence_queue(capture_op);
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::record_previous_trace(LogicalTrace *trace)
    //--------------------------------------------------------------------------
    {
      previous_trace = trace;
    }

    //--------------------------------------------------------------------------
    void InnerContext::invalidate_trace_cache(
                                    LogicalTrace *trace, Operation *invalidator)
    //--------------------------------------------------------------------------
    {
      if (!invalidator->is_internal_op() &&
          (previous_trace != NULL) && (previous_trace != trace))
        previous_trace->invalidate_trace_cache(invalidator);
    }

    //--------------------------------------------------------------------------
    void InnerContext::record_blocking_call(void)
    //--------------------------------------------------------------------------
    {
      if (current_trace != NULL)
        current_trace->record_blocking_call();
    }

    //--------------------------------------------------------------------------
    void InnerContext::issue_frame(FrameOp *frame, ApEvent frame_termination)
    //--------------------------------------------------------------------------
    {
      // This happens infrequently enough that we can just issue
      // a meta-task to see what we should do without holding the lock
      if (context_configuration.max_outstanding_frames > 0)
      {
        IssueFrameArgs args(owner_task, this, frame, frame_termination);
        // We know that the issuing is done in order because we block after
        // we launch this meta-task which blocks the application task
        RtEvent wait_on = runtime->issue_runtime_meta_task(args,
                                      LG_LATENCY_WORK_PRIORITY);
        wait_on.wait();
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::perform_frame_issue(FrameOp *frame,
                                         ApEvent frame_termination)
    //--------------------------------------------------------------------------
    {
      ApEvent wait_on, previous;
      {
        AutoLock child_lock(child_op_lock);
        const size_t current_frames = frame_events.size();
        if (current_frames > 0)
          previous = frame_events.back();
        if (current_frames > 
            (size_t)context_configuration.max_outstanding_frames)
          wait_on = frame_events[current_frames - 
                                 context_configuration.max_outstanding_frames];
        frame_events.push_back(frame_termination); 
      }
      frame->set_previous(previous);
      bool poisoned = false;
      if (!wait_on.has_triggered_faultaware(poisoned))
        wait_on.wait_faultaware(poisoned);
      if (poisoned)
        raise_poison_exception();
    }

    //--------------------------------------------------------------------------
    void InnerContext::finish_frame(ApEvent frame_termination)
    //--------------------------------------------------------------------------
    {
      // Pull off all the frame events until we reach ours
      if (context_configuration.max_outstanding_frames > 0)
      {
        AutoLock child_lock(child_op_lock);
#ifdef DEBUG_LEGION
        assert(frame_events.front() == frame_termination);
#endif
        frame_events.pop_front();
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::increment_outstanding(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert((context_configuration.min_tasks_to_schedule == 0) || 
             (context_configuration.min_frames_to_schedule == 0));
      assert((context_configuration.min_tasks_to_schedule > 0) || 
             (context_configuration.min_frames_to_schedule > 0));
#endif
      AutoLock child_lock(child_op_lock);
      if (!currently_active_context && (outstanding_subtasks == 0) && 
          (((context_configuration.min_tasks_to_schedule > 0) && 
            (pending_subtasks < 
             context_configuration.min_tasks_to_schedule)) ||
           ((context_configuration.min_frames_to_schedule > 0) &&
            (pending_frames < 
             context_configuration.min_frames_to_schedule))))
      {
        currently_active_context = true;
        runtime->activate_context(this);
      }
      outstanding_subtasks++;
    }

    //--------------------------------------------------------------------------
    void InnerContext::decrement_outstanding(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert((context_configuration.min_tasks_to_schedule == 0) || 
             (context_configuration.min_frames_to_schedule == 0));
      assert((context_configuration.min_tasks_to_schedule > 0) || 
             (context_configuration.min_frames_to_schedule > 0));
#endif
      AutoLock child_lock(child_op_lock);
#ifdef DEBUG_LEGION
      assert(outstanding_subtasks > 0);
#endif
      outstanding_subtasks--;
      if (currently_active_context && (outstanding_subtasks == 0) && 
          (((context_configuration.min_tasks_to_schedule > 0) &&
            (pending_subtasks < 
             context_configuration.min_tasks_to_schedule)) ||
           ((context_configuration.min_frames_to_schedule > 0) &&
            (pending_frames < 
             context_configuration.min_frames_to_schedule))))
      {
        currently_active_context = false;
        runtime->deactivate_context(this);
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::increment_pending(void)
    //--------------------------------------------------------------------------
    {
      // Don't need to do this if we are scheduling based on mapped frames
      if (context_configuration.min_tasks_to_schedule == 0)
        return;
      AutoLock child_lock(child_op_lock);
      pending_subtasks++;
      if (currently_active_context && (outstanding_subtasks > 0) &&
          (pending_subtasks == context_configuration.min_tasks_to_schedule))
      {
        currently_active_context = false;
        runtime->deactivate_context(this);
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::decrement_pending(TaskOp *child)
    //--------------------------------------------------------------------------
    {
      // Don't need to do this if we are scheduled by frames
      if (context_configuration.min_tasks_to_schedule > 0)
        decrement_pending(true/*need deferral*/);
    }

    //--------------------------------------------------------------------------
    void InnerContext::decrement_pending(bool need_deferral)
    //--------------------------------------------------------------------------
    {
      AutoLock child_lock(child_op_lock);
#ifdef DEBUG_LEGION
      assert(pending_subtasks > 0);
#endif
      pending_subtasks--;
      if (!currently_active_context && (outstanding_subtasks > 0) &&
          (pending_subtasks < context_configuration.min_tasks_to_schedule))
      {
        currently_active_context = true;
        runtime->activate_context(this);
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::increment_frame(void)
    //--------------------------------------------------------------------------
    {
      // Don't need to do this if we are scheduling based on mapped tasks
      if (context_configuration.min_frames_to_schedule == 0)
        return;
      AutoLock child_lock(child_op_lock);
      pending_frames++;
      if (currently_active_context && (outstanding_subtasks > 0) &&
          (pending_frames == context_configuration.min_frames_to_schedule))
      {
        currently_active_context = false;
        runtime->deactivate_context(this);
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::decrement_frame(void)
    //--------------------------------------------------------------------------
    {
      // Don't need to do this if we are scheduling based on mapped tasks
      if (context_configuration.min_frames_to_schedule == 0)
        return;
      AutoLock child_lock(child_op_lock);
#ifdef DEBUG_LEGION
      assert(pending_frames > 0);
#endif
      pending_frames--;
      if (!currently_active_context && (outstanding_subtasks > 0) &&
          (pending_frames < context_configuration.min_frames_to_schedule))
      {
        currently_active_context = true;
        runtime->activate_context(this);
      }
    }

    //--------------------------------------------------------------------------
#ifdef DEBUG_LEGION_COLLECTIVES
    MergeCloseOp* InnerContext::get_merge_close_op(Operation *op,
                                                   RegionTreeNode *node)
#else
    MergeCloseOp* InnerContext::get_merge_close_op(void)
#endif
    //--------------------------------------------------------------------------
    {
      return runtime->get_available_merge_close_op();
    }

    //--------------------------------------------------------------------------
    VirtualCloseOp* InnerContext::get_virtual_close_op(void)
    //--------------------------------------------------------------------------
    {
      return runtime->get_available_virtual_close_op();
    }

    //--------------------------------------------------------------------------
    void InnerContext::pack_inner_context(Serializer &rez) const
    //--------------------------------------------------------------------------
    {
      rez.serialize(did); // pack our distributed ID
      rez.serialize<DistributedID>(0); // no shard manager
    }

    //--------------------------------------------------------------------------
    /*static*/ InnerContext* InnerContext::unpack_inner_context(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DistributedID ctx_did, man_did;
      derez.deserialize(ctx_did);
      derez.deserialize(man_did);
      if ((runtime->determine_owner(ctx_did) != runtime->address_space) &&
          (man_did > 0))
      {
        ShardManager *manager =
          runtime->find_shard_manager(man_did, true/*can fail*/);
        if (manager != NULL)
          return manager->find_local_context();
      }
      return runtime->find_or_request_inner_context(ctx_did);
    }

    //--------------------------------------------------------------------------
    void InnerContext::destroy_lock(Lock l)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      // Can only be called from user land so no need to hold the lock
      context_locks.push_back(l.reservation_lock);
    }

    //--------------------------------------------------------------------------
    Grant InnerContext::acquire_grant(const std::vector<LockRequest> &requests)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      // Kind of annoying, but we need to unpack and repack the
      // Lock type here to build new requests because the C++
      // type system is dumb with nested classes.
      std::vector<GrantImpl::ReservationRequest> 
        unpack_requests(requests.size());
      for (unsigned idx = 0; idx < requests.size(); idx++)
      {
        unpack_requests[idx] = 
          GrantImpl::ReservationRequest(requests[idx].lock.reservation_lock,
                                        requests[idx].mode,
                                        requests[idx].exclusive);
      }
      return Grant(new GrantImpl(unpack_requests));
    }

    //--------------------------------------------------------------------------
    void InnerContext::release_grant(Grant grant)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      grant.impl->release_grant();
    } 

    //--------------------------------------------------------------------------
    void InnerContext::destroy_phase_barrier(PhaseBarrier pb)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      // Can only be called from user land so no need to hold the lock
      context_barriers.push_back(pb.phase_barrier);
    } 

    //--------------------------------------------------------------------------
    DynamicCollective InnerContext::create_dynamic_collective(
                                       unsigned arrivals, ReductionOpID redop,
                                       const void *init_value, size_t init_size)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      return DynamicCollective(
          ApBarrier(Realm::Barrier::create_barrier(arrivals, redop, 
                                    init_value, init_size)), redop);
    }

    //--------------------------------------------------------------------------
    void InnerContext::destroy_dynamic_collective(DynamicCollective dc)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      // Can only be called from user land so no need to hold the lock
      context_barriers.push_back(dc.phase_barrier);
    }

    //--------------------------------------------------------------------------
    void InnerContext::arrive_dynamic_collective(DynamicCollective dc,
                                const void *buffer, size_t size, unsigned count)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      Runtime::phase_barrier_arrive(dc,count,ApEvent::NO_AP_EVENT,buffer,size);
    }

    //--------------------------------------------------------------------------
    void InnerContext::defer_dynamic_collective_arrival(DynamicCollective dc,
                                           const Future &future, unsigned count)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      future.impl->contribute_to_collective(dc, count);
      // No need to register anything if this future is an application future
      // or it was made in a context above this in the region tree
      if ((future.impl->producer_op == NULL) ||
          (future.impl->producer_depth < get_depth()))
        return;
      // Record this future as a contribution to the collective
      // for future dependence analysis
      const size_t barrier_gen = 
        Realm::ID(dc.phase_barrier.id).event_generation();
      const size_t barrier_name = dc.phase_barrier.id - barrier_gen;
      AutoLock pb_lock(phase_barrier_lock);
      barrier_contributions[barrier_name].push_back(
          BarrierContribution(future.impl->producer_op, future.impl->op_gen,
#ifdef LEGION_SPY
            future.impl->producer_uid,
#else
            0/*no uid*/,
#endif
            0/*no muid*/, barrier_gen));
    }

    //--------------------------------------------------------------------------
    void InnerContext::perform_barrier_dependence_analysis(Operation *op,
                            const std::vector<PhaseBarrier> &wait_barriers,
                            const std::vector<PhaseBarrier> &arrive_barriers,
                            MustEpochOp *must_epoch)
    //--------------------------------------------------------------------------
    {
      AutoLock pb_lock(phase_barrier_lock);
      if (!wait_barriers.empty())
        analyze_barrier_dependences(op, wait_barriers, must_epoch, true);
      if (!arrive_barriers.empty())
        analyze_barrier_dependences(op, arrive_barriers, must_epoch, false);
    }

    //--------------------------------------------------------------------------
    void InnerContext::analyze_barrier_dependences(Operation *op,
                              const std::vector<PhaseBarrier> &barriers,
                              MustEpochOp *must_epoch_op, bool previous_gen)
    //--------------------------------------------------------------------------
    {
      const UniqueID uid = op->get_unique_op_id();
      const GenerationID gen = op->get_generation();
      const UniqueID muid = (must_epoch_op == NULL) ? 0 :
        must_epoch_op->get_unique_op_id();
      // Record our barriers for future uses
      for (std::vector<PhaseBarrier>::const_iterator ait =
            barriers.begin(); ait != barriers.end(); ait++)
      {
        // Figure out the generic barrier ID
        const ApBarrier barrier = previous_gen ? ait->phase_barrier :
          Runtime::get_previous_phase(ait->phase_barrier);
        const size_t barrier_gen = Realm::ID(barrier.id).event_generation();
        const size_t barrier_name = barrier.id - barrier_gen;
        std::list<BarrierContribution> &previous =
          barrier_contributions[barrier_name];
        for (std::list<BarrierContribution>::iterator it =
              previous.begin(); it != previous.end(); /*nothing*/)
        {
          // skip anything with a larger barrier generation
          if (it->bargen >= barrier_gen)
          {
            it++;
            continue;
          }
          // If must epoch and same uid then skip it
          if ((muid > 0) && (muid == it->muid))
          {
            it++;
            continue;
          }
#ifdef LEGION_SPY
          // No pruning for Legion Spy
          op->register_dependence(it->op, it->gen);
          LegionSpy::log_mapping_dependence(get_unique_id(), 
              it->uid, 0, uid, 0, TRUE_DEPENDENCE);
          it++;
#else
          if (op->register_dependence(it->op, it->gen))
            it++;
          else
            it = previous.erase(it);
#endif        
        }
        previous.push_back(BarrierContribution(op,gen,uid,muid,barrier_gen));
      }
    }

    //--------------------------------------------------------------------------
    Future InnerContext::get_dynamic_collective_result(DynamicCollective dc,
                                                       Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
#ifdef DEBUG_LEGION
      log_run.debug("Get dynamic collective result in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      DynamicCollectiveOp *collective =
        runtime->get_available_dynamic_collective_op();
      Future result = collective->initialize(this, dc, provenance);
      add_to_dependence_queue(collective);
      return result;
    }

    //--------------------------------------------------------------------------
    DynamicCollective InnerContext::advance_dynamic_collective(
                                                           DynamicCollective dc)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      DynamicCollective result = dc;
      Runtime::advance_barrier(result);
#ifdef LEGION_SPY
      LegionSpy::log_event_dependence(dc.phase_barrier, result.phase_barrier);
#endif
      return result;
    }

    //--------------------------------------------------------------------------
    TaskPriority InnerContext::get_current_priority(void) const
    //--------------------------------------------------------------------------
    {
      return current_priority;
    }

    //--------------------------------------------------------------------------
    void InnerContext::set_current_priority(TaskPriority priority)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(mutable_priority);
      assert(realm_done_event.exists());
#endif
      // This can be racy but that is the mappers problem
      realm_done_event.set_operation_priority(priority);
      current_priority = priority;
    }

    //--------------------------------------------------------------------------
    void InnerContext::configure_context(MapperManager *mapper, TaskPriority p)
    //--------------------------------------------------------------------------
    {
      mapper->invoke_configure_context(owner_task, context_configuration);
      // Do a little bit of checking on the output.  Make
      // sure that we only set one of the two cases so we
      // are counting by frames or by outstanding tasks.
      if ((context_configuration.min_tasks_to_schedule == 0) && 
          (context_configuration.min_frames_to_schedule == 0))
        REPORT_LEGION_ERROR(ERROR_INVALID_MAPPER_OUTPUT,
                      "Invalid mapper output from call 'configure_context' "
                      "on mapper %s. One of 'min_tasks_to_schedule' and "
                      "'min_frames_to_schedule' must be non-zero for task "
                      "%s (ID %lld)", mapper->get_mapper_name(),
                      get_task_name(), get_unique_id())
      // Hysteresis percentage is an unsigned so can't be less than 0
      if (context_configuration.hysteresis_percentage > 100)
        REPORT_LEGION_ERROR(ERROR_INVALID_MAPPER_OUTPUT,
                      "Invalid mapper output from call 'configure_context' "
                      "on mapper %s. The 'hysteresis_percentage' %d is not "
                      "a value between 0 and 100 for task %s (ID %lld)",
                      mapper->get_mapper_name(), 
                      context_configuration.hysteresis_percentage,
                      get_task_name(), get_unique_id())
      if (context_configuration.meta_task_vector_width == 0)
        REPORT_LEGION_ERROR(ERROR_INVALID_MAPPER_OUTPUT,
                      "Invalid mapper output from call 'configure context' "
                      "on mapper %s for task %s (ID %lld). The "
                      "'meta_task_vector_width' must be a non-zero value.",
                      mapper->get_mapper_name(),
                      get_task_name(), get_unique_id())
      if (context_configuration.max_templates_per_trace == 0)
        REPORT_LEGION_ERROR(ERROR_INVALID_MAPPER_OUTPUT,
                      "Invalid mapper output from call 'configure context' "
                      "on mapper %s for task %s (ID %lld). The "
                      "'max_templates_per_trace' must be a non-zero value.",
                      mapper->get_mapper_name(),
                      get_task_name(), get_unique_id())

      // If we're counting by frames set min_tasks_to_schedule to zero
      if (context_configuration.min_frames_to_schedule > 0)
        context_configuration.min_tasks_to_schedule = 0;
      // otherwise we know min_frames_to_schedule is zero

      // See if we permit priority mutations from child operation mapppers
      mutable_priority = context_configuration.mutable_priority;
      current_priority = p;
    }

    //--------------------------------------------------------------------------
    void InnerContext::initialize_region_tree_contexts(
                      const std::vector<RegionRequirement> &clone_requirements,
                      const LegionVector<VersionInfo> &version_infos,
                      const std::vector<ApUserEvent> &unmap_events)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, INITIALIZE_REGION_TREE_CONTEXTS_CALL);
      // Save to cast to single task here because this will never
      // happen during inlining of index space tasks
#ifdef DEBUG_LEGION
      assert(owner_task != NULL);
#endif
      const std::deque<InstanceSet> &physical_instances = 
        owner_task->get_physical_instances();
      const std::vector<bool> &no_access_regions = 
        owner_task->get_no_access_regions();
#ifdef DEBUG_LEGION
      assert(regions.size() <= physical_instances.size());
      assert(regions.size() <= virtual_mapped.size());
      assert(regions.size() <= no_access_regions.size());
#endif
      // Initialize all of the logical contexts no matter what
      //
      // For all of the physical contexts that were mapped, initialize them
      // with a specified reference to the current instance, otherwise
      // they were a virtual reference and we can ignore it.
      const ShardID local_shard = get_shard_id();
      const UniqueID context_uid = get_unique_id();
      std::map<PhysicalManager*,IndividualView*> top_views;
      for (unsigned idx1 = 0; idx1 < regions.size(); idx1++)
      {
#ifdef DEBUG_LEGION
        // this better be true for single tasks
        assert(regions[idx1].handle_type == LEGION_SINGULAR_PROJECTION);
#endif
        // If this is a NO_ACCESS or had no privilege fields we can skip this
        if (no_access_regions[idx1])
          continue;
        const RegionRequirement &req = clone_requirements[idx1];
        const RegionUsage usage(req);
#ifdef DEBUG_LEGION
        assert(req.handle_type == LEGION_SINGULAR_PROJECTION);
#endif
        // Make our equivalence set kd tree for look-ups
        RegionNode *region_node = runtime->forest->get_node(req.region);
        EqKDTree *tree = 
          region_node->row_source->create_equivalence_set_kd_tree(
                                                get_total_shards());
        equivalence_set_trees.emplace(idx1, EqKDRoot(tree));
        const FieldMask user_mask = 
          region_node->column_source->get_field_mask(req.privilege_fields);
        // For virtual mappings, there are two approaches here
        // 1. For read-write privileges we can do copy-in/copy-out
        // on the equivalence sets since we know that we're the 
        // only one that is going to be mutating them, this will
        // allow us to do things like refinements for them
        // 2. For any other kind of privilege, we need to make sure
        // that we see updates from other tasks potentially running
        // and mapping in parallel on the same equivalence sets, so
        // we aren't going to make our own equivalence set
        if (virtual_mapped[idx1] && !IS_WRITE(usage))
        {
          // Handle the case where we have a virtual mapping for a
          // non-write privilege and therefore we're just going to
          // seed our state with the equivalence sets and not allow
          // them to ever be refined in this context
#ifdef DEBUG_LEGION
          assert(idx1 < version_infos.size());
#endif
          const FieldMaskSet<EquivalenceSet> &eq_sets =
            version_infos[idx1].get_equivalence_sets();
          for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                eq_sets.begin(); it != eq_sets.end(); it++)
            it->first->set_expr->initialize_equivalence_set_kd_tree(
                tree, it->first, it->second, local_shard, true/*current*/);
          // In this case we also tell the region tree that this is
          // already refined so that no read or reduce refinements can
          // be performed in this context
          region_node->initialize_refined_fields(tree_context, user_mask); 
          continue;
        }
        // Only need to initialize the context if this is
        // not a leaf and it wasn't virtual mapped
        if (!virtual_mapped[idx1])
        {
          EquivalenceSet *eq_set = create_initial_equivalence_set(idx1, req); 
          const InstanceSet &sources = physical_instances[idx1];
#ifdef DEBUG_LEGION
          assert(!sources.empty());
#endif
          // Find or make views for each of our instances and then 
          // add initial users for each of them
          std::vector<IndividualView*> corresponding(sources.size());
          // Build our set of corresponding views
          for (unsigned idx2 = 0; idx2 < sources.size(); idx2++)
          {
            const InstanceRef &src_ref = sources[idx2];
            PhysicalManager *manager = src_ref.get_physical_manager();
            const FieldMask &view_mask = src_ref.get_valid_fields();
#ifdef DEBUG_LEGION
            assert(!(view_mask - user_mask)); // should be dominated
#endif
            // Check to see if the view exists yet or not
            std::map<PhysicalManager*,IndividualView*>::const_iterator 
              finder = top_views.find(manager);
            if (finder == top_views.end())
            {
              IndividualView *new_view =
                create_instance_top_view(manager, runtime->address_space);
              top_views[manager] = new_view;
              corresponding[idx2] = new_view;
              // Record the initial user for the instance
              new_view->add_initial_user(unmap_events[idx1], usage, view_mask,
                                  region_node->row_source, context_uid, idx1);
            }
            else
            {
              corresponding[idx2] = finder->second;
              // Record the initial user for the instance
              finder->second->add_initial_user(unmap_events[idx1], usage,
                   view_mask, region_node->row_source, context_uid, idx1);
            }
          }
          // Only need to do the initialization if we're the logical owner
          if (eq_set->is_logical_owner())
          {
            // The parent region requirement is restricted if it is
            // simultaneous or it is reduce-only. Simultaneous is 
            // restricted because of normal Legion coherence semantics.
            // Reduce-only is restricted because we don't issue close
            // operations at the end of a context for reduce-only cases
            // right now so by making it restricted things are eagerly
            // flushed out to the parent task's instance.
            const bool restricted = 
              IS_SIMULT(regions[idx1]) || IS_REDUCE(regions[idx1]);
            eq_set->initialize_set(usage, user_mask, restricted, sources,
                                   corresponding);
          }
          region_node->row_source->initialize_equivalence_set_kd_tree(tree,
              eq_set, user_mask, local_shard, false/*current*/);
          // Each equivalence set here comes with a reference that we
          // need to remove after we've registered it
          if (eq_set->remove_base_gc_ref(CONTEXT_REF))
            assert(false); // should never hit this
        }
        else
        {
#ifdef DEBUG_LEGION
          assert(IS_WRITE(usage));
          assert(idx1 < version_infos.size());
#endif
          // virtual mapping case, just clone all the equivalence sets
          // into our tree but put them in the previous so we'll copy 
          // from them as we make refinements
          const FieldMaskSet<EquivalenceSet> &eq_sets = 
            version_infos[idx1].get_equivalence_sets();
          for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                eq_sets.begin(); it != eq_sets.end(); it++)
            it->first->set_expr->initialize_equivalence_set_kd_tree(
                tree, it->first, it->second, local_shard, false/*current*/);
        }
      }
    }

    //--------------------------------------------------------------------------
    EquivalenceSet* InnerContext::create_initial_equivalence_set(unsigned idx,
                                                   const RegionRequirement &req)
    //--------------------------------------------------------------------------
    {
      // This is the normal equivalence set creation pathway for single tasks
      IndexSpaceNode *node = runtime->forest->get_node(req.region.index_space);
      EquivalenceSet *result =
        new EquivalenceSet(runtime, runtime->get_available_distributed_id(),
            runtime->address_space, node, req.region.get_tree_id(),
            find_parent_physical_context(idx), true/*register now*/);
      // Add a context ref that will be removed after this is registered
      result->add_base_gc_ref(CONTEXT_REF);
      return result;
    }

    //--------------------------------------------------------------------------
    void InnerContext::invalidate_region_tree_contexts(
                       const bool is_top_level_task, std::set<RtEvent> &applied,
                       const ShardMapping *mapping, ShardID source_shard)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, INVALIDATE_REGION_TREE_CONTEXTS_CALL);
      // Invalidate all our region contexts
      for (unsigned idx = 0; idx < regions.size(); idx++)
      {
        if (IS_NO_ACCESS(regions[idx]))
          continue;
        RegionNode *node = runtime->forest->get_node(regions[idx].region);
        runtime->forest->invalidate_current_context(tree_context, 
            regions[idx], false/*filter specific fields*/);
        std::map<unsigned,EqKDRoot>::iterator finder = 
          equivalence_set_trees.find(idx);
        if (finder == equivalence_set_trees.end())
          continue;
        // State is copied out by the virtual close ops if this is a
        // virtual mapped region so we invalidate like normal now
        const FieldMask close_mask = 
          node->column_source->get_field_mask(regions[idx].privilege_fields);
        std::vector<RtEvent> applied_events;
        node->row_source->invalidate_equivalence_set_kd_tree(
            finder->second.tree, finder->second.lock,
            close_mask, applied_events, false/*move to previous*/);
        equivalence_set_trees.erase(finder);
        if (!applied_events.empty())
          applied.insert(applied_events.begin(), applied_events.end());
      }
      if (!created_requirements.empty())
        invalidate_created_requirement_contexts(is_top_level_task, applied,
                                                mapping, source_shard);
    }

    //--------------------------------------------------------------------------
    void InnerContext::invalidate_created_requirement_contexts(
        const bool is_top, std::set<RtEvent> &applied_events,
        const ShardMapping *shard_mapping, ShardID source_shard)
    //--------------------------------------------------------------------------
    {
      std::set<LogicalRegion> invalidated_regions;
      std::map<LogicalRegion,EqKDTree*> return_regions;
      const FieldMask all_ones_mask(LEGION_FIELD_MASK_FIELD_ALL_ONES);
      for (std::map<unsigned,RegionRequirement>::const_iterator it = 
            created_requirements.begin(); it != 
            created_requirements.end(); it++)
      {
#ifdef DEBUG_LEGION
        assert(returnable_privileges.find(it->first) !=
                returnable_privileges.end());
#endif
        // Always invalidate the logical contexts
        RegionNode *node = runtime->forest->get_node(it->second.region);
        if (invalidated_regions.find(it->second.region) == 
            invalidated_regions.end())
        {
          // Little tricky here, this is safe to invaliate the whole
          // tree even if we only had privileges on a field because
          // if we had privileges on the whole region in this context
          // it would have merged the created_requirement and we wouldn't
          // have a non returnable privilege requirement in this context
          runtime->forest->invalidate_current_context(tree_context,
              it->second, false/*filter specific fields*/);
          invalidated_regions.insert(it->second.region);
        }
        // See if we're a returnable privilege or not
        std::map<unsigned,EqKDRoot>::iterator finder = 
          equivalence_set_trees.find(it->first);
        if (returnable_privileges[it->first] && !is_top)
        {
#ifdef DEBUG_LEGION
          assert(return_regions.find(it->second.region) == 
                  return_regions.end());
#endif
          if (finder != equivalence_set_trees.end())
          {
            finder->second.tree->add_reference();
            return_regions[it->second.region] = finder->second.tree;
            equivalence_set_trees.erase(finder);
          }
        }
        else if (finder != equivalence_set_trees.end())
        {
          // Not returning so invalidate the full thing
#ifdef DEBUG_LEGION
          assert(return_regions.find(it->second.region) == 
                  return_regions.end());
#endif
          std::vector<RtEvent> applied;
          node->row_source->invalidate_equivalence_set_kd_tree(
                finder->second.tree, finder->second.lock,
                all_ones_mask, applied, false/*move to previous*/);
          if (!applied.empty())
            applied_events.insert(applied.begin(), applied.end());
          equivalence_set_trees.erase(finder);
        }
      }
      if (!return_regions.empty())
      {
        std::vector<RegionNode*> created_nodes;
        created_nodes.reserve(return_regions.size());
        std::vector<EqKDTree*> created_trees;
        created_trees.reserve(return_regions.size());
        for (std::map<LogicalRegion,EqKDTree*>::const_iterator it =
              return_regions.begin(); it != return_regions.end(); it++)
        {
          created_nodes.push_back(runtime->forest->get_node(it->first));
          created_trees.push_back(it->second);
        }
        InnerContext *parent_ctx = find_parent_context();   
        parent_ctx->receive_created_region_contexts(
            created_nodes, created_trees, applied_events, 
            shard_mapping, source_shard);
        for (std::vector<EqKDTree*>::const_iterator it =
              created_trees.begin(); it != created_trees.end(); it++)
          if (((*it) != NULL) && (*it)->remove_reference())
            delete (*it);
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::receive_created_region_contexts(
                              const std::vector<RegionNode*> &created_nodes,
                              const std::vector<EqKDTree*> &created_trees,
                              std::set<RtEvent> &applied_events,
                              const ShardMapping *mapping, ShardID source_shard)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(mapping == NULL);
      assert(created_nodes.size() == created_trees.size());
#endif
      AutoLock priv_lock(privilege_lock);
      for (unsigned idx = 0; idx < created_trees.size(); idx++)
      {
        unsigned index = add_created_region(created_nodes[idx]->handle,
                            false/*task local*/, false/*output region*/);
        if (created_trees[idx] == NULL)
          continue;
        // Check to see if we're the first one or whether we're merging
        std::map<unsigned,EqKDRoot>::const_iterator finder =
          equivalence_set_trees.find(index);
        if (finder != equivalence_set_trees.end())
        {
          // This happens when we're merging multiple trees coming back
          // from a sub-task that was control replicated, so we extract
          // the equivalence sets and add them into our tree
          FieldMaskSet<EquivalenceSet> eq_sets;
          created_trees[idx]->find_local_equivalence_sets(eq_sets,source_shard);
          const ShardID local_shard = get_shard_id();
          for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                eq_sets.begin(); it != eq_sets.end(); it++)
            it->first->set_expr->initialize_equivalence_set_kd_tree(
                finder->second.tree,it->first, it->second, 
                local_shard, true/*current*/);
        }
        else
        {
          finder = equivalence_set_trees.emplace(index, 
              EqKDRoot(created_trees[idx])).first;
          // Filter all the current equivalence sets on to the previous
          const FieldMask all_ones_mask(LEGION_FIELD_MASK_FIELD_ALL_ONES);
          std::vector<RtEvent> applied;
          created_nodes[idx]->row_source->invalidate_equivalence_set_kd_tree(
              finder->second.tree, finder->second.lock, all_ones_mask, applied,
              true/*move to previous*/);
          if (!applied.empty())
            applied_events.insert(applied.begin(), applied.end());
        }
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::invalidate_region_tree_context(
                 const RegionRequirement &req, unsigned req_index,
                 std::set<RtEvent> &applied_events, bool filter_specific_fields)
    //--------------------------------------------------------------------------
    {
      if (!req.privilege_fields.empty())
      {
        LocalLock *tree_lock = NULL;
        EqKDTree *tree = find_equivalence_set_kd_tree(req_index,
            tree_lock, true/*null if doesn't exist*/);
        if (tree != NULL)
        {
          std::vector<RtEvent> applied;
          RegionNode *node = runtime->forest->get_node(req.region);
          const FieldMask invalidate_mask =
            node->column_source->get_field_mask(req.privilege_fields);
          node->row_source->invalidate_equivalence_set_kd_tree(tree, tree_lock,
              invalidate_mask, applied, false/*move to previous*/);
          if (!applied.empty())
            applied_events.insert(applied.begin(), applied.end());
        }
      }
      // Check to see if we should actually invalidate this tree
      if (!filter_specific_fields)
      {
        // Need the lock before doing this invalidation in case the 
        // equivalence set trees data structure resizes
        AutoLock priv_lock(privilege_lock);
        equivalence_set_trees.erase(req_index);
        // Also need to remove the returnable privileges information
        // and any created region requirements
        returnable_privileges.erase(req_index);
        created_requirements.erase(req_index);
      }
    }

    //--------------------------------------------------------------------------
    ProjectionSummary* InnerContext::construct_projection_summary(Operation *op,
            unsigned index, const RegionRequirement &req, 
            LogicalState *state, const ProjectionInfo &proj_info) 
    //--------------------------------------------------------------------------
    {
      ProjectionNode *tree = proj_info.projection->construct_projection_tree(
          op, index, req, 0/*local shard*/, state->owner, proj_info);
      return new ProjectionSummary(proj_info, tree, op, index, req, state);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::has_interfering_shards(ProjectionSummary *one,
                                              ProjectionSummary *two)
    //--------------------------------------------------------------------------
    {
      return one->get_tree()->interferes(two->get_tree(), 0/*local shard*/);
    }

    //--------------------------------------------------------------------------
    bool InnerContext::match_timeouts(std::vector<LogicalUser*> &timeouts,
                                      std::vector<LogicalUser*> &to_delete,
                                      TimeoutMatchExchange *&exchange)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(exchange == NULL);
      assert(to_delete.empty());
#endif
      to_delete.swap(timeouts);
      return false;
    }

    //--------------------------------------------------------------------------
    void InnerContext::convert_individual_views(
                                   const std::vector<PhysicalManager*> &sources,
                                   std::vector<IndividualView*> &source_views,
                                   CollectiveMapping *mapping)
    //--------------------------------------------------------------------------
    {
      source_views.resize(sources.size());
      std::vector<unsigned> still_needed;
      {
        AutoLock inst_lock(instance_view_lock,1,false/*exclusive*/); 
        for (unsigned idx = 0; idx < sources.size(); idx++)
        {
          // See if we can find it
          PhysicalManager *manager = sources[idx];
          std::map<PhysicalManager*,IndividualView*>::const_iterator finder = 
            instance_top_views.find(manager);     
          if (finder != instance_top_views.end())
          {
#ifdef DEBUG_LEGION
            // A little sanity check that the mappings match, if they don't
            // then that will lead to bigger problems
            assert((mapping == NULL) || 
                (mapping == finder->second->collective_mapping) ||
                (*mapping == *(finder->second->collective_mapping)));
#endif
            source_views[idx] = finder->second;
          }
          else
            still_needed.push_back(idx);
        }
      }
      if (!still_needed.empty())
      {
        const AddressSpaceID local_space = runtime->address_space;
        for (std::vector<unsigned>::const_iterator it = 
              still_needed.begin(); it != still_needed.end(); it++)
        {
          PhysicalManager *manager = sources[*it];
          source_views[*it] =
            create_instance_top_view(manager, local_space, mapping);
        }
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::convert_individual_views(const InstanceSet &sources,
                                     std::vector<IndividualView*> &source_views,
                                     CollectiveMapping *mapping)
    //--------------------------------------------------------------------------
    {
      source_views.resize(sources.size());
      std::vector<unsigned> still_needed;
      {
        AutoLock inst_lock(instance_view_lock,1,false/*exclusive*/); 
        for (unsigned idx = 0; idx < sources.size(); idx++)
        {
          // See if we can find it
          PhysicalManager *manager = sources[idx].get_physical_manager();
          std::map<PhysicalManager*,IndividualView*>::const_iterator finder = 
            instance_top_views.find(manager);     
          if (finder != instance_top_views.end())
          {
#ifdef DEBUG_LEGION
            // A little sanity check that the mappings match, if they don't
            // then that will lead to bigger problems
            assert((mapping == NULL) || 
                (mapping == finder->second->collective_mapping) ||
                (*mapping == *(finder->second->collective_mapping)));
#endif
            source_views[idx] = finder->second;
          }
          else
            still_needed.push_back(idx);
        }
      }
      if (!still_needed.empty())
      {
        const AddressSpaceID local_space = runtime->address_space;
        for (std::vector<unsigned>::const_iterator it = 
              still_needed.begin(); it != still_needed.end(); it++)
        {
          PhysicalManager *manager = sources[*it].get_physical_manager();
          source_views[*it] =
            create_instance_top_view(manager, local_space, mapping);
        }
      }
      // No need to invalidate the collective views here, we know that
      // source views are never going to be registered with the physical
      // analysis state so we can safely give out the individual views
    }

    //--------------------------------------------------------------------------
    void InnerContext::send_context(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      update_remote_instances(target);
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        pack_remote_context(rez, target);
      }
      runtime->send_remote_context_response(target, rez);
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_compute_equivalence_sets_request(
                   Deserializer &derez, Runtime *runtime, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID context_did;
      derez.deserialize(context_did);
      // This should always be coming back to the owner node so there's no
      // need to defer this is at should always be here
      InnerContext *local_ctx = static_cast<InnerContext*>(
        runtime->find_distributed_collectable(context_did));
      std::vector<EqSetTracker*> targets(1);
      derez.deserialize(targets.back());
      IndexSpaceExpression *expr = 
        IndexSpaceExpression::unpack_expression(derez, runtime->forest, source);
      FieldMask mask;
      derez.deserialize(mask);
      unsigned req_index;
      derez.deserialize(req_index);
      RtUserEvent ready_event;
      derez.deserialize(ready_event);
      std::vector<AddressSpaceID> target_spaces(1, source);

      const RtEvent done = local_ctx->compute_equivalence_sets(req_index,
          targets, target_spaces, source, expr, mask);
      Runtime::trigger_event(ready_event, done);
    }

    //--------------------------------------------------------------------------
    void InnerContext::convert_analysis_views(const InstanceSet &targets,
                        LegionVector<FieldMaskSet<InstanceView> > &target_views)
    //--------------------------------------------------------------------------
    {
      target_views.resize(targets.size());
      std::vector<unsigned> still_needed;
      {
        AutoLock inst_lock(instance_view_lock,1,false/*exclusive*/); 
        for (unsigned idx = 0; idx < targets.size(); idx++)
        {
          // See if we can find it
          const InstanceRef &ref = targets[idx];
          PhysicalManager *manager = ref.get_physical_manager();
          std::map<PhysicalManager*,IndividualView*>::const_iterator finder =
            instance_top_views.find(manager);     
          if (finder != instance_top_views.end())
            target_views[idx].insert(finder->second, ref.get_valid_fields());
          else
            still_needed.push_back(idx);
        }
      }
      if (!still_needed.empty())
      {
        const AddressSpaceID local_space = runtime->address_space;
        for (std::vector<unsigned>::const_iterator it = 
              still_needed.begin(); it != still_needed.end(); it++)
        {
          const InstanceRef &ref = targets[*it];
          PhysicalManager *manager = ref.get_physical_manager();
          target_views[*it].insert(create_instance_top_view(manager,
                                   local_space), ref.get_valid_fields());
        }
      }
    }

    //--------------------------------------------------------------------------
    InnerContext::CollectiveResult* 
      InnerContext::find_or_create_collective_view(RegionTreeID tid, 
          const std::vector<DistributedID> &instances, RtEvent &ready)
    //--------------------------------------------------------------------------
    {
      // Just ignore the ready event since the result will be ready now
      return find_or_create_collective_view(tid, instances);
    }

    //--------------------------------------------------------------------------
    InnerContext::CollectiveResult*
      InnerContext::find_or_create_collective_view(RegionTreeID tid,
                                    const std::vector<DistributedID> &instances)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(instances.size() > 1);
#endif
      AutoLock c_lock(collective_lock);
      std::vector<CollectiveResult*> &collectives = collective_results[tid];
      for (std::vector<CollectiveResult*>::const_iterator it =
            collectives.begin(); it != collectives.end(); it++)
      {
        if ((*it)->matches(instances))
        {
          (*it)->add_reference();
          return (*it);
        }
      }
      // If we get here then we need to make it
      std::vector<AddressSpaceID> spaces(instances.size());
      for (unsigned idx = 0; idx < spaces.size(); idx++)
        spaces[idx] = runtime->determine_owner(instances[idx]);
      std::sort(spaces.begin(), spaces.end());
      std::vector<AddressSpaceID>::iterator end = 
        std::unique(spaces.begin(), spaces.end());
      spaces.resize(std::distance(spaces.begin(),end));
      CollectiveMapping *mapping = 
        new CollectiveMapping(spaces, runtime->legion_collective_radix);
      mapping->add_reference();
      DistributedID collective_did = mapping->contains(runtime->address_space) ?
        runtime->get_available_distributed_id() :
        runtime->get_remote_distributed_id(
            mapping->find_nearest(runtime->address_space));
      const RtEvent ready =
        create_collective_view(did, collective_did, mapping, instances);
      // This is a bit subtle, we need to encode the right kind of the 
      // distributed ID (e.g. whether it is just replicated or allreduce)
      // The way we determine that is by looking at the distributed IDs of
      // the instances which also encode whether they are for reductions
      // instances or not
      const bool redop = InstanceManager::is_reduction_did(instances.back());
      if (redop)
        collective_did = LogicalView::encode_allreduce_did(collective_did);
      else
        collective_did = LogicalView::encode_replicated_did(collective_did);
      CollectiveResult *result = 
        new CollectiveResult(instances, collective_did, ready);
      result->add_reference(2/*one for us and one for result*/);
      collectives.push_back(result);
      if (mapping->remove_reference())
        delete mapping;     
      return result;
    }

    //--------------------------------------------------------------------------
    RtEvent InnerContext::create_collective_view(DistributedID creator_did,
                      DistributedID collective_did, CollectiveMapping *mapping,
                      const std::vector<DistributedID> &individual_dids)
    //--------------------------------------------------------------------------
    {
      const AddressSpaceID owner_space = 
        runtime->determine_owner(collective_did);
      if (mapping->contains(runtime->address_space))
      {
        // Send the result out to any children and then make our local copy
        std::vector<AddressSpaceID> children;
        mapping->get_children(owner_space, runtime->address_space, children);
        std::vector<RtEvent> done_events(children.size());
        for (unsigned idx = 0; idx < children.size(); idx++)
        {
          const RtUserEvent done = Runtime::create_rt_user_event();
          Serializer rez;
          {
            RezCheck z(rez);
            pack_inner_context(rez);
            rez.serialize(creator_did);
            rez.serialize(collective_did);
            mapping->pack(rez);
            rez.serialize<size_t>(individual_dids.size());
            for (std::vector<DistributedID>::const_iterator it =
                  individual_dids.begin(); it != individual_dids.end(); it++)
              rez.serialize(*it);
            rez.serialize(done);
          }
          runtime->send_collective_view_creation(children[idx], rez);
          done_events[idx] = done;
        }
        std::vector<IndividualView*> local_views;
        for (std::vector<DistributedID>::const_iterator it =
              individual_dids.begin(); it != individual_dids.end(); it++)
        {
          if (runtime->determine_owner(*it) != runtime->address_space)
            continue;
          // Should always be able to find it since we're on the owner node
          PhysicalManager *manager = static_cast<PhysicalManager*>(
              runtime->find_distributed_collectable(*it));
          local_views.push_back(
              create_instance_top_view(manager, runtime->address_space));
        }
#ifdef DEBUG_LEGION
        assert(!local_views.empty());
#endif
        ReductionOpID redop = local_views.back()->get_redop();
        CollectiveView *view = NULL;
        if (redop > 0)
          view = new AllreduceView(runtime, collective_did, creator_did,
           local_views, individual_dids, false/*register now*/, mapping, redop);
        else
          view = new ReplicatedView(runtime, collective_did, creator_did,
              local_views, individual_dids, false/*register now*/, mapping);
        if (view->is_owner())
          view->add_nested_gc_ref(creator_did);
        view->register_with_runtime();
        if (!done_events.empty())
          return Runtime::merge_events(done_events);
        else
          return RtEvent::NO_RT_EVENT;
      }
      else
      {
        // Send this to the owner node to start the broadcast tree
        const RtUserEvent done = Runtime::create_rt_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          pack_inner_context(rez);
          rez.serialize(creator_did);
          rez.serialize(collective_did);
          mapping->pack(rez);
          rez.serialize<size_t>(individual_dids.size());
          for (std::vector<DistributedID>::const_iterator it =
                individual_dids.begin(); it != individual_dids.end(); it++)
            rez.serialize(*it);
          rez.serialize(done);
        }
        runtime->send_collective_view_creation(owner_space, rez);
        return done;
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_create_collective_view(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      InnerContext *context = unpack_inner_context(derez, runtime);
      DistributedID creator_did, collective_did;
      derez.deserialize(creator_did);
      derez.deserialize(collective_did);
      size_t num_spaces;
      derez.deserialize(num_spaces);
      CollectiveMapping *mapping = new CollectiveMapping(derez, num_spaces);
      mapping->add_reference();
      size_t num_dids;
      derez.deserialize(num_dids);
      std::vector<DistributedID> individual_dids(num_dids);
      for (unsigned idx = 0; idx < num_dids; idx++)
        derez.deserialize(individual_dids[idx]);
      RtUserEvent done;
      derez.deserialize(done);
      Runtime::trigger_event(done, context->create_collective_view(
            creator_did, collective_did, mapping, individual_dids));
      if (mapping->remove_reference())
        delete mapping;
    }

    //--------------------------------------------------------------------------
    void InnerContext::notify_collective_deletion(RegionTreeID tid,
                                                  DistributedID collective_did)
    //--------------------------------------------------------------------------
    {
      bool found = false;
      {
        AutoLock c_lock(collective_lock); 
        LegionMap<RegionTreeID,std::vector<CollectiveResult*> >::iterator 
          finder = collective_results.find(tid);
        if (finder == collective_results.end())
          return;
        for (std::vector<CollectiveResult*>::iterator it =
              finder->second.begin(); it != finder->second.end(); it++)
        {
          if ((*it)->collective_did != collective_did)
            continue;
          found = true; 
          delete (*it);
          finder->second.erase(it);
          break;
        }
      }
      if (found)
        release_collective_view(runtime, did, collective_did);
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_delete_collective_view(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID collective_did;
      derez.deserialize(collective_did);
      RegionTreeID tid;
      derez.deserialize(tid);
      DistributedID context_did;
      derez.deserialize(context_did);
      // The context might already be deleted so do a weak find
      InnerContext *context = static_cast<InnerContext*>(
          runtime->weak_find_distributed_collectable(context_did));
      if (context == NULL)
        return;
      context->notify_collective_deletion(tid, collective_did);
      if (context->remove_base_resource_ref(RUNTIME_REF))
        delete context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::release_collective_view(Runtime *runtime,
                        DistributedID context_did, DistributedID collective_did)
    //--------------------------------------------------------------------------
    {
      const AddressSpaceID owner = runtime->determine_owner(collective_did);
      if (owner != runtime->address_space)
      {
        Serializer rez;
        rez.serialize(context_did);
        rez.serialize(collective_did);
        runtime->send_collective_view_release(owner, rez);
      }
      else
      {
        // Better be able to find it since we know that we're still holding
        // a resource reference to it
        CollectiveView *view = static_cast<CollectiveView*>(
            runtime->find_distributed_collectable(collective_did));
        // Now remove the resource reference that was added by the 
        // constructor for the collective view
        if (view->remove_nested_gc_ref(context_did))
          delete view;
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_release_collective_view(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DistributedID context_did, collective_did;
      derez.deserialize(context_did);
      derez.deserialize(collective_did);
      release_collective_view(runtime, context_did, collective_did);
    }

    //--------------------------------------------------------------------------
    IndividualView* InnerContext::create_instance_top_view(
                        PhysicalManager *manager, AddressSpaceID request_source,
                        CollectiveMapping *mapping)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, CREATE_INSTANCE_TOP_VIEW_CALL);
      // Check to see if we already have the 
      // instance, if we do, return it, otherwise make it and save it
      RtEvent wait_on;
      {
        AutoLock inst_lock(instance_view_lock);
        std::map<PhysicalManager*,IndividualView*>::const_iterator finder = 
          instance_top_views.find(manager);
        if (finder != instance_top_views.end())
          // We've already got the view, so we are done
          return finder->second;
        // See if someone else is already making it
        std::map<PhysicalManager*,RtUserEvent>::iterator pending_finder =
          pending_top_views.find(manager);
        if (pending_finder == pending_top_views.end())
          // mark that we are making it
          pending_top_views[manager] = RtUserEvent::NO_RT_USER_EVENT;
        else
        {
          // See if we are the first one to follow
          if (!pending_finder->second.exists())
            pending_finder->second = Runtime::create_rt_user_event();
          wait_on = pending_finder->second;
        }
      }
      if (wait_on.exists())
      {
        // Someone else is making it so we just have to wait for it
        wait_on.wait();
        // Retake the lock and read out the result
        AutoLock inst_lock(instance_view_lock, 1, false/*exclusive*/);
        std::map<PhysicalManager*,IndividualView*>::const_iterator finder = 
            instance_top_views.find(manager);
#ifdef DEBUG_LEGION
        assert(finder != instance_top_views.end());
#endif
        return finder->second;
      }
      IndividualView *result =
       manager->find_or_create_instance_top_view(this, request_source, mapping);
      // Use a gc reference here to ensure that the view is remains alive 
      // everywhere until the instance is deleted or the context ends
      result->add_nested_gc_ref(did);
      // Record the result and trigger any user event to signal that the
      // view is ready
      RtUserEvent to_trigger;
      {
        AutoLock inst_lock(instance_view_lock);
#ifdef DEBUG_LEGION
        assert(instance_top_views.find(manager) == 
                instance_top_views.end());
#endif
        instance_top_views[manager] = result;
        std::map<PhysicalManager*,RtUserEvent>::iterator pending_finder =
          pending_top_views.find(manager);
#ifdef DEBUG_LEGION
        assert(pending_finder != pending_top_views.end());
#endif
        to_trigger = pending_finder->second;
        pending_top_views.erase(pending_finder);
      }
      if (to_trigger.exists())
        Runtime::trigger_event(to_trigger);
      return result;
    }

    //--------------------------------------------------------------------------
    void InnerContext::record_fill_view_creation(FillView *view)
    //--------------------------------------------------------------------------
    {
#ifndef LEGION_SPY
      view->add_nested_valid_ref(did);
      AutoLock f_lock(fill_view_lock);
      value_fill_view_cache.push_back(view);
      if (value_fill_view_cache.size() > MAX_FILL_VIEW_CACHE_SIZE)
      {
        FillView *oldest = value_fill_view_cache.back();
        value_fill_view_cache.pop_back();
        if (oldest->remove_nested_valid_ref(did))
          delete oldest;
      }
#endif
    }

    //--------------------------------------------------------------------------
    void InnerContext::record_fill_view_creation(DistributedID future_did,
                                                 FillView *view)
    //--------------------------------------------------------------------------
    {
#ifndef LEGION_SPY
      view->add_nested_valid_ref(did);
      AutoLock f_lock(fill_view_lock);
      future_fill_view_cache.push_front(std::make_pair(view, future_did));
      if (future_fill_view_cache.size() > MAX_FILL_VIEW_CACHE_SIZE)
      {
        FillView *oldest = future_fill_view_cache.back().first;
        future_fill_view_cache.pop_back();
        if (oldest->remove_nested_valid_ref(did))
          delete oldest;
      }
#endif
    }

    //--------------------------------------------------------------------------
    FillView* InnerContext::find_or_create_fill_view(FillOp *op,
                                          const void *value, size_t value_size)
    //--------------------------------------------------------------------------
    {
      // Two versions of this method depending on whether we are doing 
      // Legion Spy or not, Legion Spy wants to know exactly which op
      // made each fill view so we can't cache them
#ifndef LEGION_SPY
      // See if we can find this in the cache first
      AutoLock f_lock(fill_view_lock);
      for (std::list<FillView*>::iterator it = 
            value_fill_view_cache.begin(); it !=
            value_fill_view_cache.end(); it++)
      {
        // Safe to do this since we know we're only comparing against other
        // fill views that were also made with eager values
        if (!(*it)->matches(value, value_size))
          continue;
        // Record a reference on it and then return
        FillView *result = (*it);
        // Move it back to the front of the list
        value_fill_view_cache.erase(it);
        value_fill_view_cache.push_front(result);
        result->add_base_valid_ref(MAPPING_ACQUIRE_REF);
        return result;
      }
#endif
      // At this point we have to make it since we couldn't find it
      FillView *fill_view = 
        new FillView(runtime, runtime->get_available_distributed_id(),
#ifdef LEGION_SPY
                     op->get_unique_op_id(),
#endif
                     value, value_size, true/*register now*/);
      fill_view->add_base_valid_ref(MAPPING_ACQUIRE_REF);
#ifndef LEGION_SPY
      // Add it to the cache since we're not doing Legion Spy
      fill_view->add_nested_valid_ref(did);
      value_fill_view_cache.push_front(fill_view);
      if (value_fill_view_cache.size() > MAX_FILL_VIEW_CACHE_SIZE)
      {
        FillView *oldest = value_fill_view_cache.back();
        value_fill_view_cache.pop_back();
        if (oldest->remove_nested_valid_ref(did))
          delete oldest;
      }
#endif
      return fill_view;
    }

    //--------------------------------------------------------------------------
    FillView* InnerContext::find_or_create_fill_view(FillOp *op,
                                           const Future &future, bool &set_view)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!set_view);
      assert(future.impl != NULL);
#endif
      // Two versions of this method depending on whether we are doing 
      // Legion Spy or not, Legion Spy wants to know exactly which op
      // made each fill view so we can't cache them
#ifndef LEGION_SPY
      const DistributedID future_did = future.impl->did;
      // See if we can find this in the cache first
      AutoLock f_lock(fill_view_lock);
      for (std::list<std::pair<FillView*,DistributedID> >::iterator it = 
            future_fill_view_cache.begin(); it !=
            future_fill_view_cache.end(); it++)
      {
        if (it->second != future_did)
          continue;
        // Record a reference on it and then return
        FillView *result = it->first;
        // Move it back to the front of the list
        future_fill_view_cache.erase(it);
        future_fill_view_cache.push_front(std::make_pair(result, future_did));
        result->add_base_valid_ref(MAPPING_ACQUIRE_REF);
        return result;
      }
#endif
      // We're going to need to set the value for this view
      set_view = true;
      FillView *fill_view = 
        new FillView(runtime, runtime->get_available_distributed_id(),
#ifdef LEGION_SPY
                     op->get_unique_op_id(),
#endif
                     true/*register now*/);
      fill_view->add_base_valid_ref(MAPPING_ACQUIRE_REF);
#ifndef LEGION_SPY
      // Add it to the cache since we're not doing Legion Spy
      fill_view->add_nested_valid_ref(did);
      future_fill_view_cache.push_front(std::make_pair(fill_view, future_did));
      if (future_fill_view_cache.size() > MAX_FILL_VIEW_CACHE_SIZE)
      {
        FillView *oldest = future_fill_view_cache.back().first;
        future_fill_view_cache.pop_back();
        if (oldest->remove_nested_valid_ref(did))
          delete oldest;
      }
#endif
      return fill_view;
    }

    //--------------------------------------------------------------------------
    FillView* InnerContext::find_fill_view(const void *value, size_t value_size)
    //--------------------------------------------------------------------------
    {
      // Two versions of this method depending on whether we are doing 
      // Legion Spy or not, Legion Spy wants to know exactly which op
      // made each fill view so we can't cache them
#ifndef LEGION_SPY
      // See if we can find this in the cache first
      AutoLock f_lock(fill_view_lock);
      for (std::list<FillView*>::iterator it = 
            value_fill_view_cache.begin(); it !=
            value_fill_view_cache.end(); it++)
      {
        // Safe to do this since we know we're only comparing against other
        // fill views that were also made with eager values
        if (!(*it)->matches(value, value_size))
          continue;
        // Record a reference on it and then return
        FillView *result = (*it);
        // Move it back to the front of the list
        value_fill_view_cache.erase(it);
        value_fill_view_cache.push_front(result);
        result->add_base_valid_ref(MAPPING_ACQUIRE_REF);
        return result;
      }
#endif
      return NULL;
    }

    //--------------------------------------------------------------------------
    FillView* InnerContext::find_fill_view(const Future &future)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(future.impl != NULL);
#endif
      // Two versions of this method depending on whether we are doing 
      // Legion Spy or not, Legion Spy wants to know exactly which op
      // made each fill view so we can't cache them
#ifndef LEGION_SPY
      const DistributedID future_did = future.impl->did;
      // See if we can find this in the cache first
      AutoLock f_lock(fill_view_lock);
      for (std::list<std::pair<FillView*,DistributedID> >::iterator it = 
            future_fill_view_cache.begin(); it !=
            future_fill_view_cache.end(); it++)
      {
        if (it->second != future_did)
          continue;
        // Record a reference on it and then return
        FillView *result = it->first;
        // Move it back to the front of the list
        future_fill_view_cache.erase(it);
        future_fill_view_cache.push_front(std::make_pair(result, future_did));
        result->add_base_valid_ref(MAPPING_ACQUIRE_REF);
        return result;
      }
#endif
      return NULL;
    }

    //--------------------------------------------------------------------------
    void InnerContext::notify_instance_deletion(PhysicalManager *deleted)
    //--------------------------------------------------------------------------
    {
      InstanceView *removed = NULL;
      {
        AutoLock inst_lock(instance_view_lock);
        std::map<PhysicalManager*,IndividualView*>::iterator finder =  
          instance_top_views.find(deleted);
        if (finder == instance_top_views.end())
          return;
        removed = finder->second;
        instance_top_views.erase(finder);
      }
      if (removed->remove_nested_gc_ref(did))
        delete removed;
    }

    //--------------------------------------------------------------------------
    bool InnerContext::attempt_children_complete(void)
    //--------------------------------------------------------------------------
    {
      AutoLock child_lock(child_op_lock);
      if (task_executed && (executing_children_count == 0) && 
          (executed_children_count == 0) && !children_complete_invoked)
      {
        children_complete_invoked = true;
        return true;
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool InnerContext::attempt_children_commit(void)
    //--------------------------------------------------------------------------
    {
      AutoLock child_lock(child_op_lock);
      if (task_executed && reorder_buffer.empty() && !children_commit_invoked)
      {
        children_commit_invoked = true;
        return true;
      }
      return false;
    }

    //--------------------------------------------------------------------------
    const std::vector<PhysicalRegion>& InnerContext::begin_task(Processor proc)
    //--------------------------------------------------------------------------
    {
      // If we have mutable priority we need to save our realm done event
      if (Processor::get_executing_processor().exists())
        realm_done_event = ApEvent(Processor::get_current_finish_event());
      // Now do the base begin task routine
      return TaskContext::begin_task(proc);
    }

    //--------------------------------------------------------------------------
    void InnerContext::end_task(const void *res, size_t res_size, bool owned,
                                PhysicalInstance deferred_result_instance, 
                                FutureFunctor *callback_functor,
                                const Realm::ExternalInstanceResource *resource,
                       void (*freefunc)(const Realm::ExternalInstanceResource&),
                                const void *metadataptr, size_t metadatasize,
                                ApEvent effects)
    //--------------------------------------------------------------------------
    {
      if (realm_done_event.exists())
      {
        // Case of a normal task
#ifdef DEBUG_LEGION
        assert(!effects.exists());
#endif
        effects = realm_done_event;
      }
      else // implicit task
        realm_done_event = effects;
      // See if we have any local regions or fields that need to be deallocated
      std::vector<LogicalRegion> local_regions_to_delete;
      std::map<FieldSpace,std::set<FieldID> > local_fields_to_delete;
      {
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
        for (std::map<LogicalRegion,bool>::const_iterator it = 
              local_regions.begin(); it != local_regions.end(); it++)
          if (!it->second)
            local_regions_to_delete.push_back(it->first);
        for (std::map<std::pair<FieldSpace,FieldID>,bool>::const_iterator it =
              local_fields.begin(); it != local_fields.end(); it++)
          if (!it->second)
            local_fields_to_delete[it->first.first].insert(it->first.second);
      }
      if (!local_regions_to_delete.empty())
      {
        for (std::vector<LogicalRegion>::const_iterator it = 
              local_regions_to_delete.begin(); it != 
              local_regions_to_delete.end(); it++)
          destroy_logical_region(*it, false/*unordered*/, NULL/*provenace*/);
      }
      if (!local_fields_to_delete.empty())
      {
        for (std::map<FieldSpace,std::set<FieldID> >::const_iterator it = 
              local_fields_to_delete.begin(); it !=
              local_fields_to_delete.end(); it++)
        {
          FieldAllocatorImpl *allocator =
            create_field_allocator(it->first, false/*unordered*/);
          allocator->add_reference();
          free_fields(allocator, it->first, it->second,
              false/*unordered*/, NULL/*provenance*/);
          if (allocator->remove_reference())
            delete allocator;
        }
      }
      if (!index_launch_spaces.empty())
      {
        // These index spaces are now local to this context so we only
        // want to invoke our local deletion and not the global deletion
        // across all the shards in the case of control replication
        for (std::map<Domain,IndexSpace>::const_iterator it = 
              index_launch_spaces.begin(); it != 
              index_launch_spaces.end(); it++)
          InnerContext::destroy_index_space(it->second, false/*unordered*/, 
              true/*recurse*/, NULL/*provenance*/);
      }
      if (overhead_profiler != NULL)
      {
        const long long current = Realm::Clock::current_time_in_nanoseconds();
        const long long diff = current - 
          overhead_profiler->previous_profiling_time;
        overhead_profiler->application_time += diff;
      }
      if (implicit_profiler != NULL)
      {
        const long long stop = Realm::Clock::current_time_in_nanoseconds();
        // log this with the profiler 
        runtime->profiler->record_implicit(get_unique_id(), owner_task->task_id,
            executing_processor, implicit_profiler->start_time, stop,
            implicit_profiler->waits, owner_task->get_completion_event());
      }
      // See if there are any runtime warnings to issue
      if (runtime->runtime_warnings)
      {
        if (total_children_count == 0)
        {
          // If there were no sub operations and this wasn't marked a
          // leaf task then signal a warning
          VariantImpl *impl = 
            runtime->find_variant_impl(owner_task->task_id, 
                                       owner_task->get_selected_variant());
          REPORT_LEGION_WARNING(LEGION_WARNING_VARIANT_TASK_NOT_MARKED,
            "Variant %s of task %s (UID %lld) was "
              "not marked as a 'leaf' variant but it didn't execute any "
              "operations. Did you forget the 'leaf' annotation?", 
              impl->get_name(), get_task_name(), get_unique_id());
        }
        else if (!owner_task->is_inner())
        {
          // If this task had sub operations and wasn't marked as inner
          // and made no accessors warn about missing 'inner' annotation
          // First check for any inline accessors that were made
          bool has_accessor = has_inline_accessor;
          if (!has_accessor)
          {
            for (unsigned idx = 0; idx < physical_regions.size(); idx++)
            {
              if (!physical_regions[idx].impl->created_accessor())
                continue;
              has_accessor = true;
              break;
            }
          }
          if (!has_accessor)
          {
            VariantImpl *impl = 
              runtime->find_variant_impl(owner_task->task_id, 
                                         owner_task->get_selected_variant());
            REPORT_LEGION_WARNING(LEGION_WARNING_VARIANT_TASK_NOT_MARKED,
              "Variant %s of task %s (UID %lld) was "
                "not marked as an 'inner' variant but it only launched "
                "operations and did not make any accessors. Did you "
                "forget the 'inner' annotation?",
                impl->get_name(), get_task_name(), get_unique_id());
          }
        }
      }
      // Quick check to make sure the user didn't forget to end a trace
      if (current_trace != NULL)
        REPORT_LEGION_ERROR(ERROR_TASK_FAILED_END_TRACE,
          "Task %s (UID %lld) failed to end trace before exiting!",
                        get_task_name(), get_unique_id()) 
      // Unmap any of our mapped regions before issuing any close operations
      unmap_all_regions(false/*external*/);
      const std::deque<InstanceSet> &physical_instances = 
        owner_task->get_physical_instances();
      // Note that this loop doesn't handle create regions
      // we deal with that case below
      for (unsigned idx = 0; idx < regions.size(); idx++)
      {
        if (!virtual_mapped[idx])
        {
          // We also don't need to close up read-only instances
          // or reduction-only instances (because they are restricted)
          // so all changes have already been propagated
          if (!IS_WRITE(regions[idx]))
            continue;
#ifdef DEBUG_LEGION
          assert(!physical_instances[idx].empty());
#endif
          PostCloseOp *close_op = 
            runtime->get_available_post_close_op();
          close_op->initialize(this, idx, physical_instances[idx]);
          add_to_dependence_queue(close_op);
        }
        else if (IS_WRITE(regions[idx]))
        {
          // Make a virtual close op to close up the instance
          // This will clone out the equivalence sets to our 
          // enclosing set of equivalence sets. Note we only
          // do this for read-write privileges where we know
          // that no one else can be modifying the enclosing
          // sets at the same time. For other privileges we're
          // already using the original set without any local
          // refinements so we don't need to do the copy out
          VirtualCloseOp *close_op = get_virtual_close_op(); 
          close_op->initialize(this, idx, regions[idx],
              &(owner_task->get_version_info(idx)));
          add_to_dependence_queue(close_op);
        }
      }
      // Check to see if we have any unordered operations that we need to inject
      progress_unordered_operations(true/*end task*/);
      // At this point we should have grabbed any references to these
      // physical regions so we can clear them at this point
      physical_regions.clear();
      TaskContext::end_task(res, res_size, owned, deferred_result_instance,
          callback_functor, resource,freefunc,metadataptr,metadatasize,effects);
    }

    //--------------------------------------------------------------------------
    void InnerContext::post_end_task(FutureInstance *instance,
                                     void *metadata, size_t metasize,
                                     FutureFunctor *callback_functor,
                                     bool own_callback_functor)
    //--------------------------------------------------------------------------
    {
      // Safe to cast to a single task here because this will never
      // be called while inlining an index space task
      // Handle the future result
      owner_task->handle_post_execution(instance, metadata, metasize, 
          callback_functor, executing_processor, own_callback_functor);
      // If we weren't a leaf task, compute the conditions for being mapped
      // which is that all of our children are now mapped
      // Also test for whether we need to trigger any of our child
      // complete or committed operations before marking that we
      // are done executing
      bool need_complete = false;
      bool need_commit = false;
      std::vector<RtEvent> preconditions;
      std::vector<ApEvent> child_completion_events;
      {
        AutoLock child_lock(child_op_lock);
        // Only need to do this for executing and executed children
        // We know that any complete children are done
        for (std::deque<ReorderBufferEntry>::const_iterator it =
              reorder_buffer.begin(); it != reorder_buffer.end(); it++)
          if ((it->stage == EXECUTING_STAGE) || (it->stage == EXECUTED_STAGE))
            preconditions.push_back(it->operation->get_mapped_event());
        // Also include the current mapping fence event, note that if there were
        // no mapping fences we might still have a real event here corresponding
        // to when the context was initialized for virtual mappings
        if (current_mapping_fence_event.exists())
          preconditions.push_back(current_mapping_fence_event);
#ifdef DEBUG_LEGION
        assert(!task_executed);
#endif
        // Now that we know the last registration has taken place we
        // can mark that we are done executing
        task_executed = true;
        if ((executing_children_count == 0) && (executed_children_count == 0))
        {
          if (!children_complete_invoked)
          {
            need_complete = true;
            children_complete_invoked = true;
#ifdef LEGION_SPY
            child_completion_events.insert(child_completion_events.end(),
                cummulative_child_completion_events.begin(),
                cummulative_child_completion_events.end());
            cummulative_child_completion_events.clear();
#endif
            for (std::deque<ReorderBufferEntry>::const_iterator it =
                  reorder_buffer.begin(); it != reorder_buffer.end(); it++)
            {
#ifdef DEBUG_LEGION
              assert(it->stage != EXECUTING_STAGE);
              assert(it->stage != EXECUTED_STAGE);
#endif
              if (it->stage == COMPLETED_STAGE)
                it->operation->find_completion_effects(child_completion_events);
            }
          }
          if (reorder_buffer.empty() && !children_commit_invoked)
          {
            need_commit = true;
            children_commit_invoked = true;
          }
        }
      }
      if (!preconditions.empty())
        owner_task->handle_post_mapped(Runtime::merge_events(preconditions));
      else
        owner_task->handle_post_mapped();
      if (need_complete)
      {
        if (!child_completion_events.empty())
        {
          if (realm_done_event.exists())
            child_completion_events.push_back(realm_done_event);
          owner_task->record_inner_termination(
              Runtime::merge_events(NULL, child_completion_events));
        }
        else
          owner_task->record_inner_termination(realm_done_event);
        owner_task->trigger_children_complete();
      }
      if (need_commit)
        owner_task->trigger_children_committed();
    }

    //--------------------------------------------------------------------------
    void InnerContext::handle_mispredication(void)
    //--------------------------------------------------------------------------
    {
      owner_task->handle_post_mapped();
      owner_task->record_inner_termination(ApEvent::NO_AP_EVENT);
      TaskContext::handle_mispredication();
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_prepipeline_stage(const void *args)
    //--------------------------------------------------------------------------
    {
      const PrepipelineArgs *pargs = (const PrepipelineArgs*)args;
      if (pargs->context->process_prepipeline_stage() &&
          pargs->context->remove_base_resource_ref(META_TASK_REF))
        delete pargs->context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_dependence_stage(const void *args)
    //--------------------------------------------------------------------------
    {
      const DependenceArgs *dargs = (const DependenceArgs*)args;
      dargs->context->process_dependence_stage();
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_ready_queue(const void *args)
    //--------------------------------------------------------------------------
    {
      const TriggerReadyArgs *targs = (const TriggerReadyArgs*)args;
      if (targs->context->process_ready_queue() &&
          targs->context->remove_base_resource_ref(META_TASK_REF))
        delete targs->context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_enqueue_task_queue(const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferredEnqueueTaskArgs *dargs = 
        (const DeferredEnqueueTaskArgs*)args;
      if (dargs->context->process_enqueue_task_queue() &&
          dargs->context->remove_base_resource_ref(META_TASK_REF))
        delete dargs->context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_distribute_task_queue(const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferredDistributeTaskArgs *dargs = 
        (const DeferredDistributeTaskArgs*)args;
      if (dargs->context->process_distribute_task_queue() &&
          dargs->context->remove_base_resource_ref(META_TASK_REF))
        delete dargs->context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_launch_task_queue(const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferredLaunchTaskArgs *dargs = 
        (const DeferredLaunchTaskArgs*)args;
      if (dargs->context->process_launch_task_queue() &&
          dargs->context->remove_base_resource_ref(META_TASK_REF))
        delete dargs->context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_resolution_queue(const void *args)
    //--------------------------------------------------------------------------
    {
      const TriggerResolutionArgs *targs = (const TriggerResolutionArgs*)args;
      if (targs->context->process_resolution_queue() &&
          targs->context->remove_base_resource_ref(META_TASK_REF))
        delete targs->context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_trigger_execution_queue(
                                                               const void *args)
    //--------------------------------------------------------------------------
    {
      const TriggerExecutionArgs *targs = (const TriggerExecutionArgs*)args;
      if (targs->context->process_trigger_execution_queue() &&
          targs->context->remove_base_resource_ref(META_TASK_REF))
        delete targs->context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_deferred_execution_queue(
                                                               const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferredExecutionArgs *dargs = (const DeferredExecutionArgs*)args;
      if (dargs->context->process_deferred_execution_queue() &&
          dargs->context->remove_base_resource_ref(META_TASK_REF))
        delete dargs->context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_trigger_completion_queue(
                                                               const void *args)
    //--------------------------------------------------------------------------
    {
      const TriggerCompletionArgs *targs = (const TriggerCompletionArgs*)args;
      if (targs->context->process_trigger_completion_queue() &&
          targs->context->remove_base_resource_ref(META_TASK_REF))
        delete targs->context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_deferred_completion_queue(
                                                               const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferredCompletionArgs *dargs = (const DeferredCompletionArgs*)args;
      if (dargs->context->process_deferred_completion_queue() &&
          dargs->context->remove_base_resource_ref(META_TASK_REF))
        delete dargs->context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_trigger_commit_queue(const void *args)
    //--------------------------------------------------------------------------
    {
      const TriggerCommitArgs *targs = (const TriggerCommitArgs*)args;
      if (targs->context->process_trigger_commit_queue() &&
          targs->context->remove_base_resource_ref(META_TASK_REF))
        delete targs->context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_deferred_commit_queue(const void *args)
    //--------------------------------------------------------------------------
    {
      const DeferredCommitArgs *dargs = (const DeferredCommitArgs*)args;
      if (dargs->context->process_deferred_commit_queue() &&
          dargs->context->remove_base_resource_ref(META_TASK_REF))
        delete dargs->context;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InnerContext::handle_post_end_task(const void *args)
    //--------------------------------------------------------------------------
    {
      const PostEndArgs *pargs = (const PostEndArgs*)args;
      if (pargs->proxy_this->process_post_end_tasks() && 
          pargs->proxy_this->remove_base_resource_ref(META_TASK_REF))
        delete pargs->proxy_this;
    }

    //--------------------------------------------------------------------------
    bool InnerContext::inline_child_task(TaskOp *child)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, INLINE_CHILD_TASK_CALL);
      if (runtime->legion_spy_enabled)
        LegionSpy::log_inline_task(child->get_unique_id());
      // Check to see if the child is predicated
      // If it is wait for it to resolve
      if (child->is_predicated_op())
      {
        // See if the predicate speculates false, if so return false
        // and then we are done.
        if (!child->get_predicate_value())
          return true;
      }
      // Find the mapped physical regions associated with each of the
      // child task's region requirements. If we don't have one then
      // it's not legal to inline the child task
      std::vector<PhysicalRegion> child_regions(child->regions.size());
      for (unsigned childidx = 0; childidx < child_regions.size(); childidx++)
      {
        const RegionRequirement &child_req = child->regions[childidx]; 
        bool found = false;
        for (unsigned our_idx = 0; our_idx < physical_regions.size(); our_idx++)
        {
          if (!physical_regions[our_idx].is_mapped())
            continue;
          const RegionRequirement &our_req = regions[our_idx];
          const RegionTreeID our_tid = our_req.region.get_tree_id();
          const IndexSpace our_space = our_req.region.get_index_space();
          const RegionUsage our_usage(our_req);
          if (!check_region_dependence(our_tid, our_space, our_req,
                  our_usage, child_req, false/*ignore privileges*/))
            continue;
          child_regions[childidx] = physical_regions[our_idx];
          found = true;
          break;
        }
        if (found)
          continue;
        // Need the lock here because of unordered detach operations
        AutoLock i_lock(inline_lock,1,false/*exclusive*/);
        for (std::list<PhysicalRegion>::const_iterator it =
              inline_regions.begin(); it != inline_regions.end(); it++)
        {
#ifdef DEBUG_LEGION
          assert(it->is_mapped());
#endif
          const RegionRequirement &our_req = it->impl->get_requirement();
          const RegionTreeID our_tid = our_req.region.get_tree_id();
          const IndexSpace our_space = our_req.region.get_index_space();
          const RegionUsage our_usage(our_req);
          if (!check_region_dependence(our_tid, our_space, our_req,
                  our_usage, child_req, false/*ignore privileges*/))
            continue;
          child_regions[childidx] = *it;
          found = true;
          break;
        }
        // If we didn't find any physical region then report the warning
        // and return because we couldn't find a mapped physical region
        if (!found)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_FAILED_INLINING,
              "Failed to inline task %s (UID %lld) into parent task "
              "%s (UID %lld) because there was no mapped region for "
              "region requirement %d to use. Currently all regions "
              "must be mapped in the parent task in order to allow "
              "for inlining. If you believe you have a compelling use "
              "case for inline a task with virtually mapped regions "
              "then please contact the Legion developers.", 
              child->get_task_name(), child->get_unique_id(), 
              owner_task->get_task_name(), owner_task->get_unique_id(),childidx)
          return false;
        }
      }
      register_executing_child(child);
      // Now select the variant for task based on the regions 
      std::deque<InstanceSet> physical_instances(child_regions.size());
      VariantImpl *variant = 
        select_inline_variant(child, child_regions, physical_instances); 
      child->perform_inlining(variant, physical_instances);
      // Finish the inlining of the child task to execute, note this doesn't
      // wait for the effects of the children to be done, it just blocks to
      // make sure the code for the children are done running on this processor
      wait_for_inlined();
      return true;
    } 

    //--------------------------------------------------------------------------
    void InnerContext::analyze_free_local_fields(FieldSpace handle,
                                     const std::vector<FieldID> &local_to_free,
                                     std::vector<unsigned> &local_field_indexes)
    //--------------------------------------------------------------------------
    {
      AutoLock local_lock(local_field_lock,1,false/*exclusive*/);
      std::map<FieldSpace,std::vector<LocalFieldInfo> >::const_iterator 
        finder = local_field_infos.find(handle);
#ifdef DEBUG_LEGION
      assert(finder != local_field_infos.end());
#endif
      for (unsigned idx = 0; idx < local_to_free.size(); idx++)
      {
#ifdef DEBUG_LEGION
        bool found = false;
#endif
        for (std::vector<LocalFieldInfo>::const_iterator it = 
              finder->second.begin(); it != finder->second.end(); it++)
        {
          if (it->fid == local_to_free[idx])
          {
            // Can't remove it yet
            local_field_indexes.push_back(it->index);
#ifdef DEBUG_LEGION
            found = true;
#endif
            break;
          }
        }
#ifdef DEBUG_LEGION
        assert(found);
#endif
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::remove_deleted_local_fields(FieldSpace space,
                                          const std::vector<FieldID> &to_remove)
    //--------------------------------------------------------------------------
    {
      AutoLock local_lock(local_field_lock);
      std::map<FieldSpace,std::vector<LocalFieldInfo> >::iterator 
        finder = local_field_infos.find(space);
#ifdef DEBUG_LEGION
      assert(finder != local_field_infos.end());
#endif
      for (unsigned idx = 0; idx < to_remove.size(); idx++)
      {
#ifdef DEBUG_LEGION
        bool found = false;
#endif
        for (std::vector<LocalFieldInfo>::iterator it = 
              finder->second.begin(); it != finder->second.end(); it++)
        {
          if (it->fid == to_remove[idx])
          {
            finder->second.erase(it);
#ifdef DEBUG_LEGION
            found = true;
#endif
            break;
          }
        }
#ifdef DEBUG_LEGION
        assert(found);
#endif
      }
      if (finder->second.empty())
        local_field_infos.erase(finder);
    } 

    //--------------------------------------------------------------------------
    void InnerContext::execute_task_launch(TaskOp *task, bool index,
                            LogicalTrace *current_trace, Provenance *provenance,
                            bool silence_warnings, bool inlining_enabled)
    //--------------------------------------------------------------------------
    {
      bool perform_inlining = false;
      if (inlining_enabled)
        perform_inlining = task->select_task_options(true/*prioritize*/);
      // Now check to see if we're inling the task or just performing
      // a normal asynchronous task launch
      if (!perform_inlining || !inline_child_task(task))
      {
        // Normal task launch, iterate over the context task's
        // regions and see if we need to unmap any of them
        std::vector<PhysicalRegion> unmapped_regions;
        if (!runtime->unsafe_launch)
          find_conflicting_regions(task, unmapped_regions);
        if (!unmapped_regions.empty())
        {
          if (runtime->runtime_warnings && !silence_warnings)
          {
            if (index)
            {
              REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
                "WARNING: Runtime is unmapping and remapping "
                  "physical regions around execute_index_space call in "
                  "task %s (UID %lld).", get_task_name(), get_unique_id());
            }
            else
            {
              REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
                "WARNING: Runtime is unmapping and remapping "
                  "physical regions around execute_task call in "
                  "task %s (UID %lld).", get_task_name(), get_unique_id());
            }
          }
          for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
            unmapped_regions[idx].impl->unmap_region();
        }
        // Issue the task call
        add_to_dependence_queue(task);
        // Remap any unmapped regions
        if (!unmapped_regions.empty())
          remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      }
    }

    //--------------------------------------------------------------------------
    void InnerContext::clone_local_fields(
           std::map<FieldSpace,std::vector<LocalFieldInfo> > &child_local) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(child_local.empty());
#endif
      AutoLock local_lock(local_field_lock,1,false/*exclusive*/);
      if (local_field_infos.empty())
        return;
      for (std::map<FieldSpace,std::vector<LocalFieldInfo> >::const_iterator
            fit = local_field_infos.begin(); 
            fit != local_field_infos.end(); fit++)
      {
        std::vector<LocalFieldInfo> &child = child_local[fit->first];
        child.resize(fit->second.size());
        for (unsigned idx = 0; idx < fit->second.size(); idx++)
        {
          LocalFieldInfo &field = child[idx];
          field = fit->second[idx];
          field.ancestor = true; // mark that this is an ancestor field
        }
      }
    }

#ifdef DEBUG_LEGION
    //--------------------------------------------------------------------------
    Operation* InnerContext::get_earliest(void) const
    //--------------------------------------------------------------------------
    {
      if (reorder_buffer.empty())
        return NULL;
      return reorder_buffer.front().operation;
    }
#endif

#ifdef LEGION_SPY
    //--------------------------------------------------------------------------
    void InnerContext::register_implicit_replay_dependence(Operation *op)
    //--------------------------------------------------------------------------
    {
      LegionSpy::log_mapping_dependence(get_unique_id(), 
          current_fence_uid, 0/*idx*/, op->get_unique_op_id(),
          0/*idx*/, LEGION_TRUE_DEPENDENCE);
    }
#endif

    //--------------------------------------------------------------------------
    RtEvent 
      InnerContext::total_hack_function_for_inorder_concurrent_replay_analysis(
                                                           RtEvent mapped_event)
    //--------------------------------------------------------------------------
    {
      inorder_concurrent_replay_analysis =
        runtime->acquire_concurrent_reservation(mapped_event,
            inorder_concurrent_replay_analysis);
      return inorder_concurrent_replay_analysis;
    }

    /////////////////////////////////////////////////////////////
    // Top Level Context 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    TopLevelContext::TopLevelContext(Runtime *rt, Processor p, DistributedID id,
                                     CollectiveMapping *mapping)
      : InnerContext(rt, NULL, -1, false/*full inner*/,
                     dummy_requirements, dummy_output_requirements,
                     dummy_indexes, dummy_mapped, ApEvent::NO_AP_EVENT,
                     id, false, false, false, mapping),
        root_uid(rt->get_unique_operation_id())
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(p.exists());
#endif
      set_executing_processor(p);
    }

    //--------------------------------------------------------------------------
    TopLevelContext::~TopLevelContext(void)
    //--------------------------------------------------------------------------
    { 
    }

    //--------------------------------------------------------------------------
    void TopLevelContext::pack_remote_context(Serializer &rez, 
                                          AddressSpaceID target, bool replicate)
    //--------------------------------------------------------------------------
    {
      rez.serialize(depth);
    }

    //--------------------------------------------------------------------------
    InnerContext* TopLevelContext::find_parent_context(void)
    //--------------------------------------------------------------------------
    {
      return NULL;
    }

    //--------------------------------------------------------------------------
    void TopLevelContext::receive_created_region_contexts(
                 const std::vector<RegionNode*> &created_nodes,
                 const std::vector<EqKDTree*> &created_trees,
                 std::set<RtEvent> &applied_events,
                 const ShardMapping *mapping, ShardID source_shard)
    //--------------------------------------------------------------------------
    {
      assert(false);
    }

    //--------------------------------------------------------------------------
    RtEvent TopLevelContext::compute_equivalence_sets(unsigned req_index,
                       const std::vector<EqSetTracker*> &targets,
                       const std::vector<AddressSpaceID> &target_spaces,
                       AddressSpaceID creation_target_space,
                       IndexSpaceExpression *expr, const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      assert(false);
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    RtEvent TopLevelContext::record_output_equivalence_set(EqSetTracker *source,
                       AddressSpaceID source_space, unsigned req_index,
                       EquivalenceSet *expr, const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      assert(false);
      return RtEvent::NO_RT_EVENT;
    }

    //--------------------------------------------------------------------------
    InnerContext* TopLevelContext::find_outermost_local_context(
                                                         InnerContext *previous)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(previous != NULL);
#endif
      return previous;
    }

    //--------------------------------------------------------------------------
    InnerContext* TopLevelContext::find_top_context(InnerContext *previous)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(previous != NULL);
#endif
      return previous;
    }

    /////////////////////////////////////////////////////////////
    // Replicate Context 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ReplicateContext::ReplicateContext(Runtime *rt, 
                                 ShardTask *owner, int d, bool full,
                                 const std::vector<RegionRequirement> &reqs,
                                 const std::vector<OutputRequirement> &out_reqs,
                                 const std::vector<unsigned> &parent_indexes,
                                 const std::vector<bool> &virt_mapped,
                                 ApEvent exec_fence,
                                 ShardManager *manager, bool inline_task,
                                 bool implicit_task, bool concurrent)
      : InnerContext(rt, owner, d, full, reqs, out_reqs, parent_indexes,
         virt_mapped, exec_fence, 0, inline_task, implicit_task, concurrent),
        owner_shard(owner), shard_manager(manager),
        total_shards(shard_manager->total_shards),
        next_close_mapped_bar_index(0), next_refinement_ready_bar_index(0),
        next_refinement_mapped_bar_index(0), next_indirection_bar_index(0), 
        next_collective_map_bar_index(0), distributed_id_allocator_shard(0),
        index_space_allocator_shard(0), index_partition_allocator_shard(0),
        field_space_allocator_shard(0), field_allocator_shard(0),
        logical_region_allocator_shard(0), dynamic_id_allocator_shard(0),
        equivalence_set_allocator_shard(0), next_available_collective_index(0),
        next_logical_collective_index(1), next_physical_template_index(0), 
        next_replicate_bar_index(0), next_logical_bar_index(0),
        unordered_ops_counter(0), unordered_ops_epoch(MIN_UNORDERED_OPS_EPOCH),
        unordered_collective(NULL)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION_COLLECTIVES
      collective_guard_reentrant = false;
      logical_guard_reentrant = false;
#endif
      shard_manager->add_nested_resource_ref(did);
      size_t num_barriers = LEGION_CONTROL_REPLICATION_COMMUNICATION_BARRIERS;
      close_mapped_barriers.resize(num_barriers);
      refinement_ready_barriers.resize(num_barriers);
      refinement_mapped_barriers.resize(num_barriers);
      indirection_barriers.resize(num_barriers);
      collective_map_barriers.resize(num_barriers);
      // Configure our collective settings
      shard_collective_radix = runtime->legion_collective_radix;
      configure_collective_settings(total_shards, owner->shard_id,
          shard_collective_radix, shard_collective_log_radix,
          shard_collective_stages, shard_collective_participating_shards,
          shard_collective_last_radix);
    }

    //--------------------------------------------------------------------------
    ReplicateContext::~ReplicateContext(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(unordered_collective == NULL);
#endif
      if (shard_manager->remove_nested_resource_ref(did))
        delete shard_manager;
      if (returned_resource_ready_barrier.exists())
        returned_resource_ready_barrier.destroy_barrier();
      if (returned_resource_mapped_barrier.exists())
        returned_resource_mapped_barrier.destroy_barrier();
      if (returned_resource_execution_barrier.exists())
        returned_resource_execution_barrier.destroy_barrier();
    }

    //--------------------------------------------------------------------------
    ContextID ReplicateContext::get_physical_tree_context(void) const
    //--------------------------------------------------------------------------
    {
      // We have all the shards on the same node use the same physical
      // tree context. This is vital for the correct implementation of
      // some parts of physical analysis equivalence set discovery.
      return shard_manager->get_first_shard_tree_context();
    }

    //--------------------------------------------------------------------------
    DistributedID ReplicateContext::get_replication_id(void) const
    //--------------------------------------------------------------------------
    {
      return shard_manager->did;
    }

#ifdef LEGION_USE_LIBDL
    //--------------------------------------------------------------------------
    void ReplicateContext::perform_global_registration_callbacks(
                     Realm::DSOReferenceImplementation *dso, const void *buffer,
                     size_t buffer_size, bool withargs, size_t dedup_tag,
                     RtEvent local_done, RtEvent global_done, 
                     std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_PERFORM_REGISTRATION_CALLBACK, __func__);
        hasher.hash(dso->dso_name.c_str(), dso->dso_name.size(), "dso_name");
        hasher.hash(dso->symbol_name.c_str(), dso->symbol_name.size(),
                    "symbol_name");
        hasher.hash(withargs, "withargs");
        hasher.hash(dedup_tag, "dedup_tag");
        if (runtime->safe_control_replication > 1)
          hasher.hash(buffer, buffer_size, "buffer");
        if (hasher.verify(__func__))
          break;
      }
      shard_manager->perform_global_registration_callbacks(dso, buffer, 
          buffer_size,withargs,dedup_tag,local_done,global_done,preconditions);
    }
#endif

    //--------------------------------------------------------------------------
    void ReplicateContext::print_once(FILE *f, const char *message) const
    //--------------------------------------------------------------------------
    {
      // Only print from shard 0
      if (owner_shard->shard_id == 0)
        fprintf(f, "%s", message);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::log_once(Realm::LoggerMessage &message) const
    //--------------------------------------------------------------------------
    {
      // Deactivate all the messages except shard 0
      if (owner_shard->shard_id != 0)
        message.deactivate();
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::from_value(const void *value, size_t size,
                           bool owned, Provenance *provenance, bool shard_local)
    //--------------------------------------------------------------------------
    {
      Future result = 
        TaskContext::from_value(value, size, owned, provenance, shard_local);
      for (int i = 0; runtime->safe_control_replication && !shard_local &&
        (i < 2) && ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_FUTURE_FROM_VALUE, __func__);
        hash_future(hasher, runtime->safe_control_replication, result,"future");
        hasher.hash(size, "size");
        hasher.hash(owned, "owned");
        if (hasher.verify(__func__))
          break;
      }
      return result;
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::from_value(const void *buffer, size_t size,
                   bool owned, const Realm::ExternalInstanceResource &resource,
                   void (*freefunc)(const Realm::ExternalInstanceResource&),
                   Provenance *provenance, bool shard_local)
    //--------------------------------------------------------------------------
    {
      Future result = TaskContext::from_value(buffer, size, owned,
          resource, freefunc, provenance, shard_local);
      for (int i = 0; runtime->safe_control_replication && !shard_local &&
        (i < 2) && ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_FUTURE_FROM_VALUE, __func__);
        hash_future(hasher, runtime->safe_control_replication, result,"future");
        hasher.hash(size, "size");
        hasher.hash(owned, "owned");
        if (hasher.verify(__func__))
          break;
      }
      return result;
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::consensus_match(const void *input, void *output,
               size_t num_elements, size_t element_size, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_CONSENSUS_MATCH, __func__);
        if (hasher.verify(__func__))
          break;
      }
      const size_t future_size = sizeof(num_elements);
      Future result(new FutureImpl(this, runtime, true/*register*/,
            runtime->get_available_distributed_id(), provenance));
      result.impl->set_future_result_size(future_size, runtime->address_space);
      switch (element_size)
      {
        case 1:
          {
            ConsensusMatchExchange<uint8_t> *collective = 
              new ConsensusMatchExchange<uint8_t>(this, COLLECTIVE_LOC_89,
                                                  result, output);
            if (collective->match_elements_async(input, num_elements))
              delete collective;
            break;
          }
        case 2:
          {
            ConsensusMatchExchange<uint16_t> *collective = 
              new ConsensusMatchExchange<uint16_t>(this, COLLECTIVE_LOC_89,
                                                   result, output);
            if (collective->match_elements_async(input, num_elements))
              delete collective;
            break;
          }
        case 4:
          {
            ConsensusMatchExchange<uint32_t> *collective = 
              new ConsensusMatchExchange<uint32_t>(this, COLLECTIVE_LOC_89,
                                                   result, output);
            if (collective->match_elements_async(input, num_elements))
              delete collective;
            break;
          }
        case 8:
          {
            ConsensusMatchExchange<uint64_t> *collective = 
              new ConsensusMatchExchange<uint64_t>(this, COLLECTIVE_LOC_89,
                                                   result, output);
            if (collective->match_elements_async(input, num_elements))
              delete collective;
            break;
          }
        default:
          REPORT_LEGION_FATAL(LEGION_FATAL_UNSUPPORTED_CONSENSUS_SIZE,
              "Unsupported size %zd for consensus match in %s (UID %lld)",
              element_size, get_task_name(), get_unique_id())
      }
      return result;
    }

    //--------------------------------------------------------------------------
    VariantID ReplicateContext::register_variant(
          const TaskVariantRegistrar &registrar, const void *user_data,
          size_t user_data_size, const CodeDescriptor &desc, 
          size_t ret_size, bool has_ret_size, VariantID vid, bool check_task_id)
    //--------------------------------------------------------------------------
    {
      // If we're inside a registration callback we don't care
      if (inside_registration_callback)
        return TaskContext::register_variant(registrar, user_data, 
            user_data_size, desc, ret_size, has_ret_size, vid, check_task_id);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_REGISTER_TASK_VARIANT, __func__);
        hasher.hash(registrar.task_id, "task_id");
        hasher.hash(registrar.global_registration, "global_registration");
        if (registrar.task_variant_name != NULL)
          hasher.hash(registrar.task_variant_name, 
                      strlen(registrar.task_variant_name), "task_variant_name");
        hash_execution_constraints(hasher, registrar.execution_constraints);
        for (std::multimap<unsigned,LayoutConstraintID>::const_iterator it =
              registrar.layout_constraints.layouts.begin(); it !=
              registrar.layout_constraints.layouts.end(); it++)
        {
          hasher.hash(it->first, "layout constraints");
          hasher.hash(it->second, "layout_constraints");
        }
        for (std::set<TaskID>::const_iterator it = 
              registrar.generator_tasks.begin(); it !=
              registrar.generator_tasks.end(); it++)
          hasher.hash(*it, "generator_tasks");
        hasher.hash(registrar.leaf_variant, "leaf_variant");
        hasher.hash(registrar.inner_variant, "inner_variant");
        hasher.hash(registrar.idempotent_variant, "idempotent_variant");
        hasher.hash(registrar.replicable_variant, "replicable_variant");
        if (has_ret_size)
          hasher.hash(ret_size, "ret_size");
        if ((user_data != NULL) && (runtime->safe_control_replication > 1))
          hasher.hash(user_data, user_data_size, "user_data");
        hasher.hash(vid, "vid");
        if (hasher.verify(__func__))
          break;
      }
      // If the task registration is marked as global, then one shard will do
      // the registration and broadcast the variant information to all other
      // shards. If not, all shards performing the registration will
      // independently register the variant.
      VariantID result;
      if (registrar.global_registration)
      {
        if (owner_shard->shard_id == dynamic_id_allocator_shard)
        {
          ValueBroadcast<VariantID> collective(this, COLLECTIVE_LOC_17);
          result = runtime->register_variant(registrar, user_data, 
                                             user_data_size, desc, 
                                             ret_size, has_ret_size, 
                                             vid, check_task_id, 
                                             false/*check context*/);
          collective.broadcast(result);
        }
        else
        {
          ValueBroadcast<VariantID> collective(this, 
                  dynamic_id_allocator_shard, COLLECTIVE_LOC_17);
          result = collective.get_value();
        }
        if (++dynamic_id_allocator_shard == total_shards)
          dynamic_id_allocator_shard = 0;
      }
      else
      {
        // We have to be a little careful here when assigning the variant ID for
        // the registered task. If the user specified it already, then can just
        // use the ID passed in. However, if we are supposed to generate the 
        // variant ID, then we'll need to pick an ID (on one shard) and tell 
        // everyone else to register the variant with this ID.
        if (vid == LEGION_AUTO_GENERATE_ID)
        {
          auto impl = 
              this->runtime->find_or_create_task_impl(registrar.task_id);
          if (this->owner_shard->shard_id == this->dynamic_id_allocator_shard)
          {
            vid = impl->get_unique_variant_id();
            ValueBroadcast<VariantID> collective(this, COLLECTIVE_LOC_17);
            collective.broadcast(vid);
          }
          else
          {
            ValueBroadcast<VariantID> collective(this, 
                    this->dynamic_id_allocator_shard, COLLECTIVE_LOC_17);
            vid = collective.get_value();
          }
          if (++dynamic_id_allocator_shard == total_shards)
            dynamic_id_allocator_shard = 0;
        }

        // Finally, if there are multiple shards in the same address space, only
        // one of the shards needs to do the registration.
        if (this->shard_manager->is_first_local_shard(this->owner_shard))
        {
          result = runtime->register_variant(registrar, user_data, 
                                             user_data_size, desc, 
                                             ret_size, has_ret_size, 
                                             vid, check_task_id, 
                                             false/*check context*/);
        }
        else
        {
          result = vid;
        }
      }
      return result;
    }

    //--------------------------------------------------------------------------
    VariantImpl* ReplicateContext::select_inline_variant(TaskOp *child,
                              const std::vector<PhysicalRegion> &parent_regions,
                              std::deque<InstanceSet> &physical_instances)
    //--------------------------------------------------------------------------
    {
      VariantImpl *variant_impl = TaskContext::select_inline_variant(child,
                                        parent_regions, physical_instances);
      if (!variant_impl->is_replicable())
      {
        MapperManager *child_mapper = 
          runtime->find_mapper(executing_processor, child->map_id);
        REPORT_LEGION_ERROR(ERROR_INVALID_MAPPER_OUTPUT,
                      "Invalid mapper output from invoction of "
                      "'select_task_variant' on mapper %s. Mapper selected "
                      "an invalid variant ID %d for inlining of task %s "
                      "(UID %lld). Parent task %s (UID %lld) is a control-"
                      "replicated task but mapper selected non-replicable "
                      "variant %d for task %s.",child_mapper->get_mapper_name(),
                      variant_impl->vid, child->get_task_name(), 
                      child->get_unique_id(), owner_task->get_task_name(),
                      owner_task->get_unique_id(), variant_impl->vid,
                      child->get_task_name())
      }
      return variant_impl;
    }

    //--------------------------------------------------------------------------
    TraceID ReplicateContext::generate_dynamic_trace_id(void)
    //--------------------------------------------------------------------------
    {
      // If we're inside a registration callback we don't care
      if (inside_registration_callback)
        return TaskContext::generate_dynamic_trace_id();
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_GENERATE_DYNAMIC_TRACE_ID, __func__);
        if (hasher.verify(__func__))
          break;
      }
      // Otherwise have one shard make it and broadcast it to everyone else
      TraceID result;
      if (owner_shard->shard_id == dynamic_id_allocator_shard)
      {
        ValueBroadcast<TraceID> collective(this, COLLECTIVE_LOC_9);
        result = runtime->generate_dynamic_trace_id(false/*check context*/);
        collective.broadcast(result);
      }
      else
      {
        ValueBroadcast<TraceID> collective(this, dynamic_id_allocator_shard,
                                           COLLECTIVE_LOC_9);
        result = collective.get_value();
      }
      if (++dynamic_id_allocator_shard == total_shards)
        dynamic_id_allocator_shard = 0;
      return result;
    }

    //--------------------------------------------------------------------------
    MapperID ReplicateContext::generate_dynamic_mapper_id(void)
    //--------------------------------------------------------------------------
    {
      // If we're inside a registration callback we don't care
      if (inside_registration_callback)
        return TaskContext::generate_dynamic_mapper_id();
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_GENERATE_DYNAMIC_MAPPER_ID, __func__);
        if (hasher.verify(__func__))
          break;
      }
      // Otherwise have one shard make it and broadcast it to everyone else
      MapperID result;
      if (owner_shard->shard_id == dynamic_id_allocator_shard)
      {
        ValueBroadcast<MapperID> collective(this, COLLECTIVE_LOC_10);
        result = runtime->generate_dynamic_mapper_id(false/*check context*/);
        collective.broadcast(result);
      }
      else
      {
        ValueBroadcast<MapperID> collective(this, dynamic_id_allocator_shard,
                                            COLLECTIVE_LOC_10);
        result = collective.get_value();
      }
      if (++dynamic_id_allocator_shard == total_shards)
        dynamic_id_allocator_shard = 0;
      return result;
    }

    //--------------------------------------------------------------------------
    ProjectionID ReplicateContext::generate_dynamic_projection_id(void)
    //--------------------------------------------------------------------------
    {
      // If we're inside a registration callback we don't care
      if (inside_registration_callback)
        return TaskContext::generate_dynamic_projection_id();
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_GENERATE_DYNAMIC_PROJECTION_ID, __func__);
        if (hasher.verify(__func__))
          break;
      }
      // Otherwise have one shard make it and broadcast it to everyone else
      ProjectionID result;
      if (owner_shard->shard_id == dynamic_id_allocator_shard)
      {
        ValueBroadcast<ProjectionID> collective(this, COLLECTIVE_LOC_11);
        result = 
          runtime->generate_dynamic_projection_id(false/*check context*/);
        collective.broadcast(result);
      }
      else
      {
        ValueBroadcast<ProjectionID> collective(this,dynamic_id_allocator_shard,
                                                COLLECTIVE_LOC_11);
        result = collective.get_value();
      }
      if (++dynamic_id_allocator_shard == total_shards)
        dynamic_id_allocator_shard = 0;
      return result;
    }

    //--------------------------------------------------------------------------
    ShardingID ReplicateContext::generate_dynamic_sharding_id(void)
    //--------------------------------------------------------------------------
    {
      // If we're inside a registration callback we don't care
      if (inside_registration_callback)
        return TaskContext::generate_dynamic_sharding_id();
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_GENERATE_DYNAMIC_SHARDING_ID, __func__);
        if (hasher.verify(__func__))
          break;
      }
      // Otherwise have one shard make it and broadcast it to everyone else
      ShardingID result;
      if (owner_shard->shard_id == dynamic_id_allocator_shard)
      {
        ValueBroadcast<ShardingID> collective(this, COLLECTIVE_LOC_12);
        result = runtime->generate_dynamic_sharding_id(false/*check context*/);
        collective.broadcast(result);
      }
      else
      {
        ValueBroadcast<ShardingID> collective(this,dynamic_id_allocator_shard,
                                              COLLECTIVE_LOC_12);
        result = collective.get_value();
      }
      if (++dynamic_id_allocator_shard == total_shards)
        dynamic_id_allocator_shard = 0;
      return result;
    }

    //--------------------------------------------------------------------------
    TaskID ReplicateContext::generate_dynamic_task_id(void)
    //--------------------------------------------------------------------------
    {
      // If we're inside a registration callback we don't care
      if (inside_registration_callback)
        return TaskContext::generate_dynamic_task_id();
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_GENERATE_DYNAMIC_TASK_ID, __func__);
        if (hasher.verify(__func__))
          break;
      }
      // Otherwise have one shard make it and broadcast it to everyone else
      TaskID result;
      if (owner_shard->shard_id == dynamic_id_allocator_shard)
      {
        ValueBroadcast<TaskID> collective(this, COLLECTIVE_LOC_13);
        result = runtime->generate_dynamic_task_id(false/*check context*/);
        collective.broadcast(result);
      }
      else
      {
        ValueBroadcast<TaskID> collective(this, dynamic_id_allocator_shard,
                                          COLLECTIVE_LOC_13);
        result = collective.get_value();
      }
      if (++dynamic_id_allocator_shard == total_shards)
        dynamic_id_allocator_shard = 0;
      return result;
    }

    //--------------------------------------------------------------------------
    ReductionOpID ReplicateContext::generate_dynamic_reduction_id(void)
    //--------------------------------------------------------------------------
    {
      // If we're inside a registration callback we don't care
      if (inside_registration_callback)
        return TaskContext::generate_dynamic_reduction_id();
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_GENERATE_DYNAMIC_REDUCTION_ID, __func__);
        if (hasher.verify(__func__))
          break;
      }
      // Otherwise have one shard make it and broadcast it to everyone else
      ReductionOpID result;
      if (owner_shard->shard_id == dynamic_id_allocator_shard)
      {
        ValueBroadcast<ReductionOpID> collective(this, COLLECTIVE_LOC_14);
        result = runtime->generate_dynamic_reduction_id(false/*check context*/);
        collective.broadcast(result);
      }
      else
      {
        ValueBroadcast<ReductionOpID> collective(this, 
            dynamic_id_allocator_shard, COLLECTIVE_LOC_14);
        result = collective.get_value();
      }
      if (++dynamic_id_allocator_shard == total_shards)
        dynamic_id_allocator_shard = 0;
      return result;
    }

    //--------------------------------------------------------------------------
    CustomSerdezID ReplicateContext::generate_dynamic_serdez_id(void)
    //--------------------------------------------------------------------------
    {
      // If we're inside a registration callback we don't care
      if (inside_registration_callback)
        return TaskContext::generate_dynamic_serdez_id();
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_GENERATE_DYNAMIC_SERDEZ_ID, __func__);
        if (hasher.verify(__func__))
          break;
      }
      // Otherwise have one shard make it and broadcast it to everyone else
      CustomSerdezID result;
      if (owner_shard->shard_id == dynamic_id_allocator_shard)
      {
        ValueBroadcast<CustomSerdezID> collective(this, COLLECTIVE_LOC_16);
        result = runtime->generate_dynamic_serdez_id(false/*check context*/);
        collective.broadcast(result);
      }
      else
      {
        ValueBroadcast<CustomSerdezID> collective(this, 
            dynamic_id_allocator_shard, COLLECTIVE_LOC_16);
        result = collective.get_value();
      }
      if (++dynamic_id_allocator_shard == total_shards)
        dynamic_id_allocator_shard = 0;
      return result;
    }

    //--------------------------------------------------------------------------
    bool ReplicateContext::perform_semantic_attach(const char *func, 
        unsigned kind, const void *arg, size_t arglen, SemanticTag tag,
        const void *buffer, size_t size, bool is_mutable, bool &global,
        const void *arg2, size_t arg2len)
    //--------------------------------------------------------------------------
    {
      if (inside_registration_callback)
        return TaskContext::perform_semantic_attach(func, kind, arg, arglen,
            tag, buffer, size, is_mutable, global, arg2, arg2len);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(kind, func);
        hasher.hash(arg, arglen, 
            (kind == REPLICATE_ATTACH_TASK_INFO) ? "task_id" : "handle");
        hasher.hash(tag, "tag");
        if (runtime->safe_control_replication > 1)
          hasher.hash(buffer, size, "buffer");
        hasher.hash(is_mutable, "is_mutable");
        hasher.hash(global, "send_to_owner");
        if (arg2 != NULL)
          hasher.hash(arg2, arg2len, "fid");
        if (hasher.verify(func))
          break;
      }
      // Before we do anything else here, we need to make sure that all
      // the shards are done reading before we attempt to mutate the value
      const RtBarrier bar = semantic_attach_barrier.next(this);
      Runtime::phase_barrier_arrive(bar, 1/*count*/);
      // Check to see if we can downgrade this to a local_only update
      if (global && shard_manager->is_total_sharding())
        global = false;
      // Wait until all the reads of the semantic info are done 
      if (!bar.has_triggered())
        bar.wait();
      if (global)
      {
        // If we're still global then just have shard 0 do this for now
        if (owner_shard->shard_id == 0)
          return true;
        post_semantic_attach();
        return false;
      }
      else
      {
        // See if we're the local shard to perform the attach operation
        if (shard_manager->perform_semantic_attach())
          return true;
        post_semantic_attach(); 
        return false;
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::post_semantic_attach(void)
    //--------------------------------------------------------------------------
    {
      if (inside_registration_callback)
        return;
      const RtBarrier bar = semantic_attach_barrier.next(this);
      Runtime::phase_barrier_arrive(bar, 1/*count*/);
      if (!bar.has_triggered())
        bar.wait();
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::hash_future(Murmur3Hasher &hasher,
                                       const unsigned safe_level,
                                       const Future &future, 
                                       const char *description) const
    //--------------------------------------------------------------------------
    {
      if (future.impl == NULL)
        return;
      TaskTreeCoordinates coordinates;
      future.impl->get_future_coordinates(coordinates);
      if (!coordinates.empty())
      {
        for (TaskTreeCoordinates::const_iterator it =
              coordinates.begin(); it != coordinates.end(); it++)
        {
          hasher.hash(it->context_index, description);
          for (int idx = 0; idx < it->index_point.get_dim(); idx++)
            hasher.hash(it->index_point[idx], description);
        }
      }
      else if (safe_level > 1)
      {
        size_t size = 0;
        const void *result = future.impl->get_buffer(executing_processor,
            Memory::SYSTEM_MEM, &size, false/*check*/, true/*silence warn*/);
        hasher.hash(result, size, description);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReplicateContext::hash_future_map(Murmur3Hasher &hasher,
                                  const FutureMap &map, const char *description)
    //--------------------------------------------------------------------------
    {
      if (map.impl == NULL)
        return;
      hasher.hash(map.impl->op_ctx_index, description);
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReplicateContext::hash_index_space_requirements(
          Murmur3Hasher &hasher, const std::vector<IndexSpaceRequirement> &reqs)
    //--------------------------------------------------------------------------
    {
      if (reqs.empty())
        return;
      Serializer rez;
      for (std::vector<IndexSpaceRequirement>::const_iterator it = 
            reqs.begin(); it != reqs.end(); it++)
        ExternalMappable::pack_index_space_requirement(*it, rez);
      hasher.hash(rez.get_buffer(), rez.get_used_bytes(), 
                  "index space requirement");
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReplicateContext::hash_region_requirements(
           Murmur3Hasher &hasher, const std::vector<RegionRequirement> &regions)
    //--------------------------------------------------------------------------
    {
      if (regions.empty())
        return;
      Serializer rez;
      for (std::vector<RegionRequirement>::const_iterator it = 
            regions.begin(); it != regions.end(); it++)
        ExternalMappable::pack_region_requirement(*it, rez);
      hasher.hash(rez.get_buffer(), rez.get_used_bytes(), "region requirement");
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReplicateContext::hash_output_requirements(
           Murmur3Hasher &hasher, const std::vector<OutputRequirement> &outputs)
    //--------------------------------------------------------------------------
    {
      if (outputs.empty())
        return;
      Serializer rez;
      for (std::vector<OutputRequirement>::const_iterator it =
            outputs.begin(); it != outputs.end(); it++)
        ExternalTask::pack_output_requirement(*it, rez);
      hasher.hash(rez.get_buffer(), rez.get_used_bytes(), "output requirement");
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReplicateContext::hash_grants(Murmur3Hasher &hasher,
                                               const std::vector<Grant> &grants)
    //--------------------------------------------------------------------------
    {
      if (grants.empty())
        return;
      Serializer rez;
      for (std::vector<Grant>::const_iterator it = 
            grants.begin(); it != grants.end(); it++)
        ExternalMappable::pack_grant(*it, rez);
      hasher.hash(rez.get_buffer(), rez.get_used_bytes(), "grants");
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReplicateContext::hash_phase_barriers(Murmur3Hasher &hasher,
                                      const std::vector<PhaseBarrier> &barriers)
    //--------------------------------------------------------------------------
    {
      if (barriers.empty())
        return;
      // We're not handling phase barriers that come from hanshakes correctly
      // right now because those can be safely different across the shards
      // so only check this with precise checks for now
      if (!hasher.precise)
        return;
      Serializer rez;
      for (std::vector<PhaseBarrier>::const_iterator it = 
            barriers.begin(); it != barriers.end(); it++)
        ExternalMappable::pack_phase_barrier(*it, rez);
      hasher.hash(rez.get_buffer(), rez.get_used_bytes(), "phase barriers");
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReplicateContext::hash_argument(Murmur3Hasher &hasher,
    unsigned safe_level, const UntypedBuffer &argument, const char *description)
    //--------------------------------------------------------------------------
    {
      if (safe_level == 1)
        return;
      if (argument.get_size() > 0)
        hasher.hash(argument.get_ptr(), argument.get_size(), description);
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReplicateContext::hash_predicate(Murmur3Hasher &hasher,
                                 const Predicate &pred, const char *description)
    //--------------------------------------------------------------------------
    {
      if (pred == Predicate::TRUE_PRED)
        hasher.hash(0, description);
      else if (pred == Predicate::FALSE_PRED)
        hasher.hash(SIZE_MAX, description);
      else
        hasher.hash(pred.impl->creator_ctx_index, description);
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReplicateContext::hash_static_dependences(
        Murmur3Hasher &hasher, const std::vector<StaticDependence> *dependences)
    //--------------------------------------------------------------------------
    {
      if ((dependences == NULL) || dependences->empty())
        return;
      Serializer rez;
      for (std::vector<StaticDependence>::const_iterator it = 
            dependences->begin(); it != dependences->end(); it++)
      {
        hasher.hash(it->previous_offset, "static dependence previous_offset");
        hasher.hash(it->previous_req_index,
                    "static dependence previous_req_index");
        hasher.hash(it->current_req_index, 
                    "static dependence current_req_index");
        hasher.hash(it->dependence_type, "static dependence dependence_type");
        hasher.hash(it->validates, "static dependence validates");
        hasher.hash(it->shard_only, "static dependence shard_only");
        for (std::set<FieldID>::const_iterator fit = 
              it->dependent_fields.begin(); fit != 
              it->dependent_fields.end(); fit++)
          hasher.hash(*fit, "static dependence field");
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::hash_task_launcher(Murmur3Hasher &hasher,
                  const unsigned safe_level, const TaskLauncher &launcher) const
    //--------------------------------------------------------------------------
    {
      hasher.hash(launcher.task_id, "task_id");
      hash_index_space_requirements(hasher, launcher.index_requirements);
      hash_region_requirements(hasher, launcher.region_requirements);
      for (std::vector<Future>::const_iterator it = 
            launcher.futures.begin(); it != launcher.futures.end(); it++)
        hash_future(hasher, safe_level, *it, "futures");
      hash_grants(hasher, launcher.grants);
      hash_phase_barriers(hasher, launcher.wait_barriers);
      hash_phase_barriers(hasher, launcher.arrive_barriers);
      hash_argument(hasher, safe_level, launcher.argument, "argument");
      hash_predicate(hasher, launcher.predicate, "predicate");
      hasher.hash(launcher.map_id, "map_id");
      hasher.hash(launcher.tag, "tag");
      hash_argument(hasher, safe_level, launcher.map_arg, "map_arg");
      for (int idx = 0; idx < launcher.point.get_dim(); idx++)
        hasher.hash(launcher.point[idx], "point");
      hasher.hash(launcher.sharding_space, "sharding_space");
      hash_future(hasher, safe_level, launcher.predicate_false_future,
                  "predicate_false_future");
      hash_argument(hasher, safe_level, launcher.predicate_false_result,
                   "predicate_false_result");
      hash_static_dependences(hasher, launcher.static_dependences);
      hasher.hash(launcher.enable_inlining, "enable_inlining");
      hasher.hash(launcher.local_function_task, "local_function_task");
      hasher.hash(launcher.independent_requirements,"independent_requirements");
      hasher.hash(launcher.silence_warnings, "silence_warnings");
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::hash_index_launcher(Murmur3Hasher &hasher,
                   const unsigned safe_level, const IndexTaskLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      hasher.hash(launcher.task_id, "task_id");
      hasher.hash(launcher.launch_domain, "launch_domain");
      hasher.hash(launcher.launch_space, "launch_space");
      hasher.hash(launcher.sharding_space, "sharding_space");
      hash_index_space_requirements(hasher, launcher.index_requirements);
      hash_region_requirements(hasher, launcher.region_requirements);
      for (std::vector<Future>::const_iterator it =
            launcher.futures.begin(); it != launcher.futures.end(); it++)
        hash_future(hasher, safe_level, *it, "futures");
      for (std::vector<ArgumentMap>::const_iterator it =
            launcher.point_futures.begin(); it != 
            launcher.point_futures.end(); it++)
        hash_future_map(hasher, 
            it->impl->freeze(this, hasher.provenance), "point_futures");
      hash_grants(hasher, launcher.grants);
      hash_phase_barriers(hasher, launcher.wait_barriers);
      hash_phase_barriers(hasher, launcher.arrive_barriers);
      hash_argument(hasher, safe_level, launcher.global_arg, "global_arg");
      if (launcher.argument_map.impl != NULL)
        hash_future_map(hasher,
            launcher.argument_map.impl->freeze(this, hasher.provenance),
                        "argument_map");
      hash_predicate(hasher, launcher.predicate, "predicate");
      hasher.hash(launcher.must_parallelism, "must_parallelism");
      hasher.hash(launcher.map_id, "map_id");
      hasher.hash(launcher.tag, "tag");
      hash_argument(hasher, safe_level, launcher.map_arg, "map_arg");
      hash_future(hasher, safe_level, launcher.predicate_false_future,
                  "predicate_false_future");
      hash_future(hasher, safe_level, launcher.initial_value,
                  "initial_value");
      hash_argument(hasher, safe_level, launcher.predicate_false_result,
                    "predicate_false_result");
      hash_static_dependences(hasher, launcher.static_dependences);
      hasher.hash(launcher.enable_inlining, "enable_inlining");
      hasher.hash(launcher.independent_requirements,"independent_requirements");
      hasher.hash(launcher.silence_warnings, "silence_warnings");
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::hash_execution_constraints(Murmur3Hasher &hasher,
                                      const ExecutionConstraintSet &constraints)
    //--------------------------------------------------------------------------
    {
      hasher.hash(constraints.isa_constraint.isa_prop, "ISA Constraint");
      for (std::vector<Processor::Kind>::const_iterator it =
            constraints.processor_constraint.valid_kinds.begin(); it !=
            constraints.processor_constraint.valid_kinds.end(); it++)
        hasher.hash(*it, "Processor Constraint");
      for (std::vector<ResourceConstraint>::const_iterator it =
            constraints.resource_constraints.begin(); it !=
            constraints.resource_constraints.end(); it++)
      {
        hasher.hash(it->resource_kind, "Resource Constraint resource_kind");
        hasher.hash(it->equality_kind, "Resource Constraint equality_kind");
        hasher.hash(it->value, "Resource Constraint value");
      }
      for (std::vector<LaunchConstraint>::const_iterator it =
            constraints.launch_constraints.begin(); it !=
            constraints.launch_constraints.end(); it++)
      {
        hasher.hash(it->launch_kind, "Launch Constraint launch_kind");
        hasher.hash(it->dims, "Launch Constraint dims");
        for (int i = 0; i < it->dims; i++)
          hasher.hash(it->values[i], "Launch Constraint value");
      }
      for (std::vector<ColocationConstraint>::const_iterator cit =
            constraints.colocation_constraints.begin(); cit !=
            constraints.colocation_constraints.end(); cit++)
      {
        for (std::set<FieldID>::const_iterator it =
              cit->fields.begin(); it != cit->fields.end(); it++)
          hasher.hash(*it, "Colocation Constraint fields");
        for (std::set<unsigned>::const_iterator it =
              cit->indexes.begin(); it != cit->indexes.end(); it++)
          hasher.hash(*it, "Colocation Constraint indexes");
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::hash_layout_constraints(Murmur3Hasher &hasher,
                     const LayoutConstraintSet &constraints, bool hash_pointers)
    //--------------------------------------------------------------------------
    {
      hasher.hash(constraints.specialized_constraint.kind,
          "Specialized Constraint kind");
      hasher.hash(constraints.specialized_constraint.redop,
          "Specialized Constraint redop");
      hasher.hash(constraints.specialized_constraint.max_pieces,
          "Specialized Constraint max_pieces");
      hasher.hash(constraints.specialized_constraint.max_overhead,
          "Specialized Constraint max_overhead");
      hasher.hash(constraints.specialized_constraint.no_access,
          "Specialized Constraint no_access");
      hasher.hash(constraints.specialized_constraint.exact,
          "Specialized Constraint exact");
      for (std::vector<FieldID>::const_iterator it =
            constraints.field_constraint.field_set.begin(); it !=
            constraints.field_constraint.field_set.end(); it++)
        hasher.hash(*it, "Field Constraint fields");
      hasher.hash(constraints.field_constraint.contiguous, 
          "Field Constraint contiguous");
      hasher.hash(constraints.field_constraint.inorder, 
          "Field Constraint inorder");
      if (constraints.memory_constraint.has_kind)
        hasher.hash(constraints.memory_constraint.kind, 
            "Memory Constraint kind");
      if (hash_pointers && constraints.pointer_constraint.is_valid)
      {
        hasher.hash(constraints.pointer_constraint.memory,
            "Pointer Constraint memory");
        hasher.hash(constraints.pointer_constraint.ptr,
            "Pointer Constraint ptr");
      }
      for (std::vector<DimensionKind>::const_iterator it =
            constraints.ordering_constraint.ordering.begin(); it !=
            constraints.ordering_constraint.ordering.end(); it++)
        hasher.hash(*it, "Ordering Constraint ordering");
      hasher.hash(constraints.ordering_constraint.contiguous,
          "Ordering Constraint contiguous");
      for (std::vector<TilingConstraint>::const_iterator it =
            constraints.tiling_constraints.begin(); it !=
            constraints.tiling_constraints.end(); it++)
      {
        hasher.hash(it->dim, "Tiling Constraint dim");
        hasher.hash(it->value, "Tiling Constraint value");
        hasher.hash(it->tiles, "Tiling Constraint tiles");
      }
      for (std::vector<DimensionConstraint>::const_iterator it =
            constraints.dimension_constraints.begin(); it !=
            constraints.dimension_constraints.end(); it++)
      {
        hasher.hash(it->kind, "Dimension Constraint kind");
        hasher.hash(it->eqk, "Dimension Constraint eqk");
        hasher.hash(it->value, "Splitting Constraint value");
      }
      for (std::vector<AlignmentConstraint>::const_iterator it =
            constraints.alignment_constraints.begin(); it !=
            constraints.alignment_constraints.end(); it++)
      {
        hasher.hash(it->fid, "Alignment Constraint fid");
        hasher.hash(it->eqk, "Alignment Constraint eqk");
        hasher.hash(it->alignment, "Alignment Constraint alignment");
      }
      for (std::vector<OffsetConstraint>::const_iterator it =
            constraints.offset_constraints.begin(); it !=
            constraints.offset_constraints.end(); it++)
      {
        hasher.hash(it->fid, "Offset Constraint fid");
        hasher.hash(it->offset, "Offset Constraint offset");
      }
    }

    //--------------------------------------------------------------------------
    bool ReplicateContext::verify_hash(const uint64_t hash[2],
        const char *description, Provenance *provenance, bool verify_every_call)
    //--------------------------------------------------------------------------
    {
      VerifyReplicableExchange exchange(COLLECTIVE_LOC_82, this);
      const VerifyReplicableExchange::ShardHashes &hashes =
        exchange.exchange(hash);
      // If all shards had the same hashes then we are done
      if (hashes.size() == 1)
        return InnerContext::verify_hash(hash, description, 
                                        provenance, verify_every_call);
      if (!verify_every_call)
      {
        // First pass, we detected a violation so go around again and see
        // if we can find out exactly which member is bad
        // Report the warning on one of the lowest hashes
        const std::pair<uint64_t,uint64_t> key(hash[0],hash[1]);
        const VerifyReplicableExchange::ShardHashes::const_iterator
          finder = hashes.find(key);
#ifdef DEBUG_LEGION
        assert(finder != hashes.end());
#endif
        if (finder->second == owner_shard->shard_id)
          log_run.error(
           "Detected control replication violation when invoking %s in "
           "task %s (UID %lld) on shard %d [Provenance: %s]. The hash summary "
           "for the function does not align with the hash summaries from other "
           "call sites. We'll run the hash algorithm again to try to recognize "
           "what value differs between the shards, hang tight...",
           description, get_task_name(), get_unique_id(), owner_shard->shard_id,
           (provenance == NULL) ? "unknown" : provenance->human_str());
      }
      else
        REPORT_LEGION_ERROR(ERROR_CONTROL_REPLICATION_VIOLATION,
            "Specific control replication violation occurred from member %s",
            description);
      return false;
    }

    //--------------------------------------------------------------------------
    EquivalenceSet* ReplicateContext::create_initial_equivalence_set(
                                     unsigned idx, const RegionRequirement &req)
    //--------------------------------------------------------------------------
    {
      return shard_manager->get_initial_equivalence_set(idx, req.region,
          find_parent_physical_context(idx), (owner_shard->shard_id == 0));
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::receive_created_region_contexts(
           const std::vector<RegionNode*> &created_nodes,
           const std::vector<EqKDTree*> &created_trees,
           std::set<RtEvent> &applied_events,
           const ShardMapping *mapping, ShardID source_shard)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(created_nodes.size() == created_trees.size());
#endif
      if ((mapping == NULL) || (mapping->size() != total_shards))
      {
        // Do the volumetric extraction to send all of the equivalence sets
        // from the source shard to the right shards in this context
        std::map<ShardID,LegionMap<RegionNode*,FieldMaskSet<EquivalenceSet> > >
          eq_sets;
        for (unsigned idx = 0; idx < created_nodes.size(); idx++)
          if (created_trees[idx] != NULL)
            created_trees[idx]->find_shard_equivalence_sets(eq_sets,
                source_shard, 0/*lower shard id*/, 
                total_shards-1/*upper shard id*/, created_nodes[idx]);
        for (std::map<ShardID,LegionMap<RegionNode*,
                      FieldMaskSet<EquivalenceSet> > >::const_iterator sit =
              eq_sets.begin(); sit != eq_sets.end(); sit++)
        {
          Serializer rez;
          rez.serialize(shard_manager->did);
          rez.serialize(sit->first);
          rez.serialize<size_t>(sit->second.size());
          for (LegionMap<RegionNode*,
                         FieldMaskSet<EquivalenceSet> >::const_iterator
                rit = sit->second.begin(); rit != sit->second.end(); rit++)
          {
            rez.serialize(rit->first->handle);
            rez.serialize(rit->second.size());
            for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                  rit->second.begin(); it != rit->second.end(); it++)
            {
              it->first->pack_global_ref();
              rez.serialize(it->first->did);
              rez.serialize(it->second);
            }
          }
          shard_manager->send_created_region_contexts(sit->first, rez,
                                                      applied_events);
        }
      }
      else
      {
        // If we have the same number of shards then we know that the 
        // equivalence set k-d trees will be the same so we can just 
        // use the base case. However we still need to send the 
        // information to the right shard
        if (source_shard != owner_shard->shard_id)
        {
          // Pack it up and send it to the source shard
          Serializer rez;
          rez.serialize(shard_manager->did);
          rez.serialize(source_shard);
          rez.serialize<size_t>(created_nodes.size());
          for (unsigned idx = 0; idx < created_nodes.size(); idx++)
          {
            RegionNode *region = created_nodes[idx];
            rez.serialize(region->handle);
            FieldMaskSet<EquivalenceSet> eq_sets;
            if (created_trees[idx] != NULL)
              created_trees[idx]->find_local_equivalence_sets(eq_sets,
                                                        source_shard);
            rez.serialize<size_t>(eq_sets.size());
            for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                  eq_sets.begin(); it != eq_sets.end(); it++)
            {
              it->first->pack_global_ref();
              rez.serialize(it->first->did);
              rez.serialize(it->second);
            }
          }
          shard_manager->send_created_region_contexts(source_shard, rez,
                                                      applied_events);
        }
        else
          InnerContext::receive_created_region_contexts(created_nodes,
                created_trees, applied_events, mapping, source_shard);
      }
    }

    //--------------------------------------------------------------------------
    bool ReplicateContext::compute_shard_to_shard_mapping(
                                   const ShardMapping &src_mapping, 
                                   std::multimap<ShardID,ShardID> &result) const
    //--------------------------------------------------------------------------
    {
      // Build a mapping between the shards of the source context and
      // the shards of this context and then decide what to do with 
      // the data from the source shard. Note that this mapping needs 
      // to be constructed deterministically across all the shards so 
      // that they all agree to do the same thing. The resulting mapping
      // needs to satisfy two properties:
      // 1. Every shard in the destination context must get at least
      //    one update from a shard in the source context (surjective)
      // 2. Every shard in the source context has to got to at least
      //    one shard in the destination context 
      const ShardMapping &dst_mapping = shard_manager->get_mapping();
      const CollectiveMapping &dst_spaces = 
        shard_manager->get_collective_mapping();
      std::vector<ShardID> address_space_shard_offsets(dst_spaces.size(), 0);
      std::vector<bool> targeted(dst_mapping.size(), false);
      size_t total_targets = 0;
      for (ShardID src = 0; src < src_mapping.size(); src++)
      {
        AddressSpaceID src_space = src_mapping[src];
        AddressSpaceID dst_space = dst_spaces.contains(src_space) ?
          src_space : dst_spaces.find_nearest(src_space);
        ShardID dst = address_space_shard_offsets[dst_space];
        // Find the next shard in our map at the dst space
        for (unsigned offset = 0; offset < dst_mapping.size(); offset++,dst++)
        {
#ifdef DEBUG_LEGION
          assert(dst <= dst_mapping.size());
#endif
          if (dst == dst_mapping.size())
            dst = 0; // reset back to the first shard on wrap around
          if (dst_mapping[dst] != dst_space)
            continue;
          result.insert(std::make_pair(src,dst));
          address_space_shard_offsets[dst_space] = dst+1;
          if (!targeted[dst])
          {
            targeted[dst] = true;
            total_targets++;
          }
          break;
        }
#ifdef DEBUG_LEGION
        // Should have assigned something for this shard
        assert(result.lower_bound(src)->first == src);
#endif
      }
      if (total_targets < dst_mapping.size())
      {
        // Not all the destination shards have targets
        // Find the nearest shards in the source to the destination
        // and have them send their results to those destinations too
        // If the source shard is the one we're receiving then
        // add to the destination shard to the targets
        const CollectiveMapping src_spaces(src_mapping,
                      runtime->legion_collective_radix);
        address_space_shard_offsets.resize(src_spaces.size());
        for (ShardID shard = 0; shard < src_spaces.size(); shard++)
          address_space_shard_offsets[shard] = 0;
        for (ShardID dst = 0; dst < targeted.size(); dst++)
        {
          if (targeted[dst])
            continue;
          AddressSpace dst_space = dst_mapping[dst];
          AddressSpace src_space = src_spaces.contains(dst_space) ?
            dst_space : src_spaces.find_nearest(dst_space);
          ShardID src = address_space_shard_offsets[src_space];
          for (unsigned offset = 0; 
                offset < src_mapping.size(); offset++, src++)
          {
#ifdef DEBUG_LEGION
            assert(src <= src_mapping.size());
#endif
            if (src == src_mapping.size())
              src = 0; // reset back to the first shard on wrap around
            if (src_mapping[src] != src_space)
              continue;
            result.insert(std::make_pair(src,dst));
            address_space_shard_offsets[src_space] = src+1;
            break;
          }
#ifdef DEBUG_LEGION
          assert((src % src_mapping.size()) != 
              address_space_shard_offsets[src_space]);
#endif
        }
      }
      // Do the identity check
      for (std::multimap<ShardID,ShardID>::const_iterator it =
            result.begin(); it != result.end(); it++)
        if (it->first != it->second)
          return false;
      // If we get here we passed the identity check so clear the result
      result.clear();
      return true;
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::create_index_space(const Domain &domain, 
                                       TypeTag type_tag, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                             (i > 0), provenance);
        hasher.hash(REPLICATE_CREATE_INDEX_SPACE, __func__);
        hasher.hash(domain, "domain");
        hasher.hash(type_tag, "type_tag");
        if (hasher.verify(__func__))
          break;
      }
      return create_index_space_replicated(&domain, type_tag, provenance); 
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::create_index_space_replicated(
                 const Domain *domain, TypeTag type_tag, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      // Seed this with the first index space broadcast
      if (pending_index_spaces.empty())
      {
        increase_pending_index_spaces(1/*count*/, false/*double*/);
        pending_index_space_check = 0;
      }
      IndexSpace handle;
      bool double_next = false;
      bool double_buffer = false;
      std::pair<ValueBroadcast<ISBroadcast>*,bool> &collective = 
        pending_index_spaces.front();
      CollectiveMapping &collective_mapping =
        shard_manager->get_collective_mapping();
      const RtBarrier creation_bar = creation_barrier.next(this); 
      if (collective.second)
      {
        const ISBroadcast value = collective.first->get_value(false);
        handle = IndexSpace(value.space_id, value.tid, type_tag);
        double_buffer = value.double_buffer;
        runtime->forest->create_index_space(handle, domain, value.did, 
            provenance, &collective_mapping, value.expr_id,
            ApEvent::NO_AP_EVENT, creation_bar);
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
        runtime->forest->revoke_pending_index_space(value.space_id);
#ifdef DEBUG_LEGION
        log_index.debug("Creating index space %x in task%s (ID %lld)",
                        handle.id, get_task_name(), get_unique_id());
#endif
        if (runtime->legion_spy_enabled)
          LegionSpy::log_top_index_space(handle.id, runtime->address_space,
              (provenance == NULL) ? NULL : provenance->human_str());
      }
      else
      {
        const RtEvent done = collective.first->get_done_event();
        if (!done.has_triggered())
        {
          double_next = true;
          done.wait();
        }
        const ISBroadcast value = collective.first->get_value(false);
        handle = IndexSpace(value.space_id, value.tid, type_tag);
#ifdef DEBUG_LEGION
        assert(handle.exists());
#endif
        double_buffer = value.double_buffer;
        runtime->forest->create_index_space(handle, domain, value.did,
            provenance, &collective_mapping, value.expr_id,
            ApEvent::NO_AP_EVENT, creation_bar);
        // Arrive on the creation barrier
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
      }
      // Record this in our context
      register_index_space_creation(handle);
      if (++pending_index_space_check == pending_index_spaces.size())
        pending_index_space_check = 0;
      else
        double_buffer = false;
      // Get new handles in flight for the next time we need them
      // Always add a new one to replace the old one, but double the number
      // in flight if we're not hiding the latency
      increase_pending_index_spaces(double_buffer ? 
          pending_index_spaces.size() + 1 : 1, double_next);
      delete collective.first;
      pending_index_spaces.pop_front();
      return handle;
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::create_unbound_index_space(TypeTag type_tag,
                                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                             i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_UNBOUND_INDEX_SPACE, __func__);
        hasher.hash(type_tag, "type_tag");
        if (hasher.verify(__func__))
          break;
      }
      return create_index_space_replicated(NULL, type_tag, provenance); 
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::increase_pending_index_spaces(unsigned count,
                                                         bool double_next)
    //--------------------------------------------------------------------------
    {
      for (unsigned idx = 0; idx < count; idx++)
      {
        if (owner_shard->shard_id == index_space_allocator_shard)
        {
          const IndexSpaceID space_id = runtime->get_unique_index_space_id(); 
          const DistributedID did = runtime->get_available_distributed_id();
          // We're the owner, so make it locally and then broadcast it
          runtime->forest->record_pending_index_space(space_id);
          // Do our arrival on this generation, should be the last one
          ValueBroadcast<ISBroadcast> *collective = 
            new ValueBroadcast<ISBroadcast>(this, COLLECTIVE_LOC_3);
          collective->broadcast(ISBroadcast(space_id,
                runtime->get_unique_index_tree_id(),
                runtime->get_unique_index_space_expr_id(), did, double_next));
          pending_index_spaces.push_back(
              std::pair<ValueBroadcast<ISBroadcast>*,bool>(collective, true));
        }
        else
        {
          ValueBroadcast<ISBroadcast> *collective = 
            new ValueBroadcast<ISBroadcast>(this, index_space_allocator_shard,
                                            COLLECTIVE_LOC_3);
          register_collective(collective);
          pending_index_spaces.push_back(
              std::pair<ValueBroadcast<ISBroadcast>*,bool>(collective, false));
        }
        index_space_allocator_shard++;
        if (index_space_allocator_shard == total_shards)
          index_space_allocator_shard = 0;
        double_next = false;
      }
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::create_index_space(const Future &future, 
                                       TypeTag type_tag, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                             i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_INDEX_SPACE, __func__);
        hash_future(hasher, runtime->safe_control_replication, future,"future");
        hasher.hash(type_tag, "type_tag");
        if (hasher.verify(__func__))
          break;
      }
      // Seed this with the first index space broadcast
      if (pending_index_spaces.empty())
      {
        increase_pending_index_spaces(1/*count*/, false/*double*/);
        pending_index_space_check = 0;
      }
      IndexSpace handle;
      bool double_next = false;
      bool double_buffer = false;
      std::pair<ValueBroadcast<ISBroadcast>*,bool> &collective = 
        pending_index_spaces.front();
      IndexSpaceNode *node = NULL;
      // Get a new creation operation
      CreationOp *creator_op = runtime->get_available_creation_op();
      const ApEvent ready = creator_op->get_completion_event();
      CollectiveMapping &collective_mapping =
        shard_manager->get_collective_mapping();
      const RtBarrier creation_bar = creation_barrier.next(this);
      if (collective.second)
      {
        const ISBroadcast value = collective.first->get_value(false);
        handle = IndexSpace(value.space_id, value.tid, type_tag);
        double_buffer = value.double_buffer;
        node = runtime->forest->create_index_space(handle, NULL, value.did, 
                                provenance, &collective_mapping, value.expr_id,
                                ready, creation_bar);
        // Arrive on the creation barrier
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
        runtime->forest->revoke_pending_index_space(value.space_id);
#ifdef DEBUG_LEGION
        log_index.debug("Creating index space %x in task%s (ID %lld)",
                        handle.id, get_task_name(), get_unique_id());
#endif
        if (runtime->legion_spy_enabled)
          LegionSpy::log_top_index_space(handle.id, runtime->address_space,
              (provenance == NULL) ? NULL : provenance->human_str());
      }
      else
      {
        const RtEvent done = collective.first->get_done_event();
        if (!done.has_triggered())
        {
          double_next = true;
          done.wait();
        }
        const ISBroadcast value = collective.first->get_value(false);
        handle = IndexSpace(value.space_id, value.tid, type_tag);
#ifdef DEBUG_LEGION
        assert(handle.exists());
#endif
        double_buffer = value.double_buffer;
        node = runtime->forest->create_index_space(handle, NULL, value.did,
                                provenance, &collective_mapping, value.expr_id,
                                ready, creation_bar);
        // Arrive on the creation barrier
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
      }
      creator_op->initialize_index_space(this, node, future, provenance,
          shard_manager->is_first_local_shard(owner_shard), 
          &(shard_manager->get_collective_mapping()));
      add_to_dependence_queue(creator_op);
      // Record this in our context
      register_index_space_creation(handle);
      if (++pending_index_space_check == pending_index_spaces.size())
        pending_index_space_check = 0;
      else
        double_buffer = false;
      // Get new handles in flight for the next time we need them
      // Always add a new one to replace the old one, but double the number
      // in flight if we're not hiding the latency
      increase_pending_index_spaces(double_buffer ? 
          pending_index_spaces.size() + 1 : 1, double_next);
      delete collective.first;
      pending_index_spaces.pop_front();
      return handle;
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::create_index_space(
                 const std::vector<DomainPoint> &points, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                             i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_INDEX_SPACE, __func__);
        for (unsigned idx = 0; idx < points.size(); idx++)
          hasher.hash(points[idx], "points");
        if (hasher.verify(__func__))
          break;
      }
      switch (points[0].get_dim())
      {
#define DIMFUNC(DIM) \
        case DIM: \
          { \
            std::vector<Realm::Point<DIM,coord_t> > \
              realm_points(points.size()); \
            for (unsigned idx = 0; idx < points.size(); idx++) \
              realm_points[idx] = Point<DIM,coord_t>(points[idx]); \
            const DomainT<DIM,coord_t> realm_is( \
                (Realm::IndexSpace<DIM,coord_t>(realm_points))); \
            const Domain bounds(realm_is); \
            return create_index_space_replicated(&bounds, \
                NT_TemplateHelper::encode_tag<DIM,coord_t>(), provenance); \
          }
        LEGION_FOREACH_N(DIMFUNC)
#undef DIMFUNC
        default:
          assert(false);
      }
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::create_index_space(
                       const std::vector<Domain> &rects, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
          ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                             i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_INDEX_SPACE, __func__);
        for (unsigned idx = 0; idx < rects.size(); idx++)
          hasher.hash(rects[idx], "rects");
        if (hasher.verify(__func__))
          break;
      }
      switch (rects[0].get_dim())
      {
#define DIMFUNC(DIM) \
        case DIM: \
          { \
            std::vector<Realm::Rect<DIM,coord_t> > realm_rects(rects.size()); \
            for (unsigned idx = 0; idx < rects.size(); idx++) \
              realm_rects[idx] = Rect<DIM,coord_t>(rects[idx]); \
            const DomainT<DIM,coord_t> realm_is( \
                (Realm::IndexSpace<DIM,coord_t>(realm_rects))); \
            const Domain bounds(realm_is); \
            return create_index_space_replicated(&bounds, \
                NT_TemplateHelper::encode_tag<DIM,coord_t>(), provenance); \
          }
        LEGION_FOREACH_N(DIMFUNC)
#undef DIMFUNC
        default:
          assert(false);
      }
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::union_index_spaces(
                  const std::vector<IndexSpace> &spaces, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                             i > 0, provenance);
        hasher.hash(REPLICATE_UNION_INDEX_SPACES, __func__);
        for (std::vector<IndexSpace>::const_iterator it = 
              spaces.begin(); it != spaces.end(); it++)
          hasher.hash(*it, "spaces");
        if (hasher.verify(__func__))
          break;
      }
      if (spaces.empty())
        return IndexSpace::NO_SPACE;
      bool none_exists = true;
      for (std::vector<IndexSpace>::const_iterator it = 
            spaces.begin(); it != spaces.end(); it++)
      {
        if (none_exists && it->exists())
          none_exists = false;
        if (spaces[0].get_type_tag() != it->get_type_tag())
          REPORT_LEGION_ERROR(ERROR_DYNAMIC_TYPE_MISMATCH,
                        "Dynamic type mismatch in 'union_index_spaces' "
                        "performed in task %s (UID %lld)",
                        get_task_name(), get_unique_id())
      }
      if (none_exists)
        return IndexSpace::NO_SPACE;
      // Seed this with the first index space broadcast
      if (pending_index_spaces.empty())
      {
        increase_pending_index_spaces(1/*count*/, false/*double*/);
        pending_index_space_check = 0;
      }
      IndexSpace handle;
      bool double_next = false;
      bool double_buffer = false;
      std::pair<ValueBroadcast<ISBroadcast>*,bool> &collective = 
        pending_index_spaces.front();
      CollectiveMapping &collective_mapping =
        shard_manager->get_collective_mapping();
      const RtBarrier creation_bar = creation_barrier.next(this);
      if (collective.second)
      {
        const ISBroadcast value = collective.first->get_value(false);
        handle = IndexSpace(value.space_id, value.tid,spaces[0].get_type_tag());
        double_buffer = value.double_buffer;
        runtime->forest->create_union_space(handle, value.did, provenance,
            spaces,creation_bar, &collective_mapping, value.expr_id);
        // Arrive on the creation barrier
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
        runtime->forest->revoke_pending_index_space(value.space_id);
#ifdef DEBUG_LEGION
        log_index.debug("Creating index space %x in task%s (ID %lld)",
                        handle.id, get_task_name(), get_unique_id());
#endif
        if (runtime->legion_spy_enabled)
          LegionSpy::log_top_index_space(handle.id, runtime->address_space,
              (provenance == NULL) ? NULL : provenance->human_str());
      }
      else
      {
        const RtEvent done = collective.first->get_done_event();
        if (!done.has_triggered())
        {
          double_next = true;
          done.wait();
        }
        const ISBroadcast value = collective.first->get_value(false);
        handle = IndexSpace(value.space_id, value.tid,spaces[0].get_type_tag());
#ifdef DEBUG_LEGION
        assert(handle.exists());
#endif
        double_buffer = value.double_buffer;
        runtime->forest->create_union_space(handle, value.did, provenance,
            spaces,creation_bar, &collective_mapping, value.expr_id);
        // Arrive on the creation barrier
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
      }
      // Record this in our context
      register_index_space_creation(handle);
      if (++pending_index_space_check == pending_index_spaces.size())
        pending_index_space_check = 0;
      else
        double_buffer = false;
      // Get new handles in flight for the next time we need them
      // Always add a new one to replace the old one, but double the number
      // in flight if we're not hiding the latency
      increase_pending_index_spaces(double_buffer ? 
          pending_index_spaces.size() + 1 : 1, double_next);
      delete collective.first;
      pending_index_spaces.pop_front();
      return handle;
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::intersect_index_spaces(
                  const std::vector<IndexSpace> &spaces, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                             i > 0, provenance);
        hasher.hash(REPLICATE_INTERSECT_INDEX_SPACES, __func__);
        for (std::vector<IndexSpace>::const_iterator it = 
              spaces.begin(); it != spaces.end(); it++)
          hasher.hash(*it, "spaces");
        if (hasher.verify(__func__))
          break;
      }
      if (spaces.empty())
        return IndexSpace::NO_SPACE;
      bool none_exists = true;
      for (std::vector<IndexSpace>::const_iterator it = 
            spaces.begin(); it != spaces.end(); it++)
      {
        if (none_exists && it->exists())
          none_exists = false;
        if (spaces[0].get_type_tag() != it->get_type_tag())
          REPORT_LEGION_ERROR(ERROR_DYNAMIC_TYPE_MISMATCH,
                        "Dynamic type mismatch in 'intersect_index_spaces' "
                        "performed in task %s (UID %lld)",
                        get_task_name(), get_unique_id())
      }
      if (none_exists)
        return IndexSpace::NO_SPACE;
      // Seed this with the first index space broadcast
      if (pending_index_spaces.empty())
      {
        increase_pending_index_spaces(1/*count*/, false/*double*/);
        pending_index_space_check = 0;
      }
      IndexSpace handle;
      bool double_next = false;
      bool double_buffer = false;
      std::pair<ValueBroadcast<ISBroadcast>*,bool> &collective = 
        pending_index_spaces.front();
      CollectiveMapping &collective_mapping =
        shard_manager->get_collective_mapping();
      const RtBarrier creation_bar = creation_barrier.next(this);
      if (collective.second)
      {
        const ISBroadcast value = collective.first->get_value(false);
        handle = IndexSpace(value.space_id, value.tid,spaces[0].get_type_tag());
        double_buffer = value.double_buffer;
        runtime->forest->create_intersection_space(handle, value.did,provenance,
            spaces,creation_bar, &collective_mapping, value.expr_id);
        // Arrive on the creation barrier
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
        runtime->forest->revoke_pending_index_space(value.space_id);
#ifdef DEBUG_LEGION
        log_index.debug("Creating index space %x in task%s (ID %lld)",
                        handle.id, get_task_name(), get_unique_id());
#endif
        if (runtime->legion_spy_enabled)
          LegionSpy::log_top_index_space(handle.id, runtime->address_space,
              (provenance == NULL) ? NULL : provenance->human_str());
      }
      else
      {
        const RtEvent done = collective.first->get_done_event();
        if (!done.has_triggered())
        {
          double_next = true;
          done.wait();
        }
        const ISBroadcast value = collective.first->get_value(false);
        handle = IndexSpace(value.space_id, value.tid,spaces[0].get_type_tag());
#ifdef DEBUG_LEGION
        assert(handle.exists());
#endif
        double_buffer = value.double_buffer;
        runtime->forest->create_intersection_space(handle, value.did,provenance,
            spaces,creation_bar, &collective_mapping, value.expr_id);
        // Arrive on the creation barrier
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
      }
      // Record this in our context
      register_index_space_creation(handle);
      if (++pending_index_space_check == pending_index_spaces.size())
        pending_index_space_check = 0;
      else
        double_buffer = false;
      // Get new handles in flight for the next time we need them
      // Always add a new one to replace the old one, but double the number
      // in flight if we're not hiding the latency
      increase_pending_index_spaces(double_buffer ? 
          pending_index_spaces.size() + 1 : 1, double_next);
      delete collective.first;
      pending_index_spaces.pop_front();
      return handle;
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::subtract_index_spaces(
                      IndexSpace left, IndexSpace right, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                             i > 0, provenance);
        hasher.hash(REPLICATE_SUBTRACT_INDEX_SPACES, __func__);
        hasher.hash(left, "left");
        hasher.hash(right, "right");
        if (hasher.verify(__func__))
          break;
      }
      if (!left.exists())
        return IndexSpace::NO_SPACE;
      if (right.exists() && left.get_type_tag() != right.get_type_tag())
        REPORT_LEGION_ERROR(ERROR_DYNAMIC_TYPE_MISMATCH,
                        "Dynamic type mismatch in 'create_difference_spaces' "
                        "performed in task %s (UID %lld)",
                        get_task_name(), get_unique_id())
      // Seed this with the first index space broadcast
      if (pending_index_spaces.empty())
      {
        increase_pending_index_spaces(1/*count*/, false/*double*/);
        pending_index_space_check = 0;
      }
      IndexSpace handle;
      bool double_next = false;
      bool double_buffer = false;
      std::pair<ValueBroadcast<ISBroadcast>*,bool> &collective = 
        pending_index_spaces.front();
      CollectiveMapping &collective_mapping =
        shard_manager->get_collective_mapping();
      const RtBarrier creation_bar = creation_barrier.next(this);
      if (collective.second)
      {
        const ISBroadcast value = collective.first->get_value(false);
        handle = IndexSpace(value.space_id, value.tid, left.get_type_tag());
        double_buffer = value.double_buffer;
        runtime->forest->create_difference_space(handle, value.did, provenance,
            left, right, creation_bar, &collective_mapping, value.expr_id);
        // Arrive on the creation barrier
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
        runtime->forest->revoke_pending_index_space(value.space_id);
#ifdef DEBUG_LEGION
        log_index.debug("Creating index space %x in task%s (ID %lld)",
                        handle.id, get_task_name(), get_unique_id());
#endif
        if (runtime->legion_spy_enabled)
          LegionSpy::log_top_index_space(handle.id, runtime->address_space,
              (provenance == NULL) ? NULL : provenance->human_str());
      }
      else
      {
        const RtEvent done = collective.first->get_done_event();
        if (!done.has_triggered())
        {
          double_next = true;
          done.wait();
        }
        const ISBroadcast value = collective.first->get_value(false);
        handle = IndexSpace(value.space_id, value.tid, left.get_type_tag());
#ifdef DEBUG_LEGION
        assert(handle.exists());
#endif
        double_buffer = value.double_buffer;
        runtime->forest->create_difference_space(handle, value.did, provenance,
            left, right, creation_bar, &collective_mapping, value.expr_id);
        // Arrive on the creation barrier
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
      }
      // Record this in our context
      register_index_space_creation(handle);
      if (++pending_index_space_check == pending_index_spaces.size())
        pending_index_space_check = 0;
      else
        double_buffer = false;
      // Get new handles in flight for the next time we need them
      // Always add a new one to replace the old one, but double the number
      // in flight if we're not hiding the latency
      increase_pending_index_spaces(double_buffer ? 
          pending_index_spaces.size() + 1 : 1, double_next);
      delete collective.first;
      pending_index_spaces.pop_front();
      return handle;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::create_shared_ownership(IndexSpace handle)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
          ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_CREATE_SHARED_OWNERSHIP, __func__);
        hasher.hash(handle, "handle");
        if (hasher.verify(__func__))
          break;
      }
      if (!handle.exists())
        return;
      // Check to see if this is a top-level index space, if not then
      // we shouldn't even be destroying it
      if (!runtime->forest->is_top_level_index_space(handle))
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_SHARED_OWNERSHIP,
            "Illegal call to create shared ownership for index space %x in " 
            "task %s (UID %lld) which is not a top-level index space. Legion "
            "only permits top-level index spaces to have shared ownership.", 
            handle.get_id(), get_task_name(), get_unique_id())
      if (shard_manager->is_total_sharding() &&
          shard_manager->is_first_local_shard(owner_shard))
        runtime->create_shared_ownership(handle, true/*total sharding*/);
      else if (owner_shard->shard_id == 0)
        runtime->create_shared_ownership(handle);
      AutoLock priv_lock(privilege_lock);
      std::map<IndexSpace,unsigned>::iterator finder = 
        created_index_spaces.find(handle);
      if (finder != created_index_spaces.end())
        finder->second++;
      else
        created_index_spaces[handle] = 1;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::destroy_index_space(IndexSpace handle,
               const bool unordered, const bool recurse, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && !unordered && (i < 2)
           && ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                             i > 0, provenance);
        hasher.hash(REPLICATE_DESTROY_INDEX_SPACE, __func__);
        hasher.hash(handle, "handle");
        hasher.hash(recurse, "recurse");
        if (hasher.verify(__func__))
          break;
      }
      if (!handle.exists())
        return;
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_index.debug("Destroying index space %x in task %s (ID %lld)", 
                        handle.id, get_task_name(), get_unique_id());
#endif
      // Check to see if this is a top-level index space, if not then
      // we shouldn't even be destroying it
      if (!runtime->forest->is_top_level_index_space(handle))
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_RESOURCE_DESTRUCTION,
            "Illegal call to destroy index space %x in task %s (UID %lld) "
            "which is not a top-level index space. Legion only permits "
            "top-level index spaces to be destroyed.", handle.get_id(),
            get_task_name(), get_unique_id())
      // Check to see if this is one that we should be allowed to destory
      std::vector<IndexPartition> sub_partitions;
      {
        AutoLock priv_lock(privilege_lock);
        std::map<IndexSpace,unsigned>::iterator finder = 
          created_index_spaces.find(handle);
        if (finder == created_index_spaces.end())
        {
          // If we didn't make the index space in this context, just
          // record it and keep going, it will get handled later
          deleted_index_spaces.push_back(
              DeletedIndexSpace(handle, recurse, provenance));
          return;
        }
        else
        {
#ifdef DEBUG_LEGION
          assert(finder->second > 0);
#endif
          if (--finder->second == 0)
            created_index_spaces.erase(finder);
          else
            return;
        }
        if (recurse)
        {
          // Also remove any index partitions for this index space tree
          for (std::map<IndexPartition,unsigned>::iterator it = 
                created_index_partitions.begin(); it !=
                created_index_partitions.end(); /*nothing*/)
          {
            if (it->first.get_tree_id() == handle.get_tree_id()) 
            {
              sub_partitions.push_back(it->first);
#ifdef DEBUG_LEGION
              assert(it->second > 0);
#endif
              if (--it->second == 0)
              {
                std::map<IndexPartition,unsigned>::iterator to_delete = it++;
                created_index_partitions.erase(to_delete);
              }
              else
                it++;
            }
            else
              it++;
          }
        }
      }
      ReplDeletionOp *op = runtime->get_available_repl_deletion_op();
      op->initialize_index_space_deletion(this, handle, sub_partitions,
                                          unordered, provenance);
      op->initialize_replication(this,
          shard_manager->is_first_local_shard(owner_shard));
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered index space deletion performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::create_shared_ownership(IndexPartition handle)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_CREATE_SHARED_OWNERSHIP, __func__); 
        hasher.hash(handle, "handle");
        if (hasher.verify(__func__))
          break;
      }
      if (!handle.exists())
        return;
      if (shard_manager->is_total_sharding() &&
          shard_manager->is_first_local_shard(owner_shard))
        runtime->create_shared_ownership(handle, true/*total sharding*/);
      else if (owner_shard->shard_id == 0)
        runtime->create_shared_ownership(handle);
      AutoLock priv_lock(privilege_lock);
      std::map<IndexPartition,unsigned>::iterator finder = 
        created_index_partitions.find(handle);
      if (finder != created_index_partitions.end())
        finder->second++;
      else
        created_index_partitions[handle] = 1;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::destroy_index_partition(IndexPartition handle,
               const bool unordered, const bool recurse, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && !unordered && (i < 2)
           && ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                             i > 0, provenance);
        hasher.hash(REPLICATE_DESTROY_INDEX_PARTITION, __func__);
        hasher.hash(handle, "handle");
        hasher.hash(recurse, "recurse");
        if (hasher.verify(__func__))
          break;
      }
      if (!handle.exists())
        return;
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_index.debug("Destroying index partition %x in task %s (ID %lld)", 
                        handle.id, get_task_name(), get_unique_id());
#endif
      std::vector<IndexPartition> sub_partitions;
      {
        AutoLock priv_lock(privilege_lock);
        std::map<IndexPartition,unsigned>::iterator finder = 
          created_index_partitions.find(handle);
        if (finder != created_index_partitions.end())
        {
#ifdef DEBUG_LEGION
          assert(finder->second > 0);
#endif
          if (--finder->second == 0)
            created_index_partitions.erase(finder);
          else
            return;
          if (recurse)
          {
            // Remove any other partitions that this partition dominates
            for (std::map<IndexPartition,unsigned>::iterator it = 
                  created_index_partitions.begin(); it !=
                  created_index_partitions.end(); /*nothing*/)
            {
              if ((handle.get_tree_id() == it->first.get_tree_id()) &&
                  runtime->forest->is_dominated_tree_only(it->first, handle))
              {
                sub_partitions.push_back(it->first);
#ifdef DEBUG_LEGION
                assert(it->second > 0);
#endif
                if (--it->second == 0)
                {
                  std::map<IndexPartition,unsigned>::iterator to_delete = it++;
                  created_index_partitions.erase(to_delete);
                }
                else
                  it++;
              }
              else
                it++;
            }
          }
        }
        else
        {
          // If we didn't make the partition, record it and keep going
          deleted_index_partitions.push_back(
              DeletedPartition(handle, recurse, provenance));
          return;
        }
      }
      ReplDeletionOp *op = runtime->get_available_repl_deletion_op();
      op->initialize_index_part_deletion(this, handle, sub_partitions,
                                         unordered, provenance);
      op->initialize_replication(this,
          shard_manager->is_first_local_shard(owner_shard));
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered index partition deletion performed after task %s"
            " (UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::increase_pending_partitions(unsigned count,
                                                       bool double_next)
    //--------------------------------------------------------------------------
    {
      for (unsigned idx = 0; idx < count; idx++)
      {
        if (owner_shard->shard_id == index_partition_allocator_shard)
        {
          const IndexPartitionID pid = runtime->get_unique_index_partition_id();
          const DistributedID did = runtime->get_available_distributed_id();
          // We're the owner, so make it locally and then broadcast it
          runtime->forest->record_pending_partition(pid);
          // Do our arrival on this generation, should be the last one
          ValueBroadcast<IPBroadcast> *collective = 
            new ValueBroadcast<IPBroadcast>(this, COLLECTIVE_LOC_7);
          collective->broadcast(IPBroadcast(pid, did, double_next));
          pending_index_partitions.push_back(
              std::pair<ValueBroadcast<IPBroadcast>*,ShardID>(collective, 
                                        index_partition_allocator_shard));
        }
        else
        {
          ValueBroadcast<IPBroadcast> *collective = 
           new ValueBroadcast<IPBroadcast>(this,index_partition_allocator_shard,
                                           COLLECTIVE_LOC_7);
          register_collective(collective);
          pending_index_partitions.push_back(
              std::pair<ValueBroadcast<IPBroadcast>*,ShardID>(collective, 
                                        index_partition_allocator_shard));
        }
        index_partition_allocator_shard++;
        if (index_partition_allocator_shard == total_shards)
          index_partition_allocator_shard = 0;
        double_next = false;
      }
    }

    //--------------------------------------------------------------------------
    bool ReplicateContext::create_shard_partition(Operation *op, 
           IndexPartition &pid, IndexSpace parent, 
           IndexSpace color_space, Provenance *provenance,
           PartitionKind part_kind, LegionColor partition_color,
           bool color_generated)
    //--------------------------------------------------------------------------
    {
      if (pending_index_partitions.empty())
      {
        increase_pending_partitions(1/*count*/, false/*double*/);
        pending_index_partition_check = 0;
      }
      bool double_next = false;
      bool double_buffer = false;
      std::pair<ValueBroadcast<IPBroadcast>*,ShardID> &collective = 
        pending_index_partitions.front();
      const bool is_owner = (collective.second == owner_shard->shard_id);
      CollectiveMapping &collective_mapping = 
        shard_manager->get_collective_mapping();
      const RtBarrier creation_bar = creation_barrier.next(this);
      if (is_owner)
      {
        const IPBroadcast value = collective.first->get_value(false);
        pid.id = value.pid;
        double_buffer = value.double_buffer;
        // Have to do our registration before broadcasting
        RtEvent safe_event = runtime->forest->create_pending_partition(this,
                                           pid, parent, color_space, 
                                           partition_color, part_kind,
                                           value.did, provenance,
                                           &collective_mapping, creation_bar);
        // Broadcast the color if we have to generate it
        if (color_generated)
        {
#ifdef DEBUG_LEGION
          assert(partition_color != INVALID_COLOR); // we should have an ID
#endif
          ValueBroadcast<LegionColor> color_collective(this, COLLECTIVE_LOC_8);
          color_collective.broadcast(partition_color);
        }
        // Signal that we're done our creation
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/, safe_event);
        runtime->forest->revoke_pending_partition(value.pid);
      }
      else
      {
        const RtEvent done = collective.first->get_done_event();
        if (!done.has_triggered())
        {
          double_next = true;
          done.wait();
        }
        const IPBroadcast value = collective.first->get_value(false);
        pid.id = value.pid;
        double_buffer = value.double_buffer;
#ifdef DEBUG_LEGION
        assert(pid.exists());
#endif
        // If we need a color then we can get that too
        if (color_generated)
        {
          ValueBroadcast<LegionColor> color_collective(this, collective.second,
                                                       COLLECTIVE_LOC_8);
          partition_color = color_collective.get_value();
#ifdef DEBUG_LEGION
          assert(partition_color != INVALID_COLOR);
#endif
        }
        // Do our registration
        RtEvent safe_event = runtime->forest->create_pending_partition(this,
                                         pid, parent, color_space, 
                                         partition_color, part_kind,
                                         value.did, provenance,
                                         &collective_mapping, creation_bar);
        // Signal that we're done our creation
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/, safe_event);
      }
      if (++pending_index_partition_check == pending_index_partitions.size()) 
        pending_index_partition_check = 0;
      else
        double_buffer = false;
      // Get new handles in flight for the next time we need them
      // Always add a new one to replace the old one, but double the number
      // in flight if we're not hiding the latency
      increase_pending_partitions(double_buffer ? 
        pending_index_partitions.size() + 1 : 1, double_next);
      // Clean up the collective
      delete collective.first;
      pending_index_partitions.pop_front();
      return is_owner;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_equal_partition(
                                                      IndexSpace parent,
                                                      IndexSpace color_space,
                                                      size_t granularity,
                                                      Color color,
                                                      Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);  
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_EQUAL_PARTITION, __func__);
        hasher.hash(parent, "parent");
        hasher.hash(color_space, "color_space");
        hasher.hash(granularity, "granularity");
        hasher.hash(color, "color");
        if (hasher.verify(__func__))
          break;
      }
      IndexPartition pid(0/*temp*/,parent.get_tree_id(),parent.get_type_tag());
      LegionColor partition_color = INVALID_COLOR;
      bool color_generated = false; 
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      else
        color_generated = true;
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      if (create_shard_partition(part_op, pid, parent, color_space, provenance,
            LEGION_DISJOINT_COMPLETE_KIND, partition_color, color_generated))
        log_index.debug("Creating equal partition %d with parent index space %x"
                        " in task %s (ID %lld)", pid.id, parent.id,
                        get_task_name(), get_unique_id());
      part_op->initialize_equal_partition(this, pid, granularity, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_partition_by_weights(
                                                IndexSpace parent,
                                                const FutureMap &weights, 
                                                IndexSpace color_space,
                                                size_t granularity, Color color,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);  
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PARTITION_BY_WEIGHTS, __func__);
        hasher.hash(parent, "parent");
        hash_future_map(hasher, weights, "weights");
        hasher.hash(color_space, "color_space");
        hasher.hash(granularity, "granularity");
        hasher.hash(color, "color");
        if (hasher.verify(__func__))
          break;
      }
      IndexPartition pid(0/*temp*/,parent.get_tree_id(),parent.get_type_tag());
      LegionColor partition_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      else
        color_generated = true;
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      if (create_shard_partition(part_op, pid, parent, color_space, provenance,
            LEGION_DISJOINT_COMPLETE_KIND, partition_color, color_generated))
        log_index.debug("Creating equal partition %d with parent index space %x"
                        " in task %s (ID %lld)", pid.id, parent.id,
                        get_task_name(), get_unique_id());
      part_op->initialize_weight_partition(this, pid, weights, 
                                           granularity, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_partition_by_union(
                                          IndexSpace parent,
                                          IndexPartition handle1,
                                          IndexPartition handle2,
                                          IndexSpace color_space,
                                          PartitionKind kind, Color color,
                                          Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PARTITION_BY_UNION, __func__);
        hasher.hash(parent, "parent");
        hasher.hash(handle1, "handle1");
        hasher.hash(handle2, "handle2");
        hasher.hash(color_space, "color_space");
        hasher.hash(kind, "kind");
        hasher.hash(color, "color");
        if (hasher.verify(__func__))
          break;
      }
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, kind)
#ifdef DEBUG_LEGION 
      if (parent.get_tree_id() != handle1.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEX_TREE_MISMATCH,
                        "IndexPartition %d is not part of the same "
                        "index tree as IndexSpace %d in create "
                        "partition by union!", handle1.id, parent.id)
      if (parent.get_tree_id() != handle2.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEX_TREE_MISMATCH,
                        "IndexPartition %d is not part of the same "
                        "index tree as IndexSpace %d in create "
                        "partition by union!", handle2.id, parent.id)
#endif
      LegionColor partition_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      else
        color_generated = true; 
      // If either partition is aliased the result is aliased
      if ((kind == LEGION_COMPUTE_KIND) || 
          (kind == LEGION_COMPUTE_COMPLETE_KIND) ||
          (kind == LEGION_COMPUTE_INCOMPLETE_KIND))
      {
        // If one of these partitions is aliased then the result is aliased
        IndexPartNode *p1 = runtime->forest->get_node(handle1);
        if (p1->is_disjoint(true/*from app*/))
        {
          IndexPartNode *p2 = runtime->forest->get_node(handle2);
          if (!p2->is_disjoint(true/*from app*/))
          {
            if (kind == LEGION_COMPUTE_KIND)
              kind = LEGION_ALIASED_KIND;
            else if (kind == LEGION_COMPUTE_COMPLETE_KIND)
              kind = LEGION_ALIASED_COMPLETE_KIND;
            else
              kind = LEGION_ALIASED_INCOMPLETE_KIND;
          }
        }
        else
        {
          if (kind == LEGION_COMPUTE_KIND)
            kind = LEGION_ALIASED_KIND;
          else if (kind == LEGION_COMPUTE_COMPLETE_KIND)
            kind = LEGION_ALIASED_COMPLETE_KIND;
          else
            kind = LEGION_ALIASED_INCOMPLETE_KIND;
        }
      }
      IndexPartition pid(0/*temp*/,parent.get_tree_id(),parent.get_type_tag());
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      if (create_shard_partition(part_op, pid, parent, color_space, provenance,
            kind, partition_color, color_generated))
        log_index.debug("Creating union partition %d with parent index "
                        "space %x in task %s (ID %lld)", pid.id, parent.id,
                        get_task_name(), get_unique_id());
      part_op->initialize_union_partition(this, pid, handle1, 
                                          handle2, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_partition_by_intersection(
                                              IndexSpace parent,
                                              IndexPartition handle1,
                                              IndexPartition handle2,
                                              IndexSpace color_space,
                                              PartitionKind kind, Color color,
                                              Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PARTITION_BY_INTERSECTION, __func__);
        hasher.hash(parent, "parent");
        hasher.hash(handle1, "handle1");
        hasher.hash(handle2, "handle2");
        hasher.hash(color_space, "color_space");
        hasher.hash(kind, "kind");
        hasher.hash(color, "color");
        if (hasher.verify(__func__))
          break;
      }
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, kind)
#ifdef DEBUG_LEGION 
      if (parent.get_tree_id() != handle1.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEX_TREE_MISMATCH,
                        "IndexPartition %d is not part of the same "
                        "index tree as IndexSpace %d in create partition by "
                        "intersection!", handle1.id, parent.id)
      if (parent.get_tree_id() != handle2.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEX_TREE_MISMATCH,
                        "IndexPartition %d is not part of the same "
                        "index tree as IndexSpace %d in create partition by "
                        "intersection!", handle2.id, parent.id)
#endif
      LegionColor partition_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      else
        color_generated = true; 
      // If either partition is disjoint then the result is disjoint
      if ((kind == LEGION_COMPUTE_KIND) || 
          (kind == LEGION_COMPUTE_COMPLETE_KIND) ||
          (kind == LEGION_COMPUTE_INCOMPLETE_KIND))
      {
        IndexPartNode *p1 = runtime->forest->get_node(handle1);
        if (!p1->is_disjoint(true/*from app*/))
        {
          IndexPartNode *p2 = runtime->forest->get_node(handle2);
          if (p2->is_disjoint(true/*from app*/))
          {
            if (kind == LEGION_COMPUTE_KIND)
              kind = LEGION_DISJOINT_KIND;
            else if (kind == LEGION_COMPUTE_COMPLETE_KIND)
              kind = LEGION_DISJOINT_COMPLETE_KIND;
            else
              kind = LEGION_DISJOINT_INCOMPLETE_KIND;
          }
        }
        else
        {
          if (kind == LEGION_COMPUTE_KIND)
            kind = LEGION_DISJOINT_KIND;
          else if (kind == LEGION_COMPUTE_COMPLETE_KIND)
            kind = LEGION_DISJOINT_COMPLETE_KIND;
          else
            kind = LEGION_DISJOINT_INCOMPLETE_KIND;
        }
      }
      IndexPartition pid(0/*temp*/,parent.get_tree_id(),parent.get_type_tag());
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      if (create_shard_partition(part_op, pid, parent, color_space, provenance,
            kind, partition_color, color_generated))
        log_index.debug("Creating intersection partition %d with parent "
                        "index space %x in task %s (ID %lld)", pid.id, 
                        parent.id, get_task_name(), get_unique_id());
      part_op->initialize_intersection_partition(this, pid, handle1,
                                                 handle2, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_partition_by_intersection(
                                                IndexSpace parent,
                                                IndexPartition partition,
                                                PartitionKind kind, Color color,
                                                bool dominates,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PARTITION_BY_INTERSECTION, __func__);
        hasher.hash(parent, "parent");
        hasher.hash(partition, "partition");
        hasher.hash(kind, "kind");
        hasher.hash(color, "color");
        hasher.hash(dominates, "dominates");
        if (hasher.verify(__func__))
          break;
      }
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, kind)
#ifdef DEBUG_LEGION
      if (parent.get_type_tag() != partition.get_type_tag())
        REPORT_LEGION_ERROR(ERROR_INDEXPARTITION_NOT_SAME_INDEX_TREE,
            "IndexPartition %d does not have the same type as the "
            "parent index space %x in task %s (UID %lld)", partition.id,
            parent.id, get_task_name(), get_unique_id())
#endif
      LegionColor partition_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      else
        color_generated = true;
      IndexPartNode *part_node = runtime->forest->get_node(partition);
      // See if we can determine disjointness if we weren't told
      if ((kind == LEGION_COMPUTE_KIND) || 
          (kind == LEGION_COMPUTE_COMPLETE_KIND) ||
          (kind == LEGION_COMPUTE_INCOMPLETE_KIND))
      {
        if (part_node->is_disjoint(true/*from app*/))
        {
          if (kind == LEGION_COMPUTE_KIND)
            kind = LEGION_DISJOINT_KIND;
          else if (kind == LEGION_COMPUTE_COMPLETE_KIND)
            kind = LEGION_DISJOINT_COMPLETE_KIND;
          else
            kind = LEGION_DISJOINT_INCOMPLETE_KIND;
        }
      }
      IndexPartition pid(0/*temp*/,parent.get_tree_id(),parent.get_type_tag());
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      if (create_shard_partition(part_op, pid, parent,
            part_node->color_space->handle, provenance, kind, partition_color,
            color_generated))
        log_index.debug("Creating intersection partition %d with parent "
                        "index space %x in task %s (ID %lld)", pid.id, 
                        parent.id, get_task_name(), get_unique_id());
      part_op->initialize_intersection_partition(this, pid, partition,
                                                 dominates, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_partition_by_difference(
                                                  IndexSpace parent,
                                                  IndexPartition handle1,
                                                  IndexPartition handle2,
                                                  IndexSpace color_space,
                                                  PartitionKind kind, 
                                                  Color color,
                                                  Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this); 
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PARTITION_BY_DIFFERENCE, __func__);
        hasher.hash(parent, "parent");
        hasher.hash(handle1, "handle1");
        hasher.hash(handle2, "handle2");
        hasher.hash(color_space, "color_space");
        hasher.hash(kind, "kind");
        hasher.hash(color, "color");
        if (hasher.verify(__func__))
          break;
      }
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, kind)
#ifdef DEBUG_LEGION 
      if (parent.get_tree_id() != handle1.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEX_TREE_MISMATCH,
                            "IndexPartition %d is not part of the same "
                            "index tree as IndexSpace %d in create "
                            "partition by difference!",
                            handle1.id, parent.id)
      if (parent.get_tree_id() != handle2.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEX_TREE_MISMATCH,
                            "IndexPartition %d is not part of the same "
                            "index tree as IndexSpace %d in create "
                            "partition by difference!",
                            handle2.id, parent.id)
#endif
      LegionColor partition_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      else
        color_generated = true; 
      // If the left-hand-side is disjoint the result is disjoint
      if ((kind == LEGION_COMPUTE_KIND) || 
          (kind == LEGION_COMPUTE_COMPLETE_KIND) ||
          (kind == LEGION_COMPUTE_INCOMPLETE_KIND))
      {
        IndexPartNode *p1 = runtime->forest->get_node(handle1);
        if (p1->is_disjoint(true/*from app*/))
        {
          if (kind == LEGION_COMPUTE_KIND)
            kind = LEGION_DISJOINT_KIND;
          else if (kind == LEGION_COMPUTE_COMPLETE_KIND)
            kind = LEGION_DISJOINT_COMPLETE_KIND;
          else
            kind = LEGION_DISJOINT_INCOMPLETE_KIND;
        }
      }
      IndexPartition pid(0/*temp*/,parent.get_tree_id(),parent.get_type_tag());
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      if (create_shard_partition(part_op, pid, parent, color_space, provenance,
            kind, partition_color, color_generated))
        log_index.debug("Creating difference partition %d with parent "
                        "index space %x in task %s (ID %lld)", pid.id, 
                        parent.id, get_task_name(), get_unique_id());
      part_op->initialize_difference_partition(this, pid, handle1, 
                                               handle2, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    Color ReplicateContext::create_cross_product_partitions(
                                              IndexPartition handle1,
                                              IndexPartition handle2,
                                std::map<IndexSpace,IndexPartition> &handles,
                                              PartitionKind kind,
                                              Color color,
                                              Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_CROSS_PRODUCT_PARTITIONS, __func__);
        hasher.hash(handle1, "handle1");
        hasher.hash(handle2, "handle2");
        hasher.hash(kind, "kind");
        hasher.hash(color, "color");
        if (hasher.verify(__func__))
          break;
      }
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, kind)
#ifdef DEBUG_LEGION
      log_index.debug("Creating cross product partitions in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
      if (handle1.get_tree_id() != handle2.get_tree_id())
        REPORT_LEGION_ERROR(ERROR_INDEX_TREE_MISMATCH,
                            "IndexPartition %d is not part of the same "
                            "index tree as IndexPartition %d in create "
                            "cross product partitions!",
                            handle1.id, handle2.id)
#endif
      LegionColor partition_color = INVALID_COLOR;
      if (color != LEGION_AUTO_GENERATE_ID)
        partition_color = color;
      std::set<RtEvent> safe_events;
      // We need an owner node to decide which color everyone is going to use
      if (owner_shard->shard_id == index_partition_allocator_shard)
      {
        // Do the call on the owner node
        if (partition_color == INVALID_COLOR)
        {
          ValueBroadcast<LegionColor> color_collective(this, COLLECTIVE_LOC_15);
          runtime->forest->create_pending_cross_product(this, handle1, handle2, 
                                             handles, kind, provenance, 
                                             partition_color, safe_events,
                                             owner_shard->shard_id,
                                             &shard_manager->get_mapping(),
                                             &color_collective);
        }
        else
          runtime->forest->create_pending_cross_product(this, handle1, handle2, 
                                             handles, kind, provenance, 
                                             partition_color, safe_events,
                                             owner_shard->shard_id,
                                             &shard_manager->get_mapping());
      }
      else
      {
        // Get the color result from the owner node
        if (partition_color == INVALID_COLOR)
        {
          ValueBroadcast<LegionColor> color_collective(this,
                            index_partition_allocator_shard, COLLECTIVE_LOC_15);
          partition_color = color_collective.get_value();
#ifdef DEBUG_LEGION
          assert(partition_color != INVALID_COLOR);
#endif
        }
        // Now we can do the call from this node
        runtime->forest->create_pending_cross_product(this, handle1, handle2, 
                                           handles, kind, provenance,
                                           partition_color, safe_events,
                                           owner_shard->shard_id,
                                           &shard_manager->get_mapping());
      }
      // Signal that we're done with our creation
      RtEvent safe_event;
      if (!safe_events.empty())
        safe_event = Runtime::merge_events(safe_events);
      const RtBarrier creation_bar = creation_barrier.next(this);
      Runtime::phase_barrier_arrive(creation_bar, 1/*count*/, safe_event);
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      part_op->initialize_cross_product(this, handle1, handle2, partition_color,
          provenance, owner_shard->shard_id, &shard_manager->get_mapping());
      // Also have to wait for creation to finish on all shards because
      // any shard can handle requests for any cross-product partition
      creation_bar.wait();
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      // Perform the exchange of all the handle names so that we can record
      // all the valid partition names here in this context
      {
        CrossProductCollective collective(this, COLLECTIVE_LOC_36);
        collective.exchange_partitions(handles);
        // Record these partition handles that were created
        AutoLock priv_lock(privilege_lock);
        for (std::map<IndexSpace,IndexPartition>::const_iterator it =
              handles.begin(); it != handles.end(); it++)
        {
#ifdef DEBUG_LEGION
          assert((created_index_partitions.find(it->second) ==
                  created_index_partitions.end()) ||
              (created_index_partitions[it->second] == 1));
#endif
          created_index_partitions[it->second] = 1;
        }
      }
      // Update our allocation shard
      index_partition_allocator_shard++;
      if (index_partition_allocator_shard == total_shards)
        index_partition_allocator_shard = 0;
      if (runtime->verify_partitions)
      {
        Domain color_space = runtime->get_index_partition_color_space(handle1);
        // This code will only work if the color space has type coord_t
        TypeTag type_tag;
        switch (color_space.get_dim())
        {
#define DIMFUNC(DIM) \
          case DIM: \
            { \
              type_tag = NT_TemplateHelper::encode_tag<DIM,coord_t>(); \
              break; \
            }
          LEGION_FOREACH_N(DIMFUNC)
#undef DIMFUNC
          default:
            assert(false);
        }
        for (Domain::DomainPointIterator itr(color_space); itr; itr++)
        {
          IndexSpace subspace;
          switch (color_space.get_dim())
          {
#define DIMFUNC(DIM) \
            case DIM: \
              { \
                const Point<DIM,coord_t> p(itr.p); \
                subspace = runtime->get_index_subspace(handle1, &p, type_tag); \
                break; \
              }
            LEGION_FOREACH_N(DIMFUNC)
#undef DIMFUNC
            default:
              assert(false);
          }
          IndexPartition part = 
            runtime->get_index_partition(subspace, partition_color);
          verify_partition(part, verify_kind, __func__);
        }
      }
      return partition_color;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::create_association(LogicalRegion domain,
                                              LogicalRegion domain_parent,
                                              FieldID domain_fid,
                                              IndexSpace range,
                                              MapperID id, MappingTagID tag,
                                              const UntypedBuffer &marg,
                                              Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_ASSOCIATION, __func__);
        hasher.hash(domain, "domain");
        hasher.hash(domain_parent, "domain_parent");
        hasher.hash(domain_fid, "domain_fid");
        hasher.hash(range, "range");
        hasher.hash(id, "id");
        hasher.hash(tag, "tag");
        hash_argument(hasher, runtime->safe_control_replication, marg, "marg");
        if (hasher.verify(__func__))
          break;
      }
      ReplDependentPartitionOp *part_op = 
        runtime->get_available_repl_dependent_partition_op();
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_index.debug("Creating association in task %s (ID %lld)", 
                        get_task_name(), get_unique_id());
      part_op->set_sharding_collective(new ShardingGatherCollective(this, 
                                    0/*owner shard*/, COLLECTIVE_LOC_37));
#endif
      part_op->initialize_by_association(this, domain, domain_parent, 
          domain_fid, range, id, tag, marg, provenance);
      // Now figure out if we need to unmap and re-map any inline mappings
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(part_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings)
          log_run.warning("WARNING: Runtime is unmapping and remapping "
              "physical regions around create_association call "
              "in task %s (UID %lld).", get_task_name(), get_unique_id());
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(part_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_restricted_partition(
                                              IndexSpace parent,
                                              IndexSpace color_space,
                                              const void *transform,
                                              size_t transform_size,
                                              const void *extent,
                                              size_t extent_size,
                                              PartitionKind part_kind,
                                              Color color,
                                              Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_RESTRICTED_PARTITION, __func__);
        hasher.hash(parent, "parent");
        hasher.hash(color_space, "color_space");
        hasher.hash(transform, transform_size, "transform_size");
        hasher.hash(extent, extent_size, "extent_size");
        hasher.hash(part_kind, "part_kind");
        hasher.hash(color, "color");
        if (hasher.verify(__func__))
          break;
      }
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      LegionColor part_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color; 
      else
        color_generated = true;
      IndexPartition pid(0/*temp*/,parent.get_tree_id(),parent.get_type_tag());
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      if (create_shard_partition(part_op, pid, parent, color_space, provenance,
            part_kind, part_color, color_generated))
        log_index.debug("Creating restricted partition in task %s (ID %lld)", 
                        get_task_name(), get_unique_id());
      part_op->initialize_restricted_partition(this, pid, transform, 
                    transform_size, extent, extent_size, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_partition_by_domain(
                                                IndexSpace parent,
                                    const std::map<DomainPoint,Domain> &domains,
                                                IndexSpace color_space,
                                                bool perform_intersections,
                                                PartitionKind part_kind,
                                                Color color,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PARTITION_BY_DOMAIN, __func__);
        hasher.hash(parent, "parent");
        for (std::map<DomainPoint,Domain>::const_iterator it = 
              domains.begin(); it != domains.end(); it++)
        {
          hasher.hash(it->first, "domains");
          hasher.hash(it->second, "domains");
        }
        hasher.hash(color_space, "color_space");
        hasher.hash(perform_intersections, "perform_intersections");
        hasher.hash(part_kind, "part_kind");
        hasher.hash(color, "color");
        if (hasher.verify(__func__))
          break;
      }
      const DistributedID did = runtime->get_available_distributed_id();
      IndexSpaceNode *color_node = runtime->forest->get_node(color_space); 
      FutureMap future_map(new FutureMapImpl(this, runtime, color_node, did,
            total_children_count++,
            ApEvent::NO_AP_EVENT, provenance, true/*reg now*/));
      // Prune out every N-th one for this shard and then pass through
      // the subset to the normal InnerContext variation of this
      std::map<DomainPoint,Future> shard_futures;
      for (std::map<DomainPoint,Domain>::const_iterator it = 
            domains.begin(); it != domains.end(); it++)
        shard_futures[it->first] = TaskContext::from_value(
            &it->second, sizeof(it->second), false/*owned*/, 
            provenance, false/*shard local*/);
      future_map.impl->set_all_futures(shard_futures);
      return create_partition_by_domain(parent, future_map, color_space, 
       perform_intersections, part_kind, color, provenance, true/*skip check*/);
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_partition_by_domain(
                                                    IndexSpace parent,
                                                    const FutureMap &domains,
                                                    IndexSpace color_space,
                                                    bool perform_intersections,
                                                    PartitionKind part_kind,
                                                    Color color,
                                                    Provenance *provenance,
                                                    bool skip_check)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && !skip_check &&(i < 2)
           && ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PARTITION_BY_DOMAIN, __func__);
        hasher.hash(parent, "parent");
        hash_future_map(hasher, domains, "domains");
        hasher.hash(color_space, "color_space");
        hasher.hash(perform_intersections, "perform_intersections");
        hasher.hash(part_kind, "part_kind");
        hasher.hash(color, "color");
        if (hasher.verify(__func__))
          break;
      }
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      LegionColor part_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color; 
      else
        color_generated = true;
      IndexPartition pid(0/*temp*/,parent.get_tree_id(),parent.get_type_tag());
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      if (create_shard_partition(part_op, pid, parent, color_space, provenance,
            part_kind, part_color, color_generated))
        log_index.debug("Creating partition by domain in task %s (ID %lld)", 
                        get_task_name(), get_unique_id());
      part_op->initialize_by_domain(this, pid, domains, 
                                    perform_intersections, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_partition_by_field(
                                              LogicalRegion handle,
                                              LogicalRegion parent_priv,
                                              FieldID fid,
                                              IndexSpace color_space,
                                              Color color,
                                              MapperID id, MappingTagID tag,
                                              PartitionKind part_kind,
                                              const UntypedBuffer &marg,
                                              Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PARTITION_BY_FIELD, __func__);
        hasher.hash(handle, "handle");
        hasher.hash(parent_priv, "parent_priv");
        hasher.hash(fid, "fid");
        hasher.hash(color_space, "color_space");
        hasher.hash(color, "color");
        hasher.hash(id, "id");
        hasher.hash(tag, "tag");
        hasher.hash(part_kind, "part_kind");
        hash_argument(hasher, runtime->safe_control_replication, marg, "marg");
        if (hasher.verify(__func__))
          break;
      }
      // Partition by field is disjoint by construction
      PartitionKind verify_kind = LEGION_DISJOINT_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      IndexSpace parent = handle.get_index_space();  
      LegionColor part_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color;
      else
        color_generated = true;
      IndexPartition pid(0/*temp*/,parent.get_tree_id(),parent.get_type_tag());
      ReplDependentPartitionOp *part_op = 
        runtime->get_available_repl_dependent_partition_op();
      if (create_shard_partition(part_op, pid, parent, color_space, provenance,
            part_kind, part_color, color_generated))
        log_index.debug("Creating partition by field in task %s (ID %lld)", 
                        get_task_name(), get_unique_id());
      part_op->initialize_by_field(this, pid, handle, parent_priv, color_space,
                                   fid, id, tag, marg, provenance);
      part_op->initialize_replication(this);
#ifdef DEBUG_LEGION
      part_op->set_sharding_collective(new ShardingGatherCollective(this, 
                                    0/*owner shard*/, COLLECTIVE_LOC_38));
#endif
      // Now figure out if we need to unmap and re-map any inline mappings
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(part_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings)
          log_run.warning("WARNING: Runtime is unmapping and remapping "
              "physical regions around create_partition_by_field call "
              "in task %s (UID %lld).", get_task_name(), get_unique_id());
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(part_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_partition_by_image(
                                                    IndexSpace handle,
                                                    LogicalPartition projection,
                                                    LogicalRegion parent,
                                                    FieldID fid,
                                                    IndexSpace color_space,
                                                    PartitionKind part_kind,
                                                    Color color,
                                                    MapperID id, 
                                                    MappingTagID tag,
                                                    const UntypedBuffer &marg,
                                                    Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);  
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PARTITION_BY_IMAGE, __func__);
        hasher.hash(handle, "handle");
        hasher.hash(projection, "projection");
        hasher.hash(parent, "parent");
        hasher.hash(fid, "fid");
        hasher.hash(color_space, "color_space");
        hasher.hash(part_kind, "part_kind");
        hasher.hash(color, "color");
        hasher.hash(id, "id");
        hasher.hash(tag, "tag");
        hash_argument(hasher, runtime->safe_control_replication, marg, "marg");
        if (hasher.verify(__func__))
          break;
      }
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      LegionColor part_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color;
      else
        color_generated = true;
      IndexPartition pid(0/*temp*/, handle.get_tree_id(),handle.get_type_tag());
      ReplDependentPartitionOp *part_op = 
        runtime->get_available_repl_dependent_partition_op();
      if (create_shard_partition(part_op, pid, handle, color_space, provenance,
            part_kind, part_color, color_generated))
        log_index.debug("Creating partition by image in task %s (ID %lld)", 
                        get_task_name(), get_unique_id());
      part_op->initialize_by_image(this, pid, handle, projection, parent, fid,
                                   id, tag, marg, provenance);
      part_op->initialize_replication(this);
#ifdef DEBUG_LEGION
      part_op->set_sharding_collective(new ShardingGatherCollective(this, 
                                    0/*owner shard*/, COLLECTIVE_LOC_39));
#endif
      // Now figure out if we need to unmap and re-map any inline mappings
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(part_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings)
          log_run.warning("WARNING: Runtime is unmapping and remapping "
              "physical regions around create_partition_by_image call "
              "in task %s (UID %lld).", get_task_name(), get_unique_id());
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(part_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_partition_by_image_range(
                                                    IndexSpace handle,
                                                    LogicalPartition projection,
                                                    LogicalRegion parent,
                                                    FieldID fid,
                                                    IndexSpace color_space,
                                                    PartitionKind part_kind,
                                                    Color color,
                                                    MapperID id, 
                                                    MappingTagID tag,
                                                    const UntypedBuffer &marg,
                                                    Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);  
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PARTITION_BY_IMAGE_RANGE, __func__);
        hasher.hash(handle, "handle");
        hasher.hash(projection, "projection");
        hasher.hash(parent, "parent");
        hasher.hash(fid, "fid");
        hasher.hash(color_space, "color_space");
        hasher.hash(part_kind, "part_kind");
        hasher.hash(color, "color");
        hasher.hash(id, "id");
        hasher.hash(tag, "tag");
        hash_argument(hasher, runtime->safe_control_replication, marg, "marg");
        if (hasher.verify(__func__))
          break;
      }
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      LegionColor part_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color;
      else
        color_generated = true;
      IndexPartition pid(0/*temp*/, handle.get_tree_id(),handle.get_type_tag());
      ReplDependentPartitionOp *part_op = 
        runtime->get_available_repl_dependent_partition_op();
      if (create_shard_partition(part_op, pid, handle, color_space, provenance,
            part_kind, part_color, color_generated))
        log_index.debug("Creating partition by image range in task %s "
                        "(ID %lld)", get_task_name(), get_unique_id());
      part_op->initialize_by_image_range(this, pid, handle, projection, parent, 
                                         fid, id, tag, marg, provenance);
      part_op->initialize_replication(this);
#ifdef DEBUG_LEGION
      part_op->set_sharding_collective(new ShardingGatherCollective(this, 
                                    0/*owner shard*/, COLLECTIVE_LOC_40));
#endif
      // Now figure out if we need to unmap and re-map any inline mappings
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(part_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings)
          log_run.warning("WARNING: Runtime is unmapping and remapping "
              "physical regions around create_partition_by_image_range call "
              "in task %s (UID %lld).", get_task_name(), get_unique_id());
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(part_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_partition_by_preimage(
                                                  IndexPartition projection,
                                                  LogicalRegion handle,
                                                  LogicalRegion parent,
                                                  FieldID fid,
                                                  IndexSpace color_space,
                                                  PartitionKind part_kind,
                                                  Color color,
                                                  MapperID id, MappingTagID tag,
                                                  const UntypedBuffer &marg,
                                                  Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);  
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PARTITION_BY_PREIMAGE, __func__);
        hasher.hash(projection, "projection");
        hasher.hash(handle, "handle");
        hasher.hash(parent, "parent");
        hasher.hash(fid, "fid");
        hasher.hash(color_space, "color_space");
        hasher.hash(part_kind, "part_kind");
        hasher.hash(color, "color");
        hasher.hash(id, "id");
        hasher.hash(tag, "tag");
        hash_argument(hasher, runtime->safe_control_replication, marg, "marg");
        if (hasher.verify(__func__))
          break;
      }
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      LegionColor part_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color;
      else
        color_generated = true; 
      // If the source of the preimage is disjoint then the result is disjoint
      // Note this only applies here and not to range
      if ((part_kind == LEGION_COMPUTE_KIND) || 
          (part_kind == LEGION_COMPUTE_COMPLETE_KIND) ||
          (part_kind == LEGION_COMPUTE_INCOMPLETE_KIND))
      {
        IndexPartNode *p = runtime->forest->get_node(projection);
        if (p->is_disjoint(true/*from app*/))
        {
          if (part_kind == LEGION_COMPUTE_KIND)
            part_kind = LEGION_DISJOINT_KIND;
          else if (part_kind == LEGION_COMPUTE_COMPLETE_KIND)
            part_kind = LEGION_DISJOINT_COMPLETE_KIND;
          else
            part_kind = LEGION_DISJOINT_INCOMPLETE_KIND;
        }
      }
      IndexPartition pid(0/*temp*/,
          handle.get_index_space().get_tree_id(), handle.get_type_tag());
      ReplDependentPartitionOp *part_op = 
        runtime->get_available_repl_dependent_partition_op();
      if (create_shard_partition(part_op, pid, handle.get_index_space(),
            color_space, provenance, part_kind, part_color, color_generated))
        log_index.debug("Creating partition by preimage in task %s (ID %lld)",
                        get_task_name(), get_unique_id());
      part_op->initialize_by_preimage(this, pid, projection, handle, parent,
                                      fid, id, tag, marg, provenance);
      part_op->initialize_replication(this);
#ifdef DEBUG_LEGION
      part_op->set_sharding_collective(new ShardingGatherCollective(this, 
                                    0/*owner shard*/, COLLECTIVE_LOC_41));
#endif
      // Now figure out if we need to unmap and re-map any inline mappings
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(part_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings)
          log_run.warning("WARNING: Runtime is unmapping and remapping "
              "physical regions around create_partition_by_preimage call "
              "in task %s (UID %lld).", get_task_name(), get_unique_id());
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(part_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_partition_by_preimage_range(
                                                  IndexPartition projection,
                                                  LogicalRegion handle,
                                                  LogicalRegion parent,
                                                  FieldID fid,
                                                  IndexSpace color_space,
                                                  PartitionKind part_kind,
                                                  Color color,
                                                  MapperID id, MappingTagID tag,
                                                  const UntypedBuffer &marg,
                                                  Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);  
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PARTITION_BY_PREIMAGE_RANGE, __func__);
        hasher.hash(projection, "projection");
        hasher.hash(handle, "handle");
        hasher.hash(parent, "parent");
        hasher.hash(fid, "fid");
        hasher.hash(color_space, "color_space");
        hasher.hash(part_kind, "part_kind");
        hasher.hash(color, "color");
        hasher.hash(id, "id");
        hasher.hash(tag, "tag");
        hash_argument(hasher, runtime->safe_control_replication, marg, "marg");
        if (hasher.verify(__func__))
          break;
      }
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions)
        SWAP_PART_KINDS(verify_kind, part_kind)
      LegionColor part_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color;
      else
        color_generated = true; 
      IndexPartition pid(0/*temp*/,
          handle.get_index_space().get_tree_id(), handle.get_type_tag());
      ReplDependentPartitionOp *part_op = 
        runtime->get_available_repl_dependent_partition_op();
      if (create_shard_partition(part_op, pid, handle.get_index_space(),
            color_space, provenance, part_kind, part_color, color_generated))
        log_index.debug("Creating partition by preimage range in task %s "
                        "(ID %lld)", get_task_name(), get_unique_id());
      part_op->initialize_by_preimage_range(this, 
                                            pid, projection, handle, parent,
                                            fid, id, tag, marg, provenance);
      part_op->initialize_replication(this);
#ifdef DEBUG_LEGION
      part_op->set_sharding_collective(new ShardingGatherCollective(this, 
                                    0/*owner shard*/, COLLECTIVE_LOC_42));
#endif
      // Now figure out if we need to unmap and re-map any inline mappings
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(part_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings)
          log_run.warning("WARNING: Runtime is unmapping and remapping "
              "physical regions around create_partition_by_preimage_range call "
              "in task %s (UID %lld).", get_task_name(), get_unique_id());
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(part_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      if (runtime->verify_partitions)
        verify_partition(pid, verify_kind, __func__);
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexPartition ReplicateContext::create_pending_partition(
                                                      IndexSpace parent,
                                                      IndexSpace color_space,
                                                      PartitionKind part_kind,
                                                      Color color,
                                                      Provenance *provenance,
                                                      bool trust)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && !trust && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_PENDING_PARTITION, __func__); 
        hasher.hash(parent, "parent");
        hasher.hash(color_space, "color_space");
        hasher.hash(part_kind, "part_kind");
        hasher.hash(color, "color");
        if (hasher.verify(__func__))
          break;
      }
      PartitionKind verify_kind = LEGION_COMPUTE_KIND;
      if (runtime->verify_partitions && !trust)
        SWAP_PART_KINDS(verify_kind, part_kind)
      LegionColor part_color = INVALID_COLOR;
      bool color_generated = false;
      if (color != LEGION_AUTO_GENERATE_ID)
        part_color = color; 
      else
        color_generated = true;
      IndexPartition pid(0/*temp*/,parent.get_tree_id(),parent.get_type_tag());
      if (create_shard_partition(NULL/*op*/, pid, parent, color_space,
            provenance, part_kind, part_color, color_generated))
        log_index.debug("Creating pending partition in task %s (ID %lld)", 
                        get_task_name(), get_unique_id());
      if (runtime->verify_partitions && !trust)
      {
        // We can't block to check this here because the user needs 
        // control back in order to fill in the pieces of the partitions
        // so just launch a meta-task to check it when we can
        VerifyPartitionArgs args(this, pid, verify_kind, __func__);
        runtime->issue_runtime_meta_task(args, LG_LOW_PRIORITY); 
      }
      return pid;
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::create_index_space_union(
                                                    IndexPartition parent,
                                                    const void *realm_color,
                                                    size_t color_size,
                                                    TypeTag type_tag,
                                        const std::vector<IndexSpace> &handles,
                                                    Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_INDEX_SPACE_UNION, __func__);
        hasher.hash(parent, "parent");
        hasher.hash(realm_color, color_size, "realm_color");
        hasher.hash(type_tag, "type_tag");
        for (std::vector<IndexSpace>::const_iterator it = 
              handles.begin(); it != handles.end(); it++)
          hasher.hash(*it, "handles");
        if (hasher.verify(__func__))
          break;
      }
#ifdef DEBUG_LEGION
      log_index.debug("Creating index space union in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      IndexSpace result = 
        runtime->forest->instantiate_subspace(parent, realm_color, type_tag);
      part_op->initialize_index_space_union(this, result, handles, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::create_index_space_union(
                                                      IndexPartition parent,
                                                      const void *realm_color,
                                                      size_t color_size,
                                                      TypeTag type_tag,
                                                      IndexPartition handle,
                                                      Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_INDEX_SPACE_UNION, __func__);
        hasher.hash(parent, "parent");
        hasher.hash(realm_color, color_size, "realm_color");
        hasher.hash(type_tag, "type_tag");
        hasher.hash(handle, "handle");
        if (hasher.verify(__func__))
          break;
      }
#ifdef DEBUG_LEGION
      log_index.debug("Creating index space union in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      IndexSpace result = 
        runtime->forest->instantiate_subspace(parent, realm_color, type_tag);
      part_op->initialize_index_space_union(this, result, handle, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::create_index_space_intersection(
                                                    IndexPartition parent,
                                                    const void *realm_color,
                                                    size_t color_size,
                                                    TypeTag type_tag,
                                        const std::vector<IndexSpace> &handles,
                                                    Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_INDEX_SPACE_INTERSECTION, __func__);
        hasher.hash(parent, "parent");
        hasher.hash(realm_color, color_size, "realm_color");
        hasher.hash(type_tag, "type_tag");
        for (std::vector<IndexSpace>::const_iterator it = 
              handles.begin(); it != handles.end(); it++)
          hasher.hash(*it, "handles");
        if (hasher.verify(__func__))
          break;
      }
#ifdef DEBUG_LEGION
      log_index.debug("Creating index space intersection in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      IndexSpace result = 
        runtime->forest->instantiate_subspace(parent, realm_color, type_tag); 
      part_op->initialize_index_space_intersection(this, result, handles,
                                                   provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::create_index_space_intersection(
                                                    IndexPartition parent,
                                                    const void *realm_color,
                                                    size_t color_size,
                                                    TypeTag type_tag,
                                                    IndexPartition handle,
                                                    Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_INDEX_SPACE_INTERSECTION, __func__);
        hasher.hash(parent, "parent");
        hasher.hash(realm_color, color_size, "realm_color");
        hasher.hash(type_tag, "type_tag");
        hasher.hash(handle, "handle");
        if (hasher.verify(__func__))
          break;
      }
#ifdef DEBUG_LEGION
      log_index.debug("Creating index space intersection in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      IndexSpace result = 
        runtime->forest->instantiate_subspace(parent, realm_color, type_tag); 
      part_op->initialize_index_space_intersection(this, result, handle,
                                                   provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpace ReplicateContext::create_index_space_difference(
                                                    IndexPartition parent,
                                                    const void *realm_color,
                                                    size_t color_size,
                                                    TypeTag type_tag,
                                                    IndexSpace initial,
                                        const std::vector<IndexSpace> &handles,
                                                    Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_INDEX_SPACE_DIFFERENCE, __func__);
        hasher.hash(parent, "parent");
        hasher.hash(realm_color, color_size, "realm_color");
        hasher.hash(type_tag, "type_tag");
        hasher.hash(initial, "initial");
        for (std::vector<IndexSpace>::const_iterator it = 
              handles.begin(); it != handles.end(); it++)
          hasher.hash(*it, "handles");
        if (hasher.verify(__func__))
          break;
      }
#ifdef DEBUG_LEGION
      log_index.debug("Creating index space difference in task %s (ID %lld)", 
                      get_task_name(), get_unique_id());
#endif
      ReplPendingPartitionOp *part_op = 
        runtime->get_available_repl_pending_partition_op();
      IndexSpace result = 
        runtime->forest->instantiate_subspace(parent, realm_color, type_tag); 
      part_op->initialize_index_space_difference(this, result, initial,
                                                 handles, provenance);
      // Now we can add the operation to the queue
      add_to_dependence_queue(part_op);
      return result;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::verify_partition(IndexPartition pid, 
                                  PartitionKind kind, const char *function_name)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *node = runtime->forest->get_node(pid);
      // Check containment first
      for (ColorSpaceIterator itr(node, true/*local only*/); itr; itr++) 
      {
        IndexSpaceNode *child_node = node->get_child(*itr);
        IndexSpaceExpression *diff = 
          runtime->forest->subtract_index_spaces(child_node, node->parent);
        if (!diff->is_empty())
        {
          const DomainPoint bad = 
            node->color_space->delinearize_color_to_point(*itr);
          switch (bad.get_dim())
          {
            case 1:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0])
            case 2:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1])
            case 3:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2])
            case 4:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,"
                  "%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2], bad[3])
            case 5:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,"
                  "%lld,%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2], bad[3], bad[4])
            case 6:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,"
                  "%lld,%lld,%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2], bad[3], bad[4], bad[5])
            case 7:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,"
                  "%lld,%lld,%lld,%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2], bad[3], bad[4], bad[5], bad[6])
            case 8:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,"
                  "%lld,%lld,%lld,%lld,%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2], bad[3], bad[4], bad[5], bad[6],
                  bad[7])
            case 9:
              REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
                  "Call to partition function %s in %s (UID %lld) has "
                  "non-dominated child sub-region at color (%lld,%lld,"
                  "%lld,%lld,%lld,%lld,%lld,%lld,%lld).",
                  function_name, get_task_name(), get_unique_id(),
                  bad[0], bad[1], bad[2], bad[3], bad[4], bad[5], bad[6],
                  bad[7], bad[8])
            default:
              assert(false);
          }
        }
      }
      // Only need to do the rest of this on shard 0
      if (owner_shard->shard_id > 0)
        return;
      // Check disjointness
      if ((kind == LEGION_DISJOINT_KIND) || 
          (kind == LEGION_DISJOINT_COMPLETE_KIND) ||
          (kind == LEGION_DISJOINT_INCOMPLETE_KIND))
      {
        if (!node->is_disjoint(true/*from application*/))
          REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
              "Call to partitioning function %s in %s (UID %lld) specified "
              "partition was %s but the partition is aliased.",
              function_name, get_task_name(), get_unique_id(),
              (kind == LEGION_DISJOINT_KIND) ? "DISJOINT_KIND" :
              (kind == LEGION_DISJOINT_COMPLETE_KIND) ? 
              "DISJOINT_COMPLETE_KIND" : "DISJOINT_INCOMPLETE_KIND")
      }
      else if ((kind == LEGION_ALIASED_KIND) || 
               (kind == LEGION_ALIASED_COMPLETE_KIND) ||
               (kind == LEGION_ALIASED_INCOMPLETE_KIND))
      {
        if (node->is_disjoint(true/*from application*/))
          REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
              "Call to partitioning function %s in %s (UID %lld) specified "
              "partition was %s but the partition is disjoint.",
              function_name, get_task_name(), get_unique_id(),
              (kind == LEGION_ALIASED_KIND) ? "ALIASED_KIND" :
              (kind == LEGION_ALIASED_COMPLETE_KIND) ? "ALIASED_COMPLETE_KIND" :
              "ALIASED_INCOMPLETE_KIND")
      }
      // Check completeness
      if ((kind == LEGION_DISJOINT_COMPLETE_KIND) || 
          (kind == LEGION_ALIASED_COMPLETE_KIND) ||
          (kind == LEGION_COMPUTE_COMPLETE_KIND))
      {
        if (!node->is_complete(true/*from application*/))
          REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
              "Call to partitioning function %s in %s (UID %lld) specified "
              "partition was %s but the partition is incomplete.",
              function_name, get_task_name(), get_unique_id(),
              (kind == LEGION_DISJOINT_COMPLETE_KIND) ? "DISJOINT_COMPLETE_KIND"
            : (kind == LEGION_ALIASED_COMPLETE_KIND) ? "ALIASED_COMPLETE_KIND" :
              "COMPUTE_COMPLETE_KIND")
      }
      else if ((kind == LEGION_DISJOINT_INCOMPLETE_KIND) || 
               (kind == LEGION_ALIASED_INCOMPLETE_KIND) || 
               (kind == LEGION_COMPUTE_INCOMPLETE_KIND))
      {
        if (node->is_complete(true/*from application*/))
          REPORT_LEGION_ERROR(ERROR_PARTITION_VERIFICATION,
              "Call to partitioning function %s in %s (UID %lld) specified "
              "partition was %s but the partition is complete.",
              function_name, get_task_name(), get_unique_id(),
              (kind == LEGION_DISJOINT_INCOMPLETE_KIND) ? 
                "DISJOINT_INCOMPLETE_KIND" :
              (kind == LEGION_ALIASED_INCOMPLETE_KIND) ? 
              "ALIASED_INCOMPLETE_KIND" : "COMPUTE_INCOMPLETE_KIND")
      }
    }

    //--------------------------------------------------------------------------
    FieldSpace ReplicateContext::create_field_space(Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_FIELD_SPACE, __func__);
        if (hasher.verify(__func__))
          break;
      }
      return create_replicated_field_space(provenance); 
    }

    //--------------------------------------------------------------------------
    FieldSpace ReplicateContext::create_replicated_field_space(
                                       Provenance *provenance, ShardID *creator)
    //--------------------------------------------------------------------------
    {
      // Seed this with the first field space broadcast
      if (pending_field_spaces.empty())
      {
        increase_pending_field_spaces(1/*count*/, false/*double*/);
        pending_field_space_check = 0;
      }
      FieldSpace space;
      bool double_next = false;
      bool double_buffer = false;
      std::pair<ValueBroadcast<FSBroadcast>*,bool> &collective = 
        pending_field_spaces.front();
      if (creator != NULL)
        *creator = collective.first->origin;
      CollectiveMapping &collective_mapping = 
        shard_manager->get_collective_mapping();
      const RtBarrier creation_bar = creation_barrier.next(this);
      if (collective.second)
      {
        const FSBroadcast value = collective.first->get_value(false);
        space = FieldSpace(value.space_id);
        double_buffer = value.double_buffer;
        // Need to register this before broadcasting
        runtime->forest->create_field_space(space, value.did, provenance,
                                      &collective_mapping, creation_bar);
        // Arrive on the creation barrier
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/); 
        runtime->forest->revoke_pending_field_space(value.space_id);
#ifdef DEBUG_LEGION
        log_field.debug("Creating field space %x in task %s (ID %lld)", 
                        space.id, get_task_name(), get_unique_id());
#endif
        if (runtime->legion_spy_enabled)
          LegionSpy::log_field_space(space.id, runtime->address_space,
              (provenance == NULL) ? NULL : provenance->human_str());
      }
      else
      {
        const RtEvent done = collective.first->get_done_event();
        if (!done.has_triggered())
        {
          double_next = true;
          done.wait();
        }
        const FSBroadcast value = collective.first->get_value(false);
        space = FieldSpace(value.space_id);
        double_buffer = value.double_buffer;
#ifdef DEBUG_LEGION
        assert(space.exists());
#endif
        runtime->forest->create_field_space(space, value.did, provenance,
                                      &collective_mapping, creation_bar);
        // Arrive on the creation barrier
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
      }
      if (++pending_field_space_check == pending_field_spaces.size())
        pending_field_space_check = 0;
      else
        double_buffer = false;
      // Record this in our context
      register_field_space_creation(space);
      // Get new handles in flight for the next time we need them
      // Always add a new one to replace the old one, but double the number
      // in flight if we're not hiding the latency
      increase_pending_field_spaces(double_buffer ? 
          pending_field_spaces.size() + 1 : 1, double_next);
      delete collective.first;
      pending_field_spaces.pop_front();
      return space;
    }

    //--------------------------------------------------------------------------
    FieldSpace ReplicateContext::create_field_space(
                                         const std::vector<size_t> &sizes,
                                         std::vector<FieldID> &resulting_fields,
                                         CustomSerdezID serdez_id,
                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_FIELD_SPACE, __func__);
        for (std::vector<size_t>::const_iterator it = 
              sizes.begin(); it != sizes.end(); it++)
          hasher.hash(*it, "sizes");
        for (std::vector<FieldID>::const_iterator it = 
              resulting_fields.begin(); it != resulting_fields.end(); it++)
          hasher.hash(*it, "resulting_fields");
        hasher.hash(serdez_id, "serdez_id");
        if (hasher.verify(__func__))
          break;
      }
      ShardID creator_shard = 0;
      const FieldSpace space =
        create_replicated_field_space(provenance, &creator_shard);
      if (resulting_fields.size() < sizes.size())
        resulting_fields.resize(sizes.size(), LEGION_AUTO_GENERATE_ID);
      for (unsigned idx = 0; idx < resulting_fields.size(); idx++)
      {
        if (resulting_fields[idx] == LEGION_AUTO_GENERATE_ID)
        {
          if (pending_fields.empty())
          {
            increase_pending_fields(1/*count*/, false/*double*/);
            pending_field_check = 0;
          }
          bool double_next = false;
          bool double_buffer = false;
          std::pair<ValueBroadcast<FIDBroadcast>*,bool> &collective = 
            pending_fields.front();
          if (collective.second)
          {
            const FIDBroadcast value = collective.first->get_value(false);
            resulting_fields[idx] = value.field_id;
            double_buffer = value.double_buffer;
          }
          else
          {
            const RtEvent done = collective.first->get_done_event();
            if (!done.has_triggered())
            {
              double_next = true;
              done.wait();
            }
            const FIDBroadcast value = collective.first->get_value(false);
            resulting_fields[idx] = value.field_id;
            double_buffer = value.double_buffer;
          }
          if (++pending_field_check == pending_fields.size())
            pending_field_check = 0;
          else
            double_buffer = false;
          increase_pending_fields(
              double_buffer ? pending_fields.size() + 1 : 1, double_next);
          delete collective.first;
          pending_fields.pop_front();
        }
        else if (resulting_fields[idx] >= LEGION_MAX_APPLICATION_FIELD_ID)
          REPORT_LEGION_ERROR(ERROR_TASK_ATTEMPTED_ALLOCATE_FIELD,
            "Task %s (ID %lld) attempted to allocate a field with "
            "ID %d which exceeds the LEGION_MAX_APPLICATION_FIELD_ID "
            "bound set in legion_config.h", get_task_name(),
            get_unique_id(), resulting_fields[idx])
      }
      // Figure out if we're going to do the field initialization on this node
      const AddressSpaceID owner_space =
        FieldSpaceNode::get_owner_space(space, runtime);
      const bool local_shard = (owner_space == runtime->address_space) ?
        (creator_shard == owner_shard->shard_id) :
        shard_manager->is_first_local_shard(owner_shard);
      const RtBarrier creation_bar = creation_barrier.next(this);
      // This deduplicates multiple shards on the same node
      if (local_shard)
      {
        const bool non_owner = (creator_shard != owner_shard->shard_id);
        FieldSpaceNode *node = runtime->forest->get_node(space);
        node->initialize_fields(sizes, resulting_fields, serdez_id, 
                                provenance, true/*collective*/);
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
        if (runtime->legion_spy_enabled && !non_owner)
          for (unsigned idx = 0; idx < resulting_fields.size(); idx++)
            LegionSpy::log_field_creation(space.id, resulting_fields[idx],
             sizes[idx], (provenance == NULL) ? NULL : provenance->human_str());
      }
      else
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
      register_all_field_creations(space, false/*loca*/, resulting_fields);
      // Make sure all the field allocations are done on all shards
      creation_bar.wait();
      return space;
    }

    //--------------------------------------------------------------------------
    FieldSpace ReplicateContext::create_field_space(
                                         const std::vector<Future> &sizes,
                                         std::vector<FieldID> &resulting_fields,
                                         CustomSerdezID serdez_id,
                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_FIELD_SPACE, __func__);
        for (std::vector<Future>::const_iterator it = 
              sizes.begin(); it != sizes.end(); it++)
          hash_future(hasher, runtime->safe_control_replication, *it, "sizes");
        for (std::vector<FieldID>::const_iterator it = 
              resulting_fields.begin(); it != resulting_fields.end(); it++)
          hasher.hash(*it, "resulting_fields");
        hasher.hash(serdez_id, "serdez_id");
        if (hasher.verify(__func__))
          break;
      }
      ShardID creator_shard = 0;
      const FieldSpace space =
        create_replicated_field_space(provenance, &creator_shard);
      for (unsigned idx = 0; idx < resulting_fields.size(); idx++)
      {
        if (resulting_fields[idx] == LEGION_AUTO_GENERATE_ID)
        {
          if (pending_fields.empty())
          {
            increase_pending_fields(1/*count*/, false/*double*/);
            pending_field_check = 0;
          }
          bool double_next = false;
          bool double_buffer = false;
          std::pair<ValueBroadcast<FIDBroadcast>*,bool> &collective = 
            pending_fields.front();
          if (collective.second)
          {
            const FIDBroadcast value = collective.first->get_value(false);
            resulting_fields[idx] = value.field_id;
            double_buffer = value.double_buffer;
          }
          else
          {
            const RtEvent done = collective.first->get_done_event();
            if (!done.has_triggered())
            {
              double_next = true;
              done.wait();
            }
            const FIDBroadcast value = collective.first->get_value(false);
            resulting_fields[idx] = value.field_id;
            double_buffer = value.double_buffer;
          }
          if (++pending_field_check == pending_fields.size())
            pending_field_check = 0;
          else
            double_buffer = false;
          increase_pending_fields(
              double_buffer ? pending_fields.size() + 1 : 1, double_next);
          delete collective.first;
          pending_fields.pop_front();
        }
#ifdef DEBUG_LEGION
        else if (resulting_fields[idx] >= LEGION_MAX_APPLICATION_FIELD_ID)
          REPORT_LEGION_ERROR(ERROR_TASK_ATTEMPTED_ALLOCATE_FIELD,
            "Task %s (ID %lld) attempted to allocate a field with "
            "ID %d which exceeds the LEGION_MAX_APPLICATION_FIELD_ID "
            "bound set in legion_config.h", get_task_name(),
            get_unique_id(), resulting_fields[idx])
#endif
      }
      for (unsigned idx = 0; idx < sizes.size(); idx++)
        if (sizes[idx].impl == NULL)
          REPORT_LEGION_ERROR(ERROR_REQUEST_FOR_EMPTY_FUTURE,
              "Invalid empty future passed to field allocation for field %d "
              "in task %s (UID %lld)", resulting_fields[idx],
              get_task_name(), get_unique_id())
      // Figure out if we're going to do the field initialization on this node
      const AddressSpaceID owner_space =
        FieldSpaceNode::get_owner_space(space, runtime);
      const bool local_shard = (owner_space == runtime->address_space) ?
        (creator_shard == owner_shard->shard_id) :
        shard_manager->is_first_local_shard(owner_shard);
      const RtBarrier creation_bar = creation_barrier.next(this);
      // Get a new creation operation
      CreationOp *creator_op = runtime->get_available_creation_op();
      FieldSpaceNode *node = runtime->forest->get_node(space);
      // This deduplicates multiple shards on the same node
      if (local_shard)
      {
        const ApEvent ready = creator_op->get_completion_event();
        const bool owner = (creator_shard == owner_shard->shard_id);
        node->initialize_fields(ready, resulting_fields, serdez_id,
                                provenance, true/*collective*/);
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
        creator_op->initialize_fields(this, node, resulting_fields, 
                                      sizes, provenance, owner);
      }
      else
      {
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
        creator_op->initialize_fields(this, node, resulting_fields, 
                                      sizes, provenance, false/*owner*/);
      }
      register_all_field_creations(space, false/*local*/, resulting_fields);
      // Make sure the field IDs are valid everywhere
      creation_bar.wait();
      // Launch the creation op in this context to act as a fence to ensure
      // that the allocations are done on all shard nodes before anyone else
      // tries to use them or their meta-data
      add_to_dependence_queue(creator_op);
      return space;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::increase_pending_field_spaces(unsigned count,
                                                         bool double_next)
    //--------------------------------------------------------------------------
    {
      for (unsigned idx = 0; idx < count; idx++)
      {
        if (owner_shard->shard_id == field_space_allocator_shard)
        {
          const FieldSpaceID space = runtime->get_unique_field_space_id();
          const DistributedID did = runtime->get_available_distributed_id();
          // We're the owner, so make it locally and then broadcast it
          runtime->forest->record_pending_field_space(space);
          // Do our arrival on this generation, should be the last one
          ValueBroadcast<FSBroadcast> *collective = 
            new ValueBroadcast<FSBroadcast>(this, COLLECTIVE_LOC_31);
          collective->broadcast(FSBroadcast(space, did, double_next));
          pending_field_spaces.push_back(
              std::pair<ValueBroadcast<FSBroadcast>*,bool>(collective, true));
        }
        else
        {
          ValueBroadcast<FSBroadcast> *collective = 
            new ValueBroadcast<FSBroadcast>(this, field_space_allocator_shard,
                                            COLLECTIVE_LOC_31);
          register_collective(collective);
          pending_field_spaces.push_back(
              std::pair<ValueBroadcast<FSBroadcast>*,bool>(collective, false));
        }
        field_space_allocator_shard++;
        if (field_space_allocator_shard == total_shards)
          field_space_allocator_shard = 0;
        double_next = false;
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::create_shared_ownership(FieldSpace handle)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      if (!handle.exists())
        return;
      if (shard_manager->is_total_sharding() &&
          shard_manager->is_first_local_shard(owner_shard))
        runtime->create_shared_ownership(handle, true/*total sharding*/);
      else if (owner_shard->shard_id == 0)
        runtime->create_shared_ownership(handle);
      AutoLock priv_lock(privilege_lock);
      std::map<FieldSpace,unsigned>::iterator finder = 
        created_field_spaces.find(handle);
      if (finder != created_field_spaces.end())
        finder->second++;
      else
        created_field_spaces[handle] = 1;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::destroy_field_space(FieldSpace handle,
                                   const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && !unordered && (i < 2)
            && ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_DESTROY_FIELD_SPACE, __func__);
        hasher.hash(handle, "handle");
        if (hasher.verify(__func__))
          break;
      }
      if (!handle.exists())
        return;
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_field.debug("Destroying field space %x in task %s (ID %lld)", 
                        handle.id, get_task_name(), get_unique_id());
#endif
      // Check to see if this is one that we should be allowed to destory
      {
        AutoLock priv_lock(privilege_lock);
        std::map<FieldSpace,unsigned>::iterator finder = 
          created_field_spaces.find(handle);
        if (finder != created_field_spaces.end())
        {
#ifdef DEBUG_LEGION
          assert(finder->second > 0);
#endif
          if (--finder->second == 0)
            created_field_spaces.erase(finder);
          else
            return;
          // Count how many regions are still using this field space
          // that still need to be deleted before we can remove the
          // list of created fields
          std::set<LogicalRegion> latent_regions;
          for (std::map<LogicalRegion,unsigned>::const_iterator it = 
                created_regions.begin(); it != created_regions.end(); it++)
            if (it->first.get_field_space() == handle)
              latent_regions.insert(it->first);
          for (std::map<LogicalRegion,bool>::const_iterator it = 
                local_regions.begin(); it != local_regions.end(); it++)
            if (it->first.get_field_space() == handle)
              latent_regions.insert(it->first);
          if (latent_regions.empty())
          {
            // No remaining regions so we can remove any created fields now
            for (std::set<std::pair<FieldSpace,FieldID> >::iterator it = 
                  created_fields.begin(); it != 
                  created_fields.end(); /*nothing*/)
            {
              if (it->first == handle)
              {
                std::set<std::pair<FieldSpace,FieldID> >::iterator 
                  to_delete = it++;
                created_fields.erase(to_delete);
              }
              else
                it++;
            }
          }
          else
            latent_field_spaces[handle] = latent_regions;
        }
        else
        {
          // If we didn't make this field space, record the deletion
          // and keep going. It will be handled by the context that
          // made the field space
          deleted_field_spaces.push_back(handle);
          return;
        }
      }
      ReplDeletionOp *op = runtime->get_available_repl_deletion_op();
      op->initialize_field_space_deletion(this, handle, unordered, provenance);
      op->initialize_replication(this,
          shard_manager->is_first_local_shard(owner_shard));
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered field space deletion performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
    }

    //--------------------------------------------------------------------------
    FieldID ReplicateContext::allocate_field(FieldSpace space,size_t field_size,
                                             FieldID fid, bool local,
                                             CustomSerdezID serdez_id,
                                             Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_ALLOCATE_FIELD, __func__);
        hasher.hash(space, "space");
        hasher.hash(field_size, "field_size");
        hasher.hash(fid, "fid");
        hasher.hash(local, "local");
        hasher.hash(serdez_id, "serdez_id");
        if (hasher.verify(__func__))
          break;
      }
      if (local)
        REPORT_LEGION_FATAL(LEGION_FATAL_UNIMPLEMENTED_FEATURE,
                      "Local field creation is not currently supported "
                      "for control replication with task %s (UID %lld)",
                      get_task_name(), get_unique_id())
      if (fid == LEGION_AUTO_GENERATE_ID)
      {
        if (pending_fields.empty())
        {
          increase_pending_fields(1/*count*/, false/*double*/);
          pending_field_check = 0;
        }
        bool double_next = false;
        bool double_buffer = false;
        std::pair<ValueBroadcast<FIDBroadcast>*,bool> &collective = 
          pending_fields.front();
        if (collective.second)
        {
          const FIDBroadcast value = collective.first->get_value(false);
          fid = value.field_id;
          double_buffer = value.double_buffer;
        }
        else
        {
          const RtEvent done = collective.first->get_done_event();
          if (!done.has_triggered())
          {
            double_next = true;
            done.wait();
          }
          const FIDBroadcast value = collective.first->get_value(false);
          fid = value.field_id;
          double_buffer = value.double_buffer;
        }
        if (++pending_field_check == pending_fields.size())
          pending_field_check = 0;
        else
          double_buffer = false;
        increase_pending_fields(
            double_buffer ? pending_fields.size() + 1 : 1, double_next);
        delete collective.first;
        pending_fields.pop_front();
      }
      else if (fid >= LEGION_MAX_APPLICATION_FIELD_ID)
        REPORT_LEGION_ERROR(ERROR_TASK_ATTEMPTED_ALLOCATE_FIELD,
                     "Task %s (ID %lld) attempted to allocate a field with "
                     "ID %d which exceeds the LEGION_MAX_APPLICATION_FIELD_ID"
                     " bound set in legion_config.h", get_task_name(),
                     get_unique_id(), fid)
      std::map<FieldSpace,std::pair<ShardID,bool> >::const_iterator finder = 
        field_allocator_owner_shards.find(space);
#ifdef DEBUG_LEGION
      assert(finder != field_allocator_owner_shards.end());
#endif
      RtEvent precondition;
      // This deduplicates multiple shards on the same node
      if (finder->second.second)
      {
        const bool non_owner = (finder->second.first != owner_shard->shard_id);
        precondition = runtime->forest->allocate_field(space, field_size, fid, 
                                             serdez_id, provenance, non_owner);
        if (runtime->legion_spy_enabled && !non_owner)
          LegionSpy::log_field_creation(space.id, fid, field_size,
              (provenance == NULL) ? NULL : provenance->human_str());
      }
      const RtBarrier creation_bar = creation_barrier.next(this);
      Runtime::phase_barrier_arrive(creation_bar, 1/*count*/, precondition);
      register_field_creation(space, fid, local);
      // Make sure the field IDs are valid everywhere
      creation_bar.wait();
      return fid;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::increase_pending_fields(unsigned count, 
                                                   bool double_next)
    //--------------------------------------------------------------------------
    {
      for (unsigned idx = 0; idx < count; idx++)
      {
        if (owner_shard->shard_id == field_allocator_shard)
        {
          const FieldID fid = runtime->get_unique_field_id();
          // Do our arrival on this generation, should be the last one
          ValueBroadcast<FIDBroadcast> *collective = 
            new ValueBroadcast<FIDBroadcast>(this, COLLECTIVE_LOC_33);
          collective->broadcast(FIDBroadcast(fid, double_next));
          pending_fields.push_back(
              std::pair<ValueBroadcast<FIDBroadcast>*,bool>(collective, true));
        }
        else
        {
          ValueBroadcast<FIDBroadcast> *collective = 
            new ValueBroadcast<FIDBroadcast>(this, field_allocator_shard,
                                             COLLECTIVE_LOC_33);
          register_collective(collective);
          pending_fields.push_back(
              std::pair<ValueBroadcast<FIDBroadcast>*,bool>(collective, false));
        }
        field_allocator_shard++;
        if (field_allocator_shard == total_shards)
          field_allocator_shard = 0;
        double_next = false;
      }
    }

    //--------------------------------------------------------------------------
    FieldID ReplicateContext::allocate_field(FieldSpace space,
                                             const Future &field_size,
                                             FieldID fid, bool local,
                                             CustomSerdezID serdez_id,
                                             Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_ALLOCATE_FIELD, __func__);
        hasher.hash(space, "space");
        hash_future(hasher, runtime->safe_control_replication, field_size, 
                    "field_size");
        hasher.hash(fid, "fid");
        hasher.hash(local, "local");
        hasher.hash(serdez_id, "serdez_id");
        if (hasher.verify(__func__))
          break;
      }
      if (local)
        REPORT_LEGION_FATAL(LEGION_FATAL_UNIMPLEMENTED_FEATURE,
                      "Local field creation is not currently supported "
                      "for control replication with task %s (UID %lld)",
                      get_task_name(), get_unique_id())
      if (fid == LEGION_AUTO_GENERATE_ID)
      {
        if (pending_fields.empty())
        {
          increase_pending_fields(1/*count*/, false/*double*/);
          pending_field_check = 0;
        }
        bool double_next = false;
        bool double_buffer = false;
        std::pair<ValueBroadcast<FIDBroadcast>*,bool> &collective = 
          pending_fields.front();
        if (collective.second)
        {
          const FIDBroadcast value = collective.first->get_value(false);
          fid = value.field_id;
          double_buffer = value.double_buffer;
        }
        else
        {
          const RtEvent done = collective.first->get_done_event();
          if (!done.has_triggered())
          {
            double_next = true;
            done.wait();
          }
          const FIDBroadcast value = collective.first->get_value(false);
          fid = value.field_id;
          double_buffer = value.double_buffer;
        }
        if (++pending_field_check == pending_fields.size())
          pending_field_check = 0;
        else
          double_buffer = false;
        increase_pending_fields(
            double_buffer ? pending_fields.size() + 1 : 1, double_next);
        delete collective.first;
        pending_fields.pop_front();
      }
      else if (fid >= LEGION_MAX_APPLICATION_FIELD_ID)
        REPORT_LEGION_ERROR(ERROR_TASK_ATTEMPTED_ALLOCATE_FIELD,
                     "Task %s (ID %lld) attempted to allocate a field with "
                     "ID %d which exceeds the LEGION_MAX_APPLICATION_FIELD_ID"
                     " bound set in legion_config.h", get_task_name(),
                     get_unique_id(), fid)
      if (field_size.impl == NULL)
        REPORT_LEGION_ERROR(ERROR_REQUEST_FOR_EMPTY_FUTURE,
            "Invalid empty future passed to field allocation for field %d "
            "in task %s (UID %lld)", fid, get_task_name(), get_unique_id())
      std::map<FieldSpace,std::pair<ShardID,bool> >::const_iterator finder = 
        field_allocator_owner_shards.find(space);
#ifdef DEBUG_LEGION
      assert(finder != field_allocator_owner_shards.end());
#endif
      // Get a new creation operation
      CreationOp *creator_op = runtime->get_available_creation_op();
      const RtBarrier creation_bar = creation_barrier.next(this);
      // This deduplicates multiple shards on the same node
      if (finder->second.second)
      {
        const ApEvent ready = creator_op->get_completion_event();
        const bool owner = (finder->second.first == owner_shard->shard_id);
        RtEvent precondition;
        FieldSpaceNode *node = runtime->forest->allocate_field(space, ready,
            fid, serdez_id, provenance, precondition, !owner);
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/, precondition);
        creator_op->initialize_field(this, node, fid, field_size, 
                                     provenance, owner);
      }
      else
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
      register_field_creation(space, fid, local);
      // Make sure the IDs are valid everywhere
      creation_bar.wait();
      if (!finder->second.second)
      {
        FieldSpaceNode *node = runtime->forest->get_node(space);
        creator_op->initialize_field(this, node, fid, field_size, 
                                     provenance, false/*owner*/);
      }
      // Launch the creation op in this context to act as a fence to ensure
      // that the allocations are done on all shard nodes before anyone else
      // tries to use them or their meta-data
      add_to_dependence_queue(creator_op);
      return fid;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::free_field(FieldAllocatorImpl *allocator, 
    FieldSpace space, FieldID fid, const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_FREE_FIELD, __func__);
        hasher.hash(space, "space");
        hasher.hash(fid, "fid");
        if (hasher.verify(__func__))
          break;
      }
      {
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
        const std::pair<FieldSpace,FieldID> key(space, fid);
        // This field will actually be removed in analyze_destroy_fields
        std::set<std::pair<FieldSpace,FieldID> >::const_iterator finder = 
          created_fields.find(key);
        if (finder == created_fields.end())
        {
          std::map<std::pair<FieldSpace,FieldID>,bool>::iterator 
            local_finder = local_fields.find(key);
          if (local_finder == local_fields.end())
          {
            // If we didn't make this field, record the deletion and
            // then have a later context handle it
            deleted_fields.emplace_back(DeletedField(space, fid, provenance));
            return;
          }
          else
            local_finder->second = true;
        }
      }
      ReplDeletionOp *op = runtime->get_available_repl_deletion_op();
      op->initialize_field_deletion(this, space, fid, unordered, allocator,
                                    provenance, (owner_shard->shard_id != 0));
      op->initialize_replication(this,
          shard_manager->is_first_local_shard(owner_shard));
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered field free performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::allocate_fields(FieldSpace space,
                                         const std::vector<size_t> &sizes,
                                         std::vector<FieldID> &resulting_fields,
                                         bool local, CustomSerdezID serdez_id,
                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_ALLOCATE_FIELDS, __func__);
        hasher.hash(space, "space");
        for (std::vector<size_t>::const_iterator it = 
              sizes.begin(); it != sizes.end(); it++)
          hasher.hash(*it, "sizes");
        for (std::vector<FieldID>::const_iterator it = 
              resulting_fields.begin(); it != resulting_fields.end(); it++)
          hasher.hash(*it, "resulting_fields");
        hasher.hash(local, "local");
        hasher.hash(serdez_id, "serdez_id");
        if (hasher.verify(__func__))
          break;
      }
      if (local)
        REPORT_LEGION_FATAL(LEGION_FATAL_UNIMPLEMENTED_FEATURE,
                      "Local field creation is not currently supported "
                      "for control replication with task %s (UID %lld)",
                      get_task_name(), get_unique_id())
      if (resulting_fields.size() < sizes.size())
        resulting_fields.resize(sizes.size(), LEGION_AUTO_GENERATE_ID);
      for (unsigned idx = 0; idx < resulting_fields.size(); idx++)
      {
        if (resulting_fields[idx] == LEGION_AUTO_GENERATE_ID)
        {
          if (pending_fields.empty())
          {
            increase_pending_fields(1/*count*/, false/*double*/);
            pending_field_check = 0;
          }
          bool double_next = false;
          bool double_buffer = false;
          std::pair<ValueBroadcast<FIDBroadcast>*,bool> &collective = 
            pending_fields.front();
          if (collective.second)
          {
            const FIDBroadcast value = collective.first->get_value(false);
            resulting_fields[idx] = value.field_id;
            double_buffer = value.double_buffer;
          }
          else
          {
            const RtEvent done = collective.first->get_done_event();
            if (!done.has_triggered())
            {
              double_next = true;
              done.wait();
            }
            const FIDBroadcast value = collective.first->get_value(false);
            resulting_fields[idx] = value.field_id;
            double_buffer = value.double_buffer;
          }
          if (++pending_field_check == pending_fields.size())
            pending_field_check = 0;
          else
            double_buffer = false;
          increase_pending_fields(
              double_buffer ? pending_fields.size() + 1 : 1, double_next);
          delete collective.first;
          pending_fields.pop_front();
        }
        else if (resulting_fields[idx] >= LEGION_MAX_APPLICATION_FIELD_ID)
          REPORT_LEGION_ERROR(ERROR_TASK_ATTEMPTED_ALLOCATE_FIELD,
            "Task %s (ID %lld) attempted to allocate a field with "
            "ID %d which exceeds the LEGION_MAX_APPLICATION_FIELD_ID "
            "bound set in legion_config.h", get_task_name(),
            get_unique_id(), resulting_fields[idx])
      }
      std::map<FieldSpace,std::pair<ShardID,bool> >::const_iterator finder = 
        field_allocator_owner_shards.find(space);
#ifdef DEBUG_LEGION
      assert(finder != field_allocator_owner_shards.end());
#endif
      RtEvent precondition;
      // This deduplicates multiple shards on the same node
      if (finder->second.second)
      {
        const bool non_owner = (finder->second.first != owner_shard->shard_id);
        precondition = runtime->forest->allocate_fields(space, sizes, 
                  resulting_fields, serdez_id, provenance, non_owner);
        if (runtime->legion_spy_enabled && !non_owner)
          for (unsigned idx = 0; idx < resulting_fields.size(); idx++)
            LegionSpy::log_field_creation(space.id, resulting_fields[idx],
             sizes[idx], (provenance == NULL) ? NULL : provenance->human_str());
      }
      const RtBarrier creation_bar = creation_barrier.next(this);
      Runtime::phase_barrier_arrive(creation_bar, 1/*count*/, precondition);
      register_all_field_creations(space, local, resulting_fields);
      // Make sure all the field IDs are valid everywhere
      creation_bar.wait();
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::allocate_fields(FieldSpace space,
                                         const std::vector<Future> &sizes,
                                         std::vector<FieldID> &resulting_fields,
                                         bool local, CustomSerdezID serdez_id,
                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_ALLOCATE_FIELDS, __func__);
        hasher.hash(space, "space");
        for (std::vector<Future>::const_iterator it = 
              sizes.begin(); it != sizes.end(); it++)
          hash_future(hasher, runtime->safe_control_replication, *it, "sizes");
        for (std::vector<FieldID>::const_iterator it = 
              resulting_fields.begin(); it != resulting_fields.end(); it++)
          hasher.hash(*it, "resulting_fields");
        hasher.hash(local, "local");
        hasher.hash(serdez_id, "serdez_id");
        if (hasher.verify("allocate_fields"))
          break;
      }
      if (local)
        REPORT_LEGION_FATAL(LEGION_FATAL_UNIMPLEMENTED_FEATURE,
                      "Local field creation is not currently supported "
                      "for control replication with task %s (UID %lld)",
                      get_task_name(), get_unique_id())
      for (unsigned idx = 0; idx < resulting_fields.size(); idx++)
      {
        if (resulting_fields[idx] == LEGION_AUTO_GENERATE_ID)
        {
          if (pending_fields.empty())
          {
            increase_pending_fields(1/*count*/, false/*double*/);
            pending_field_check = 0;
          }
          bool double_next = false;
          bool double_buffer = false;
          std::pair<ValueBroadcast<FIDBroadcast>*,bool> &collective = 
            pending_fields.front();
          if (collective.second)
          {
            const FIDBroadcast value = collective.first->get_value(false);
            resulting_fields[idx] = value.field_id;
            double_buffer = value.double_buffer;
          }
          else
          {
            const RtEvent done = collective.first->get_done_event();
            if (!done.has_triggered())
            {
              double_next = true;
              done.wait();
            }
            const FIDBroadcast value = collective.first->get_value(false);
            resulting_fields[idx] = value.field_id;
            double_buffer = value.double_buffer;
          }
          if (++pending_field_check == pending_fields.size())
            pending_field_check = 0;
          else
            double_buffer = false;
          increase_pending_fields(
              double_buffer ? pending_fields.size() + 1 : 1, double_next);
          delete collective.first;
          pending_fields.pop_front();
        }
#ifdef DEBUG_LEGION
        else if (resulting_fields[idx] >= LEGION_MAX_APPLICATION_FIELD_ID)
          REPORT_LEGION_ERROR(ERROR_TASK_ATTEMPTED_ALLOCATE_FIELD,
            "Task %s (ID %lld) attempted to allocate a field with "
            "ID %d which exceeds the LEGION_MAX_APPLICATION_FIELD_ID "
            "bound set in legion_config.h", get_task_name(),
            get_unique_id(), resulting_fields[idx])
#endif
      }
      for (unsigned idx = 0; idx < sizes.size(); idx++)
        if (sizes[idx].impl == NULL)
          REPORT_LEGION_ERROR(ERROR_REQUEST_FOR_EMPTY_FUTURE,
              "Invalid empty future passed to field allocation for field %d "
              "in task %s (UID %lld)", resulting_fields[idx],
              get_task_name(), get_unique_id())
      std::map<FieldSpace,std::pair<ShardID,bool> >::const_iterator finder = 
        field_allocator_owner_shards.find(space);
#ifdef DEBUG_LEGION
      assert(finder != field_allocator_owner_shards.end());
#endif
      // Get a new creation operation
      CreationOp *creator_op = runtime->get_available_creation_op();
      const RtBarrier creation_bar = creation_barrier.next(this);
      // This deduplicates multiple shards on the same node
      if (finder->second.second)
      {
        const ApEvent ready = creator_op->get_completion_event();
        const bool owner = (finder->second.first == owner_shard->shard_id);
        RtEvent precondition;
        FieldSpaceNode *node = runtime->forest->allocate_fields(space, ready,
                resulting_fields, serdez_id, provenance, precondition, !owner);
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/, precondition);
        creator_op->initialize_fields(this, node, resulting_fields, 
                                      sizes, provenance, owner);
      }
      else
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
      register_all_field_creations(space, local, resulting_fields);
      // Make sure the field IDs are valid everywhere
      creation_bar.wait();
      if (!finder->second.second)
      {
        FieldSpaceNode *node = runtime->forest->get_node(space);
        creator_op->initialize_fields(this, node, resulting_fields, 
                                      sizes, provenance, false/*owner*/);
      }
      // Launch the creation op in this context to act as a fence to ensure
      // that the allocations are done on all shard nodes before anyone else
      // tries to use them or their meta-data
      add_to_dependence_queue(creator_op);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::free_fields(FieldAllocatorImpl *allocator,
                                       FieldSpace space, 
                                       const std::set<FieldID> &to_free,
                                       const bool unordered,
                                       Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && !unordered && (i < 2)
            && ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_FREE_FIELDS, __func__);
        hasher.hash(space, "space");
        for (std::set<FieldID>::const_iterator it = 
              to_free.begin(); it != to_free.end(); it++)
          hasher.hash(*it, "to_free");
        if (hasher.verify(__func__))
          break;
      }
      std::set<FieldID> free_now;
      {
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
        // These fields will actually be removed in analyze_destroy_fields
        for (std::set<FieldID>::const_iterator it = 
              to_free.begin(); it != to_free.end(); it++)
        {
          const std::pair<FieldSpace,FieldID> key(space, *it);
          std::set<std::pair<FieldSpace,FieldID> >::const_iterator finder = 
            created_fields.find(key);
          if (finder == created_fields.end())
          {
            std::map<std::pair<FieldSpace,FieldID>,bool>::iterator 
              local_finder = local_fields.find(key);
            if (local_finder != local_fields.end())
            {
              local_finder->second = true;
              free_now.insert(*it);
            }
            else
              deleted_fields.emplace_back(DeletedField(space, *it, provenance));
          }
          else
            free_now.insert(*it);
        }
      }
      if (free_now.empty())
        return;
      ReplDeletionOp *op = runtime->get_available_repl_deletion_op();
      op->initialize_field_deletions(this, space, free_now, unordered, 
                    allocator, provenance, (owner_shard->shard_id != 0));
      op->initialize_replication(this,
          shard_manager->is_first_local_shard(owner_shard));
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered free fields performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
    }

    //--------------------------------------------------------------------------
    LogicalRegion ReplicateContext::create_logical_region(
                                                      IndexSpace index_space,
                                                      FieldSpace field_space,
                                                      const bool task_local,
                                                      Provenance *provenance,
                                                      const bool output_region)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_CREATE_LOGICAL_REGION, __func__);
        hasher.hash(index_space, "index_space");
        hasher.hash(field_space, "field_space");
        hasher.hash(task_local, "task_local");
        if (hasher.verify(__func__))
          break;
      }
      // Seed this with the first field space broadcast
      if (pending_region_trees.empty())
      {
        increase_pending_region_trees(1/*count*/, false/*double*/);
        pending_region_tree_check = 0;
      }
      LogicalRegion handle(0/*temp*/, index_space, field_space);
      bool double_next = false;
      bool double_buffer = false;
      std::pair<ValueBroadcast<LRBroadcast>*,bool> &collective = 
        pending_region_trees.front();
      CollectiveMapping &collective_mapping = 
        shard_manager->get_collective_mapping();
      const RtBarrier creation_bar = creation_barrier.next(this);
      if (collective.second)
      {
        const LRBroadcast value = collective.first->get_value(false);
        handle.tree_id = value.tid;
        double_buffer = value.double_buffer;
        // Have to register this before doing the broadcast
        runtime->forest->create_logical_region(handle, value.did, provenance,
            &collective_mapping, creation_bar);
        // Arrive on the creation barrier
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
        runtime->forest->revoke_pending_region_tree(value.tid);
#ifdef DEBUG_LEGION
        log_region.debug("Creating logical region in task %s (ID %lld) with "
                         "index space %x and field space %x in new tree %d",
                         get_task_name(), get_unique_id(), index_space.id, 
                         field_space.id, handle.tree_id);
#endif
        if (runtime->legion_spy_enabled)
          LegionSpy::log_top_region(index_space.id, field_space.id,
              handle.tree_id, runtime->address_space,
              (provenance == NULL) ? NULL : provenance->human_str());
      }
      else
      {
        const RtEvent done = collective.first->get_done_event();
        if (!done.has_triggered())
        {
          double_next = true;
          done.wait();
        }
        const LRBroadcast value = collective.first->get_value(false);
        handle.tree_id = value.tid;
        double_buffer = value.double_buffer;
#ifdef DEBUG_LEGION
        assert(handle.exists());
#endif
        runtime->forest->create_logical_region(handle, value.did, provenance,
            &collective_mapping, creation_bar);
        // Signal that we are done our creation
        Runtime::phase_barrier_arrive(creation_bar, 1/*count*/);
      }
      // Register the creation of a top-level region with the context
      const unsigned created_index =
        register_region_creation(handle, task_local, output_region);
      if (output_region)
      {
        // If this is an output region make sure nobody tries to compute
        // the equivalence sets for it until we know it is ready
        AutoLock priv_lock(privilege_lock);
#ifdef DEBUG_LEGION
        assert(equivalence_set_trees.find(created_index) ==
                equivalence_set_trees.end());
        assert(pending_equivalence_set_trees.find(created_index) ==
            pending_equivalence_set_trees.end());
#endif
        // Put in a guard so that nobody else tries to make it
        pending_equivalence_set_trees[created_index] = 
          RtUserEvent::NO_RT_USER_EVENT;
      }
      if (++pending_region_tree_check == pending_region_trees.size())
        pending_region_tree_check = 0;
      else
        double_buffer = false;
      // Get new handles in flight for the next time we need them
      // Always add a new one to replace the old one, but double the number
      // in flight if we're not hiding the latency
      increase_pending_region_trees(
          double_buffer ? pending_region_trees.size() + 1 : 1, double_next);
      delete collective.first;
      pending_region_trees.pop_front();
      return handle;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::increase_pending_region_trees(unsigned count,
                                                         bool double_next)
    //--------------------------------------------------------------------------
    {
      for (unsigned idx = 0; idx < count; idx++)
      {
        if (owner_shard->shard_id == logical_region_allocator_shard)
        {
          const RegionTreeID tid = runtime->get_unique_region_tree_id();
          const DistributedID did = runtime->get_available_distributed_id();
          // We're the owner, so make it locally and then broadcast it
          runtime->forest->record_pending_region_tree(tid);
          // Do our arrival on this generation, should be the last one
          ValueBroadcast<LRBroadcast> *collective = 
            new ValueBroadcast<LRBroadcast>(this, COLLECTIVE_LOC_34);
          collective->broadcast(LRBroadcast(tid, did, double_next));
          pending_region_trees.push_back(
              std::pair<ValueBroadcast<LRBroadcast>*,bool>(collective, true));
        }
        else
        {
          ValueBroadcast<LRBroadcast> *collective = 
            new ValueBroadcast<LRBroadcast>(this,logical_region_allocator_shard,
                                            COLLECTIVE_LOC_34);
          register_collective(collective);
          pending_region_trees.push_back(
              std::pair<ValueBroadcast<LRBroadcast>*,bool>(collective, false));
        }
        logical_region_allocator_shard++;
        if (logical_region_allocator_shard == total_shards)
          logical_region_allocator_shard = 0;
        double_next = false;
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::create_shared_ownership(LogicalRegion handle)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_CREATE_SHARED_OWNERSHIP, __func__);
        hasher.hash(handle, "handle");
        if (hasher.verify("create_shared_ownership"))
          break;
      }
      if (!handle.exists())
        return;
      if (!runtime->forest->is_top_level_region(handle))
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_SHARED_OWNERSHIP,
            "Illegal call to create shared ownership for logical region "
            "(%x,%x,%x in task %s (UID %lld) which is not a top-level logical "
            "region. Legion only permits top-level logical regions to have "
            "shared ownerships.", handle.index_space.id, handle.field_space.id,
            handle.tree_id, get_task_name(), get_unique_id())
      if (shard_manager->is_total_sharding() &&
          shard_manager->is_first_local_shard(owner_shard))
        runtime->create_shared_ownership(handle, true/*total sharding*/);
      else if (owner_shard->shard_id == 0)
        runtime->create_shared_ownership(handle);
      AutoLock priv_lock(privilege_lock);
      std::map<LogicalRegion,unsigned>::iterator finder = 
        created_regions.find(handle);
      if (finder != created_regions.end())
        finder->second++;
      else
        created_regions[handle] = 1;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::destroy_logical_region(LogicalRegion handle,
                                   const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && !unordered && (i < 2)
            && ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_DESTROY_LOGICAL_REGION, __func__);
        hasher.hash(handle, "handle");
        if (hasher.verify(__func__))
          break;
      }
      if (!handle.exists())
        return;
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_region.debug("Deleting logical region (%x,%x) in task %s (ID %lld)",
                         handle.index_space.id, handle.field_space.id, 
                         get_task_name(), get_unique_id());
#endif
      // Check to see if this is a top-level logical region, if not then
      // we shouldn't even be destroying it
      if (!runtime->forest->is_top_level_region(handle))
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_RESOURCE_DESTRUCTION,
            "Illegal call to destroy logical region (%x,%x,%x in task %s "
            "(UID %lld) which is not a top-level logical region. Legion only "
            "permits top-level logical regions to be destroyed.", 
            handle.index_space.id, handle.field_space.id, handle.tree_id,
            get_task_name(), get_unique_id())
      // Check to see if this is one that we should be allowed to destory
      {
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
        std::map<LogicalRegion,unsigned>::iterator finder = 
          created_regions.find(handle);
        if (finder == created_regions.end())
        {
          // Check to see if it is a local region
          std::map<LogicalRegion,bool>::iterator local_finder = 
            local_regions.find(handle);
          // Mark that this region is deleted, safe even though this
          // is a read-only lock because we're not changing the structure
          // of the map
          if (local_finder == local_regions.end())
          {
            // Record the deletion for later and propagate it up
            deleted_regions.emplace_back(DeletedRegion(handle, provenance));
            return;
          }
          else
            local_finder->second = true;
        }
        else
        {
          if (finder->second == 0)
          {
            REPORT_LEGION_WARNING(LEGION_WARNING_DUPLICATE_DELETION,
                "Duplicate deletions were performed for region (%x,%x,%x) "
                "in task tree rooted by %s", handle.index_space.id, 
                handle.field_space.id, handle.tree_id, get_task_name())
            return;
          }
          if (--finder->second > 0)
            return;
          // Don't remove anything from created regions yet, we still might
          // need it as part of the logical dependence analysis for earlier
          // operations, but the reference count is zero so we're protected
        }
      }
      ReplDeletionOp *op = runtime->get_available_repl_deletion_op();
      op->initialize_logical_region_deletion(this, handle,unordered,provenance);
      op->initialize_replication(this,
          shard_manager->is_first_local_shard(owner_shard));
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered logical region deletion performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::reset_equivalence_sets(LogicalRegion parent,
                          LogicalRegion region, const std::set<FieldID> &fields)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_RESET_EQUIVALENCE_SETS, __func__);
        hasher.hash(parent, "parent");
        hasher.hash(region, "region");
        for (std::set<FieldID>::const_iterator it =
              fields.begin(); it != fields.end(); it++)
          hasher.hash(*it, "fields");
        if (hasher.verify(__func__))
          break;
      }
      // Ignore reset calls inside of traces replays
      if ((current_trace != NULL) && current_trace->is_fixed())
      {
        REPORT_LEGION_WARNING(
            LEGION_WARNING_IGNORING_EQUIVALENCE_SETS_RESET,
            "Ignoring equivalence sets reset in %s (UID %lld) because "
            "it was made inside of a trace.",
            get_task_name(), get_unique_id())
        return;
      }
      if (fields.empty())
      {
        REPORT_LEGION_WARNING(
            LEGION_WARNING_IGNORING_EQUIVALENCE_SETS_RESET,
            "Ignoring equivalence sets reset in %s (UID %lld) because "
            "it contains no fields.",
            get_task_name(), get_unique_id())
        return;
      }
      ReplResetOp *reset = runtime->get_available_repl_reset_op();
      reset->initialize(this, parent, region, fields);
      add_to_dependence_queue(reset);
    }

    //--------------------------------------------------------------------------
    FieldAllocatorImpl* ReplicateContext::create_field_allocator(
                                              FieldSpace handle, bool unordered)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && !unordered && (i < 2)
            && ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_CREATE_FIELD_ALLOCATOR, __func__);
        hasher.hash(handle, "handle");
        if (hasher.verify(__func__))
          break;
      }
      {
        AutoLock priv_lock(privilege_lock,1,false/*exclusive*/);
        std::map<FieldSpace,FieldAllocatorImpl*>::const_iterator finder = 
          field_allocators.find(handle);
        if (finder != field_allocators.end())
          return finder->second;
      }
      FieldSpaceNode *node = runtime->forest->get_node(handle);
      if (unordered)
      {
        // This next part is unsafe to perform in a control replicated
        // context if we are unordered, so just make a fresh allocator
        const RtEvent ready = node->create_allocator(runtime->address_space);
        // Don't have one so make a new one
        FieldAllocatorImpl *result = new FieldAllocatorImpl(node, this, ready);
        // DO NOT SAVE THIS!
        return result;
      }
      // Didn't find it, so have to make, retake the lock in exclusive mode
      AutoLock priv_lock(privilege_lock);
      // Check to see if we lost the race
      std::map<FieldSpace,FieldAllocatorImpl*>::const_iterator finder = 
        field_allocators.find(handle);
      if (finder != field_allocators.end())
        return finder->second;
      // Check to see which shard (if any) owns this field space
      const AddressSpaceID owner_space = 
        FieldSpaceNode::get_owner_space(handle, runtime);
      // Figure out which shard is the owner
      bool found = false;
      std::pair<ShardID,bool> owner(0,false);
      const ShardMapping &mapping = shard_manager->get_mapping();
      for (unsigned idx = 0; idx < mapping.size(); idx++)
      {
        if (mapping[idx] != owner_space)
          continue;
        owner.first = idx;
        found = true;
        break;
      }
      // Pick a shard to be the owner if we don't have a local shard
      if (!found)
      { 
        owner.first = field_allocator_shard++;
        if (field_allocator_shard == total_shards)
          field_allocator_shard = 0;
      }
      if (owner_space == runtime->address_space)
        owner.second = (owner.first == owner_shard->shard_id);
      else
        owner.second = shard_manager->is_first_local_shard(owner_shard);
#ifdef DEBUG_LEGION
      assert(field_allocator_owner_shards.find(handle) == 
              field_allocator_owner_shards.end());
#endif
      field_allocator_owner_shards[handle] = owner;
      RtEvent ready;
      if (owner.second)
        ready = node->create_allocator(runtime->address_space, 
            RtUserEvent::NO_RT_USER_EVENT, true/*sharded context*/, 
            (owner.first == owner_shard->shard_id));
      // Don't have one so make a new one
      FieldAllocatorImpl *result = new FieldAllocatorImpl(node, this, ready);
      // Save it for later
      field_allocators[handle] = result;
      return result;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::destroy_field_allocator(FieldSpaceNode *node,
                                                   bool from_application)
    //--------------------------------------------------------------------------
    {
      if (from_application)
      {
        AutoRuntimeCall call(this);
        destroy_field_allocator(node, false/*from application*/);
        return;
      }
      bool found = false;
      std::pair<ShardID,bool> result;
      {
        AutoLock priv_lock(privilege_lock);
        // Check to see if we still have one
        std::map<FieldSpace,FieldAllocatorImpl*>::iterator finder = 
          field_allocators.find(node->handle);
        if (finder != field_allocators.end())
        {
          found = true;
          field_allocators.erase(finder);
          std::map<FieldSpace,std::pair<ShardID,bool> >::iterator owner_finder =
            field_allocator_owner_shards.find(node->handle);
#ifdef DEBUG_LEGION
          assert(owner_finder != field_allocator_owner_shards.end());
#endif
          result = owner_finder->second;
          
          field_allocator_owner_shards.erase(owner_finder);
        }
      }
      if (found && result.second)
      {
        const RtEvent ready = node->destroy_allocator(runtime->address_space,
              true/*sharded*/, (result.first == owner_shard->shard_id));
        if (ready.exists() && !ready.has_triggered())
          ready.wait();
      }
      else if (!found)
      {
        const RtEvent ready = node->destroy_allocator(runtime->address_space);
        if (ready.exists() && !ready.has_triggered())
          ready.wait();
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::initialize_unordered_collective(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(unordered_collective == NULL);
#endif
      unordered_ops_counter = 0;
      unordered_collective = 
        new UnorderedExchange(this, COLLECTIVE_LOC_88);
      unordered_collective->start_unordered_exchange(unordered_ops);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::finalize_unordered_collective(AutoLock &d_lock)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(unordered_collective != NULL);
#endif
      const RtEvent ready =
        unordered_collective->perform_collective_wait(false/*block*/);
      if (ready.exists() && !ready.has_triggered())
      {
        d_lock.release();
        ready.wait();
        d_lock.reacquire();
      }
      std::vector<Operation*> ready_operations;
      if (!unordered_ops.empty())
        unordered_collective->find_ready_operations(ready_operations);
      delete unordered_collective;
      unordered_collective = NULL;
      if (!ready_operations.empty())
      {
        // Filter out the ready operations
        for (std::vector<Operation*>::iterator it = unordered_ops.begin();
              it != unordered_ops.end(); /*nothing*/)
        {
          bool filter = false;
          for (unsigned idx = 0; idx < ready_operations.size(); idx++)
          {
            if ((*it) != ready_operations[idx])
              continue;
            filter = true;
            break;
          }
          if (filter)
            it = unordered_ops.erase(it);
          else
            it++;
        }
        // Now we can do the insertion
        issue_unordered_operations(d_lock, ready_operations);
        // Reset the epoch counter back to the minimum since we found some
        // matches so we should probably be doing this more often
        unordered_ops_epoch = MIN_UNORDERED_OPS_EPOCH;
      }
      else if (unordered_ops_epoch < MAX_UNORDERED_OPS_EPOCH)
        // If there were no ready unordered ops then we double the epoch
        unordered_ops_epoch *= 2;
#ifdef DEBUG_LEGION
      assert(MIN_UNORDERED_OPS_EPOCH <= unordered_ops_epoch);
      assert(unordered_ops_epoch <= MAX_UNORDERED_OPS_EPOCH);
#endif
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::insert_unordered_ops(AutoLock &d_lock)
    //--------------------------------------------------------------------------
    {
      // If we have a trace then we're definitely not inserting operations
      if (current_trace != NULL)
        return;
      // For control replication, we need to have an algorithm to determine
      // when the shards try to sync up to insert operations that doesn't
      // rely on knowing if or when any one shard has unordered ops
      // We employ a sampling based algorithm here with exponential backoff
      // to detect when we are doing unordered ops since it's likely a 
      // binary state where either we are or we aren't doing unordered ops
#ifdef DEBUG_LEGION
      assert(unordered_ops_counter < unordered_ops_epoch);
#endif
      // If we're doing progress then we can skip this check and
      // reset the counter back to zero since we're doing an exchange
      if (++unordered_ops_counter < unordered_ops_epoch)
        return;
      // Check to see if the previous exchange had any matching unordered
      // operations for us to perform
      if (unordered_collective != NULL)
        finalize_unordered_collective(d_lock);
      // Start the next exchange
      initialize_unordered_collective();
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::progress_unordered_operations(bool end_task)
    //--------------------------------------------------------------------------
    {
      AutoLock d_lock(dependence_lock);
      if (end_task)
      {
        // This is the end of this parent task so mark that we're done
#ifdef DEBUG_LEGION
        assert(!finished_execution);
        assert(current_trace == NULL);
#endif
        finished_execution = true;
      }
      // No progress can occur inside of a trace
      else if (current_trace != NULL)
        return;
      // With control replication we're always doing half phases for detecting
      // when we have unordered operations across all shards that are ready to
      // be performed, but in this case the user has asked us to actually
      // perform a full phase, so finish the previous one and then start a 
      // new phase if we're not the end last phase
      if (unordered_collective != NULL)
        finalize_unordered_collective(d_lock);
      initialize_unordered_collective();
      finalize_unordered_collective(d_lock);
      if (end_task)
      {
        if (!unordered_ops.empty())
          REPORT_LEGION_WARNING(LEGION_WARNING_MISMATCHED_UNORDERED_OPERATIONS,
              "Control replicated task %s (UID %lld) had %zd mismatched "
              "unordered operations at the end of its execution that are now "
              "leaked.", get_task_name(), get_unique_id(), unordered_ops.size())
      }
      else
        initialize_unordered_collective();
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::execute_task(const TaskLauncher &launcher,
                                        std::vector<OutputRequirement> *outputs)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_EXECUTE_TASK, __func__);
        hash_task_launcher(hasher, runtime->safe_control_replication, launcher);
        if (outputs != NULL) hash_output_requirements(hasher, *outputs);
        if (hasher.verify(__func__))
          break;
      }
      // Quick out for predicate false
      if (launcher.predicate == Predicate::FALSE_PRED)
        return predicate_task_false(launcher, provenance);
      // If we're doing a local-function task then we can run that with just
      // a normal individual task in each shard since it is safe to duplicate
      if (launcher.local_function_task)
        return InnerContext::execute_task(launcher, outputs);
      ReplIndividualTask *task = 
        runtime->get_available_repl_individual_task();
      Future result = task->initialize_task(this,
                                            launcher,
                                            provenance,
                                            true /*track*/,
                                            false /*top_level*/,
                                            false /*must epoch*/,
                                            outputs);
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_task.debug("Registering new single task with unique id %lld "
                        "and task %s (ID %lld) with high level runtime in "
                        "addresss space %d",
                        task->get_unique_id(), task->get_task_name(), 
                        task->get_unique_id(), runtime->address_space);
      task->set_sharding_collective(new ShardingGatherCollective(this, 
                                    0/*owner shard*/, COLLECTIVE_LOC_43));
#endif
      // Now initialize the particular information for replication 
      task->initialize_replication(this);
      if (launcher.enable_inlining && !launcher.silence_warnings)
        REPORT_LEGION_WARNING(LEGION_WARNING_INLINING_NOT_SUPPORTED,
            "Inlining is not currently supported for replicated tasks "
            "such as %s (UID %lld)", get_task_name(), get_unique_id())
      execute_task_launch(task, false/*index*/, current_trace, provenance,
                          launcher.silence_warnings, false/*no inlining*/);
      return result;
    }

    //--------------------------------------------------------------------------
    FutureMap ReplicateContext::execute_index_space(
                                        const IndexTaskLauncher &launcher,
                                        std::vector<OutputRequirement> *outputs)
    //--------------------------------------------------------------------------
    {
      if (launcher.must_parallelism)
      {
        // Turn around and use a must epoch launcher
        MustEpochLauncher epoch_launcher(launcher.map_id, launcher.tag);
        epoch_launcher.add_index_task(launcher);
        epoch_launcher.provenance = launcher.provenance;
        FutureMap result = execute_must_epoch(epoch_launcher);
        return result;
      }
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
          ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_EXECUTE_INDEX_SPACE, __func__);
        hash_index_launcher(hasher, runtime->safe_control_replication,launcher);
        if (outputs != NULL) hash_output_requirements(hasher, *outputs);
        if (hasher.verify(__func__))
          break;
      }
      if (launcher.launch_domain.exists() && 
          (launcher.launch_domain.get_volume() == 0))
      {
        log_run.warning("Ignoring empty index task launch in task %s (ID %lld)",
                        get_task_name(), get_unique_id());
        return FutureMap();
      }
      IndexSpace launch_space = launcher.launch_space;
      if (!launch_space.exists())
        launch_space = find_index_launch_space(launcher.launch_domain,
                                               provenance);
      // Quick out for predicate false
      if (launcher.predicate == Predicate::FALSE_PRED)
        return predicate_index_task_false(total_children_count++, 
                                          launch_space, launcher, provenance);
      ReplIndexTask *task = runtime->get_available_repl_index_task();
      FutureMap result = task->initialize_task(this,
                                               launcher,
                                               launch_space,
                                               provenance,
                                               true /*track*/,
                                               outputs);
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_task.debug("Registering new index space task with unique id "
                       "%lld and task %s (ID %lld) with high level runtime in "
                       "address space %d",
                       task->get_unique_id(), task->get_task_name(), 
                       task->get_unique_id(), runtime->address_space);
      task->set_sharding_collective(new ShardingGatherCollective(this, 
                                    0/*owner shard*/, COLLECTIVE_LOC_44));
#endif
      task->initialize_replication(this);
      if (launcher.enable_inlining && !launcher.silence_warnings)
        REPORT_LEGION_WARNING(LEGION_WARNING_INLINING_NOT_SUPPORTED,
            "Inlining is not currently supported for replicated tasks "
            "such as %s (UID %lld)", get_task_name(), get_unique_id())
      execute_task_launch(task, true/*index*/, current_trace, provenance,
                          launcher.silence_warnings, false/*no inlining*/);
      return result;
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::execute_index_space(
                                        const IndexTaskLauncher &launcher,
                                        ReductionOpID redop, bool deterministic,
                                        std::vector<OutputRequirement> *outputs)
    //--------------------------------------------------------------------------
    {
      AutoProvenance provenance(launcher.provenance);
      if (launcher.must_parallelism)
      {
        // Turn around and use a must epoch launcher
        MustEpochLauncher epoch_launcher(launcher.map_id, launcher.tag);
        epoch_launcher.add_index_task(launcher);
        epoch_launcher.provenance = launcher.provenance;
        FutureMap result = execute_must_epoch(epoch_launcher);
        // Reduce the future map down to a future
        return reduce_future_map(result, redop, deterministic,
                                 launcher.map_id, launcher.tag, provenance,
                                 launcher.initial_value);
      }
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_EXECUTE_INDEX_SPACE, __func__);
        hash_index_launcher(hasher, runtime->safe_control_replication,launcher);
        hasher.hash(redop, "redop");
        hasher.hash<bool>(deterministic, "deterministic");
        if (outputs != NULL) hash_output_requirements(hasher, *outputs);
        if (hasher.verify(__func__))
          break;
      } 
      if (launcher.launch_domain.exists() &&
          (launcher.launch_domain.get_volume() == 0))
      {
        if (!launcher.initial_value.is_empty())
          return launcher.initial_value;

        REPORT_LEGION_WARNING(LEGION_WARNING_IGNORING_EMPTY_INDEX_TASK_LAUNCH,
          "Ignoring empty index task launch in task %s (ID %lld)",
                        get_task_name(), get_unique_id());
        const ReductionOp *reduction_op = runtime->get_reduction(redop);
        FutureImpl *result = new FutureImpl(this, runtime, true/*register*/,
          runtime->get_available_distributed_id(), provenance);
        result->set_local(reduction_op->identity,
                          reduction_op->sizeof_rhs, false/*own*/);
        return Future(result);
      }
      IndexSpace launch_space = launcher.launch_space;
      if (!launch_space.exists())
        launch_space = find_index_launch_space(launcher.launch_domain,
                                               provenance);
      // Quick out for predicate false
      if (launcher.predicate == Predicate::FALSE_PRED)
        return predicate_index_task_reduce_false(launcher, launch_space,
                                                 redop, provenance);
      ReplIndexTask *task = runtime->get_available_repl_index_task();
      Future result = task->initialize_task(this, launcher, launch_space, 
                                            provenance, redop, deterministic,
                                            true/*track*/, outputs);
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_task.debug("Registering new index space task with unique id "
                       "%lld and task %s (ID %lld) with high level runtime in "
                       "address space %d",
                       task->get_unique_id(), task->get_task_name(), 
                       task->get_unique_id(), runtime->address_space);
      task->set_sharding_collective(new ShardingGatherCollective(this, 
                                    0/*owner shard*/, COLLECTIVE_LOC_45));
#endif
      task->initialize_replication(this);
      if (launcher.enable_inlining && !launcher.silence_warnings)
        REPORT_LEGION_WARNING(LEGION_WARNING_INLINING_NOT_SUPPORTED,
            "Inlining is not currently supported for replicated tasks "
            "such as %s (UID %lld)", get_task_name(), get_unique_id())
      execute_task_launch(task, true/*index*/, current_trace, provenance, 
                          launcher.silence_warnings, false/*no inlining*/);
      return result;
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::reduce_future_map(const FutureMap &future_map,
                                        ReductionOpID redop, bool deterministic,
                                        MapperID mapper_id, MappingTagID tag,
                                        Provenance *provenance,
                                        Future initial_value)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this); 
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_REDUCE_FUTURE_MAP, __func__);
        hash_future_map(hasher, future_map, "future_map");
        hash_future(hasher,
                    runtime->safe_control_replication,
                    initial_value,
                    "initial_value");
        hasher.hash(redop, "redop");
        hasher.hash(deterministic, "deterministic");
        if (hasher.verify(__func__))
          break;
      }
      if (future_map.impl == NULL)
      {
        const ReductionOp *reduction_op = runtime->get_reduction(redop);
        FutureImpl *result = new FutureImpl(this, runtime, true/*register*/,
          runtime->get_available_distributed_id(), provenance);
        result->set_local(reduction_op->identity,
                          reduction_op->sizeof_rhs, false/*own*/);
        return Future(result);
      }
      // Check to see if this is just a normal future map, if so then 
      // we can just do the standard thing here
      if (!future_map.impl->is_replicate_future_map())
        return InnerContext::reduce_future_map(future_map, redop, deterministic,
                                               mapper_id, tag, provenance,
                                               initial_value);
      ReplAllReduceOp *all_reduce_op = 
        runtime->get_available_repl_all_reduce_op();
      Future result = all_reduce_op->initialize(this, future_map, redop,
                              deterministic, mapper_id, tag, provenance,
                              initial_value);
      all_reduce_op->initialize_replication(this);
      add_to_dependence_queue(all_reduce_op);
      return result;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::increase_pending_distributed_ids(unsigned count,
                                                            bool double_next)
    //--------------------------------------------------------------------------
    {
      for (unsigned idx = 0; idx < count; idx++)
      {
        if (owner_shard->shard_id == distributed_id_allocator_shard)
        {
          const DistributedID did = runtime->get_available_distributed_id();
          // Do our arrival on this generation, should be the last one
          ValueBroadcast<DIDBroadcast> *collective = 
            new ValueBroadcast<DIDBroadcast>(this, COLLECTIVE_LOC_2);
          collective->broadcast(DIDBroadcast(did, double_next));
          pending_distributed_ids.emplace_back(
              std::pair<ValueBroadcast<DIDBroadcast>*,bool>(collective, true));
        }
        else
        {
          ValueBroadcast<DIDBroadcast> *collective = 
            new ValueBroadcast<DIDBroadcast>(this,
                distributed_id_allocator_shard, COLLECTIVE_LOC_2);
          register_collective(collective);
          pending_distributed_ids.emplace_back(
              std::pair<ValueBroadcast<DIDBroadcast>*,bool>(collective, false));
        }
        distributed_id_allocator_shard++;
        if (distributed_id_allocator_shard == total_shards)
          distributed_id_allocator_shard = 0;
        double_next = false;
      }
    }

    //--------------------------------------------------------------------------
    DistributedID ReplicateContext::get_next_distributed_id(void)
    //--------------------------------------------------------------------------
    {
      if (pending_distributed_ids.empty())
      {
        increase_pending_distributed_ids(1/*count*/, false/*double*/);
        pending_distributed_id_check = 0;
      }
      bool double_next = false;
      std::pair<ValueBroadcast<DIDBroadcast>*,bool> &pending_did =
        pending_distributed_ids.front();
      if (!pending_did.second)
      {
        const RtEvent done = pending_did.first->get_done_event();
        if (!done.has_triggered())
        {
          double_next = true;
          done.wait();
        }
      }
      const DIDBroadcast value = pending_did.first->get_value(false);
      bool double_buffer = false;
      if (++pending_distributed_id_check == pending_distributed_ids.size())
      {
        double_buffer = value.double_buffer;
        pending_distributed_id_check = 0;
      }
      // Get new handles in flight for the next time we need them
      // Always add a new one to replace the old one, but double the number
      // in flight if we're not hiding the latency
      increase_pending_distributed_ids(double_buffer ? 
       pending_distributed_ids.size() + 1 : 1, double_next);
      delete pending_did.first;
      pending_distributed_ids.pop_front();
      return value.did;
    }

    //--------------------------------------------------------------------------
    FutureMap ReplicateContext::construct_future_map(IndexSpace space,
                                const std::map<DomainPoint,UntypedBuffer> &data,
                                Provenance *provenance, bool collective,
                                ShardingID sid, bool implicit, bool internal,
                                bool check_space)
    //--------------------------------------------------------------------------
    {
      if (!internal)
      {
        AutoRuntimeCall call(this);
        for (int i = 0; runtime->safe_control_replication && (i < 2) &&
              ((current_trace == NULL) || !current_trace->is_fixed()); i++)
        {
          Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                                i > 0, provenance);
          hasher.hash(REPLICATE_CONSTRUCT_FUTURE_MAP, __func__);
          if (check_space)
            hasher.hash(space, "space");
          if (!collective)
          {
            for (std::map<DomainPoint,UntypedBuffer>::const_iterator it =
                  data.begin(); it != data.end(); it++)
            {
              hasher.hash(it->first, "data");
              if (runtime->safe_control_replication > 1)
                hasher.hash(it->second.get_ptr(), it->second.get_size(),"data");
            }
          }
          else if (!implicit)
            hasher.hash(sid, "sid");
          if (hasher.verify(__func__))
            break;
        }
        return construct_future_map(space, data, provenance, collective,
            sid, implicit, true/*internal*/, check_space);
      }
      IndexSpaceNode *domain_node = runtime->forest->get_node(space);
      Domain domain;
      domain_node->get_domain(domain);
      FutureMap result;
      if (collective)
      {
        const DistributedID map_did = get_next_distributed_id();
        result = shard_manager->deduplicate_future_map_creation(
              this, domain_node, domain_node, total_children_count++,
              map_did, ApEvent::NO_AP_EVENT, provenance);
        ReplFutureMapImpl *map = static_cast<ReplFutureMapImpl*>(result.impl);
        ShardingFunction *function;
        if (implicit)
        {
          // Do an exchange between the shards to compute the implicit sharding
          // No need to wait for it to be done before continuing
          ImplicitShardingFunctor *functor = new
            ImplicitShardingFunctor(this, COLLECTIVE_LOC_101, map);
          functor->compute_sharding(data);
          function = new ShardingFunction(functor, runtime->forest, 
              shard_manager, sid, false, true/*own functor*/);
          if (!map->set_sharding_function(function, true/*own*/))
          {
            // Wait for the collective to be done before we delete it
            functor->perform_collective_wait();
            delete function;
          }
        }
        else
        {
          function = shard_manager->find_sharding_function(sid);
          map->set_sharding_function(function, false/*own*/);
        }
        // Check that all the points abide by the sharding function 
        for (std::map<DomainPoint,UntypedBuffer>::const_iterator it =
              data.begin(); it != data.end(); it++)
          if (function->find_owner(it->first, domain) != owner_shard->shard_id)
            REPORT_LEGION_ERROR(ERROR_FUTURE_MAP_COUNT_MISMATCH,
                "Sharding function does not match described sharding for "
                "future map construction in %s (UID %lld)",
                get_task_name(), get_unique_id())
      }
      else
      {
        if (data.size() != domain_node->get_volume())
          REPORT_LEGION_ERROR(ERROR_FUTURE_MAP_COUNT_MISMATCH,
            "The number of buffers passed into a future map construction (%zd) "
            "does not match the volume of the domain (%zd) for the future map "
            "in task %s (UID %lld)", data.size(), domain_node->get_volume(),
            get_task_name(), get_unique_id())
        const DistributedID did = runtime->get_available_distributed_id();
        result = FutureMap(new FutureMapImpl(this, runtime, domain_node, did,
              total_children_count++, ApEvent::NO_AP_EVENT, provenance));
      }
      for (std::map<DomainPoint,UntypedBuffer>::const_iterator it =
            data.begin(); it != data.end(); it++)
      {
        if (!domain.contains(it->first))
          REPORT_LEGION_ERROR(ERROR_FUTURE_MAP_COUNT_MISMATCH,
            "Point passed into future map construction is not contained "
            "within the bounds of the domain in task %s (UID %lld)",
            get_task_name(), get_unique_id())
        const size_t future_size = it->second.get_size();
        FutureImpl *future = new FutureImpl(this, runtime, true/*register*/,
            runtime->get_available_distributed_id(), provenance);
        future->set_local(it->second.get_ptr(), future_size);
        result.impl->set_future(it->first, future);
      }
      return result;
    }

    //--------------------------------------------------------------------------
    FutureMap ReplicateContext::construct_future_map(IndexSpace space,
                                const std::map<DomainPoint,Future> &futures,
                                Provenance *provenance, bool internal,
                                bool collective, ShardingID sid, bool implicit,
                                bool check_space)
    //--------------------------------------------------------------------------
    {
      if (!internal)
      {
        AutoRuntimeCall call(this);
        for (int i = 0; runtime->safe_control_replication && (i < 2) &&
              ((current_trace == NULL) || !current_trace->is_fixed()); i++)
        {
          Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                                i > 0, provenance);
          hasher.hash(REPLICATE_CONSTRUCT_FUTURE_MAP, __func__);
          if (check_space)
            hasher.hash(space, "space");
          if (!collective)
          {
            for (std::map<DomainPoint,Future>::const_iterator it =
                  futures.begin(); it != futures.end(); it++)
            {
              hasher.hash(it->first, "futures");
              hash_future(hasher, runtime->safe_control_replication,
                          it->second, "futures");
            }
          }
          else if (!implicit)
            hasher.hash(sid, "sid");
          if (hasher.verify(__func__))
            break;
        }
        return construct_future_map(space, futures, provenance,
            true/*internal*/, collective, sid, implicit, check_space);
      }
      IndexSpaceNode *domain_node = runtime->forest->get_node(space);
      CreationOp *creation_op = runtime->get_available_creation_op();
      creation_op->initialize_map(this, provenance, futures);
      FutureMap result;
      if (collective)
      {
        const DistributedID map_did = get_next_distributed_id();
        result = shard_manager->deduplicate_future_map_creation(
            this, creation_op, domain_node, domain_node, map_did, provenance);
        ReplFutureMapImpl *map = static_cast<ReplFutureMapImpl*>(result.impl);
        ShardingFunction *function;
        if (implicit)
        {
          // Do an exchange between the shards to compute the implicit sharding
          // No need to wait for it to be done before continuing
          ImplicitShardingFunctor *functor = new
            ImplicitShardingFunctor(this, COLLECTIVE_LOC_102, map);
          functor->compute_sharding(futures);
          function = new ShardingFunction(functor, runtime->forest,
              shard_manager, sid, false, true/*own functor*/);
          if (!map->set_sharding_function(function, true/*own*/))
          {
            // Wait for the collective to be done before we delete it
            functor->perform_collective_wait();
            delete function;
          }
        }
        else
        {
          function = shard_manager->find_sharding_function(sid);
          map->set_sharding_function(function, false/*own*/);
        }
        // Check that all the points abide by the sharding function
        Domain domain;
        domain_node->get_domain(domain);
        for (std::map<DomainPoint,Future>::const_iterator it =
              futures.begin(); it != futures.end(); it++)
          if (function->find_owner(it->first, domain) != owner_shard->shard_id)
            REPORT_LEGION_ERROR(ERROR_FUTURE_MAP_COUNT_MISMATCH,
                "Sharding function does not match described sharding for "
                "future map construction in %s (UID %lld)",
                get_task_name(), get_unique_id())
      }
      else
      {
        if (futures.size() != domain_node->get_volume())
          REPORT_LEGION_ERROR(ERROR_FUTURE_MAP_COUNT_MISMATCH,
            "The number of futures passed into a future map construction (%zd) "
            "does not match the volume of the domain (%zd) for the future map "
            "in task %s (UID %lld)", futures.size(), domain_node->get_volume(),
            get_task_name(), get_unique_id())
        const DistributedID did = runtime->get_available_distributed_id();
        result = FutureMap(new FutureMapImpl(this, creation_op, domain_node,
                          runtime, did, provenance));
      }
      add_to_dependence_queue(creation_op);
      result.impl->set_all_futures(futures);
      return result;
    }

    //--------------------------------------------------------------------------
    PhysicalRegion ReplicateContext::map_region(const InlineLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_MAP_REGION, __func__);
        Serializer rez;
        ExternalMappable::pack_region_requirement(launcher.requirement, rez);
        hasher.hash(rez.get_buffer(), rez.get_used_bytes(), "requirement");
        hash_grants(hasher, launcher.grants);
        hash_phase_barriers(hasher, launcher.wait_barriers);
        hash_phase_barriers(hasher, launcher.arrive_barriers);
        hasher.hash(launcher.map_id, "map_id");
        hasher.hash(launcher.tag, "tag");
        hash_argument(hasher,
            runtime->safe_control_replication, launcher.map_arg, "map_arg");
        hasher.hash(launcher.layout_constraint_id, "layout_constraints");
        hash_static_dependences(hasher, launcher.static_dependences);
        if (hasher.verify(__func__))
          break;
      }
      if (IS_NO_ACCESS(launcher.requirement))
        return PhysicalRegion();
      ReplMapOp *map_op = runtime->get_available_repl_map_op();
      PhysicalRegion result = map_op->initialize(this, launcher, provenance);
#ifdef DEBUG_LEGION
      log_run.debug("Registering a map operation for region "
                    "(%x,%x,%x) in task %s (ID %lld)",
                    launcher.requirement.region.index_space.id, 
                    launcher.requirement.region.field_space.id, 
                    launcher.requirement.region.tree_id, 
                    get_task_name(), get_unique_id());
#endif
      map_op->initialize_replication(this); 
      if (current_trace != NULL)
        REPORT_LEGION_ERROR(ERROR_ATTEMPTED_INLINE_MAPPING_REGION,
                      "Attempted an inline mapping of region "
                      "(%x,%x,%x) inside of trace %d of parent task %s "
                      "(ID %lld). It is illegal to perform inline mapping "
                      "operations inside of traces.",
                      launcher.requirement.region.index_space.id, 
                      launcher.requirement.region.field_space.id, 
                      launcher.requirement.region.tree_id, 
                      current_trace->tid, get_task_name(), get_unique_id())
      bool parent_conflict = false, inline_conflict = false;  
      const int index = 
        has_conflicting_regions(map_op, parent_conflict, inline_conflict);
      if (parent_conflict)
        REPORT_LEGION_ERROR(ERROR_ATTEMPTED_INLINE_MAPPING_REGION,
          "Attempted an inline mapping of region "
                      "(%x,%x,%x) that conflicts with mapped region " 
                      "(%x,%x,%x) at index %d of parent task %s "
                      "(ID %lld) that would ultimately result in "
                      "deadlock. Instead you receive this error message.",
                      launcher.requirement.region.index_space.id,
                      launcher.requirement.region.field_space.id,
                      launcher.requirement.region.tree_id,
                      regions[index].region.index_space.id,
                      regions[index].region.field_space.id,
                      regions[index].region.tree_id,
                      index, get_task_name(), get_unique_id())
      if (inline_conflict)
        REPORT_LEGION_ERROR(ERROR_ATTEMPTED_INLINE_MAPPING_REGION,
          "Attempted an inline mapping of region (%x,%x,%x) "
                      "that conflicts with previous inline mapping in "
                      "task %s (ID %lld) that would ultimately result in "
                      "deadlock.  Instead you receive this error message.",
                      launcher.requirement.region.index_space.id,
                      launcher.requirement.region.field_space.id,
                      launcher.requirement.region.tree_id,
                      get_task_name(), get_unique_id())
      register_inline_mapped_region(result);
      add_to_dependence_queue(map_op);
      return result;
    }

    //--------------------------------------------------------------------------
    ApEvent ReplicateContext::remap_region(const PhysicalRegion &region,
                                           Provenance *provenance,bool internal)
    //--------------------------------------------------------------------------
    {
      if (!internal)
      {
        AutoRuntimeCall call(this);
        for (int i = 0; runtime->safe_control_replication && (i < 2) &&
              ((current_trace == NULL) || !current_trace->is_fixed()); i++)
        {
          Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                                i > 0, provenance);
          hasher.hash(REPLICATE_REMAP_REGION, __func__);
          Serializer rez;
          ExternalMappable::pack_region_requirement(
              region.impl->get_requirement(), rez);
          hasher.hash(rez.get_buffer(), rez.get_used_bytes(), "requirement");
          hasher.hash<bool>(region.is_mapped(), "is_mapped");
          if (hasher.verify(__func__))
            break;
        }
        return remap_region(region, provenance, true/*internal*/);
      }
      // Check to see if the region is already mapped,
      // if it is then we are done
      if (region.is_mapped())
        return ApEvent::NO_AP_EVENT;
      if (current_trace != NULL)
      {
        const RegionRequirement &req = region.impl->get_requirement();
        REPORT_LEGION_ERROR(ERROR_ATTEMPTED_INLINE_MAPPING_REGION,
                      "Attempted an inline mapping of region "
                      "(%x,%x,%x) inside of trace %d of parent task %s "
                      "(ID %lld). It is illegal to perform inline mapping "
                      "operations inside of traces.", req.region.index_space.id,
                      req.region.field_space.id, req.region.tree_id, 
                      current_trace->tid, get_task_name(), get_unique_id())
      }
      ReplMapOp *map_op = runtime->get_available_repl_map_op();
      map_op->initialize(this, region, provenance);
      map_op->initialize_replication(this);
      register_inline_mapped_region(region);
      const ApEvent result = map_op->get_completion_event();
      add_to_dependence_queue(map_op);
      return result;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::fill_fields(const FillLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_FILL_FIELDS, __func__);
        hasher.hash(launcher.handle, "handle");
        hasher.hash(launcher.parent, "parent");
        hash_argument(hasher, runtime->safe_control_replication, 
                      launcher.argument, "argument");
        hash_future(hasher, runtime->safe_control_replication, 
                    launcher.future, "future");
        hash_predicate(hasher, launcher.predicate, "predicate");
        for (std::set<FieldID>::const_iterator it = 
              launcher.fields.begin(); it != launcher.fields.end(); it++)
          hasher.hash(*it, "fields");
        hash_grants(hasher, launcher.grants);
        hash_phase_barriers(hasher, launcher.wait_barriers);
        hash_phase_barriers(hasher, launcher.arrive_barriers);
        hasher.hash(launcher.map_id, "map_id");
        hasher.hash(launcher.tag, "tag");
        hash_argument(hasher,
            runtime->safe_control_replication, launcher.map_arg, "map_arg");
        for (int idx = 0; idx < launcher.point.get_dim(); idx++)
          hasher.hash(launcher.point[idx], "point");
        hasher.hash(launcher.sharding_space, "sharding_space");
        hash_static_dependences(hasher, launcher.static_dependences);
        hasher.hash(launcher.silence_warnings, "silence_warnings");
        if (hasher.verify(__func__))
          break;
      }
      if (launcher.fields.empty())
      {
        REPORT_LEGION_WARNING(LEGION_WARNING_EMPTY_FILL_FIELDS,
            "Ignoring fill request with no fields in task %s (UID %lld)",
            get_task_name(), get_unique_id())
        return;
      }
      ReplFillOp *fill_op = runtime->get_available_repl_fill_op();
      fill_op->initialize(this, launcher, provenance);
#ifdef DEBUG_LEGION
      log_run.debug("Registering a fill operation in task %s (ID %lld)",
                     get_task_name(), get_unique_id());
#endif
      fill_op->initialize_replication(this, get_next_distributed_id(),
          shard_manager->is_first_local_shard(owner_shard));
      // Check to see if we need to do any unmappings and remappings
      // before we can issue this copy operation
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(fill_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "WARNING: Runtime is unmapping and remapping "
              "physical regions around fill_fields call in task %s (UID %lld).",
              get_task_name(), get_unique_id());
        }
        // Unmap any regions which are conflicting
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(fill_op);
      // Remap any regions which we unmapped
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::fill_fields(const IndexFillLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_FILL_FIELDS, __func__);
        hasher.hash(launcher.launch_domain, "launch_domain");
        hasher.hash(launcher.launch_space, "launch_space");
        hasher.hash(launcher.sharding_space, "sharding_space");
        hasher.hash(launcher.region, "region");
        hasher.hash(launcher.partition, "partition");
        hasher.hash(launcher.parent, "parent");
        hasher.hash(launcher.projection, "projection");
        hash_argument(hasher, runtime->safe_control_replication, 
                      launcher.argument, "argument");
        hash_future(hasher, runtime->safe_control_replication,
                    launcher.future, "future");
        hash_predicate(hasher, launcher.predicate, "predicate");
        for (std::set<FieldID>::const_iterator it = 
              launcher.fields.begin(); it != launcher.fields.end(); it++)
          hasher.hash(*it, "fields");
        hash_grants(hasher, launcher.grants);
        hash_phase_barriers(hasher, launcher.wait_barriers);
        hash_phase_barriers(hasher, launcher.arrive_barriers);
        hasher.hash(launcher.map_id, "map_id");
        hasher.hash(launcher.tag, "tag");
        hash_argument(hasher,
            runtime->safe_control_replication, launcher.map_arg, "map_arg");
        hash_static_dependences(hasher, launcher.static_dependences);
        hasher.hash(launcher.silence_warnings, "silence_warnings");
        if (hasher.verify(__func__))
          break;
      }
      if (launcher.fields.empty())
      {
        REPORT_LEGION_WARNING(LEGION_WARNING_EMPTY_FILL_FIELDS,
            "Ignoring index fill request with no fields in task %s (UID %lld)",
            get_task_name(), get_unique_id())
        return;
      }
      if (launcher.launch_domain.exists() && 
          (launcher.launch_domain.get_volume() == 0))
      {
        log_run.warning("Ignoring empty index space fill in task %s (ID %lld)",
                        get_task_name(), get_unique_id());
        return;
      }
      IndexSpace launch_space = launcher.launch_space;
      if (!launch_space.exists())
        launch_space = find_index_launch_space(launcher.launch_domain,
                                               provenance);
      ReplIndexFillOp *fill_op = 
        runtime->get_available_repl_index_fill_op();
      fill_op->initialize(this, launcher, launch_space, provenance); 
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_run.debug("Registering an index fill operation in task %s "
                      "(ID %lld)", get_task_name(), get_unique_id());
      fill_op->set_sharding_collective(new ShardingGatherCollective(this, 
                                       0/*owner shard*/, COLLECTIVE_LOC_46));
#endif
      fill_op->initialize_replication(this, get_next_distributed_id());
      // Check to see if we need to do any unmappings and remappings
      // before we can issue this copy operation
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(fill_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
          log_run.warning("WARNING: Runtime is unmapping and remapping "
              "physical regions around fill_fields call in task %s (UID %lld).",
              get_task_name(), get_unique_id());
        // Unmap any regions which are conflicting
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(fill_op);
      // Remap any regions which we unmapped
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::discard_fields(const DiscardLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_DISCARD_FIELDS, __func__);
        hasher.hash(launcher.handle, "handle");
        hasher.hash(launcher.parent, "parent");
        for (std::set<FieldID>::const_iterator it = 
              launcher.fields.begin(); it != launcher.fields.end(); it++)
          hasher.hash(*it, "fields");
        hash_static_dependences(hasher, launcher.static_dependences);
        hasher.hash(launcher.silence_warnings, "silence_warnings");
        if (hasher.verify(__func__))
          break;
      }
      if (launcher.fields.empty())
      {
        REPORT_LEGION_WARNING(LEGION_WARNING_EMPTY_FILL_FIELDS,
            "Ignoring discard request with no fields in task %s (UID %lld)",
            get_task_name(), get_unique_id())
        return;
      }
      ReplDiscardOp *discard_op = runtime->get_available_repl_discard_op();
      discard_op->initialize(this, launcher, provenance);
      discard_op->initialize_replication(this,
          shard_manager->is_first_local_shard(owner_shard));
      // We still unamp conflicting regions for discard, but we wil never
      // remap them afterwards since this is invalidating the data
      if (!runtime->unsafe_launch)
      {
        std::vector<PhysicalRegion> unmapped_regions;
        find_conflicting_regions(discard_op, unmapped_regions);
        if (!unmapped_regions.empty())
        {
          if (runtime->runtime_warnings && !launcher.silence_warnings)
          {
            REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
              "Runtime is unmapping and remapping "
                "physical regions around discard_fields call in "
                "task %s (UID %lld).", get_task_name(), get_unique_id());
          }
          // Unmap any regions which are conflicting
          for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
            unmapped_regions[idx].impl->unmap_region();
        }
      }
      add_to_dependence_queue(discard_op);
      // Do not remap the previously mapped regions, they are uninitialized
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::issue_copy(const CopyLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_ISSUE_COPY, __func__);
        hash_region_requirements(hasher, launcher.src_requirements);
        hash_region_requirements(hasher, launcher.dst_requirements);
        hash_region_requirements(hasher, launcher.src_indirect_requirements);
        hash_region_requirements(hasher, launcher.dst_indirect_requirements);
        for (std::vector<bool>::const_iterator it = 
              launcher.src_indirect_is_range.begin(); it != 
              launcher.src_indirect_is_range.end(); it++)
          hasher.hash<bool>(*it, "src_indirect_is_range");
        for (std::vector<bool>::const_iterator it = 
              launcher.dst_indirect_is_range.begin(); it != 
              launcher.dst_indirect_is_range.end(); it++)
          hasher.hash<bool>(*it, "dst_indirect_is_range");
        hash_grants(hasher, launcher.grants);
        hash_phase_barriers(hasher, launcher.wait_barriers);
        hash_phase_barriers(hasher, launcher.arrive_barriers);
        hash_predicate(hasher, launcher.predicate, "predicate");
        hasher.hash(launcher.map_id, "map_id");
        hasher.hash(launcher.tag, "tag");
        hash_argument(hasher, 
            runtime->safe_control_replication, launcher.map_arg, "map_arg");
        for (int idx = 0; idx < launcher.point.get_dim(); idx++)
          hasher.hash(launcher.point[idx], "point");
        hasher.hash(launcher.sharding_space, "sharding_space");
        hash_static_dependences(hasher, launcher.static_dependences);
        hasher.hash(launcher.possible_src_indirect_out_of_range,
                    "possible_src_indirect_out_of_range");
        hasher.hash(launcher.possible_dst_indirect_out_of_range,
                    "possible_dst_indirect_out_of_range");
        hasher.hash(launcher.possible_dst_indirect_aliasing,
                    "possible_dst_indirect_aliasing");
        hasher.hash(launcher.silence_warnings, "silence_warnings");
        if (hasher.verify(__func__))
          break;
      }
      ReplCopyOp *copy_op = runtime->get_available_repl_copy_op();
      copy_op->initialize(this, launcher, provenance);
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_run.debug("Registering a copy operation in task %s (ID %lld)",
                      get_task_name(), get_unique_id());
      copy_op->set_sharding_collective(new ShardingGatherCollective(this, 
                                       0/*owner shard*/, COLLECTIVE_LOC_47));
#endif
      copy_op->initialize_replication(this);
      // Check to see if we need to do any unmappings and remappings
      // before we can issue this copy operation
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(copy_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
          log_run.warning("WARNING: Runtime is unmapping and remapping "
              "physical regions around issue_copy_operation call in "
              "task %s (UID %lld).", get_task_name(), get_unique_id());
        // Unmap any regions which are conflicting
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(copy_op);
      // Remap any regions which we unmapped
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }
    
    //--------------------------------------------------------------------------
    void ReplicateContext::issue_copy(const IndexCopyLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_ISSUE_COPY, __func__);
        hash_region_requirements(hasher, launcher.src_requirements);
        hash_region_requirements(hasher, launcher.dst_requirements);
        hash_region_requirements(hasher, launcher.src_indirect_requirements);
        hash_region_requirements(hasher, launcher.dst_indirect_requirements);
        for (std::vector<bool>::const_iterator it = 
              launcher.src_indirect_is_range.begin(); it != 
              launcher.src_indirect_is_range.end(); it++)
          hasher.hash<bool>(*it, "src_indirect_is_range");
        for (std::vector<bool>::const_iterator it = 
              launcher.dst_indirect_is_range.begin(); it != 
              launcher.dst_indirect_is_range.end(); it++)
          hasher.hash<bool>(*it, "dst_indirect_is_range");
        hash_grants(hasher, launcher.grants);
        hash_phase_barriers(hasher, launcher.wait_barriers);
        hash_phase_barriers(hasher, launcher.arrive_barriers);
        hash_predicate(hasher, launcher.predicate, "predicate");
        hasher.hash(launcher.map_id, "map_id");
        hasher.hash(launcher.tag, "tag");
        hash_argument(hasher,
            runtime->safe_control_replication, launcher.map_arg, "map_arg");
        hasher.hash(launcher.launch_domain, "launch_domain");
        hasher.hash(launcher.launch_space, "launch_space");
        hasher.hash(launcher.sharding_space, "sharding_space");
        hash_static_dependences(hasher, launcher.static_dependences);
        hasher.hash(launcher.possible_src_indirect_out_of_range,
                    "possible_src_indirect_out_of_range");
        hasher.hash(launcher.possible_dst_indirect_out_of_range,
                    "possible_dst_indirect_out_of_range");
        hasher.hash(launcher.possible_dst_indirect_aliasing,
                    "possible_dst_indirect_aliasing");
        hasher.hash(launcher.collective_src_indirect_points,
                    "collective_src_indirect_points");
        hasher.hash(launcher.collective_dst_indirect_points,
                    "collective_dst_indirect_points");
        hasher.hash(launcher.silence_warnings, "silence_warnings");
        if (hasher.verify(__func__))
          break;
      }
      if (launcher.launch_domain.exists() &&
          (launcher.launch_domain.get_volume() == 0))
      {
        log_run.warning("Ignoring empty index space copy in task %s "
                        "(ID %lld)", get_task_name(), get_unique_id());
        return;
      }
      IndexSpace launch_space = launcher.launch_space;
      if (!launch_space.exists())
        launch_space = find_index_launch_space(launcher.launch_domain,
                                               provenance);
      ReplIndexCopyOp *copy_op = 
        runtime->get_available_repl_index_copy_op();
      copy_op->initialize(this, launcher, launch_space, provenance);
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_run.debug("Registering an index copy operation in task %s "
                      "(ID %lld)", get_task_name(), get_unique_id());
      copy_op->set_sharding_collective(new ShardingGatherCollective(this, 
                                       0/*owner shard*/, COLLECTIVE_LOC_48));
#endif
      copy_op->initialize_replication(this);
      // Check to see if we need to do any unmappings and remappings
      // before we can issue this copy operation
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(copy_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
          log_run.warning("WARNING: Runtime is unmapping and remapping "
              "physical regions around issue_copy_operation call in "
              "task %s (UID %lld).", get_task_name(), get_unique_id());
        // Unmap any regions which are conflicting
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the copy operation
      add_to_dependence_queue(copy_op);
      // Remap any regions which we unmapped
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::issue_acquire(const AcquireLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_ACQUIRE, __func__);
        hasher.hash(launcher.logical_region, "logical_region");
        hasher.hash(launcher.parent_region, "parent_region");
        for (std::set<FieldID>::const_iterator it =
              launcher.fields.begin(); it != launcher.fields.end(); it++)
          hasher.hash(*it, "fields");
        hash_grants(hasher, launcher.grants);
        hash_phase_barriers(hasher, launcher.wait_barriers);
        hash_phase_barriers(hasher, launcher.arrive_barriers);
        hasher.hash(launcher.map_id, "map_id");
        hasher.hash(launcher.tag, "tag");
        hash_argument(hasher,
            runtime->safe_control_replication, launcher.map_arg, "map_arg");
        if (launcher.physical_region.impl != NULL)
        {
          Serializer rez;
          ExternalMappable::pack_region_requirement(
              launcher.physical_region.impl->get_requirement(), rez);
          hasher.hash(rez.get_buffer(), rez.get_used_bytes(),"physical_region");
          hasher.hash<bool>(launcher.physical_region.is_mapped(), "is_mapped");
        }
        if (hasher.verify(__func__))
          break;
      }
      AutoProvenance provenance(launcher.provenance);
      ReplAcquireOp *acquire_op = runtime->get_available_repl_acquire_op();
      acquire_op->initialize(this, launcher, provenance);
#ifdef DEBUG_LEGION
      log_run.debug("Issuing an acquire operation in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      acquire_op->initialize_replication(this,
          shard_manager->is_first_local_shard(owner_shard));
      // Check to see if we need to do any unmappings and remappings
      // before we can issue this acquire operation.
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(acquire_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around issue_acquire call in "
              "task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the acquire operation
      add_to_dependence_queue(acquire_op);
      // Remap any regions which we unmapped
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::issue_release(const ReleaseLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_RELEASE, __func__);
        hasher.hash(launcher.logical_region, "logical_region");
        hasher.hash(launcher.parent_region, "parent_region");
        for (std::set<FieldID>::const_iterator it =
              launcher.fields.begin(); it != launcher.fields.end(); it++)
          hasher.hash(*it, "fields");
        hash_grants(hasher, launcher.grants);
        hash_phase_barriers(hasher, launcher.wait_barriers);
        hash_phase_barriers(hasher, launcher.arrive_barriers);
        hasher.hash(launcher.map_id, "map_id");
        hasher.hash(launcher.tag, "tag");
        hash_argument(hasher,
            runtime->safe_control_replication, launcher.map_arg, "map_arg");
        if (launcher.physical_region.impl != NULL)
        {
          Serializer rez;
          ExternalMappable::pack_region_requirement(
              launcher.physical_region.impl->get_requirement(), rez);
          hasher.hash(rez.get_buffer(), rez.get_used_bytes(),"physical_region");
          hasher.hash<bool>(launcher.physical_region.is_mapped(), "is_mappped");
        }
        if (hasher.verify(__func__))
          break;
      }
      AutoProvenance provenance(launcher.provenance);
      ReplReleaseOp *release_op = runtime->get_available_repl_release_op();
      release_op->initialize(this, launcher, provenance);
#ifdef DEBUG_LEGION
      log_run.debug("Issuing a release operation in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      release_op->initialize_replication(this,
          shard_manager->is_first_local_shard(owner_shard));
      // Check to see if we need to do any unmappings and remappings
      // before we can issue the release operation
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        find_conflicting_regions(release_op, unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
        {
          REPORT_LEGION_WARNING(LEGION_WARNING_RUNTIME_UNMAPPING_REMAPPING,
            "Runtime is unmapping and remapping "
              "physical regions around issue_release call in "
              "task %s (UID %lld).", get_task_name(), get_unique_id());
        }
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Issue the release operation
      add_to_dependence_queue(release_op);
      // Remap any regions which we unmapped
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
    }

    //--------------------------------------------------------------------------
    PhysicalRegion ReplicateContext::attach_resource(
                                                 const AttachLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_ATTACH_RESOURCE, __func__);
        hasher.hash(launcher.resource, "resource");
        hasher.hash(launcher.handle, "handle");
        hasher.hash(launcher.parent, "parent");
        hasher.hash(launcher.restricted, "restricted");
        hasher.hash(launcher.mapped, "mapped");
        hasher.hash(launcher.collective, "collective");
        hasher.hash(launcher.deduplicate_across_shards,
                    "deduplicate_across_shards");
        for (std::map<FieldID,const char*>::const_iterator it = 
              launcher.field_files.begin(); it != 
              launcher.field_files.end(); it++)
        {
          hasher.hash(it->first, "field_files");
          hasher.hash(it->second, strlen(it->second), "field_files");
        }
        hash_layout_constraints(hasher, launcher.constraints,false/*pointers*/);
        for (std::set<FieldID>::const_iterator it = 
              launcher.privilege_fields.begin(); it !=
              launcher.privilege_fields.end(); it++)
          hasher.hash(*it, "privilege_fields");
        hasher.hash(launcher.footprint, "footprint");
        hash_static_dependences(hasher, launcher.static_dependences);
        if (hasher.verify(__func__))
          break;
      }
      if (launcher.restricted)
        REPORT_LEGION_ERROR(ERROR_REPLICATE_TASK_VIOLATION,
            "Attach operations in control replication context %s (UID %lld) "
            "requested a restriction. Restrictions are only permitted for "
            "attach operations in non-control-replicated contexts currently.",
            get_task_name(), get_unique_id());
      ReplAttachOp *attach_op = runtime->get_available_repl_attach_op();
      PhysicalRegion result = attach_op->initialize(this, launcher, provenance);
      attach_op->initialize_replication(this, launcher.collective, 
          launcher.deduplicate_across_shards,
          shard_manager->is_first_local_shard(owner_shard));
      bool parent_conflict = false, inline_conflict = false;
      int index = has_conflicting_regions(attach_op, 
                                          parent_conflict, inline_conflict);
      if (parent_conflict)
        REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
          "Attempted an attach hdf5 file operation on region "
                      "(%x,%x,%x) that conflicts with mapped region " 
                      "(%x,%x,%x) at index %d of parent task %s (ID %lld) "
                      "that would ultimately result in deadlock. Instead you "
                      "receive this error message. Try unmapping the region "
                      "before invoking attach_external_resource.",
                      launcher.handle.index_space.id, 
                      launcher.handle.field_space.id, 
                      launcher.handle.tree_id, 
                      regions[index].region.index_space.id,
                      regions[index].region.field_space.id,
                      regions[index].region.tree_id, index, 
                      get_task_name(), get_unique_id())
      if (inline_conflict)
        REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
          "Attempted an attach hdf5 file operation on region "
                      "(%x,%x,%x) that conflicts with previous inline "
                      "mapping in task %s (ID %lld) "
                      "that would ultimately result in deadlock. Instead you "
                      "receive this error message. Try unmapping the region "
                      "before invoking attach_external_resource.",
                      launcher.handle.index_space.id, 
                      launcher.handle.field_space.id, 
                      launcher.handle.tree_id, get_task_name(), 
                      get_unique_id())
      // If we're counting this region as mapped we need to register it
      if (launcher.mapped)
        register_inline_mapped_region(result);
      add_to_dependence_queue(attach_op);
      return result;
    }

    //--------------------------------------------------------------------------
    ExternalResources ReplicateContext::attach_resources(
                                            const IndexAttachLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_INDEX_ATTACH_RESOURCE, __func__);
        hasher.hash(launcher.resource, "resource");
        hasher.hash(launcher.parent, "parent");
        hasher.hash(launcher.restricted, "restricted");
        hasher.hash(launcher.deduplicate_across_shards,
                    "deduplicate_across_shards");
        hash_layout_constraints(hasher, launcher.constraints,false/*pointers*/);
        // Everything else other than the privilege fields is sharded already
        // Make sure we include privilege fields from the files too
        // Effectively the direct privilege fields or privilege fields 
        // mentioned by any of the other data structures need to be the same
        std::set<FieldID> all_privilege_fields(launcher.privilege_fields);
        for (std::map<FieldID,std::vector<const char*> >::const_iterator it =
              launcher.field_files.begin(); it != 
              launcher.field_files.end(); it++)
          all_privilege_fields.insert(it->first);
        for (std::set<FieldID>::const_iterator it = 
              all_privilege_fields.begin(); it !=
              all_privilege_fields.end(); it++)
          hasher.hash(*it, "privilege_fields");
        hash_static_dependences(hasher, launcher.static_dependences);
        if (hasher.verify(__func__))
          break;
      }
      std::vector<unsigned> indexes;
      if (!launcher.deduplicate_across_shards)
      {
        indexes.resize(launcher.handles.size());
        for (unsigned idx = 0; idx < indexes.size(); idx++)
          indexes[idx] = idx;
      }
      else // ask the shard manager to deduplicate here
        shard_manager->deduplicate_attaches(launcher, indexes);
      // Start this inflight before we compute the upper bound
      IndexAttachLaunchSpace collective(this, COLLECTIVE_LOC_28);
      collective.exchange_counts(indexes.size());
      // Compute the upper bound partition node from this launcher
      RegionTreeNode *node = compute_index_attach_upper_bound(launcher,indexes);
      ReplIndexAttachOp *attach_op = 
        runtime->get_available_repl_index_attach_op();
      IndexSpaceNode *launch_space = collective.get_launch_space(provenance);
      ExternalResources result = attach_op->initialize(this, node, launch_space,
                             launcher, indexes, provenance, true/*replicated*/);
      attach_op->initialize_replication(this); 
      const RegionRequirement &req = attach_op->get_requirement();
      bool parent_conflict = false, inline_conflict = false;
      int index = has_conflicting_internal(req,parent_conflict,inline_conflict);
      if (parent_conflict)
      {
        if (req.handle_type == LEGION_PARTITION_PROJECTION)
          REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
                        "Attempted an index attach operation with upper bound "
                        "partition (%x,%x,%x) that conflicts with mapped region"
                        " (%x,%x,%x) at index %d of parent task %s (ID %lld) "
                        "that would ultimately result in deadlock. Instead you "
                        "receive this error message. Try unmapping the region "
                        "before invoking 'attach_external_resources'.",
                        req.partition.index_partition.id, 
                        req.partition.field_space.id, 
                        req.partition.tree_id, 
                        regions[index].region.index_space.id,
                        regions[index].region.field_space.id,
                        regions[index].region.tree_id, index, 
                        get_task_name(), get_unique_id())
        else
          REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
                        "Attempted an index attach operation with upper bound "
                        "region (%x,%x,%x) that conflicts with mapped region "
                        "(%x,%x,%x) at index %d of parent task %s (ID %lld) "
                        "that would ultimately result in deadlock. Instead you "
                        "receive this error message. Try unmapping the region "
                        "before invoking 'attach_external_resources'.",
                        req.region.index_space.id, 
                        req.region.field_space.id, 
                        req.region.tree_id, 
                        regions[index].region.index_space.id,
                        regions[index].region.field_space.id,
                        regions[index].region.tree_id, index, 
                        get_task_name(), get_unique_id())
      }
      if (inline_conflict)
      {
        if (req.handle_type == LEGION_PARTITION_PROJECTION)
          REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
                        "Attempted an index attach operation with upper bound "
                        "partition (%x,%x,%x) that conflicts with previous "
                        "inline mapping in task %s (ID %lld) "
                        "that would ultimately result in deadlock. Instead you "
                        "receive this error message. Try unmapping the region "
                        "before invoking 'attach_external_resources'.",
                        req.partition.index_partition.id, 
                        req.partition.field_space.id, req.partition.tree_id,
                        get_task_name(), get_unique_id())
        else
          REPORT_LEGION_ERROR(ERROR_ATTEMPTED_EXTERNAL_ATTACH,
                        "Attempted an index attach operation with upper bound "
                        "region (%x,%x,%x) that conflicts with previous inline "
                        "mapping in task %s (ID %lld) "
                        "that would ultimately result in deadlock. Instead you "
                        "receive this error message. Try unmapping the region "
                        "before invoking 'attach_external_resources'.",
                        req.region.index_space.id, 
                        req.region.field_space.id, req.region.tree_id,
                        get_task_name(), get_unique_id())
      }
      add_to_dependence_queue(attach_op);
      return result;
    }

    //--------------------------------------------------------------------------
    RegionTreeNode* ReplicateContext::compute_index_attach_upper_bound(
      const IndexAttachLauncher &launcher, const std::vector<unsigned> &indexes)
    //--------------------------------------------------------------------------
    {
      // Call the base version first if our indexes are not empty
      RegionTreeNode *result = indexes.empty() ? NULL :
        InnerContext::compute_index_attach_upper_bound(launcher, indexes);
      // Do the exchange between the shards
      IndexAttachUpperBound exchange(this, COLLECTIVE_LOC_26, runtime->forest);
      result = exchange.find_upper_bound(result);
      return result;
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::detach_resource(PhysicalRegion region, 
                 const bool flush, const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && !unordered && (i < 2)
            && ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_DETACH_RESOURCE, __func__);
        Serializer rez;
        if (region.impl != NULL)
          ExternalMappable::pack_region_requirement(
              region.impl->get_requirement(), rez);
        hasher.hash(rez.get_buffer(), rez.get_used_bytes(), "requirement");
        hasher.hash<bool>(region.is_mapped(), "is_mapped");
        hasher.hash<bool>(flush, "flush");
        if (hasher.verify(__func__))
          break;
      }
      ReplDetachOp *op = runtime->get_available_repl_detach_op();
      Future result = 
        op->initialize_detach(this, region, flush, unordered, provenance);
      op->initialize_replication(this, region.impl->collective, 
          shard_manager->is_first_local_shard(owner_shard));
      // If the region is still mapped, then unmap it
      if (region.is_mapped())
      {
        unregister_inline_mapped_region(region);
        region.impl->unmap_region();
      }
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered detach operation performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
      return result;
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::detach_resources(ExternalResources resources,
                 const bool flush, const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && !unordered && (i < 2)
            && ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_INDEX_DETACH_RESOURCE, __func__);
        if (resources.impl != NULL)
        {
          hasher.hash(resources.impl->parent, "parent");
          for (std::vector<FieldID>::const_iterator it =
                resources.impl->privilege_fields.begin(); it !=
                resources.impl->privilege_fields.end(); it++)
            hasher.hash(*it, "privilege_fields");
          if (resources.impl->upper_bound->is_region())
            hasher.hash(
                resources.impl->upper_bound->as_region_node()->handle,"region");
          else
            hasher.hash(
                resources.impl->upper_bound->as_partition_node()->handle,
                "partition");
        }
        hasher.hash<bool>(flush, "flush");
        if (hasher.verify(__func__))
          break;
      }
      if (resources.impl == NULL)
        return Future();
      ReplIndexDetachOp *op = runtime->get_available_repl_index_detach_op();
      Future result =
        resources.impl->detach(this, op, flush, unordered, provenance);
      op->initialize_replication(this);
      if (!add_to_dependence_queue(op, unordered))
      {
#ifdef DEBUG_LEGION
        assert(unordered);
#endif
        REPORT_LEGION_ERROR(ERROR_POST_EXECUTION_UNORDERED_OPERATION,
            "Illegal unordered index detach operation performed after task %s "
            "(UID %lld) has finished executing. All unordered operations must "
            "be performed before the end of the execution of the parent task.",
            get_task_name(), get_unique_id())
      }
      return result;
    }

    //--------------------------------------------------------------------------
    FutureMap ReplicateContext::execute_must_epoch(
                                              const MustEpochLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) && 
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_MUST_EPOCH, __func__);
        hasher.hash(launcher.map_id, "map_id");
        hasher.hash(launcher.mapping_tag, "mapping_tag");
        for (std::vector<TaskLauncher>::const_iterator it = 
              launcher.single_tasks.begin(); it != 
              launcher.single_tasks.end(); it++)
          hash_task_launcher(hasher, runtime->safe_control_replication, *it);
        for (std::vector<IndexTaskLauncher>::const_iterator it = 
              launcher.index_tasks.begin(); it !=
              launcher.index_tasks.end(); it++)
          hash_index_launcher(hasher, runtime->safe_control_replication, *it);
        hasher.hash(launcher.launch_domain, "launch_domain");
        hasher.hash(launcher.launch_space, "launch_space");
        hasher.hash(launcher.sharding_space, "sharding_space");
        hasher.hash(launcher.silence_warnings, "silence_warnings");
        if (hasher.verify(__func__))
          break;
      }
      ReplMustEpochOp *epoch_op = runtime->get_available_repl_epoch_op();
      FutureMap result = epoch_op->initialize(this, launcher, provenance);
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_run.debug("Executing a must epoch in task %s (ID %lld)",
                      get_task_name(), get_unique_id());
      epoch_op->set_sharding_collective(new ShardingGatherCollective(this, 
                                        0/*owner shard*/, COLLECTIVE_LOC_49));
#endif
      epoch_op->initialize_replication(this);
      // Now find all the parent task regions we need to invalidate
      std::vector<PhysicalRegion> unmapped_regions;
      if (!runtime->unsafe_launch)
        epoch_op->find_conflicted_regions(unmapped_regions);
      if (!unmapped_regions.empty())
      {
        if (runtime->runtime_warnings && !launcher.silence_warnings)
          log_run.warning("WARNING: Runtime is unmapping and remapping "
              "physical regions around issue_release call in "
              "task %s (UID %lld).", get_task_name(), get_unique_id());
        for (unsigned idx = 0; idx < unmapped_regions.size(); idx++)
          unmapped_regions[idx].impl->unmap_region();
      }
      // Now we can issue the must epoch
      add_to_dependence_queue(epoch_op);
      // Remap any unmapped regions
      if (!unmapped_regions.empty())
        remap_unmapped_regions(current_trace, unmapped_regions, provenance);
      return result;
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::issue_timing_measurement(
                                                 const TimingLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_TIMING_MEASUREMENT, __func__);
        hasher.hash(launcher.measurement, "measurement");
        for (std::set<Future>::const_iterator it = 
              launcher.preconditions.begin(); it !=
              launcher.preconditions.end(); it++)
          hash_future(hasher, runtime->safe_control_replication,
                      *it, "preconditions");
        if (hasher.verify(__func__))
          break;
      }
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_run.debug("Issuing a timing measurement in task %s (ID %lld)",
                      get_task_name(), get_unique_id());
#endif
      ReplTimingOp *timing_op = runtime->get_available_repl_timing_op();
      Future result = timing_op->initialize(this, launcher, provenance);
      ValueBroadcast<long long> *timing_collective = 
        new ValueBroadcast<long long>(this, 0/*shard 0 is always the owner*/,
                                      COLLECTIVE_LOC_35);
      timing_op->set_timing_collective(timing_collective);
      add_to_dependence_queue(timing_op);
      return result;
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::select_tunable_value(
                                                const TunableLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      AutoProvenance provenance(launcher.provenance);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_TUNABLE_SELECTION, __func__);
        hasher.hash(launcher.tunable, "tunable");
        hasher.hash(launcher.mapper, "mapper");
        hasher.hash(launcher.tag, "tag");
        hash_argument(hasher, runtime->safe_control_replication,
                      launcher.arg, "arg");
        for (std::vector<Future>::const_iterator it =
              launcher.futures.begin(); it != launcher.futures.end(); it++)
          hash_future(hasher, runtime->safe_control_replication, *it,"futures");
        if (hasher.verify(__func__))
          break;
      }
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_run.debug("Issuing a tunable request in task %s (ID %lld)",
                      get_task_name(), get_unique_id());
#endif
      ReplTunableOp *tunable_op = runtime->get_available_repl_tunable_op();
      Future result = tunable_op->initialize(this, launcher, provenance);
      tunable_op->initialize_replication(this);
      add_to_dependence_queue(tunable_op);
      return result;
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::issue_mapping_fence(Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) && 
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_MAPPING_FENCE, __func__);
        if (hasher.verify(__func__))
          break;
      }
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_run.debug("Issuing a mapping fence in task %s (ID %lld)",
                      get_task_name(), get_unique_id());
#endif
      ReplFenceOp *fence_op = runtime->get_available_repl_fence_op();
      Future result = 
        fence_op->initialize(this, FenceOp::MAPPING_FENCE, true, provenance);
      add_to_dependence_queue(fence_op);
      return result;
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::issue_execution_fence(Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_EXECUTION_FENCE, __func__);
        if (hasher.verify(__func__))
          break;
      }
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_run.debug("Issuing an execution fence in task %s (ID %lld)",
                      get_task_name(), get_unique_id());
#endif
      ReplFenceOp *fence_op = runtime->get_available_repl_fence_op();
      Future result = 
        fence_op->initialize(this, FenceOp::EXECUTION_FENCE, true, provenance);
      add_to_dependence_queue(fence_op);
      return result;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::begin_trace(TraceID tid, bool logical_only,
                        bool static_trace, const std::set<RegionTreeID> *trees,
                        bool deprecated, Provenance *provenance, bool from_application)
    //--------------------------------------------------------------------------
    {
      if (from_application) {
        AutoRuntimeCall call(this);
        this->begin_trace(tid, logical_only, static_trace, trees, deprecated, provenance, false /* from_application */);
        return;
      }
      for (int i = 0; runtime->safe_control_replication && (i < 2); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_BEGIN_TRACE, __func__);
        hasher.hash(tid, "tid");
        hasher.hash<bool>(logical_only, "logical_only");
        hasher.hash<bool>(static_trace, "static_trace");
        hasher.hash<bool>(deprecated, "deprecated");
        if (trees != NULL)
          for (std::set<RegionTreeID>::const_iterator it = 
                trees->begin(); it != trees->end(); it++)
            hasher.hash(*it, "trees");
        if (hasher.verify(__func__))
          break;
      }
      if (runtime->no_tracing) return;
      if (runtime->no_physical_tracing) logical_only = true;
#ifdef DEBUG_LEGION
      log_run.debug("Beginning a trace in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      // No need to hold the lock here, this is only ever called
      // by the one thread that is running the task.
      if (current_trace != NULL)
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_NESTED_TRACE,
          "Illegal nested trace with ID %d attempted in "
           "task %s (ID %lld)", tid, get_task_name(), get_unique_id())
      std::map<TraceID,LogicalTrace*>::const_iterator finder = traces.find(tid);
      LogicalTrace *trace = NULL;
      if (finder == traces.end())
      {
        // Trace does not exist yet, so make one and record it
        trace = new LogicalTrace(this, tid, logical_only, 
                                 static_trace, provenance, trees);
        if (!deprecated)
          traces[tid] = trace;
        trace->add_reference();
      }
      else
        trace = finder->second;

#ifdef DEBUG_LEGION
      assert(trace != NULL);
#endif
      trace->clear_blocking_call();

      // Issue a begin op
      ReplTraceBeginOp *begin = runtime->get_available_repl_begin_op();
      begin->initialize_begin(this, trace, provenance);
      add_to_dependence_queue(begin);

      if (!logical_only)
      {
        // Issue a replay op
        ReplTraceReplayOp *replay = runtime->get_available_repl_replay_op();
        replay->initialize_replay(this, trace, provenance);
        // Record the event for when the trace replay is ready
        physical_trace_replay_status.store(replay->get_mapped_event().id);
        add_to_dependence_queue(replay);
      }

      // Now mark that we are starting a trace
      current_trace = trace;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::end_trace(TraceID tid, bool deprecated,
                                     Provenance *provenance, bool from_application)
    //--------------------------------------------------------------------------
    {
      if (from_application) {
        AutoRuntimeCall call(this);
        this->end_trace(tid, deprecated, provenance, false /* from application */);
        return;
      }
      for (int i = 0; runtime->safe_control_replication && (i < 2); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,
                              i > 0, provenance);
        hasher.hash(REPLICATE_END_TRACE, __func__);
        hasher.hash(tid, "tid");
        hasher.hash<bool>(deprecated, "deprecated");
        if (hasher.verify(__func__))
          break;
      }
      if (runtime->no_tracing) return;
#ifdef DEBUG_LEGION
      log_run.debug("Ending a trace in task %s (ID %lld)",
                    get_task_name(), get_unique_id());
#endif
      if (current_trace == NULL)
        REPORT_LEGION_ERROR(ERROR_UMATCHED_END_TRACE,
          "Unmatched end trace for ID %d in task %s (ID %lld)", 
          tid, get_task_name(), get_unique_id())
      else if (!deprecated && (current_trace->tid != tid))
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_END_TRACE_CALL,
          "Illegal end trace call on trace ID %d that does not match "
          "the current trace ID %d in task %s (UID %lld)", tid,
          current_trace->tid, get_task_name(), get_unique_id())
      const bool has_blocking_call = current_trace->has_blocking_call();
      if (current_trace->is_fixed())
      {
        // Already fixed, dump a complete trace op into the stream
        ReplTraceCompleteOp *complete_op = 
          runtime->get_available_repl_trace_op();
        complete_op->initialize_complete(this, provenance, has_blocking_call);
        add_to_dependence_queue(complete_op);
      }
      else
      {
        // Not fixed yet, dump a capture trace op into the stream
        ReplTraceCaptureOp *capture_op = 
          runtime->get_available_repl_capture_op();
        capture_op->initialize_capture(this, provenance,
                                       has_blocking_call, deprecated);
        // Mark that the current trace is now fixed
        current_trace->fix_trace(provenance);
        add_to_dependence_queue(capture_op);
      }
      // We no longer have a trace that we're executing 
      current_trace = NULL;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::end_task(const void *res, size_t res_size,bool owned,
                                PhysicalInstance deferred_result_instance,
                                FutureFunctor *callback_functor,
                                const Realm::ExternalInstanceResource *resource,
              void (*freefunc)(const Realm::ExternalInstanceResource &resource),
                  const void *metadataptr, size_t metadatasize, ApEvent effects)
    //--------------------------------------------------------------------------
    {
      // We have an extra one of these here to handle the case where some
      // shards do an extra runtime call than other shards. This should
      // avoid that case hanging at least.
      for (int i = 0; runtime->safe_control_replication && (i < 2); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_END_TASK, __func__);
        hasher.hash(res_size, "res_size");
        hasher.hash(metadatasize, "metadatasize");
        if (hasher.verify(__func__))
          break;
      }
      InnerContext::end_task(res, res_size, owned, deferred_result_instance,
           callback_functor,resource,freefunc,metadataptr,metadatasize,effects);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::post_end_task(FutureInstance *instance, 
                                         void *metadata, size_t metasize,
                                         FutureFunctor *callback_functor,
                                         bool own_callback_functor)
    //--------------------------------------------------------------------------
    {
      // Pull any pending collectives here on the stack so we can delete them
      // after the end task call, even though this context might be reclaimed
      std::deque<std::pair<ValueBroadcast<ISBroadcast>*,bool> > 
                                            release_index_spaces;
      if (!pending_index_spaces.empty())
        release_index_spaces.swap(pending_index_spaces);
      std::deque<std::pair<ValueBroadcast<IPBroadcast>*,ShardID> >
                                            release_index_partitions;
      if (!pending_index_partitions.empty())
        release_index_partitions.swap(pending_index_partitions);
      std::deque<std::pair<ValueBroadcast<FSBroadcast>*,bool> >
                                            release_field_spaces;
      if (!pending_field_spaces.empty())
        release_field_spaces.swap(pending_field_spaces);
      std::deque<std::pair<ValueBroadcast<FIDBroadcast>*,bool> >
                                            release_fields;
      if (!pending_fields.empty())
        release_fields.swap(pending_fields);
      std::deque<std::pair<ValueBroadcast<LRBroadcast>*,bool> >
                                            release_region_trees;
      if (!pending_region_trees.empty())
        release_region_trees.swap(pending_region_trees);
      std::deque<std::pair<ValueBroadcast<DIDBroadcast>*,bool> >
                                            release_distributed_ids;
      if (!pending_distributed_ids.empty())
        release_distributed_ids.swap(pending_distributed_ids);
      // Grab this now before the context might be deleted
      const ShardID local_shard = owner_shard->shard_id;
      // Do the base call
      InnerContext::post_end_task(instance, metadata, metasize,
                                  callback_functor, own_callback_functor);
      // Then delete all the pending collectives that we had
      while (!release_index_spaces.empty())
      {
        std::pair<ValueBroadcast<ISBroadcast>*,bool> &collective = 
          release_index_spaces.front();
        if (collective.second)
        {
          const ISBroadcast value = collective.first->get_value(false);
          runtime->forest->revoke_pending_index_space(value.space_id);
        }
        else
        {
          // Make sure this collective is done before we delete it
          const RtEvent done = collective.first->get_done_event();
          if (!done.has_triggered())
            done.wait();
        }
        delete collective.first;
        release_index_spaces.pop_front();
      }
      while (!release_index_partitions.empty())
      {
        std::pair<ValueBroadcast<IPBroadcast>*,ShardID> &collective = 
          release_index_partitions.front();
        if (collective.second == local_shard)
        {
          const IPBroadcast value = collective.first->get_value(false);
          runtime->forest->revoke_pending_partition(value.pid);
        }
        else
        {
          // Make sure this collective is done before we delete it
          const RtEvent done = collective.first->get_done_event();
          if (!done.has_triggered())
            done.wait();
        }
        delete collective.first;
        release_index_partitions.pop_front();
      }
      while (!release_field_spaces.empty())
      {
        std::pair<ValueBroadcast<FSBroadcast>*,bool> &collective = 
          release_field_spaces.front();
        if (collective.second)
        {
          const FSBroadcast value = collective.first->get_value(false);
          runtime->forest->revoke_pending_field_space(value.space_id);
        }
        else
        {
          // Make sure this collective is done before we delete it
          const RtEvent done = collective.first->get_done_event();
          if (!done.has_triggered())
            done.wait();
        }
        delete collective.first;
        release_field_spaces.pop_front();
      }
      while (!release_fields.empty())
      {
        std::pair<ValueBroadcast<FIDBroadcast>*,bool> &collective = 
          release_fields.front();
        if (!collective.second)
        {
          // Make sure this collective is done before we delete it
          const RtEvent done = collective.first->get_done_event();
          if (!done.has_triggered())
            done.wait();
        }
        delete collective.first;
        release_fields.pop_front();
      }
      while (!release_region_trees.empty())
      {
        std::pair<ValueBroadcast<LRBroadcast>*,bool> &collective = 
          release_region_trees.front();
        if (collective.second)
        {
          const LRBroadcast value = collective.first->get_value(false);
          runtime->forest->revoke_pending_region_tree(value.tid);
        }
        else
        {
          // Make sure this collective is done before we delete it
          const RtEvent done = collective.first->get_done_event();
          if (!done.has_triggered())
            done.wait();
        }
        delete collective.first;
        release_region_trees.pop_front();
      }
      while (!release_distributed_ids.empty())
      {
        std::pair<ValueBroadcast<DIDBroadcast>*,bool> &collective = 
          release_distributed_ids.front();
        if (!collective.second)
        {
          // Make sure this collective is done before we delete it
          const RtEvent done = collective.first->get_done_event();
          if (!done.has_triggered())
            done.wait();
        }
        delete collective.first;
        release_distributed_ids.pop_front();
      }
    }

    //--------------------------------------------------------------------------
    bool ReplicateContext::add_to_dependence_queue(Operation *op,
                                                 bool unordered, bool outermost)
    //--------------------------------------------------------------------------
    {
      // We disable program order execution when we are replaying a
      // fixed trace since it might not be sound to block
      if (runtime->program_order_execution && !unordered &&
           outermost && !is_replaying_physical_trace())
      {
        const ApEvent term_event = op->get_completion_event();
        InnerContext::add_to_dependence_queue(op,unordered,false/*outermost*/);
        const ApBarrier inorder_bar = inorder_barrier.next(this);
        Runtime::phase_barrier_arrive(inorder_bar, 1/*count*/, term_event); 
        bool poisoned = false;
        inorder_bar.wait_faultaware(poisoned);
        if (poisoned)
          raise_poison_exception();
        // Issue any unordered operations now
        AutoLock d_lock(dependence_lock);
        insert_unordered_ops(d_lock);
        // Not unordered so it must have succeeded
        return true;
      }
      else
        return InnerContext::add_to_dependence_queue(op, unordered, outermost);
    }

    //--------------------------------------------------------------------------
    PredicateImpl* ReplicateContext::create_predicate_impl(Operation *op)
    //--------------------------------------------------------------------------
    {
      return new ReplPredicateImpl(op,
          get_next_collective_index(COLLECTIVE_LOC_1));
    }

    //--------------------------------------------------------------------------
    InnerContext::CollectiveResult* 
      ReplicateContext::find_or_create_collective_view(RegionTreeID tid,
          const std::vector<DistributedID> &instances, RtEvent &ready)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(instances.size() > 1);
#endif
      // Find which shard is the owner
      const ShardID tid_shard = shard_manager->find_collective_owner(tid);
      if (tid_shard != owner_shard->shard_id)
      {
        const RtUserEvent to_trigger = Runtime::create_rt_user_event();
        CollectiveResult *result = new CollectiveResult(instances);
        result->add_reference();
        Serializer rez;
        rez.serialize(shard_manager->did);
        rez.serialize(tid_shard);
        rez.serialize(tid);
        rez.serialize<size_t>(instances.size());
        for (unsigned idx = 0; idx < instances.size(); idx++)
          rez.serialize(instances[idx]);
        rez.serialize(result);
        rez.serialize(runtime->address_space);
        rez.serialize(to_trigger);
        shard_manager->send_find_or_create_collective_view(tid_shard, rez);
        ready = to_trigger;
        return result;
      }
      else
        return InnerContext::find_or_create_collective_view(tid, 
                                                instances, ready);
    }

    //--------------------------------------------------------------------------
    ProjectionSummary* ReplicateContext::construct_projection_summary(
            Operation *op, unsigned index, const RegionRequirement &req, 
            LogicalState *state, const ProjectionInfo &proj_info) 
    //--------------------------------------------------------------------------
    {
      const ShardID local_shard = owner_shard->shard_id;
      ProjectionNode *result = proj_info.projection->construct_projection_tree(
                          op, index, req, local_shard, state->owner, proj_info);
      // Now we need to exchange this between the shards. The secret to this
      // function is knowing that it is only called in the logical dependence
      // analysis stage of the pipeline so we can get a collective ID here to
      // perform the exchange between the shards of their neighbor sharding
      // information. This is unfortunately a blocking process, but it should
      // be memoized in most cases to reduce the latency of it happening
      if (req.projection == 0)
      {
        // For the identity projection function we know how to compute this
        // without performing any communication between the shards
        IndexSpaceNode *launch_space = proj_info.projection_space;
        Domain launch_domain, shard_domain;
        launch_space->get_domain(launch_domain);
        if (proj_info.sharding_space != NULL)
          proj_info.sharding_space->get_domain(shard_domain);
        else
          shard_domain = launch_domain;
        if (state->owner->is_region())
        {
          // Iterate all the points in the launch space and compute their
          // shards and record them in the shard users
          ProjectionRegion *projection = result->as_region_projection();
          for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)
          {
            const ShardID shard =
              proj_info.sharding_function->find_owner(*itr, shard_domain);
            projection->add_user(shard);
          }
          return new ProjectionSummary(proj_info, result, op, index, req, 
              state, true/*disjoint*/, false/*unique*/);
        }
        else
        {
          // Iterate all the points in the launch space and linearize
          // their colors to add to the summary
          IndexPartNode *partition = 
            state->owner->as_partition_node()->row_source;
          IndexSpaceNode *color_space = partition->color_space;
          ProjectionPartition *projection = result->as_partition_projection();
          for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)
          {
            const ShardID shard =
              proj_info.sharding_function->find_owner(*itr, shard_domain);
            if (shard == local_shard)
              continue;
            const LegionColor color = color_space->linearize_color(*itr);
#ifdef DEBUG_LEGION
            assert(projection->local_children.find(color) ==
                    projection->local_children.end());
#endif
            projection->shard_children.add_child(color);
          }
          const bool disjoint = partition->is_disjoint(false/*from app*/);
          return new ProjectionSummary(proj_info, result, op, index, req,
                                       state, disjoint, true/*unique*/);
        }
      }
      else
        return new ProjectionSummary(proj_info, result, op, index, 
                                     req, state, this); 
    }

    //--------------------------------------------------------------------------
    bool ReplicateContext::has_interfering_shards(ProjectionSummary *one,
                                                  ProjectionSummary *two)
    //--------------------------------------------------------------------------
    {
      bool result = one->get_tree()->interferes(two->get_tree(), 
                                          owner_shard->shard_id);
      // Now we need to perform a collective to make sure that all the 
      // shards agree on the result of the interference
      AllReduceCollective<SumReduction<bool> > any_interfering(this,
          get_next_collective_index(COLLECTIVE_LOC_105, true/*logical*/));
      return any_interfering.sync_all_reduce(result);
    }

    //--------------------------------------------------------------------------
    bool ReplicateContext::match_timeouts(std::vector<LogicalUser*> &timeouts, 
                                          std::vector<LogicalUser*> &to_delete,
                                          TimeoutMatchExchange *&exchange)
    //--------------------------------------------------------------------------
    {
      bool previous_ready = true;
      bool double_latency = false;
      if (exchange != NULL)
      {
        RtEvent ready = exchange->perform_collective_wait(false/*block*/);
        if (ready.exists() && !ready.has_triggered())
        {
          previous_ready = false;
          ready.wait();
        }
        double_latency = exchange->complete_exchange(to_delete);
        delete exchange;
      }
      exchange = new TimeoutMatchExchange(this, COLLECTIVE_LOC_79);
      exchange->perform_exchange(timeouts, previous_ready);
      return double_latency;
    }

    //--------------------------------------------------------------------------
    Lock ReplicateContext::create_lock(void)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_REPLICATE_TASK_VIOLATION,
                    "Illegal create lock performed in "
                    "control replicated task %s (UID %lld)",
                    get_task_name(), get_unique_id())
      return Lock();
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::destroy_lock(Lock l)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_REPLICATE_TASK_VIOLATION,
                    "Illegal destroy lock performed in "
                    "control replicated task %s (UID %lld)",
                    get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    Grant ReplicateContext::acquire_grant(
                                       const std::vector<LockRequest> &requests)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_REPLICATE_TASK_VIOLATION,
                    "Illegal acquire grant performed in "
                    "control replicated task %s (UID %lld)",
                    get_task_name(), get_unique_id())
      return Grant();
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::release_grant(Grant g)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_REPLICATE_TASK_VIOLATION,
                    "Illegal release grant performed in "
                    "control replicated task %s (UID %lld)",
                    get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    PhaseBarrier ReplicateContext::create_phase_barrier(unsigned arrivals)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) && 
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_CREATE_PHASE_BARRIER, __func__);
        hasher.hash(arrivals, "arrivals");
        if (hasher.verify(__func__))
          break;
      }
      ValueBroadcast<PhaseBarrier> bar_collective(this, 0/*origin*/,
                                                  COLLECTIVE_LOC_71); 
      // Shard 0 will make the barrier and broadcast it
      if (owner_shard->shard_id == 0)
      {
        PhaseBarrier result = InnerContext::create_phase_barrier(arrivals);
        bar_collective.broadcast(result);
        return result;
      }
      else
        return bar_collective.get_value();
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::destroy_phase_barrier(PhaseBarrier pb)
    //--------------------------------------------------------------------------
    {
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) && 
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_DESTROY_PHASE_BARRIER, __func__);
        hasher.hash(pb.phase_barrier, "phase_barrier");
        if (hasher.verify(__func__))
          break;
      }
      // Shard 0 has to wait for all the other shards to get here
      // too before it can do the deletion
      ShardSyncTree sync_point(this, 0/*origin*/, COLLECTIVE_LOC_72);
      sync_point.perform_collective_sync();
      if (owner_shard->shard_id == 0)
        InnerContext::destroy_phase_barrier(pb);
    }

    //--------------------------------------------------------------------------
    PhaseBarrier ReplicateContext::advance_phase_barrier(PhaseBarrier bar)
    //--------------------------------------------------------------------------
    {
      // For now we issue a mapping fence whenever we do this because
      // we do not have any logical dependence analysis on phase barriers
      issue_mapping_fence(NULL/*provenance*/);
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) &&
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_ADVANCE_PHASE_BARRIER, __func__);
        hasher.hash(bar, "bar");
        if (hasher.verify(__func__))
          break;
      }
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_run.debug("Advancing phase barrier in task %s (ID %lld)",
                        get_task_name(), get_unique_id());
#endif
      PhaseBarrier result = bar;
      Runtime::advance_barrier(result);
#ifdef LEGION_SPY
      if (owner_shard->shard_id == 0)
        LegionSpy::log_event_dependence(bar.phase_barrier,result.phase_barrier);
#endif
      return result;
    }

    //--------------------------------------------------------------------------
    DynamicCollective ReplicateContext::create_dynamic_collective(
                                       unsigned arrivals, ReductionOpID redop,
                                       const void *init_value, size_t init_size)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_REPLICATE_TASK_VIOLATION,
                    "Illegal create dynamic collective performed in "
                    "control replicated task %s (UID %lld)",
                    get_task_name(), get_unique_id())
      return DynamicCollective();
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::destroy_dynamic_collective(DynamicCollective dc)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_REPLICATE_TASK_VIOLATION,
                    "Illegal destroy dynamic collective performed in "
                    "control replicated task %s (UID %lld)",
                    get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::arrive_dynamic_collective(DynamicCollective dc,
                                                    const void *buffer,
                                                    size_t size, unsigned count)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_REPLICATE_TASK_VIOLATION,
                    "Illegal dynamic collective arrival performed in "
                    "control replicated task %s (UID %lld)",
                    get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::defer_dynamic_collective_arrival(
                                                         DynamicCollective dc,
                                                         const Future &f,
                                                         unsigned count)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_REPLICATE_TASK_VIOLATION,
                    "Illegal defer dynamic collective arrival performed in "
                    "control replicated task %s (UID %lld)",
                    get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    Future ReplicateContext::get_dynamic_collective_result(DynamicCollective dc,
                                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_REPLICATE_TASK_VIOLATION,
                    "Illegal get dynamic collective result performed in "
                    "control replicated task %s (UID %lld)",
                    get_task_name(), get_unique_id())
      return Future();
    }

    //--------------------------------------------------------------------------
    DynamicCollective ReplicateContext::advance_dynamic_collective( 
                                                           DynamicCollective dc)
    //--------------------------------------------------------------------------
    {
      // For now we issue a mapping fence whenever we do this because
      // we do not have any logical dependence analysis on phase barriers
      issue_mapping_fence(NULL/*provenance*/);
      AutoRuntimeCall call(this);
      for (int i = 0; runtime->safe_control_replication && (i < 2) && 
            ((current_trace == NULL) || !current_trace->is_fixed()); i++)
      {
        Murmur3Hasher hasher(this, runtime->safe_control_replication > 1,i > 0);
        hasher.hash(REPLICATE_ADVANCE_DYNAMIC_COLLECTIVE, __func__);
        hasher.hash(dc, "dc");
        if (hasher.verify(__func__))
          break;
      }
#ifdef DEBUG_LEGION
      if (owner_shard->shard_id == 0)
        log_run.debug("Advancing dynamic collective in task %s (ID %lld)",
                        get_task_name(), get_unique_id());
#endif
      DynamicCollective result = dc;
      Runtime::advance_barrier(result);
#ifdef LEGION_SPY
      if (owner_shard->shard_id == 0)
        LegionSpy::log_event_dependence(dc.phase_barrier, result.phase_barrier);
#endif
      return result;
    }

    //--------------------------------------------------------------------------
#ifdef DEBUG_LEGION_COLLECTIVES
    MergeCloseOp* ReplicateContext::get_merge_close_op(Operation *op,
                                                       RegionTreeNode *node)
#else
    MergeCloseOp* ReplicateContext::get_merge_close_op(void)
#endif
    //--------------------------------------------------------------------------
    {
      ReplMergeCloseOp *result = runtime->get_available_repl_merge_close_op();
      // Get the mapped barrier for the close operation
      const RtBarrier mapped_bar = get_next_close_mapped_barrier();
#ifdef DEBUG_LEGION_COLLECTIVES
      CloseCheckReduction::RHS barrier(op, mapped_bar, 
                                       node, false/*read only*/);
      const RtBarrier close_check_bar = close_check_barrier.next(this,
          CloseCheckReduction::REDOP, &CloseCheckReduction::IDENTITY,
          sizeof(CloseCheckReduction::IDENTITY));
      Runtime::phase_barrier_arrive(close_check_bar, 1/*count*/,
                              RtEvent::NO_RT_EVENT, &barrier, sizeof(barrier));
      close_check_bar.wait();
      CloseCheckReduction::RHS actual_barrier;
      bool ready = Runtime::get_barrier_result(close_check_bar,
                                      &actual_barrier, sizeof(actual_barrier));
      assert(ready);
      assert(actual_barrier == barrier);
#endif
      result->set_repl_close_info(mapped_bar);
      return result;
    }

    //--------------------------------------------------------------------------
#ifdef DEBUG_LEGION_COLLECTIVES
    RefinementOp* ReplicateContext::get_refinement_op(Operation *op,
                                                      RegionTreeNode *node)
#else
    RefinementOp* ReplicateContext::get_refinement_op(void)
#endif
    //--------------------------------------------------------------------------
    {
      ReplRefinementOp *result = runtime->get_available_repl_refinement_op();
      // Get the mapped barrier for the refinement operation
      RtBarrier mapped_bar = get_next_refinement_mapped_barrier();
#ifdef DEBUG_LEGION_COLLECTIVES
      CloseCheckReduction::RHS barrier(op, mapped_bar,
                                       node, false/*read only*/);
      const RtBarrier refinement_check_bar = refinement_check_barrier.next(this,
          CloseCheckReduction::REDOP, &CloseCheckReduction::IDENTITY, 
          sizeof(CloseCheckReduction::IDENTITY));
      Runtime::phase_barrier_arrive(refinement_check_bar, 1/*count*/,
                              RtEvent::NO_RT_EVENT, &barrier, sizeof(barrier));
      refinement_check_bar.wait();
      CloseCheckReduction::RHS actual_barrier;
      bool ready = Runtime::get_barrier_result(refinement_check_bar,
                                      &actual_barrier, sizeof(actual_barrier));
      assert(ready);
      assert(actual_barrier == barrier);
#endif
      const RtBarrier next_refinement_bar = get_next_refinement_barrier();
      result->set_repl_refinement_info(mapped_bar, next_refinement_bar);
      return result;
    }

    //--------------------------------------------------------------------------
    VirtualCloseOp* ReplicateContext::get_virtual_close_op(void)
    //--------------------------------------------------------------------------
    {
      return runtime->get_available_repl_virtual_close_op();
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::pack_task_context(Serializer &rez) const
    //--------------------------------------------------------------------------
    {
      rez.serialize(did); // pack our distributed ID
      rez.serialize<DistributedID>(shard_manager->did);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::pack_remote_context(Serializer &rez,
                                          AddressSpaceID target, bool replicate)
    //--------------------------------------------------------------------------
    {
      // Do the normal inner pack with replicate true
      InnerContext::pack_remote_context(rez, target, true/*replicate*/);
      // Then pack our additional information
      rez.serialize(owner_shard->shard_id);
      rez.serialize<size_t>(total_shards);
      rez.serialize(shard_manager->shard_points[owner_shard->shard_id]);
      rez.serialize(shard_manager->shard_domain);
      rez.serialize(shard_manager->did);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::handle_collective_message(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      ShardCollective *collective = find_or_buffer_collective(derez);   
      if (collective != NULL)
        collective->handle_collective_message(derez);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::register_rendezvous(ShardRendezvous *rendezvous)
    //--------------------------------------------------------------------------
    {
      std::vector<std::pair<void*,size_t> > to_handle;
      {
        AutoLock repl_lock(replication_lock);
#ifdef DEBUG_LEGION
        assert(shard_rendezvous.find(rendezvous->origin_shard) == 
                shard_rendezvous.end());
#endif
        shard_rendezvous[rendezvous->origin_shard] = rendezvous;
        std::map<ShardID,std::vector<std::pair<void*,size_t> > >::iterator
          finder = pending_rendezvous_updates.find(rendezvous->origin_shard);
        if (finder != pending_rendezvous_updates.end())
        {
          to_handle.swap(finder->second);
          pending_rendezvous_updates.erase(finder);
        }
      }
      for (std::vector<std::pair<void*,size_t> >::const_iterator it =
            to_handle.begin(); it != to_handle.end(); it++)
      {
        Deserializer derez(it->first, it->second);
        if (rendezvous->receive_message(derez))
        {
          AutoLock repl_lock(replication_lock);
          std::map<ShardID,ShardRendezvous*>::iterator finder =
            shard_rendezvous.find(rendezvous->origin_shard);
#ifdef DEBUG_LEGION
          assert(finder != shard_rendezvous.end());
          assert(finder->second == rendezvous);
#endif
          shard_rendezvous.erase(finder);
        }
        free(it->first);
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::handle_rendezvous_message(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      ShardRendezvous *rendezvous = find_or_buffer_rendezvous(derez);
      if ((rendezvous != NULL) && rendezvous->receive_message(derez))
      {
        AutoLock repl_lock(replication_lock);
        std::map<ShardID,ShardRendezvous*>::iterator finder =
          shard_rendezvous.find(rendezvous->origin_shard);
#ifdef DEBUG_LEGION
        assert(finder != shard_rendezvous.end());
        assert(finder->second == rendezvous);
#endif
        shard_rendezvous.erase(finder);
      }
    }

    //--------------------------------------------------------------------------
    ShardRendezvous* ReplicateContext::find_or_buffer_rendezvous(
                                                            Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      ShardID origin_shard;
      derez.deserialize(origin_shard);
      AutoLock repl_lock(replication_lock);
      // See if we already have a rendezvous here to rendezvous with
      std::map<ShardID,ShardRendezvous*>::const_iterator finder =
        shard_rendezvous.find(origin_shard);
      if (finder != shard_rendezvous.end())
        return finder->second;
      // If we couldn't find it then we have to buffer it for the future
      const size_t remaining_bytes = derez.get_remaining_bytes();
      void *buffer = malloc(remaining_bytes);
      memcpy(buffer, derez.get_current_pointer(), remaining_bytes);
      derez.advance_pointer(remaining_bytes);
      pending_rendezvous_updates[origin_shard].push_back(
          std::pair<void*,size_t>(buffer, remaining_bytes));
      return NULL;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::handle_resource_update(Deserializer &derez,
                                                  std::set<RtEvent> &applied)
    //--------------------------------------------------------------------------
    {
      size_t return_index;
      derez.deserialize(return_index);
      RtBarrier ready_barrier, mapped_barrier, execution_barrier;
      derez.deserialize(ready_barrier);
      derez.deserialize(mapped_barrier);
      derez.deserialize(execution_barrier);
      size_t num_created_regions;
      derez.deserialize(num_created_regions);
      std::map<LogicalRegion,unsigned> created_regs;
      for (unsigned idx = 0; idx < num_created_regions; idx++)
      {
        LogicalRegion reg;
        derez.deserialize(reg);
        derez.deserialize(created_regs[reg]);
      }
      size_t num_deleted_regions;
      derez.deserialize(num_deleted_regions);
      std::vector<DeletedRegion> deleted_regs(num_deleted_regions);
      for (unsigned idx = 0; idx < num_deleted_regions; idx++)
        deleted_regs[idx].deserialize(derez);
      size_t num_created_fields;
      derez.deserialize(num_created_fields);
      std::set<std::pair<FieldSpace,FieldID> > created_fids;
      for (unsigned idx = 0; idx < num_created_fields; idx++)
      {
        std::pair<FieldSpace,FieldID> key;
        derez.deserialize(key.first);
        derez.deserialize(key.second);
        created_fids.insert(key);
      }
      size_t num_deleted_fields;
      derez.deserialize(num_deleted_fields);
      std::vector<DeletedField> deleted_fids(num_deleted_fields);
      for (unsigned idx = 0; idx < num_deleted_fields; idx++)
        deleted_fields[idx].deserialize(derez);
      size_t num_created_field_spaces;
      derez.deserialize(num_created_field_spaces);
      std::map<FieldSpace,unsigned> created_fs;
      for (unsigned idx = 0; idx < num_created_field_spaces; idx++)
      {
        FieldSpace sp;
        derez.deserialize(sp);
        derez.deserialize(created_fs[sp]);
      }
      size_t num_latent_field_spaces;
      derez.deserialize(num_latent_field_spaces);
      std::map<FieldSpace,std::set<LogicalRegion> > latent_fs;
      for (unsigned idx = 0; idx < num_latent_field_spaces; idx++)
      {
        FieldSpace sp;
        derez.deserialize(sp);
        std::set<LogicalRegion> &regions = latent_fs[sp];
        size_t num_regions;
        derez.deserialize(num_regions);
        for (unsigned idx2 = 0; idx2 < num_regions; idx2++)
        {
          LogicalRegion region;
          derez.deserialize(region);
          regions.insert(region);
        }
      }
      size_t num_deleted_field_spaces;
      derez.deserialize(num_deleted_field_spaces);
      std::vector<DeletedFieldSpace> deleted_fs(num_deleted_field_spaces);
      for (unsigned idx = 0; idx < num_deleted_field_spaces; idx++)
        deleted_fs[idx].deserialize(derez);
      size_t num_created_index_spaces;
      derez.deserialize(num_created_index_spaces);
      std::map<IndexSpace,unsigned> created_is;
      for (unsigned idx = 0; idx < num_created_index_spaces; idx++)
      {
        IndexSpace sp;
        derez.deserialize(sp);
        derez.deserialize(created_is[sp]);
      }
      size_t num_deleted_index_spaces;
      derez.deserialize(num_deleted_index_spaces);
      std::vector<DeletedIndexSpace> deleted_is(num_deleted_index_spaces);
      for (unsigned idx = 0; idx < num_deleted_index_spaces; idx++)
        deleted_is[idx].deserialize(derez);
      size_t num_created_index_partitions;
      derez.deserialize(num_created_index_partitions);
      std::map<IndexPartition,unsigned> created_partitions;
      for (unsigned idx = 0; idx < num_created_index_partitions; idx++)
      {
        IndexPartition ip;
        derez.deserialize(ip);
        derez.deserialize(created_partitions[ip]);
      }
      size_t num_deleted_index_partitions;
      derez.deserialize(num_deleted_index_partitions);
      std::vector<DeletedPartition> 
        deleted_partitions(num_deleted_index_partitions);
      for (unsigned idx = 0; idx < num_deleted_index_partitions; idx++)
        deleted_partitions[idx].deserialize(derez);
      // Send this down to the base class to avoid re-broadcasting
      receive_replicate_resources(return_index, created_regs, deleted_regs,
          created_fids, deleted_fids, created_fs, latent_fs, deleted_fs,
          created_is, deleted_is, created_partitions, deleted_partitions,
          applied, ready_barrier, mapped_barrier, execution_barrier);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::handle_created_region_contexts(Deserializer &derez,
                                              std::set<RtEvent> &applied_events)
    //--------------------------------------------------------------------------
    {
      size_t num_regions;
      derez.deserialize(num_regions);
      std::vector<RegionNode*> created_nodes(num_regions);
      for (unsigned idx1 = 0; idx1 < num_regions; idx1++)
      {
        LogicalRegion handle;
        derez.deserialize(handle);
        RegionNode *node = runtime->forest->get_node(handle);
        created_nodes[idx1] = node;
        size_t num_sets;
        derez.deserialize(num_sets);
        std::vector<RtEvent> ready_events;
        FieldMaskSet<EquivalenceSet> eq_sets;
        for (unsigned idx2 = 0; idx2 < num_sets; idx2++)
        {
          DistributedID did;
          derez.deserialize(did);
          RtEvent ready;
          EquivalenceSet *set = 
            runtime->find_or_request_equivalence_set(did, ready);
          FieldMask mask;
          derez.deserialize(mask);
          eq_sets.insert(set, mask);
          if (ready.exists())
            ready_events.push_back(ready);
        }
        if (!ready_events.empty())
        {
          const RtEvent wait_on = Runtime::merge_events(ready_events);
          if (wait_on.exists() && !wait_on.has_triggered())
            wait_on.wait();
        }
        EqKDTree *current = NULL;
        {
          AutoLock priv_lock(privilege_lock);
          unsigned index = add_created_region(handle,
              false/*task local*/, false/*output region*/);
          std::map<unsigned,EqKDRoot>::const_iterator finder =
            equivalence_set_trees.find(index);
          if (finder == equivalence_set_trees.end())
          {
            // If we're the first make the KD-tree here
            current = 
              node->row_source->create_equivalence_set_kd_tree(total_shards);
            equivalence_set_trees.emplace(index, EqKDRoot(current));
          }
          else
            current = finder->second.tree;
        }
        const ShardID local_shard = get_shard_id(); 
        // Put the equivalence sets in the tree but in the previous set
        // of equivalence sets so new accesses will make new sets
        for (FieldMaskSet<EquivalenceSet>::const_iterator it =
              eq_sets.begin(); it != eq_sets.end(); it++)
        {
          it->first->set_expr->initialize_equivalence_set_kd_tree(
              current, it->first, it->second, local_shard, false/*current*/);
          it->first->unpack_global_ref();
        }
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::handle_trace_update(Deserializer &derez, 
                                               AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      ShardedPhysicalTemplate *tpl = find_or_buffer_trace_update(derez, source);
      // If the template is NULL then the request was buffered
      if (tpl == NULL)
        return;
      tpl->handle_trace_update(derez, source);
    }

    //--------------------------------------------------------------------------
    ApBarrier ReplicateContext::handle_find_trace_shard_event(
                     size_t template_index, ApEvent event, ShardID remote_shard)
    //--------------------------------------------------------------------------
    {
      ShardedPhysicalTemplate *physical_template = NULL;
      {
        AutoLock r_lock(replication_lock);
        std::map<size_t,ShardedPhysicalTemplate*>::const_iterator finder = 
          physical_templates.find(template_index);
        // If we can't find the template index that means it hasn't been
        // started here so it can't have produced the event we're looking for
        // Note it also can't have been reclaimed yet as all the shard
        // templates need to come to the same decision on whether they 
        // are replayable before any of them can be deleted and so if one
        // is still tracing then they all are
        if (finder == physical_templates.end())
          return ApBarrier::NO_AP_BARRIER;
        physical_template = finder->second;
      }
      return physical_template->find_trace_shard_event(event, remote_shard);
    }

    //--------------------------------------------------------------------------
    ApBarrier ReplicateContext::handle_find_trace_shard_frontier(
                     size_t template_index, ApEvent event, ShardID remote_shard)
    //--------------------------------------------------------------------------
    {
      ShardedPhysicalTemplate *physical_template = NULL;
      {
        AutoLock r_lock(replication_lock);
        std::map<size_t,ShardedPhysicalTemplate*>::const_iterator finder = 
          physical_templates.find(template_index);
        // If we can't find the template index that means it hasn't been
        // started here so it can't have produced the event we're looking for
        // Note it also can't have been reclaimed yet as all the shard
        // templates need to come to the same decision on whether they 
        // are replayable before any of them can be deleted and so if one
        // is still tracing then they all are
        if (finder == physical_templates.end())
          return ApBarrier::NO_AP_BARRIER;
        physical_template = finder->second;
      }
      return physical_template->find_trace_shard_frontier(event, remote_shard);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::record_intra_space_dependence(size_t context_index,
        const DomainPoint &point, RtEvent point_mapped, ShardID next_shard)
    //--------------------------------------------------------------------------
    {
      const std::pair<size_t,DomainPoint> key(context_index,point);
      AutoLock r_lock(replication_lock);
      IntraSpaceDeps &deps = intra_space_deps[key];
      // Check to see if someone has already registered this
      std::map<ShardID,RtUserEvent>::iterator finder = 
        deps.pending_deps.find(next_shard);
      if (finder != deps.pending_deps.end())
      {
        Runtime::trigger_event(finder->second, point_mapped);
        deps.pending_deps.erase(finder);
        if (deps.pending_deps.empty() && deps.ready_deps.empty())
          intra_space_deps.erase(key);
      }
      else
      {
        // Not seen yet so just record our entry for this shard
#ifdef DEBUG_LEGION
        assert(deps.ready_deps.find(next_shard) == deps.ready_deps.end());
#endif
        deps.ready_deps[next_shard] = point_mapped;
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::handle_intra_space_dependence(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      std::pair<size_t,DomainPoint> key;
      derez.deserialize(key.first);
      derez.deserialize(key.second);
      RtUserEvent pending_event;
      derez.deserialize(pending_event);
      ShardID requesting_shard;
      derez.deserialize(requesting_shard);

      AutoLock r_lock(replication_lock);
      IntraSpaceDeps &deps = intra_space_deps[key];
      // Check to see if someone has already registered this shard
      std::map<ShardID,RtEvent>::iterator finder = 
        deps.ready_deps.find(requesting_shard);
      if (finder != deps.ready_deps.end())
      {
        Runtime::trigger_event(pending_event, finder->second);
        deps.ready_deps.erase(finder);
        if (deps.ready_deps.empty() && deps.pending_deps.empty())
          intra_space_deps.erase(key);
      }
      else
      {
        // Not seen yet so just record our entry for this shard
#ifdef DEBUG_LEGION
        assert(deps.pending_deps.find(requesting_shard) == 
                deps.pending_deps.end());
#endif
        deps.pending_deps[requesting_shard] = pending_event;
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::receive_resources(size_t return_index,
              std::map<LogicalRegion,unsigned> &created_regs,
              std::vector<DeletedRegion> &deleted_regs,
              std::set<std::pair<FieldSpace,FieldID> > &created_fids,
              std::vector<DeletedField> &deleted_fids,
              std::map<FieldSpace,unsigned> &created_fs,
              std::map<FieldSpace,std::set<LogicalRegion> > &latent_fs,
              std::vector<DeletedFieldSpace> &deleted_fs,
              std::map<IndexSpace,unsigned> &created_is,
              std::vector<DeletedIndexSpace> &deleted_is,
              std::map<IndexPartition,unsigned> &created_partitions,
              std::vector<DeletedPartition> &deleted_partitions,
              std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      // We need to broadcast these updates out to other shards
      Serializer rez;
      // If we have any deletions make barriers for use with
      // the deletion operations we may need to perform
      if (!deleted_regs.empty() || !deleted_fids.empty() || 
          !deleted_fs.empty() || !deleted_is.empty() || 
          !deleted_partitions.empty())
      {
        if (!returned_resource_ready_barrier.exists())
          returned_resource_ready_barrier = RtBarrier(
              Realm::Barrier::create_barrier(shard_manager->total_shards));
        if (!returned_resource_mapped_barrier.exists())
          returned_resource_mapped_barrier = RtBarrier(
              Realm::Barrier::create_barrier(shard_manager->total_shards));
        if (!returned_resource_execution_barrier.exists())
          returned_resource_execution_barrier = RtBarrier(
              Realm::Barrier::create_barrier(shard_manager->total_shards));
      }
      rez.serialize(return_index);
      rez.serialize(returned_resource_ready_barrier);
      rez.serialize(returned_resource_mapped_barrier);
      rez.serialize(returned_resource_execution_barrier);
      rez.serialize<size_t>(created_regs.size());
      if (!created_regs.empty())
      {
        for (std::map<LogicalRegion,unsigned>::const_iterator it =
              created_regs.begin(); it != created_regs.end(); it++)
        {
          rez.serialize(it->first);
          rez.serialize(it->second);
        }
      }
      rez.serialize<size_t>(deleted_regs.size());
      if (!deleted_regs.empty())
      {
        for (std::vector<DeletedRegion>::const_iterator it = 
              deleted_regs.begin(); it != deleted_regs.end(); it++)
          it->serialize(rez);
      }
      rez.serialize<size_t>(created_fids.size());
      if (!created_fids.empty())
      {
        for (std::set<std::pair<FieldSpace,FieldID> >::const_iterator 
              it = created_fids.begin(); it != created_fids.end(); it++)
        {
          rez.serialize(it->first);
          rez.serialize(it->second);
        }
      }
      rez.serialize<size_t>(deleted_fids.size());
      if (!deleted_fids.empty())
      {
        for (std::vector<DeletedField>::const_iterator it =
              deleted_fids.begin(); it != deleted_fids.end(); it++)
          it->serialize(rez);
      }
      rez.serialize<size_t>(created_fs.size());
      if (!created_fs.empty())
      {
        for (std::map<FieldSpace,unsigned>::const_iterator it = 
              created_fs.begin(); it != created_fs.end(); it++)
        {
          rez.serialize(it->first);
          rez.serialize(it->second);
        }
      } 
      rez.serialize<size_t>(latent_fs.size());
      if (!latent_fs.empty())
      {
        for (std::map<FieldSpace,std::set<LogicalRegion> >::const_iterator it =
              latent_fs.begin(); it != latent_fs.end(); it++)
        {
          rez.serialize(it->first);
          rez.serialize<size_t>(it->second.size());
          for (std::set<LogicalRegion>::const_iterator it2 = 
                it->second.begin(); it2 != it->second.end(); it2++)
            rez.serialize(*it2);
        }
      }
      rez.serialize<size_t>(deleted_fs.size());
      if (!deleted_fs.empty())
      {
        for (std::vector<DeletedFieldSpace>::const_iterator it = 
              deleted_fs.begin(); it != deleted_fs.end(); it++)
          it->serialize(rez);
      }
      rez.serialize<size_t>(created_is.size());
      if (!created_is.empty())
      {
        for (std::map<IndexSpace,unsigned>::const_iterator it = 
              created_is.begin(); it != created_is.end(); it++)
        {
          rez.serialize(it->first);
          rez.serialize(it->second);
        }
      }
      rez.serialize<size_t>(deleted_is.size());
      if (!deleted_is.empty())
      {
        for (std::vector<DeletedIndexSpace>::const_iterator it = 
              deleted_is.begin(); it != deleted_is.end(); it++)
          it->serialize(rez);
      }
      rez.serialize<size_t>(created_partitions.size());
      if (!created_partitions.empty())
      {
        for (std::map<IndexPartition,unsigned>::const_iterator it = 
              created_partitions.begin(); it != 
              created_partitions.end(); it++)
        {
          rez.serialize(it->first);
          rez.serialize(it->second);
        }
      }
      rez.serialize<size_t>(deleted_partitions.size());
      if (!deleted_partitions.empty())
      {
        for (std::vector<DeletedPartition>::const_iterator it = 
              deleted_partitions.begin(); it != deleted_partitions.end(); it++)
          it->serialize(rez);
      }
      shard_manager->broadcast_resource_update(owner_shard, rez, preconditions);
      // Now we can handle this for ourselves
      receive_replicate_resources(return_index, created_regs, deleted_regs,
          created_fids, deleted_fids, created_fs, latent_fs, deleted_fs,
          created_is, deleted_is, created_partitions, deleted_partitions,
          preconditions, returned_resource_ready_barrier,
          returned_resource_mapped_barrier,returned_resource_execution_barrier);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::receive_replicate_resources(size_t return_index,
              std::map<LogicalRegion,unsigned> &created_regs,
              std::vector<DeletedRegion> &deleted_regs,
              std::set<std::pair<FieldSpace,FieldID> > &created_fids,
              std::vector<DeletedField> &deleted_fids,
              std::map<FieldSpace,unsigned> &created_fs,
              std::map<FieldSpace,std::set<LogicalRegion> > &latent_fs,
              std::vector<DeletedFieldSpace> &deleted_fs,
              std::map<IndexSpace,unsigned> &created_is,
              std::vector<DeletedIndexSpace> &deleted_is,
              std::map<IndexPartition,unsigned> &created_partitions,
              std::vector<DeletedPartition> &deleted_partitions,
              std::set<RtEvent> &preconditions, RtBarrier &ready_barrier, 
              RtBarrier &mapped_barrier, RtBarrier &execution_barrier)
    //--------------------------------------------------------------------------
    {
      bool need_deletion_dependences = true;
      ApEvent precondition;
      std::map<Operation*,GenerationID> dependences;
      if (!created_regs.empty())
        register_region_creations(created_regs);
      if (!deleted_regs.empty())
      {
        precondition = 
          compute_return_deletion_dependences(return_index, dependences);
        need_deletion_dependences = false;
        register_region_deletions(precondition, dependences, 
                                  deleted_regs, preconditions, ready_barrier,
                                  mapped_barrier, execution_barrier); 
      }
      if (!created_fids.empty())
        register_field_creations(created_fids);
      if (!deleted_fids.empty())
      {
        if (need_deletion_dependences)
        {
          precondition = 
            compute_return_deletion_dependences(return_index, dependences);
          need_deletion_dependences = false;
        }
        register_field_deletions(precondition, dependences, 
                                 deleted_fids, preconditions, ready_barrier,
                                 mapped_barrier, execution_barrier);
      }
      if (!created_fs.empty())
        register_field_space_creations(created_fs);
      if (!latent_fs.empty())
        register_latent_field_spaces(latent_fs);
      if (!deleted_fs.empty())
      {
        if (need_deletion_dependences)
        {
          precondition = 
            compute_return_deletion_dependences(return_index, dependences);
          need_deletion_dependences = false;
        }
        register_field_space_deletions(precondition, dependences,
                                       deleted_fs, preconditions, ready_barrier,
                                       mapped_barrier, execution_barrier);
      }
      if (!created_is.empty())
        register_index_space_creations(created_is);
      if (!deleted_is.empty())
      {
        if (need_deletion_dependences)
        {
          precondition = 
            compute_return_deletion_dependences(return_index, dependences);
          need_deletion_dependences = false;
        }
        register_index_space_deletions(precondition, dependences,
                                       deleted_is, preconditions, ready_barrier,
                                       mapped_barrier, execution_barrier);
      }
      if (!created_partitions.empty())
        register_index_partition_creations(created_partitions);
      if (!deleted_partitions.empty())
      {
        if (need_deletion_dependences)
        {
          precondition = 
            compute_return_deletion_dependences(return_index, dependences);
          need_deletion_dependences = false;
        }
        register_index_partition_deletions(precondition, dependences,
                                           deleted_partitions, preconditions,
                                           ready_barrier, mapped_barrier, 
                                           execution_barrier);
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::register_region_deletions(ApEvent precondition,
                           const std::map<Operation*,GenerationID> &dependences,
                                            std::vector<DeletedRegion> &regions,
                                            std::set<RtEvent> &preconditions,
                                            RtBarrier &ready_barrier,
                                            RtBarrier &mapped_barrier,
                                            RtBarrier &execution_barrier)
    //--------------------------------------------------------------------------
    {
      std::vector<DeletedRegion> delete_now;
      {
        AutoLock priv_lock(privilege_lock);
        for (std::vector<DeletedRegion>::const_iterator rit =
              regions.begin(); rit != regions.end(); rit++)
        {
          std::map<LogicalRegion,unsigned>::iterator region_finder = 
            created_regions.find(rit->region);
          if (region_finder == created_regions.end())
          {
            if (local_regions.find(rit->region) != local_regions.end())
              REPORT_LEGION_ERROR(ERROR_ILLEGAL_RESOURCE_DESTRUCTION,
                  "Local logical region (%x,%x,%x) in task %s (UID %lld) was "
                  "not deleted by this task. Local regions can only be deleted "
                  "by the task that made them.", rit->region.index_space.id,
                  rit->region.field_space.id, rit->region.tree_id, 
                  get_task_name(), get_unique_id())
            // Deletion keeps going up
            deleted_regions.push_back(*rit);
          }
          else
          {
            // One of ours to delete
#ifdef DEBUG_LEGION
            assert(region_finder->second > 0);
#endif
            if (--region_finder->second == 0)
            {
              // Don't remove this from created regions yet,
              // That will happen when we make the deletion operation
              delete_now.push_back(*rit);
            }
          }
        }
      }
      if (!delete_now.empty())
      {
        for (std::vector<DeletedRegion>::const_iterator it = 
              delete_now.begin(); it != delete_now.end(); it++)
        {
          ReplDeletionOp *op = runtime->get_available_repl_deletion_op();
          op->initialize_logical_region_deletion(this, it->region, 
              true/*unordered*/, it->provenance,
              true/*skip dependence analysis*/);
          op->initialize_replication(this,
                            shard_manager->is_first_local_shard(owner_shard),
                            &ready_barrier,&mapped_barrier,&execution_barrier);
          op->set_deletion_preconditions(precondition, dependences);
          preconditions.insert(
              Runtime::protect_event(op->get_completion_event()));
          op->execute_dependence_analysis();
        }
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::register_field_deletions(ApEvent precondition,
                           const std::map<Operation*,GenerationID> &dependences,
                           std::vector<DeletedField> &fields,
                           std::set<RtEvent> &preconditions,
                           RtBarrier &ready_barrier, RtBarrier &mapped_barrier,
                           RtBarrier &execution_barrier)
    //--------------------------------------------------------------------------
    {
      std::map<std::pair<FieldSpace,Provenance*>,std::set<FieldID> > delete_now;
      {
        AutoLock priv_lock(privilege_lock);
        for (std::vector<DeletedField>::const_iterator fit =
              fields.begin(); fit != fields.end(); fit++)
        {
          const std::pair<FieldSpace,FieldID> key(fit->space, fit->fid);
          std::set<std::pair<FieldSpace,FieldID> >::const_iterator 
            field_finder = created_fields.find(key);
          if (field_finder == created_fields.end())
          {
            std::map<std::pair<FieldSpace,FieldID>,bool>::iterator 
              local_finder = local_fields.find(key);
            if (local_finder != local_fields.end())
              REPORT_LEGION_ERROR(ERROR_ILLEGAL_RESOURCE_DESTRUCTION,
                  "Local field %d in field space %x in task %s (UID %lld) was "
                  "not deleted by this task. Local fields can only be deleted "
                  "by the task that made them.", fit->fid, fit->space.id,
                  get_task_name(), get_unique_id())
            deleted_fields.push_back(*fit);
          }
          else
          {
            // One of ours to delete
            std::pair<FieldSpace,Provenance*> now_key(fit->space,
                                                      fit->provenance);
            delete_now[now_key].insert(fit->fid);
            // No need to delete this now, it will be deleted
            // when the deletion op makes its region requirements
          }
        }
      }
      if (!delete_now.empty())
      {
        for (std::map<std::pair<FieldSpace,Provenance*>,
                      std::set<FieldID> >::const_iterator it = 
              delete_now.begin(); it != delete_now.end(); it++)
        {
          ReplDeletionOp *op = runtime->get_available_repl_deletion_op();
          FieldAllocatorImpl *allocator = 
            create_field_allocator(it->first.first, true/*unordered*/);
          op->initialize_field_deletions(this, it->first.first, it->second, 
              true/*unordered*/, allocator, it->first.second, 
              (owner_shard->shard_id != 0), true/*skip dependence analysis*/);
          op->initialize_replication(this,
                            shard_manager->is_first_local_shard(owner_shard),
                            &ready_barrier,&mapped_barrier,&execution_barrier);
          op->set_deletion_preconditions(precondition, dependences);
          preconditions.insert(
              Runtime::protect_event(op->get_completion_event()));
          op->execute_dependence_analysis();
        }
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::register_field_space_deletions(ApEvent precondition,
                           const std::map<Operation*,GenerationID> &dependences,
                                         std::vector<DeletedFieldSpace> &spaces,
                                               std::set<RtEvent> &preconditions,
                                               RtBarrier &ready_barrier,
                                               RtBarrier &mapped_barrier,
                                               RtBarrier &execution_barrier)
    //--------------------------------------------------------------------------
    {
      std::vector<DeletedFieldSpace> delete_now;
      {
        AutoLock priv_lock(privilege_lock);
        for (std::vector<DeletedFieldSpace>::const_iterator fit = 
              spaces.begin(); fit != spaces.end(); fit++)
        {
          std::map<FieldSpace,unsigned>::iterator finder = 
            created_field_spaces.find(fit->space);
          if (finder != created_field_spaces.end())
          {
#ifdef DEBUG_LEGION
            assert(finder->second > 0);
#endif
            if (--finder->second == 0)
            {
              delete_now.push_back(*fit);
              created_field_spaces.erase(finder);
              // Count how many regions are still using this field space
              // that still need to be deleted before we can remove the
              // list of created fields
              std::set<LogicalRegion> remaining_regions;
              for (std::map<LogicalRegion,unsigned>::const_iterator it = 
                    created_regions.begin(); it != created_regions.end(); it++)
                if (it->first.get_field_space() == fit->space)
                  remaining_regions.insert(it->first);
              for (std::map<LogicalRegion,bool>::const_iterator it = 
                    local_regions.begin(); it != local_regions.end(); it++)
                if (it->first.get_field_space() == fit->space)
                  remaining_regions.insert(it->first);
              if (remaining_regions.empty())
              {
                // No remaining regions so we can remove any created fields now
                for (std::set<std::pair<FieldSpace,FieldID> >::iterator it = 
                      created_fields.begin(); it != 
                      created_fields.end(); /*nothing*/)
                {
                  if (it->first == fit->space)
                  {
                    std::set<std::pair<FieldSpace,FieldID> >::iterator 
                      to_delete = it++;
                    created_fields.erase(to_delete);
                  }
                  else
                    it++;
                }
              }
              else
                latent_field_spaces[fit->space] = remaining_regions;
            }
          }
          else
            // If we didn't make this field space, record the deletion
            // and keep going. It will be handled by the context that
            // made the field space
            deleted_field_spaces.push_back(*fit);
        }
      }
      if (!delete_now.empty())
      {
        for (std::vector<DeletedFieldSpace>::const_iterator it = 
              delete_now.begin(); it != delete_now.end(); it++)
        {
          ReplDeletionOp *op = runtime->get_available_repl_deletion_op();
          op->initialize_field_space_deletion(this, it->space, 
                            true/*unordered*/, it->provenance);
          op->initialize_replication(this,
                            shard_manager->is_first_local_shard(owner_shard),
                            &ready_barrier,&mapped_barrier,&execution_barrier);
          op->set_deletion_preconditions(precondition, dependences);
          preconditions.insert(
              Runtime::protect_event(op->get_completion_event()));
          op->execute_dependence_analysis();
        }
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::register_index_space_deletions(ApEvent precondition,
                           const std::map<Operation*,GenerationID> &dependences,
                                         std::vector<DeletedIndexSpace> &spaces,
                                               std::set<RtEvent> &preconditions,
                                               RtBarrier &ready_barrier,
                                               RtBarrier &mapped_barrier,
                                               RtBarrier &execution_barrier)
    //--------------------------------------------------------------------------
    {
      std::vector<DeletedIndexSpace> delete_now;
      std::vector<std::vector<IndexPartition> > sub_partitions;
      {
        AutoLock priv_lock(privilege_lock);
        for (std::vector<DeletedIndexSpace>::const_iterator sit =
              spaces.begin(); sit != spaces.end(); sit++)
        {
          std::map<IndexSpace,unsigned>::iterator finder = 
            created_index_spaces.find(sit->space);
          if (finder != created_index_spaces.end())
          {
#ifdef DEBUG_LEGION
            assert(finder->second > 0);
#endif
            if (--finder->second == 0)
            {
              delete_now.push_back(*sit);
              sub_partitions.resize(sub_partitions.size() + 1);
              created_index_spaces.erase(finder);
              if (sit->recurse)
              {
                std::vector<IndexPartition> &subs = sub_partitions.back();
                // Also remove any index partitions for this index space tree
                for (std::map<IndexPartition,unsigned>::iterator it = 
                      created_index_partitions.begin(); it !=
                      created_index_partitions.end(); /*nothing*/)
                {
                  if (it->first.get_tree_id() == sit->space.get_tree_id()) 
                  {
#ifdef DEBUG_LEGION
                    assert(it->second > 0);
#endif
                    if (--it->second == 0)
                    {
                      subs.push_back(it->first);
                      std::map<IndexPartition,unsigned>::iterator 
                        to_delete = it++;
                      created_index_partitions.erase(to_delete);
                    }
                    else
                      it++;
                  }
                  else
                    it++;
                }
              }
            }
          }
          else
            // If we didn't make the index space in this context, just
            // record it and keep going, it will get handled later
            deleted_index_spaces.push_back(*sit);
        }
      }
      if (!delete_now.empty())
      {
#ifdef DEBUG_LEGION
        assert(delete_now.size() == sub_partitions.size());
#endif
        for (unsigned idx = 0; idx < delete_now.size(); idx++)
        {
          ReplDeletionOp *op = runtime->get_available_repl_deletion_op();
          op->initialize_index_space_deletion(this, delete_now[idx].space,
            sub_partitions[idx], true/*unordered*/, delete_now[idx].provenance);
          op->initialize_replication(this,
                            shard_manager->is_first_local_shard(owner_shard),
                            &ready_barrier,&mapped_barrier,&execution_barrier);
          op->set_deletion_preconditions(precondition, dependences);
          preconditions.insert(
              Runtime::protect_event(op->get_completion_event()));
          op->execute_dependence_analysis();
        }
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::register_index_partition_deletions(ApEvent precond,
                           const std::map<Operation*,GenerationID> &dependences,
                                           std::vector<DeletedPartition> &parts,
                                               std::set<RtEvent> &preconditions,
                                               RtBarrier &ready_barrier,
                                               RtBarrier &mapped_barrier,
                                               RtBarrier &execution_barrier)
    //--------------------------------------------------------------------------
    {
      std::vector<DeletedPartition> delete_now;
      std::vector<std::vector<IndexPartition> > sub_partitions;
      {
        AutoLock priv_lock(privilege_lock);
        for (std::vector<DeletedPartition>::const_iterator pit =
              parts.begin(); pit != parts.end(); pit++)
        {
          std::map<IndexPartition,unsigned>::iterator finder = 
            created_index_partitions.find(pit->partition);
          if (finder != created_index_partitions.end())
          {
#ifdef DEBUG_LEGION
            assert(finder->second > 0);
#endif
            if (--finder->second == 0)
            {
              delete_now.push_back(*pit);
              sub_partitions.resize(sub_partitions.size() + 1);
              created_index_partitions.erase(finder);
              if (pit->recurse)
              {
                std::vector<IndexPartition> &subs = sub_partitions.back();
                // Remove any other partitions that this partition dominates
                for (std::map<IndexPartition,unsigned>::iterator it = 
                      created_index_partitions.begin(); it !=
                      created_index_partitions.end(); /*nothing*/)
                {
                  if ((pit->partition.get_tree_id() == it->first.get_tree_id())
                      && runtime->forest->is_dominated_tree_only(it->first, 
                                                                pit->partition))
                  {
#ifdef DEBUG_LEGION
                    assert(it->second > 0);
#endif
                    if (--it->second == 0)
                    {
                      subs.push_back(it->first);
                      std::map<IndexPartition,unsigned>::iterator 
                        to_delete = it++;
                      created_index_partitions.erase(to_delete);
                    }
                    else
                      it++;
                  }
                  else
                    it++;
                }
              }
            }
          }
          else
            // If we didn't make the partition, record it and keep going
            deleted_index_partitions.push_back(*pit);
        }
      }
      if (!delete_now.empty())
      {
#ifdef DEBUG_LEGION
        assert(delete_now.size() == sub_partitions.size());
#endif
        for (unsigned idx = 0; idx < delete_now.size(); idx++)
        {
          ReplDeletionOp *op = runtime->get_available_repl_deletion_op();
          op->initialize_index_part_deletion(this, delete_now[idx].partition,
            sub_partitions[idx], true/*unordered*/, delete_now[idx].provenance);
          op->initialize_replication(this,
                            shard_manager->is_first_local_shard(owner_shard),
                            &ready_barrier,&mapped_barrier,&execution_barrier);
          op->set_deletion_preconditions(precond, dependences);
          preconditions.insert(
              Runtime::protect_event(op->get_completion_event()));
          op->execute_dependence_analysis();
        }
      }
    }

    //--------------------------------------------------------------------------
    CollectiveID ReplicateContext::get_next_collective_index(
                                      CollectiveIndexLocation loc, bool logical)
    //--------------------------------------------------------------------------
    {
      // No need for a lock, should only be coming from the creation
      // of operations directly from the application and therefore
      // should be deterministic
      // Count by 2s to avoid conflicts with the collectives from the 
      // logical depedence analysis stage of the pipeline
      if (logical)
      {
#ifdef DEBUG_LEGION_COLLECTIVES
        if (!logical_guard_reentrant)
        {
          CollectiveCheckReduction::RHS location = loc;
          // Guard against coming back in here when advancing the barrier
          logical_guard_reentrant = true;
          const RtBarrier logical_check_bar = logical_check_barrier.next(this,
              CollectiveCheckReduction::REDOP, 
              &CollectiveCheckReduction::IDENTITY,
              sizeof(CollectiveCheckReduction::IDENTITY));
          logical_guard_reentrant = false;
          Runtime::phase_barrier_arrive(logical_check_bar, 1/*count*/,
                             RtEvent::NO_RT_EVENT, &location, sizeof(location));
          logical_check_bar.wait();
          CollectiveCheckReduction::RHS actual_location;
          bool ready = Runtime::get_barrier_result(logical_check_bar,
                                     &actual_location, sizeof(actual_location));
          assert(ready);
          assert(location == actual_location);
        }
#endif
        const CollectiveID result = next_logical_collective_index;
        next_logical_collective_index += 2;
        return result;
      }
      else
      {
#ifdef DEBUG_LEGION_COLLECTIVES
        if (!collective_guard_reentrant)
        {
          CollectiveCheckReduction::RHS location = loc;
          // Guard against coming back in here when advancing the barrier
          collective_guard_reentrant = true;
          const RtBarrier collective_check_bar = collective_check_barrier.next(
              this, CollectiveCheckReduction::REDOP, 
              &CollectiveCheckReduction::IDENTITY,
              sizeof(CollectiveCheckReduction::IDENTITY));
          collective_guard_reentrant = false;
          Runtime::phase_barrier_arrive(collective_check_bar, 1/*count*/,
                             RtEvent::NO_RT_EVENT, &location, sizeof(location));
          collective_check_bar.wait();
          CollectiveCheckReduction::RHS actual_location;
          bool ready = Runtime::get_barrier_result(collective_check_bar,
                                     &actual_location, sizeof(actual_location));
          assert(ready);
          assert(location == actual_location);
        }
#endif
        const CollectiveID result = next_available_collective_index;
        next_available_collective_index += 2; 
        return result;
      }
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::register_collective(ShardCollective *collective)
    //--------------------------------------------------------------------------
    {
      std::vector<std::pair<void*,size_t> > to_apply;
      {
        AutoLock repl_lock(replication_lock);
#ifdef DEBUG_LEGION
        assert(collectives.find(collective->collective_index) == 
               collectives.end());
        assert(shard_manager != NULL);
#endif
        // If the collectives are empty then we add a reference to the
        // shard manager to prevent it being collected before we're
        // done handling all the collectives
        if (collectives.empty())
          shard_manager->add_nested_gc_ref(did);
        collectives[collective->collective_index] = collective;
        std::map<CollectiveID,std::vector<std::pair<void*,size_t> > >::
          iterator finder = pending_collective_updates.find(
                                                collective->collective_index);
        if (finder != pending_collective_updates.end())
        {
          to_apply.swap(finder->second);
          pending_collective_updates.erase(finder);
        }
      }
      if (!to_apply.empty())
      {
        for (std::vector<std::pair<void*,size_t> >::const_iterator it = 
              to_apply.begin(); it != to_apply.end(); it++)
        {
          Deserializer derez(it->first, it->second);
          collective->handle_collective_message(derez);
          free(it->first);
        }
      }
    }

    //--------------------------------------------------------------------------
    ShardCollective* ReplicateContext::find_or_buffer_collective(
                                                            Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      CollectiveID collective_index;
      derez.deserialize(collective_index);
      AutoLock repl_lock(replication_lock);
      // See if we already have the collective in which case we can just
      // return it, otherwise we need to buffer the deserializer
      std::map<CollectiveID,ShardCollective*>::const_iterator finder = 
        collectives.find(collective_index);
      if (finder != collectives.end())
        return finder->second;
      // If we couldn't find it then we have to buffer it for the future
      const size_t remaining_bytes = derez.get_remaining_bytes();
      void *buffer = malloc(remaining_bytes);
      memcpy(buffer, derez.get_current_pointer(), remaining_bytes);
      derez.advance_pointer(remaining_bytes);
      pending_collective_updates[collective_index].push_back(
          std::pair<void*,size_t>(buffer, remaining_bytes));
      return NULL;
    } 

    //--------------------------------------------------------------------------
    void ReplicateContext::unregister_collective(ShardCollective *collective)
    //--------------------------------------------------------------------------
    {
      bool remove_reference = false;
      {
        AutoLock repl_lock(replication_lock); 
        std::map<CollectiveID,ShardCollective*>::iterator finder =
          collectives.find(collective->collective_index);
        // Sometimes collectives are not used
        if (finder != collectives.end())
        {
          collectives.erase(finder);
          // Once we've done all our collectives then we can remove the
          // reference that we added on the shard manager
          remove_reference = collectives.empty();
        }
      }
      if (remove_reference && shard_manager->remove_nested_gc_ref(did))
        delete shard_manager;
    }

    //--------------------------------------------------------------------------
    size_t ReplicateContext::register_trace_template(
                                     ShardedPhysicalTemplate *physical_template)
    //--------------------------------------------------------------------------
    {
      size_t index;
      std::vector<PendingTemplateUpdate> to_apply;
      {
        AutoLock r_lock(replication_lock);
        index = next_physical_template_index++;
#ifdef DEBUG_LEGION
        assert(physical_templates.find(index) == physical_templates.end());
#endif
        physical_templates[index] = physical_template;
        // Check to see if we have any pending updates to perform
        std::map<size_t,std::vector<PendingTemplateUpdate> >::iterator
          finder = pending_template_updates.find(index);
        if (finder != pending_template_updates.end())
        {
          to_apply.swap(finder->second);
          pending_template_updates.erase(finder);
        }
      }
      if (!to_apply.empty())
      {
        for (std::vector<PendingTemplateUpdate>::const_iterator it = 
              to_apply.begin(); it != to_apply.end(); it++)
        {
          Deserializer derez(it->ptr, it->size);
          physical_template->handle_trace_update(derez, it->source);
          free(it->ptr);
        }
      }
      return index;
    }

    //--------------------------------------------------------------------------
    ShardedPhysicalTemplate* ReplicateContext::find_or_buffer_trace_update(
                                     Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      size_t trace_index;
      derez.deserialize(trace_index);
      AutoLock r_lock(replication_lock); 
      std::map<size_t,ShardedPhysicalTemplate*>::const_iterator finder = 
        physical_templates.find(trace_index);
      if (finder != physical_templates.end())
        return finder->second;
#ifdef DEBUG_LEGION
      assert(next_physical_template_index <= trace_index);
#endif
      // If we couldn't find it then we have to buffer it for the future
      const size_t remaining_bytes = derez.get_remaining_bytes();
      void *buffer = malloc(remaining_bytes);
      memcpy(buffer, derez.get_current_pointer(), remaining_bytes);
      derez.advance_pointer(remaining_bytes);
      pending_template_updates[trace_index].push_back(
          PendingTemplateUpdate(buffer, remaining_bytes, source));
      return NULL;
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::unregister_trace_template(size_t index)
    //--------------------------------------------------------------------------
    {
      AutoLock r_lock(replication_lock);
#ifdef DEBUG_LEGION
      std::map<size_t,ShardedPhysicalTemplate*>::iterator finder = 
        physical_templates.find(index);
      assert(finder != physical_templates.end());
      physical_templates.erase(finder);
#else
      physical_templates.erase(index);
#endif
    }

    //--------------------------------------------------------------------------
    ShardID ReplicateContext::get_next_equivalence_set_origin(void)
    //--------------------------------------------------------------------------
    {
      const ShardID result = equivalence_set_allocator_shard++;
      if (equivalence_set_allocator_shard == total_shards)
        equivalence_set_allocator_shard = 0;
      return result;
    }

    //--------------------------------------------------------------------------
    RtEvent ReplicateContext::compute_equivalence_sets(unsigned req_index,
                             const std::vector<EqSetTracker*> &targets,
                             const std::vector<AddressSpaceID> &target_spaces,
                             AddressSpaceID creation_target_space,
                             IndexSpaceExpression *expr, const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(targets.size() == target_spaces.size());
      assert(std::is_sorted(target_spaces.begin(), target_spaces.end()));
      assert(std::binary_search(target_spaces.begin(), target_spaces.end(),
                                creation_target_space));
#endif
      // Find the equivalence set tree for this region requirement
      LocalLock *tree_lock = NULL;
      EqKDTree *tree = find_equivalence_set_kd_tree(req_index, tree_lock);
      // Then ask the index space expression to traverse the tree for
      // all of its rectangles and find the equivalence sets that are needed
      FieldMaskSet<EqKDTree> to_create;
      FieldMaskSet<EquivalenceSet> eq_sets;
      std::vector<RtEvent> pending_sets;
      FieldMaskSet<EqKDTree> new_subscriptions;
      std::map<EqKDTree*,Domain> creation_rects;
      std::map<EquivalenceSet*,LegionMap<Domain,FieldMask> > creation_srcs;
      std::map<ShardID,LegionMap<Domain,FieldMask> > remote_shard_rects;
      std::vector<unsigned> new_target_references(targets.size(), 0);
      expr->compute_equivalence_sets(tree, tree_lock, mask, targets,
          target_spaces, new_target_references, eq_sets, pending_sets, 
          new_subscriptions, to_create, creation_rects, creation_srcs,
          remote_shard_rects, owner_shard->shard_id);
#ifdef DEBUG_LEGION
      assert(to_create.size() == creation_rects.size());
#endif
      // Send out messages to any shards we need to compute remotely
      for (std::map<ShardID,LegionMap<Domain,FieldMask> >::const_iterator sit =
            remote_shard_rects.begin(); sit != remote_shard_rects.end(); sit++)
      {
        const RtUserEvent ready = Runtime::create_rt_user_event();
        Serializer rez;
        rez.serialize(shard_manager->did);
        rez.serialize(sit->first);
        rez.serialize(targets.size());
        for (unsigned idx = 0; idx < targets.size(); idx++)
        {
          rez.serialize(targets[idx]);
          rez.serialize(target_spaces[idx]);
        }
        rez.serialize(creation_target_space);
        rez.serialize(req_index);
        rez.serialize(mask);
        rez.serialize<size_t>(sit->second.size());
        for (LegionMap<Domain,FieldMask>::const_iterator it =
              sit->second.begin(); it != sit->second.end(); it++)
        {
          rez.serialize(it->first);
          rez.serialize(it->second);
        }
        rez.serialize<size_t>(remote_shard_rects.size()+1);
        rez.serialize(ready);
        shard_manager->send_compute_equivalence_sets(sit->first, rez);
        pending_sets.push_back(ready);
      }
      const CollectiveMapping target_mapping(target_spaces,
                          runtime->legion_collective_radix);
      return report_equivalence_sets(target_mapping, targets,
          creation_target_space, mask, new_target_references, eq_sets,
          new_subscriptions, to_create, creation_rects, creation_srcs,
          remote_shard_rects.size() + 1, pending_sets);
    }

    //--------------------------------------------------------------------------
    RtEvent ReplicateContext::record_output_equivalence_set(
        EqSetTracker *source, AddressSpaceID source_space, unsigned req_index,
        EquivalenceSet *set, const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      LocalLock *tree_lock = NULL;
      EqKDTree *tree = find_or_create_output_set_kd_tree(req_index, tree_lock); 
      FieldMaskSet<EqKDTree> new_subscriptions;
      std::map<ShardID,LegionMap<Domain,FieldMask> > remote_shard_rects;
      unsigned references = set->set_expr->record_output_equivalence_set(tree,
          tree_lock, set, mask, source, source_space, new_subscriptions,
          remote_shard_rects, owner_shard->shard_id);
      std::vector<RtEvent> recorded_events;
      // Send out messages to any shards we need to do the recording on
      for (std::map<ShardID,LegionMap<Domain,FieldMask> >::const_iterator sit =
            remote_shard_rects.begin(); sit != remote_shard_rects.end(); sit++)
      {
        const RtUserEvent ready = Runtime::create_rt_user_event();
        Serializer rez;
        rez.serialize(shard_manager->did);
        rez.serialize(sit->first);
        rez.serialize(source);
        rez.serialize(source_space);
        rez.serialize(req_index);
        rez.serialize(set->did);
        set->pack_global_ref();
        rez.serialize<size_t>(sit->second.size());
        for (LegionMap<Domain,FieldMask>::const_iterator it =
              sit->second.begin(); it != sit->second.end(); it++)
        {
          rez.serialize(it->first);
          rez.serialize(it->second);
        }
        rez.serialize(ready);
        shard_manager->send_output_equivalence_set(sit->first, rez);
        recorded_events.push_back(ready);
      }
      if (!new_subscriptions.empty())
      {
        RtEvent recorded = report_output_registrations(source, source_space,
                                             references, new_subscriptions);
        if (recorded.exists())
          recorded_events.push_back(recorded);
      }
      return Runtime::merge_events(recorded_events);
    }

    //--------------------------------------------------------------------------
    EqKDTree* ReplicateContext::create_equivalence_set_kd_tree(
                                                           IndexSpaceNode *node)
    //--------------------------------------------------------------------------
    {
      // Tell it how many shards we have so it can create an initial spatial
      // partitioning for that number of shards
      return node->create_equivalence_set_kd_tree(total_shards);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::refine_equivalence_sets(unsigned req_index,
                        IndexSpaceNode *node, const FieldMask &refinement_mask,
                        std::vector<RtEvent> &applied_events, bool sharded)
    //--------------------------------------------------------------------------
    {
      if (sharded)
      {
        LocalLock *tree_lock = NULL;
        EqKDTree *tree = find_equivalence_set_kd_tree(req_index, tree_lock);
        std::map<ShardID,LegionMap<Domain,FieldMask> > remote_shard_rects;
        node->invalidate_shard_equivalence_set_kd_tree(tree, tree_lock,
            refinement_mask, applied_events, remote_shard_rects,
            owner_shard->shard_id);
        // If there are any remote then send them to the target shard
        for (std::map<ShardID,LegionMap<Domain,FieldMask> >::const_iterator 
              sit = remote_shard_rects.begin();
              sit != remote_shard_rects.end(); sit++)
        {
          const RtUserEvent refined_event = Runtime::create_rt_user_event();
          Serializer rez;
          rez.serialize(shard_manager->did);
          rez.serialize(sit->first);
          rez.serialize(req_index);
          rez.serialize<size_t>(sit->second.size());
          for (LegionMap<Domain,FieldMask>::const_iterator it =
                sit->second.begin(); it != sit->second.end(); it++)
          {
            rez.serialize(it->first);
            rez.serialize(it->second);
          }
          rez.serialize(refined_event);
          shard_manager->send_refine_equivalence_sets(sit->first, rez);
          applied_events.push_back(refined_event);
        }
      }
      else
        InnerContext::refine_equivalence_sets(req_index, node, refinement_mask,
                                              applied_events, sharded);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::handle_refine_equivalence_sets(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      unsigned req_index;
      derez.deserialize(req_index);
      LocalLock *tree_lock = NULL;
      EqKDTree *tree = find_equivalence_set_kd_tree(req_index, tree_lock);
      size_t num_rects;
      derez.deserialize(num_rects);
      std::vector<RtEvent> invalidated;
      // Need exclusive access for invalidations
      AutoLock t_lock(*tree_lock);
      for (unsigned idx = 0; idx < num_rects; idx++)
      {
        Domain domain;
        derez.deserialize(domain);
        FieldMask mask;
        derez.deserialize(mask);
        tree->invalidate_shard_tree(domain, mask, runtime, invalidated);
      }
      RtUserEvent done_event;
      derez.deserialize(done_event);
      if (!invalidated.empty())
        Runtime::trigger_event(done_event, Runtime::merge_events(invalidated));
      else
        Runtime::trigger_event(done_event);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::handle_compute_equivalence_sets(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      size_t num_targets;
      derez.deserialize(num_targets);
      std::vector<EqSetTracker*> targets(num_targets);
      std::vector<AddressSpaceID> target_spaces(num_targets);
      for (unsigned idx = 0; idx < num_targets; idx++)
      {
        derez.deserialize(targets[idx]);
        derez.deserialize(target_spaces[idx]);
      }
      AddressSpaceID creation_target_space;
      derez.deserialize(creation_target_space);
      unsigned req_index;
      derez.deserialize(req_index);
      FieldMask mask;
      derez.deserialize(mask);
      size_t num_rects;
      derez.deserialize(num_rects);

      FieldMaskSet<EqKDTree> to_create;
      FieldMaskSet<EquivalenceSet> eq_sets;
      std::vector<RtEvent> pending_sets;
      FieldMaskSet<EqKDTree> new_subscriptions;
      std::map<EqKDTree*,Domain> creation_rects;
      std::map<EquivalenceSet*,LegionMap<Domain,FieldMask> > creation_srcs;
      LocalLock *tree_lock = NULL;
      EqKDTree *tree = find_equivalence_set_kd_tree(req_index, tree_lock);
      std::vector<unsigned> new_target_references(num_targets, 0);
      {
        // Non-exclusive access to the tree for 
        AutoLock t_lock(*tree_lock,1,false/*exclusive*/);
        for (unsigned idx = 0; idx < num_rects; idx++)
        {
          Domain rect;
          derez.deserialize(rect);
          FieldMask rect_mask;
          derez.deserialize(rect_mask);
          tree->compute_shard_equivalence_sets(rect, rect_mask, targets, 
              target_spaces, new_target_references, eq_sets, pending_sets,
              new_subscriptions, to_create, creation_rects, creation_srcs,
              owner_shard->shard_id);
        }
      }
      size_t expected_responses;
      derez.deserialize(expected_responses);
      RtUserEvent ready_event;
      derez.deserialize(ready_event);
      // Now we can send the responses
      const CollectiveMapping target_mapping(target_spaces,
                          runtime->legion_collective_radix);
      RtEvent ready = report_equivalence_sets(target_mapping, targets,
          creation_target_space, mask, new_target_references, eq_sets,
          new_subscriptions, to_create, creation_rects, creation_srcs,
          expected_responses, pending_sets);
      Runtime::trigger_event(ready_event, ready);
    }

    //--------------------------------------------------------------------------
    void ReplicateContext::handle_output_equivalence_set(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      EqSetTracker *source;
      derez.deserialize(source);
      AddressSpaceID source_space;
      derez.deserialize(source_space);
      unsigned req_index;
      derez.deserialize(req_index);
      DistributedID did;
      derez.deserialize(did);
      RtEvent set_ready;
      EquivalenceSet *set =
        runtime->find_or_request_equivalence_set(did, set_ready);
      size_t num_rects;
      derez.deserialize(num_rects);
      
      FieldMaskSet<EqKDTree> new_subscriptions;
      LocalLock *tree_lock = NULL;
      EqKDTree *tree = find_or_create_output_set_kd_tree(req_index, tree_lock);
      if (set_ready.exists() && !set_ready.has_triggered())
        set_ready.wait();
      unsigned references = 0;
      {
        // Non exclusive accessor for recording shard output sets
        AutoLock t_lock(*tree_lock,1,false/*exclusive*/);
        for (unsigned idx = 0; idx < num_rects; idx++)
        {
          Domain rect;
          derez.deserialize(rect);
          FieldMask rect_mask;
          derez.deserialize(rect_mask);
          references += tree->record_shard_output_equivalence_set(set, rect,
            rect_mask, source, source_space, new_subscriptions,
            owner_shard->shard_id);
        }
      }
      RtUserEvent recorded_event;
      derez.deserialize(recorded_event);
      Runtime::trigger_event(recorded_event, report_output_registrations(source,
            source_space, references, new_subscriptions));
      set->unpack_global_ref();
    }

    //--------------------------------------------------------------------------
    bool ReplicateContext::create_new_replicate_barrier(RtBarrier &bar, 
#ifdef DEBUG_LEGION_COLLECTIVES
                ReductionOpID redop, const void *init, size_t init_size,
#endif
                                                        size_t arrivals)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!bar.exists());
      assert(next_replicate_bar_index < total_shards);
#endif
      bool created = false;
      ValueBroadcast<RtBarrier> 
        collective(this, next_replicate_bar_index, COLLECTIVE_LOC_83);
      if (owner_shard->shard_id == next_replicate_bar_index++)
      {
#ifdef DEBUG_LEGION_COLLECTIVES
        bar = RtBarrier(Realm::Barrier::create_barrier(arrivals, redop,
                                                       init, init_size));
#else
        bar = RtBarrier(Realm::Barrier::create_barrier(arrivals));
#endif
        collective.broadcast(bar);
        created = true;
      }
      else
        bar = collective.get_value();
      // Check to see if we need to reset the next_replicate_bar_index
      if (next_replicate_bar_index == total_shards)
       next_replicate_bar_index = 0;
      return created;
    }

    //--------------------------------------------------------------------------
    bool ReplicateContext::create_new_replicate_barrier(ApBarrier &bar,
#ifdef DEBUG_LEGION_COLLECTIVES
                ReductionOpID redop, const void *init, size_t init_size,
#endif
                                                        size_t arrivals)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!bar.exists());
      assert(next_replicate_bar_index < total_shards);
#endif
      bool created = false;
      ValueBroadcast<ApBarrier> 
        collective(this, next_replicate_bar_index, COLLECTIVE_LOC_84);
      if (owner_shard->shard_id == next_replicate_bar_index++)
      {
#ifdef DEBUG_LEGION_COLLECTIVES
        bar = ApBarrier(Realm::Barrier::create_barrier(arrivals, redop,
                                                       init, init_size));
#else
        bar = ApBarrier(Realm::Barrier::create_barrier(arrivals));
#endif
        collective.broadcast(bar);
        created = true;
      }
      else
        bar = collective.get_value();
      // Check to see if we need to reset the next_replicate_bar_index
      if (next_replicate_bar_index == total_shards)
        next_replicate_bar_index = 0;
      return created;
    }

    //--------------------------------------------------------------------------
    bool ReplicateContext::create_new_logical_barrier(RtBarrier &bar, 
#ifdef DEBUG_LEGION_COLLECTIVES
                ReductionOpID redop, const void *init, size_t init_size,
#endif
                                                      size_t arrivals)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!bar.exists());
      assert(next_logical_bar_index < total_shards);
#endif
      bool created = false;
      const CollectiveID cid =
        get_next_collective_index(COLLECTIVE_LOC_18, true/*logical*/);
      ValueBroadcast<RtBarrier> collective(cid, this, next_logical_bar_index);
      if (owner_shard->shard_id == next_logical_bar_index++)
      {
#ifdef DEBUG_LEGION_COLLECTIVES
        bar = RtBarrier(Realm::Barrier::create_barrier(arrivals, redop,
                                                       init, init_size));
#else
        bar = RtBarrier(Realm::Barrier::create_barrier(arrivals));
#endif
        collective.broadcast(bar);
        created = true;
      }
      else
        bar = collective.get_value();
      // Check to see if we need to reset the next_replicate_bar_index
      if (next_logical_bar_index == total_shards)
        next_logical_bar_index = 0;
      return created;
    }

    //--------------------------------------------------------------------------
    bool ReplicateContext::create_new_logical_barrier(ApBarrier &bar, 
#ifdef DEBUG_LEGION_COLLECTIVES
                ReductionOpID redop, const void *init, size_t init_size,
#endif
                                                      size_t arrivals)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!bar.exists());
      assert(next_logical_bar_index < total_shards);
#endif
      bool created = false;
      const CollectiveID cid =
        get_next_collective_index(COLLECTIVE_LOC_24, true/*logical*/);
      ValueBroadcast<ApBarrier> collective(cid, this, next_logical_bar_index);
      if (owner_shard->shard_id == next_logical_bar_index++)
      {
#ifdef DEBUG_LEGION_COLLECTIVES
        bar = ApBarrier(Realm::Barrier::create_barrier(arrivals, redop,
                                                       init, init_size));
#else
        bar = ApBarrier(Realm::Barrier::create_barrier(arrivals));
#endif
        collective.broadcast(bar);
        created = true;
      }
      else
        bar = collective.get_value();
      // Check to see if we need to reset the next_replicate_bar_index
      if (next_logical_bar_index == total_shards)
        next_logical_bar_index = 0;
      return created;
    }

    //--------------------------------------------------------------------------
    const DomainPoint& ReplicateContext::get_shard_point(void) const
    //--------------------------------------------------------------------------
    {
      return shard_manager->shard_points[owner_shard->shard_id];
    }

    //--------------------------------------------------------------------------
    ShardID ReplicateContext::AttachDetachShardingFunctor::shard(
      const DomainPoint &point, const Domain &domain, const size_t total_shards)
    //--------------------------------------------------------------------------
    {
      const Point<2> p = point; 
      return p[0]; // First dimension is always the shard dimension
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReplicateContext::register_attach_detach_sharding_functor(
                                                               Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      // See Runtime::get_current_static_sharding_id for how we get this ID
      runtime->register_sharding_functor(LEGION_MAX_APPLICATION_SHARDING_ID,
          new AttachDetachShardingFunctor(), false/*need check*/,
          true/*silence warnings*/, NULL, true/*preregistered*/);
    }

    //--------------------------------------------------------------------------
    ShardingFunction* 
                     ReplicateContext::get_attach_detach_sharding_function(void)
    //--------------------------------------------------------------------------
    {
      // See Runtime::get_current_static_sharding_id for how we get this ID
      return shard_manager->find_sharding_function(
                LEGION_MAX_APPLICATION_SHARDING_ID);
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode* ReplicateContext::compute_index_attach_launch_spaces(
                       std::vector<size_t> &shard_sizes, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      // No need for a lock, we're in the logical dependence stage
      for (std::vector<AttachLaunchSpace*>::const_iterator it =
            index_attach_launch_spaces.begin(); it !=
            index_attach_launch_spaces.end(); it++)
      {
        const AttachLaunchSpace *space = *it;
#ifdef DEBUG_LEGION
        assert(space->shard_sizes.size() == shard_sizes.size());
#endif
        bool match = true;
        for (unsigned idx = 0; idx < shard_sizes.size(); idx++)
        {
          if (space->shard_sizes[idx] == shard_sizes[idx])
            continue;
          match = false;
          break;
        }
        if (match)
          return space->launch_space;
      }
      // Make the index space first
      // See if we can make this a rect or a ragged collection of rects
      coord_t upper_bound = 0;
      for (std::vector<size_t>::const_iterator it =
            shard_sizes.begin(); it != shard_sizes.end(); it++)
      {
        if (it != shard_sizes.begin())
        {
          if (coord_t(*it) != upper_bound)
            upper_bound = -1;
        }
        else
          upper_bound = *it;
      }
      IndexSpace handle;
      if (upper_bound > 0)
      {
        const Domain domain =
          Rect<2>(Point<2>(0,0),Point<2>(shard_sizes.size()-1,upper_bound-1));
        handle = InnerContext::create_index_space(domain,
            NT_TemplateHelper::encode_tag<2,coord_t>(), provenance);
      }
      else
      {
        std::vector<Rect<2> > rects(shard_sizes.size());
        // Use prefix sum here so we know where each shard begins and ends
        // in the color space of the projection
        coord_t offset = 0;
        for (unsigned idx = 0; idx < shard_sizes.size(); idx++)
        {
          rects[idx] = Rect<2>(Point<2>(idx, offset),
                               Point<2>(idx, offset + shard_sizes[idx] - 1));
          offset += shard_sizes[idx];
        }
        const Domain domain = Realm::IndexSpace<2,coord_t>(rects);
        handle = InnerContext::create_index_space(domain,
            NT_TemplateHelper::encode_tag<2,coord_t>(), provenance);
      }
      IndexSpaceNode *node = runtime->forest->get_node(handle);
      AttachLaunchSpace *space = new AttachLaunchSpace(node);
      space->shard_sizes.swap(shard_sizes);
      index_attach_launch_spaces.push_back(space);
      return node;
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReplicateContext::register_universal_sharding_functor(
                                                               Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      // See Runtime::get_current_static_sharding_id for how we get this ID
      runtime->register_sharding_functor(LEGION_MAX_APPLICATION_SHARDING_ID + 1,
          new UniversalShardingFunctor(), false/*need check*/,
          true/*silence warnings*/, NULL, true/*preregistered*/);
    }

    //--------------------------------------------------------------------------
    ShardingFunction* ReplicateContext::get_universal_sharding_function(void)
    //--------------------------------------------------------------------------
    {
      // See Runtime::get_current_static_sharding_id for how we get this ID
      // Note the universal sharding function is special and can skip checks
      // on the output because it's not actually used for sharding
      return shard_manager->find_sharding_function(
                LEGION_MAX_APPLICATION_SHARDING_ID + 1, true/*skip checks*/);
    }

    /////////////////////////////////////////////////////////////
    // Remote Task 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RemoteTask::RemoteTask(RemoteContext *own)
      : owner(own), context_index(0)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RemoteTask::RemoteTask(const RemoteTask &rhs)
      : owner(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    RemoteTask::~RemoteTask(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RemoteTask& RemoteTask::operator=(const RemoteTask &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    UniqueID RemoteTask::get_unique_id(void) const
    //--------------------------------------------------------------------------
    {
      return owner->get_unique_id();
    }

    //--------------------------------------------------------------------------
    Domain RemoteTask::get_slice_domain(void) const
    //--------------------------------------------------------------------------
    {
      return Domain(index_point, index_point);
    }

    //--------------------------------------------------------------------------
    ShardID RemoteTask::get_shard_id(void) const
    //--------------------------------------------------------------------------
    {
      return owner->shard_id;
    }

    //--------------------------------------------------------------------------
    size_t RemoteTask::get_total_shards(void) const
    //--------------------------------------------------------------------------
    {
      return owner->total_shards;
    }

    //--------------------------------------------------------------------------
    DomainPoint RemoteTask::get_shard_point(void) const
    //--------------------------------------------------------------------------
    {
      return owner->shard_point;
    }

    //--------------------------------------------------------------------------
    Domain RemoteTask::get_shard_domain(void) const
    //--------------------------------------------------------------------------
    {
      return owner->shard_domain;
    }

    //--------------------------------------------------------------------------
    size_t RemoteTask::get_context_index(void) const
    //--------------------------------------------------------------------------
    {
      return context_index;
    }

    //--------------------------------------------------------------------------
    void RemoteTask::set_context_index(size_t index)
    //--------------------------------------------------------------------------
    {
      context_index = index;
    }

    //--------------------------------------------------------------------------
    bool RemoteTask::has_parent_task(void) const
    //--------------------------------------------------------------------------
    {
      return (get_depth() > 0);
    }

    //--------------------------------------------------------------------------
    const Task* RemoteTask::get_parent_task(void) const
    //--------------------------------------------------------------------------
    {
      if ((parent_task == NULL) && has_parent_task())
        parent_task = owner->get_parent_task();
      return parent_task;
    }

    //--------------------------------------------------------------------------
    const std::string& RemoteTask::get_provenance_string(bool human) const
    //--------------------------------------------------------------------------
    {
      Provenance *provenance = owner->get_provenance();
      if (provenance != NULL)
        return human ? provenance->human : provenance->machine;
      else
        return Provenance::no_provenance;
    }
    
    //--------------------------------------------------------------------------
    int RemoteTask::get_depth(void) const
    //--------------------------------------------------------------------------
    {
      return owner->get_depth();
    }

    //--------------------------------------------------------------------------
    const char* RemoteTask::get_task_name(void) const
    //--------------------------------------------------------------------------
    {
      TaskImpl *task_impl = owner->runtime->find_task_impl(task_id);
      return task_impl->get_name();
    }

    //--------------------------------------------------------------------------
    bool RemoteTask::has_trace(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    /////////////////////////////////////////////////////////////
    // Remote Context 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RemoteContext::RemoteContext(DistributedID id, Runtime *rt,
                                 CollectiveMapping *mapping)
      : InnerContext(rt, NULL, -1, false/*full inner*/, remote_task.regions,
                     remote_task.output_regions, local_parent_req_indexes,
                     local_virtual_mapped, ApEvent::NO_AP_EVENT, id,
                     false, false, false, mapping),
        parent_ctx(NULL), shard_manager(NULL), provenance(NULL),
        top_level_context(false), remote_task(RemoteTask(this)),
        remote_uid(0), repl_id(0)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RemoteContext::~RemoteContext(void)
    //--------------------------------------------------------------------------
    {
      if (!local_field_infos.empty())
      {
        // If we have any local fields then tell field space that
        // we can remove them and then clear them 
        for (std::map<FieldSpace,std::vector<LocalFieldInfo> >::const_iterator
              it = local_field_infos.begin(); 
              it != local_field_infos.end(); it++)
        {
          const std::vector<LocalFieldInfo> &infos = it->second;
          std::vector<FieldID> to_remove;
          for (unsigned idx = 0; idx < infos.size(); idx++)
          {
            if (infos[idx].ancestor)
              continue;
            to_remove.push_back(infos[idx].fid);
          }
          if (!to_remove.empty())
            runtime->forest->remove_local_fields(it->first, to_remove);
        }
        local_field_infos.clear();
      } 
      if ((provenance != NULL) && provenance->remove_reference())
        delete provenance;
    }

    //--------------------------------------------------------------------------
    Task* RemoteContext::get_task(void)
    //--------------------------------------------------------------------------
    {
      return &remote_task;
    }

    //--------------------------------------------------------------------------
    UniqueID RemoteContext::get_unique_id(void) const
    //--------------------------------------------------------------------------
    {
      return remote_uid;
    }

    //--------------------------------------------------------------------------
    InnerContext* RemoteContext::find_top_context(InnerContext *previous)
    //--------------------------------------------------------------------------
    {
      if (!top_level_context)
        return find_parent_context()->find_top_context(this);
 #ifdef DEBUG_LEGION
      assert(previous != NULL);
#endif
      return previous;     
    }
    
    //--------------------------------------------------------------------------
    InnerContext* RemoteContext::find_parent_context(void)
    //--------------------------------------------------------------------------
    {
      if (top_level_context)
        return NULL;
      // See if we already have it
      InnerContext *result = parent_ctx.load();
      if (result != NULL)
        return result;
#ifdef DEBUG_LEGION
      assert(parent_context_did != 0);
#endif
      // THIS IS ONLY SAFE BECAUSE THIS FUNCTION IS NEVER CALLED BY
      // A MESSAGE IN THE CONTEXT_VIRTUAL_CHANNEL
      result = runtime->find_or_request_inner_context(parent_context_did);
#ifdef DEBUG_LEGION
      assert(result != NULL);
#endif
      if (parent_ctx.exchange(result) == NULL)
        remote_task.parent_task = result->get_task();
      return result;
    }

    //--------------------------------------------------------------------------
    RtEvent RemoteContext::compute_equivalence_sets(unsigned req_index,
                       const std::vector<EqSetTracker*> &targets,
                       const std::vector<AddressSpaceID> &target_spaces,
                       AddressSpaceID creation_target_space,
                       IndexSpaceExpression *expr, const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!top_level_context);
      assert(targets.size() == 1);
      assert(targets.size() == target_spaces.size());
      assert(creation_target_space == runtime->address_space);
      // should always be local
      assert(target_spaces.front() == runtime->address_space); 
#endif
      RtUserEvent ready_event = Runtime::create_rt_user_event();
      // Send off a request to the owner node to handle it
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        rez.serialize(targets.front());
        expr->pack_expression(rez, owner_space);
        rez.serialize(mask);
        rez.serialize(req_index);
        rez.serialize(ready_event);
      }
      // Send it to the owner space 
      runtime->send_compute_equivalence_sets_request(owner_space, rez);
      return ready_event;
    }

    //--------------------------------------------------------------------------
    RtEvent RemoteContext::record_output_equivalence_set(EqSetTracker *source,
                              AddressSpaceID source_space, unsigned req_index,
                              EquivalenceSet *set, const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      const RtUserEvent recorded = Runtime::create_rt_user_event();
      Serializer rez;
      {
        RezCheck z(rez);
        pack_inner_context(rez);
        rez.serialize(source);
        rez.serialize(source_space);
        rez.serialize(req_index);
        rez.serialize(set->did);
        set->pack_global_ref();
        rez.serialize(mask);
        rez.serialize(recorded);
      }
      runtime->send_output_equivalence_set_request(owner_space, rez);
      return recorded;
    }

    //--------------------------------------------------------------------------
    InnerContext* RemoteContext::find_parent_physical_context(unsigned index)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(regions.size() == virtual_mapped.size());
      assert(regions.size() == parent_req_indexes.size());
#endif     
      if (index < virtual_mapped.size())
      {
        // See if it is virtual mapped
        if (virtual_mapped[index])
          return find_parent_context()->find_parent_physical_context(
                                            parent_req_indexes[index]);
        else // We mapped a physical instance so we're it
          return this;
      }
      else // We created it
      {
        // But we're the remote note, so we don't have updated created
        // requirements or returnable privileges so we need to see if
        // we already know the answer and if not, ask the owner context
        RtEvent wait_on;
        RtUserEvent request;
        {
          AutoLock rem_lock(remote_lock);
          std::map<unsigned,InnerContext*>::const_iterator finder = 
            physical_contexts.find(index);
          if (finder != physical_contexts.end())
            return finder->second;
          std::map<unsigned,RtEvent>::const_iterator pending_finder = 
            pending_physical_contexts.find(index);
          if (pending_finder == pending_physical_contexts.end())
          {
            // Make a new request
            request = Runtime::create_rt_user_event();
            pending_physical_contexts[index] = request;
            wait_on = request;
          }
          else // Already sent it so just get the wait event
            wait_on = pending_finder->second;
        }
        if (request.exists())
        {
          // Send the request
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize(index);
            rez.serialize(this);
            rez.serialize(request);
          }
          runtime->send_remote_context_physical_request(owner_space, rez);
        }
        // Wait for the result to come back to us
        wait_on.wait();
        // When we wake up it should be there
        AutoLock rem_lock(remote_lock, 1, false/*exclusive*/);
#ifdef DEBUG_LEGION
        assert(physical_contexts.find(index) != physical_contexts.end());
#endif
        return physical_contexts[index]; 
      }
    }

    //--------------------------------------------------------------------------
    void RemoteContext::pack_inner_context(Serializer &rez) const
    //--------------------------------------------------------------------------
    {
      rez.serialize(did); // pack our distributed ID
      rez.serialize<DistributedID>(repl_id); // shard manager ID
    }

    //--------------------------------------------------------------------------
    InnerContext::CollectiveResult* 
      RemoteContext::find_or_create_collective_view(RegionTreeID tid,
          const std::vector<DistributedID> &instances, RtEvent &ready)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(instances.size() > 1);
#endif
      const RtUserEvent to_trigger = Runtime::create_rt_user_event();
      CollectiveResult *result = new CollectiveResult(instances);
      result->add_reference();
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        rez.serialize(tid);
        rez.serialize<size_t>(instances.size());
        for (unsigned idx = 0; idx < instances.size(); idx++)
          rez.serialize(instances[idx]);
        rez.serialize(result);
        rez.serialize(to_trigger);
      }
      runtime->send_remote_context_find_collective_view_request(owner_space, 
                                                                rez);
      ready = to_trigger;
      return result;
    }

    //--------------------------------------------------------------------------
    void RemoteContext::invalidate_region_tree_contexts(
                       const bool is_top_level_task, std::set<RtEvent> &applied,
                       const ShardMapping *mapping, ShardID source_shard)
    //--------------------------------------------------------------------------
    {
      // Should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    void RemoteContext::receive_created_region_contexts(
                        const std::vector<RegionNode*> &created_nodes,
                        const std::vector<EqKDTree*> &created_trees,
                        std::set<RtEvent> &applied_events,
                        const ShardMapping *shard_mapping, ShardID source_shard)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(created_nodes.size() == created_trees.size());
#endif
      const RtUserEvent done_event = Runtime::create_rt_user_event();
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        if (shard_mapping != NULL)
        {
          shard_mapping->pack_mapping(rez);
          rez.serialize(source_shard);
        }
        else
          ShardMapping::pack_empty(rez);
        rez.serialize<size_t>(created_nodes.size());
        for (unsigned idx = 0; idx < created_nodes.size(); idx++)
        {
          RegionNode *region = created_nodes[idx];
          rez.serialize(region->handle);
          FieldMaskSet<EquivalenceSet> eq_sets;
          created_trees[idx]->find_local_equivalence_sets(eq_sets,source_shard);
          rez.serialize<size_t>(eq_sets.size());
          for (FieldMaskSet<EquivalenceSet>::const_iterator it =
                eq_sets.begin(); it != eq_sets.end(); it++)
          {
            it->first->pack_global_ref();
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
        }
        rez.serialize(done_event);
      }
      pack_global_ref();
      runtime->send_created_region_contexts(owner_space, rez);
      applied_events.insert(done_event);
    }

    //--------------------------------------------------------------------------
    /*static*/ void RemoteContext::handle_created_region_contexts(
                   Runtime *runtime, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID context_did;
      derez.deserialize(context_did);
      ShardMapping src_mapping;
      src_mapping.unpack_mapping(derez);
      ShardID source_shard = 0;
      if (!src_mapping.empty())
        derez.deserialize(source_shard);
      size_t num_regions;
      derez.deserialize(num_regions);
      std::vector<RegionNode*> created_nodes(num_regions);
      std::vector<EqKDTree*> created_trees(num_regions);
      std::set<RtEvent> applied_events;
      for (unsigned idx1 = 0; idx1 < num_regions; idx1++)
      {
        LogicalRegion handle;
        derez.deserialize(handle);
        RegionNode *node = runtime->forest->get_node(handle);
        created_nodes[idx1] = node;
        EqKDTree *tree = node->row_source->create_equivalence_set_kd_tree(
                            src_mapping.empty() ? 1 : src_mapping.size());
        size_t num_sets;
        derez.deserialize(num_sets);
        for (unsigned idx2 = 0; idx2 < num_sets; idx2++)
        {
          DistributedID did;
          derez.deserialize(did);
          RtEvent ready;
          EquivalenceSet *set = 
            runtime->find_or_request_equivalence_set(did, ready);
          FieldMask mask;
          derez.deserialize(mask);
          if (ready.exists() && !ready.has_triggered())
            ready.wait();
          set->set_expr->initialize_equivalence_set_kd_tree(
              tree, set, mask, source_shard, true/*current*/);
          set->unpack_global_ref();
        }
        created_trees[idx1] = tree;
      }
      RtUserEvent done_event;
      derez.deserialize(done_event);

      InnerContext *context = static_cast<InnerContext*>(
          runtime->find_distributed_collectable(context_did));
      context->receive_created_region_contexts(created_nodes,
          created_trees, applied_events, 
          src_mapping.empty() ? NULL : &src_mapping, source_shard);
      if (!applied_events.empty())
        Runtime::trigger_event(done_event, 
            Runtime::merge_events(applied_events));
      else
        Runtime::trigger_event(done_event);
      context->unpack_global_ref();
    }

    //--------------------------------------------------------------------------
    void RemoteContext::unpack_remote_context(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DETAILED_PROFILER(runtime, REMOTE_UNPACK_CONTEXT_CALL);
      derez.deserialize(depth);
      top_level_context = (depth < 0);
      // If we're the top-level context then we're already done
      if (top_level_context)
        return;
      remote_task.unpack_external_task(derez, runtime);
      local_parent_req_indexes.resize(remote_task.regions.size()); 
      for (unsigned idx = 0; idx < local_parent_req_indexes.size(); idx++)
        derez.deserialize(local_parent_req_indexes[idx]);
      size_t num_virtual;
      derez.deserialize(num_virtual);
      local_virtual_mapped.resize(regions.size(), false);
      for (unsigned idx = 0; idx < num_virtual; idx++)
      {
        unsigned index;
        derez.deserialize(index);
        local_virtual_mapped[index] = true;
      }
      derez.deserialize(parent_context_did);
      size_t num_coordinates;
      derez.deserialize(num_coordinates);
      context_coordinates.resize(num_coordinates);
      for (unsigned idx = 0; idx < num_coordinates; idx++)
        context_coordinates[idx].deserialize(derez);
      provenance = Provenance::deserialize(derez);
      if (provenance != NULL)
        provenance->add_reference();
      derez.deserialize(remote_uid);
      // Unpack any local fields that we have
      unpack_local_field_update(derez);
      derez.deserialize(concurrent_context);
      bool replicate;
      derez.deserialize(replicate);
      if (replicate)
      {
        derez.deserialize(shard_id);
        derez.deserialize(total_shards);
        derez.deserialize(shard_point);
        derez.deserialize(shard_domain);
        derez.deserialize(repl_id);
        // See if we have a local shard manager
        shard_manager = runtime->find_shard_manager(repl_id, true/*can fail*/);
      }
      // See if we can find our parent task, if not don't worry about it
      // DO NOT CHANGE THIS UNLESS YOU THINK REALLY HARD ABOUT VIRTUAL 
      // CHANNELS AND HOW CONTEXT META-DATA IS MOVED!
      InnerContext *parent = static_cast<InnerContext*>(
        runtime->weak_find_distributed_collectable(parent_context_did));
      if (parent != NULL)
      {
        parent_ctx.store(parent);
        remote_task.parent_task = parent->get_task();
        if (parent->remove_base_resource_ref(RUNTIME_REF))
          delete parent;
      }
    }

    //--------------------------------------------------------------------------
    const Task* RemoteContext::get_parent_task(void)
    //--------------------------------------------------------------------------
    {
      // Note that it safe to actually perform the find_context call here
      // because we are no longer in the virtual channel for unpacking
      // remote contexts therefore we can page in the context
      InnerContext *parent = parent_ctx.load();
      if (parent == NULL)
      {
        parent = runtime->find_or_request_inner_context(parent_context_did);
        const Task *result = parent->get_task();
        if (parent_ctx.exchange(parent) == NULL)
          remote_task.parent_task = result;
        return result;
      }
      else
        return parent->get_task();
    }

    //--------------------------------------------------------------------------
    void RemoteContext::unpack_local_field_update(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      size_t num_field_spaces;
      derez.deserialize(num_field_spaces);
      if (num_field_spaces == 0)
        return;
      for (unsigned fidx = 0; fidx < num_field_spaces; fidx++)
      {
        FieldSpace handle;
        derez.deserialize(handle);
        Provenance *provenance = Provenance::deserialize(derez);
        if (provenance != NULL)
          provenance->add_reference();
        size_t num_local;
        derez.deserialize(num_local); 
        std::vector<FieldID> fields(num_local);
        std::vector<size_t> field_sizes(num_local);
        std::vector<CustomSerdezID> serdez_ids(num_local);
        std::vector<unsigned> indexes(num_local);
        {
          // Take the lock for updating this data structure
          AutoLock local_lock(local_field_lock);
          std::vector<LocalFieldInfo> &infos = local_field_infos[handle];
          infos.resize(num_local);
          for (unsigned idx = 0; idx < num_local; idx++)
          {
            LocalFieldInfo &info = infos[idx];
            derez.deserialize(info);
            // Update data structures for notifying the field space
            fields[idx] = info.fid;
            field_sizes[idx] = info.size;
            serdez_ids[idx] = info.serdez;
            indexes[idx] = info.index;
          }
        }
        runtime->forest->update_local_fields(handle, fields, field_sizes,
                                             serdez_ids, indexes, provenance);
        if ((provenance != NULL) && provenance->remove_reference())
          delete provenance;
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void RemoteContext::handle_local_field_update(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      RemoteContext *context = static_cast<RemoteContext*>(
          runtime->find_or_request_inner_context(did));
      context->unpack_local_field_update(derez);
      RtUserEvent done_event;
      derez.deserialize(done_event);
      Runtime::trigger_event(done_event);
      context->unpack_global_ref();
    }

    //--------------------------------------------------------------------------
    /*static*/ void RemoteContext::handle_physical_request(Deserializer &derez,
                                        Runtime *runtime, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID context_did;
      derez.deserialize(context_did);
      unsigned index;
      derez.deserialize(index);
      RemoteContext *target;
      derez.deserialize(target);
      RtUserEvent to_trigger;
      derez.deserialize(to_trigger);
      RtEvent ctx_ready;
      InnerContext *local = runtime->find_or_request_inner_context(context_did);
      InnerContext *result = local->find_parent_physical_context(index);
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(target);
        rez.serialize(index);
        result->pack_inner_context(rez);
        rez.serialize(to_trigger);
      }
      runtime->send_remote_context_physical_response(source, rez);
    }

    //--------------------------------------------------------------------------
    void RemoteContext::set_physical_context_result(unsigned index,
                                                    InnerContext *result)
    //--------------------------------------------------------------------------
    {
      AutoLock rem_lock(remote_lock);
#ifdef DEBUG_LEGION
      assert(physical_contexts.find(index) == physical_contexts.end());
#endif
      physical_contexts[index] = result;
      std::map<unsigned,RtEvent>::iterator finder = 
        pending_physical_contexts.find(index);
#ifdef DEBUG_LEGION
      assert(finder != pending_physical_contexts.end());
#endif
      pending_physical_contexts.erase(finder);
    }

    //--------------------------------------------------------------------------
    /*static*/ void RemoteContext::handle_physical_response(Deserializer &derez,
                                                            Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      RemoteContext *target;
      derez.deserialize(target);
      unsigned index;
      derez.deserialize(index);
      InnerContext *result = unpack_inner_context(derez, runtime);
      RtUserEvent to_trigger;
      derez.deserialize(to_trigger);
      target->set_physical_context_result(index, result);
      Runtime::trigger_event(to_trigger);
    }

    //--------------------------------------------------------------------------
    /*static*/ void RemoteContext::handle_find_collective_view_request(
                   Deserializer &derez, Runtime *runtime, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID context_did;
      derez.deserialize(context_did);
      InnerContext *local = runtime->find_or_request_inner_context(context_did);
      RegionTreeID tid;
      derez.deserialize(tid);
      size_t num_insts;
      derez.deserialize(num_insts);
      std::vector<DistributedID> instances(num_insts);
      for (unsigned idx = 0; idx < num_insts; idx++)
        derez.deserialize(instances[idx]);
      CollectiveResult *target;
      derez.deserialize(target);
      RtUserEvent to_trigger;
      derez.deserialize(to_trigger);

      RtEvent result_ready;
      CollectiveResult *result = local->find_or_create_collective_view(tid, 
                                                  instances, result_ready);
      if (result_ready.exists() && !result_ready.has_triggered())
        result_ready.wait();
      Serializer rez;
      {
        RezCheck z2(rez);
        rez.serialize(target);
        rez.serialize(result->collective_did);
        rez.serialize(result->ready_event);
        rez.serialize(to_trigger);
      }
      runtime->send_remote_context_find_collective_view_response(source, rez);
      if (result->remove_reference())
        delete result;
    }

    //--------------------------------------------------------------------------
    /*static*/ void RemoteContext::handle_find_collective_view_response(
                                          Deserializer &derez, Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      CollectiveResult *target;
      derez.deserialize(target);
      derez.deserialize(target->collective_did);
      derez.deserialize(target->ready_event);
      RtUserEvent to_trigger;
      derez.deserialize(to_trigger);
#ifdef DEBUG_LEGION
      assert(to_trigger.exists());
#endif
      Runtime::trigger_event(to_trigger);
    }

    //--------------------------------------------------------------------------
    /*static*/ void RemoteContext::handle_context_request(Deserializer &derez,
                                                          Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      AddressSpaceID source;
      derez.deserialize(source);
      DistributedCollectable *dc = runtime->find_distributed_collectable(did);
#ifdef DEBUG_LEGION
      InnerContext *context = dynamic_cast<InnerContext*>(dc);
      assert(context != NULL);
#else
      InnerContext *context = static_cast<InnerContext*>(dc);
#endif
      context->send_context(source);
    }

    //--------------------------------------------------------------------------
    /*static*/ void RemoteContext::handle_context_response(Deserializer &derez,
                                                           Runtime *runtime)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      void *location = runtime->find_or_create_pending_collectable_location<
                                                          RemoteContext>(did);
      RemoteContext *context = new(location) RemoteContext(did, runtime);
      context->unpack_remote_context(derez);
      context->register_with_runtime();
    }

    /////////////////////////////////////////////////////////////
    // Leaf Context 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    LeafContext::LeafContext(Runtime *rt, SingleTask *owner, bool inline_task)
      : TaskContext(rt, owner, owner->get_depth(), owner->regions,
                    owner->output_regions, LEGION_DISTRIBUTED_HELP_ENCODE(
                      rt->get_available_distributed_id(), LEAF_CONTEXT_DC),
                    false/*perform registration*/, inline_task),
        inlined_tasks(0)
    //--------------------------------------------------------------------------
    {
#ifdef LEGION_GC
      log_garbage.info("GC Leaf Context %lld %d", 
          LEGION_DISTRIBUTED_ID_FILTER(this->did), local_space); 
#endif
    }

    //--------------------------------------------------------------------------
    LeafContext::~LeafContext(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    ContextID LeafContext::get_logical_tree_context(void) const
    //--------------------------------------------------------------------------
    {
      assert(false);
      return 0;
    }

    //--------------------------------------------------------------------------
    ContextID LeafContext::get_physical_tree_context(void) const
    //--------------------------------------------------------------------------
    {
      assert(false);
      return 0;
    }

    //--------------------------------------------------------------------------
    void LeafContext::compute_task_tree_coordinates(
                                         TaskTreeCoordinates &coordinates) const
    //--------------------------------------------------------------------------
    {
      InnerContext *parent_ctx = owner_task->get_context();
      parent_ctx->compute_task_tree_coordinates(coordinates);
      coordinates.push_back(ContextCoordinate(
            owner_task->get_context_index(), owner_task->index_point));
    }

    //--------------------------------------------------------------------------
    bool LeafContext::attempt_children_complete(void)
    //--------------------------------------------------------------------------
    {
      AutoLock leaf(leaf_lock);
      if (task_executed && !children_complete_invoked)
      {
        children_complete_invoked = true;
        return true;
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool LeafContext::attempt_children_commit(void)
    //--------------------------------------------------------------------------
    {
      AutoLock leaf(leaf_lock);
      if (task_executed && !children_commit_invoked)
      {
        children_commit_invoked = true;
        return true;
      }
      return false;
    }

    //--------------------------------------------------------------------------
    void LeafContext::inline_child_task(TaskOp *child)
    //--------------------------------------------------------------------------
    {
      if (runtime->check_privileges)
        child->perform_privilege_checks();
      if (runtime->legion_spy_enabled)
        LegionSpy::log_inline_task(child->get_unique_id());
      // Find the mapped physical regions associated with each of the
      // child task's region requirements. If they aren't mapped then
      // we need a mapping fence to ensure that all the mappings are
      // done before we attempt to run this task. If they are all mapped
      // though then we can run this right away.
      std::vector<PhysicalRegion> child_regions(child->regions.size());
      for (unsigned childidx = 0; childidx < child_regions.size(); childidx++)
      {
        const RegionRequirement &child_req = child->regions[childidx];
#ifdef DEBUG_LEGION
        bool found = false;
#endif
        for (unsigned our_idx = 0; our_idx < physical_regions.size(); our_idx++)
        {
          if (!physical_regions[our_idx].is_mapped())
            continue;
          const RegionRequirement &our_req = regions[our_idx];
          const RegionTreeID our_tid = our_req.region.get_tree_id();
          const IndexSpace our_space = our_req.region.get_index_space();
          const RegionUsage our_usage(our_req);
          if (!check_region_dependence(our_tid, our_space, our_req,
                  our_usage, child_req, false/*ignore privileges*/))
            continue;
          child_regions[childidx] = physical_regions[our_idx];
#ifdef DEBUG_LEGION
          found = true;
#endif
          break;
        }
#ifdef DEBUG_LEGION
        assert(found);
#endif
      }
      // Now select the variant for task based on the regions 
      std::deque<InstanceSet> physical_instances(child_regions.size());
      VariantImpl *variant = 
        select_inline_variant(child, child_regions, physical_instances); 
      child->perform_inlining(variant, physical_instances);
      // Finish the inlining of the child task to execute, note this doesn't
      // wait for the effects of the children to be done, it just blocks to
      // make sure the code for the children are done running on this processor
      wait_for_inlined();
    }

    //--------------------------------------------------------------------------
    VariantImpl* LeafContext::select_inline_variant(TaskOp *child,
                              const std::vector<PhysicalRegion> &parent_regions,
                              std::deque<InstanceSet> &physical_instances)
    //--------------------------------------------------------------------------
    {
      VariantImpl *variant_impl = TaskContext::select_inline_variant(child,
                                        parent_regions, physical_instances);
      if (!variant_impl->is_leaf())
      {
        MapperManager *child_mapper = 
          runtime->find_mapper(executing_processor, child->map_id);
        REPORT_LEGION_ERROR(ERROR_INVALID_MAPPER_OUTPUT,
                      "Invalid mapper output from invoction of "
                      "'select_task_variant' on mapper %s. Mapper selected "
                      "an invalid variant ID %d for inlining of task %s "
                      "(UID %lld). Parent task %s (UID %lld) is a leaf task "
                      "but mapper selected non-leaf variant %d for task %s.",
                      child_mapper->get_mapper_name(),
                      variant_impl->vid, child->get_task_name(), 
                      child->get_unique_id(), owner_task->get_task_name(),
                      owner_task->get_unique_id(), variant_impl->vid,
                      child->get_task_name())
      }
      return variant_impl;
    }

    //--------------------------------------------------------------------------
    bool LeafContext::is_leaf_context(void) const
    //--------------------------------------------------------------------------
    {
      return true;
    }

    //--------------------------------------------------------------------------
    void LeafContext::return_resources(ResourceTracker *target, 
                                       size_t return_index,
                                       std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      // Nothing to do
    }

    //--------------------------------------------------------------------------
    void LeafContext::pack_return_resources(Serializer &rez, 
                                            size_t return_index)
    //--------------------------------------------------------------------------
    {
      ResourceTracker::pack_empty_resources(rez, return_index);
    }

    //--------------------------------------------------------------------------
    void LeafContext::log_created_requirements(void)
    //--------------------------------------------------------------------------
    {
      // Nothing to do
    }

    //--------------------------------------------------------------------------
    void LeafContext::report_leaks_and_duplicates(
                                               std::set<RtEvent> &preconditions)
    //--------------------------------------------------------------------------
    {
      // Nothing to do
    }

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::create_index_space(const Domain &bounds, 
                                       TypeTag type_tag, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal index space creation performed in leaf task %s (ID %lld)",
        get_task_name(), get_unique_id())
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::create_index_space(
                 const std::vector<DomainPoint> &points, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal index space creation performed in leaf task %s (ID %lld)",
        get_task_name(), get_unique_id())
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::create_index_space(const std::vector<Domain> &rects,
                                               Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal index space creation performed in leaf task %s (ID %lld)",
        get_task_name(), get_unique_id())
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::create_index_space(const Future &f, TypeTag tag,
                                               Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal index space creation performed in leaf task %s (ID %lld)",
        get_task_name(), get_unique_id())
      return IndexSpace::NO_SPACE;
    } 

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::create_unbound_index_space(TypeTag tag,
                                                       Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal unbound index space creation performed in leaf task %s "
        "(ID %lld)", get_task_name(), get_unique_id())
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    void LeafContext::create_shared_ownership(IndexSpace handle)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal index space create shared ownership performed in leaf task "
        "%s (ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::union_index_spaces(
                  const std::vector<IndexSpace> &spaces, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal union index spaces performed in leaf task %s (ID %lld)",
        get_task_name(), get_unique_id())
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::intersect_index_spaces(
                  const std::vector<IndexSpace> &spaces, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal intersect index spaces performed in leaf task %s (ID %lld)",
        get_task_name(), get_unique_id())
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::subtract_index_spaces(
                      IndexSpace left, IndexSpace right, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal subtract index spaces performed in leaf task %s (ID %lld)",
        get_task_name(), get_unique_id())
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    void LeafContext::destroy_index_space(IndexSpace handle, 
               const bool unordered, const bool recurse, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal index space destruction performed in leaf task %s (ID %lld)",
        get_task_name(), get_unique_id())
    } 

    //--------------------------------------------------------------------------
    void LeafContext::create_shared_ownership(IndexPartition handle)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal index partition create shared ownership performed in leaf "
        "task %s (ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::destroy_index_partition(IndexPartition handle,
               const bool unordered, const bool recurse, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal index partition destruction performed in leaf task %s "
        "(ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_equal_partition(
                                             IndexSpace parent,
                                             IndexSpace color_space,
                                             size_t granularity, Color color,
                                             Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_EQUAL_PARTITION_CREATION,
        "Illegal equal partition creation performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_partition_by_weights(IndexSpace parent,
                                                const FutureMap &weights,
                                                IndexSpace color_space,
                                                size_t granularity, Color color,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_EQUAL_PARTITION_CREATION,
        "Illegal create partition by weights performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_partition_by_union(
                                          IndexSpace parent,
                                          IndexPartition handle1,
                                          IndexPartition handle2,
                                          IndexSpace color_space,
                                          PartitionKind kind, Color color,
                                          Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_UNION_PARTITION_CREATION,
        "Illegal union partition creation performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_partition_by_intersection(
                                                IndexSpace parent,
                                                IndexPartition handle1,
                                                IndexPartition handle2,
                                                IndexSpace color_space,
                                                PartitionKind kind, Color color,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_INTERSECTION_PARTITION_CREATION,
        "Illegal intersection partition creation performed in "
                     "leaf task %s (ID %lld)", get_task_name(),get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_partition_by_intersection(
                                                IndexSpace parent,
                                                IndexPartition partition,
                                                PartitionKind kind, Color color,
                                                bool dominates,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_INTERSECTION_PARTITION_CREATION,
        "Illegal intersection partition creation performed in "
                     "leaf task %s (ID %lld)", get_task_name(),get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_partition_by_difference(
                                                      IndexSpace parent,
                                                      IndexPartition handle1,
                                                      IndexPartition handle2,
                                                      IndexSpace color_space,
                                                      PartitionKind kind,
                                                      Color color,
                                                      Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_DIFFERENCE_PARTITION_CREATION,
        "Illegal difference partition creation performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    Color LeafContext::create_cross_product_partitions(IndexPartition handle1,
                                                       IndexPartition handle2,
                                   std::map<IndexSpace,IndexPartition> &handles,
                                                       PartitionKind kind,
                                                       Color color,
                                                       Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_CREATE_CROSS_PRODUCT_PARTITION,
        "Illegal create cross product partitions performed in "
                     "leaf task %s (ID %lld)", get_task_name(),get_unique_id())
      return 0;
    }

    //--------------------------------------------------------------------------
    void LeafContext::create_association(LogicalRegion domain,
                                         LogicalRegion domain_parent,
                                         FieldID domain_fid, IndexSpace range,
                                         MapperID id, MappingTagID tag,
                                         const UntypedBuffer &marg,
                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_CREATE_ASSOCIATION,
        "Illegal create association performed in leaf task "
                     "%s (ID %lld)", get_task_name(),get_unique_id())
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_restricted_partition(
                                                IndexSpace parent,
                                                IndexSpace color_space,
                                                const void *transform,
                                                size_t transform_size,
                                                const void *extent,
                                                size_t extent_size,
                                                PartitionKind part_kind,
                                                Color color,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_CREATE_RESTRICTED_PARTITION,
        "Illegal create restricted partition performed in "
                     "leaf task %s (ID %lld)", get_task_name(),get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_partition_by_domain(
                                                IndexSpace parent,
                                    const std::map<DomainPoint,Domain> &domains,
                                                IndexSpace color_space,
                                                bool perform_intersections,
                                                PartitionKind part_kind,
                                                Color color,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_PARTITION_BY_DOMAIN,
          "Illegal create partition by domain performed in leaf "
          "task %s (UID %lld)", get_task_name(), get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_partition_by_domain(
                                                IndexSpace parent,
                                                const FutureMap &domains,
                                                IndexSpace color_space,
                                                bool perform_intersections,
                                                PartitionKind part_kind,
                                                Color color,
                                                Provenance *provenance,
                                                bool skip_check)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_PARTITION_BY_DOMAIN,
          "Illegal create partition by domain performed in leaf "
          "task %s (UID %lld)", get_task_name(), get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_partition_by_field(
                                                LogicalRegion handle,
                                                LogicalRegion parent_priv,
                                                FieldID fid,
                                                IndexSpace color_space,
                                                Color color,
                                                MapperID id, MappingTagID tag,
                                                PartitionKind part_kind,
                                                const UntypedBuffer &marg,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_PARTITION_FIELD,
        "Illegal partition by field performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_partition_by_image(
                                              IndexSpace handle,
                                              LogicalPartition projection,
                                              LogicalRegion parent,
                                              FieldID fid,
                                              IndexSpace color_space,
                                              PartitionKind part_kind,
                                              Color color,
                                              MapperID id, MappingTagID tag,
                                              const UntypedBuffer &marg,
                                              Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_PARTITION_IMAGE,
        "Illegal partition by image performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_partition_by_image_range(
                                              IndexSpace handle,
                                              LogicalPartition projection,
                                              LogicalRegion parent,
                                              FieldID fid,
                                              IndexSpace color_space,
                                              PartitionKind part_kind,
                                              Color color,
                                              MapperID id, MappingTagID tag,
                                              const UntypedBuffer &marg,
                                              Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_PARTITION_IMAGE_RANGE,
        "Illegal partition by image range performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_partition_by_preimage(
                                                IndexPartition projection,
                                                LogicalRegion handle,
                                                LogicalRegion parent,
                                                FieldID fid,
                                                IndexSpace color_space,
                                                PartitionKind part_kind,
                                                Color color,
                                                MapperID id, MappingTagID tag,
                                                const UntypedBuffer &marg,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_PARTITION_PREIMAGE,
        "Illegal partition by preimage performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_partition_by_preimage_range(
                                                IndexPartition projection,
                                                LogicalRegion handle,
                                                LogicalRegion parent,
                                                FieldID fid,
                                                IndexSpace color_space,
                                                PartitionKind part_kind,
                                                Color color,
                                                MapperID id, MappingTagID tag,
                                                const UntypedBuffer &marg,
                                                Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_PARTITION_PREIMAGE_RANGE,
        "Illegal partition by preimage range performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexPartition LeafContext::create_pending_partition(
                                              IndexSpace parent,
                                              IndexSpace color_space,
                                              PartitionKind part_kind,
                                              Color color, Provenance *prov,
                                              bool trust)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_CREATE_PENDING_PARTITION,
        "Illegal create pending partition performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexPartition::NO_PART;
    }

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::create_index_space_union(IndexPartition parent,
                                                     const void *realm_color,
                                                     size_t color_size,
                                                     TypeTag type_tag,
                                        const std::vector<IndexSpace> &handles,
                                                     Provenance *provenance) 
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_CREATE_INDEX_SPACE_UNION,
        "Illegal create index space union performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::create_index_space_union(IndexPartition parent,
                                                     const void *realm_color,
                                                     size_t color_size,
                                                     TypeTag type_tag,
                                                     IndexPartition handle,
                                                     Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_CREATE_INDEX_SPACE_UNION,
        "Illegal create index space union performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::create_index_space_intersection(
                                                     IndexPartition parent,
                                                     const void *realm_color,
                                                     size_t color_size,
                                                     TypeTag type_tag,
                                        const std::vector<IndexSpace> &handles,
                                                      Provenance *provenance) 
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_CREATE_INDEX_SPACE_INTERSECTION,
        "Illegal create index space intersection performed in "
                     "leaf task %s (ID %lld)", get_task_name(),get_unique_id())
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::create_index_space_intersection(
                                                     IndexPartition parent,
                                                     const void *realm_color,
                                                     size_t color_size,
                                                     TypeTag type_tag,
                                                     IndexPartition handle,
                                                     Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_CREATE_INDEX_SPACE_INTERSECTION,
        "Illegal create index space intersection performed in "
                     "leaf task %s (ID %lld)", get_task_name(),get_unique_id())
      return IndexSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    IndexSpace LeafContext::create_index_space_difference(
                                                  IndexPartition parent,
                                                  const void *realm_color,
                                                  size_t color_size,
                                                  TypeTag type_tag,
                                                  IndexSpace initial,
                                          const std::vector<IndexSpace> &handles,
                                                  Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_CREATE_INDEX_SPACE_DIFFERENCE,
        "Illegal create index space difference performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return IndexSpace::NO_SPACE;
    } 

    //--------------------------------------------------------------------------
    FieldSpace LeafContext::create_field_space(Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
       "Illegal field space creation performed in leaf task %s (ID %lld)",
       get_task_name(), get_unique_id())
      return FieldSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    FieldSpace LeafContext::create_field_space(
                                         const std::vector<size_t> &sizes,
                                         std::vector<FieldID> &resulting_fields,
                                         CustomSerdezID serdez_id,
                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
       "Illegal field space creation performed in leaf task %s (ID %lld)",
       get_task_name(), get_unique_id())
      return FieldSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    FieldSpace LeafContext::create_field_space(const std::vector<Future> &sizes,
                                         std::vector<FieldID> &resulting_fields,
                                         CustomSerdezID serdez_id,
                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
       "Illegal field space creation performed in leaf task %s (ID %lld)",
       get_task_name(), get_unique_id())
      return FieldSpace::NO_SPACE;
    }

    //--------------------------------------------------------------------------
    void LeafContext::create_shared_ownership(FieldSpace handle)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal field space create shared ownership performed in leaf task "
        "%s (ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::destroy_field_space(FieldSpace handle, 
                                   const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
       "Illegal field space destruction performed in leaf task %s (ID %lld)",
       get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    FieldAllocatorImpl* LeafContext::create_field_allocator(FieldSpace handle,
                                                            bool unordered)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
       "Illegal field allocator creation performed in leaf task %s (ID %lld)",
       get_task_name(), get_unique_id())
      return NULL;
    }

    //--------------------------------------------------------------------------
    void LeafContext::destroy_field_allocator(FieldSpaceNode *node,
                                              bool from_application)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
       "Illegal field allocator destruction performed in leaf task %s "
       "(ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    FieldID LeafContext::allocate_field(FieldSpace space, size_t field_size,
                                        FieldID fid, bool local,
                                        CustomSerdezID serdez_id,
                                        Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
       "Illegal field allocation performed in leaf task %s (ID %lld)",
       get_task_name(), get_unique_id())
      return 0;
    }

    //--------------------------------------------------------------------------
    void LeafContext::allocate_fields(FieldSpace space,
                                      const std::vector<size_t> &sizes,
                                      std::vector<FieldID> &resulting_fields,
                                      bool local, CustomSerdezID serdez_id,
                                      Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
       "Illegal field allocations performed in leaf task %s (ID %lld)",
       get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::free_field(FieldAllocatorImpl *allocator,FieldSpace space, 
                                 FieldID fid, const bool unordered,
                                 Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
       "Illegal field free performed in leaf task %s (ID %lld)",
       get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::free_fields(FieldAllocatorImpl *allocator, 
                                  FieldSpace space, 
                                  const std::set<FieldID> &to_free,
                                  const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
       "Illegal field free performed in leaf task %s (ID %lld)",
       get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    FieldID LeafContext::allocate_field(FieldSpace space, 
                                        const Future &field_size,
                                        FieldID fid, bool local,
                                        CustomSerdezID serdez_id,
                                        Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_NONLOCAL_FIELD_ALLOCATION,
        "Illegal deferred field allocation performed in leaf task %s (ID %lld)",
        get_task_name(), get_unique_id())
      return 0;
    }

    //--------------------------------------------------------------------------
    void LeafContext::allocate_local_field(FieldSpace space, size_t field_size,
                                     FieldID fid, CustomSerdezID serdez_id,
                                     std::set<RtEvent> &done_events,
                                     Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_NONLOCAL_FIELD_ALLOCATION,
          "Illegal local field allocation performed in leaf task %s (ID %lld)",
          get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::allocate_fields(FieldSpace space,
                                      const std::vector<Future> &sizes,
                                      std::vector<FieldID> &resuling_fields,
                                      bool local, CustomSerdezID serdez_id,
                                      Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_NONLOCAL_FIELD_ALLOCATION2,
       "Illegal deferred field allocations performed in leaf task %s (ID %lld)",
       get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::allocate_local_fields(FieldSpace space,
                                   const std::vector<size_t> &sizes,
                                   const std::vector<FieldID> &resuling_fields,
                                   CustomSerdezID serdez_id,
                                   std::set<RtEvent> &done_events,
                                   Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_NONLOCAL_FIELD_ALLOCATION2,
          "Illegal local field allocations performed in leaf task %s (ID %lld)",
          get_task_name(), get_unique_id())
    } 

    //--------------------------------------------------------------------------
    LogicalRegion LeafContext::create_logical_region(IndexSpace index_space,
                                                     FieldSpace field_space,
                                                     const bool task_local,
                                                     Provenance *provenance,
                                                     const bool output_region)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
       "Illegal logical region creation performed in leaf task %s (ID %lld)",
       get_task_name(), get_unique_id())
      return LogicalRegion::NO_REGION;
    }

    //--------------------------------------------------------------------------
    void LeafContext::create_shared_ownership(LogicalRegion handle)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal logical region create shared ownership performed in leaf task "
        "%s (ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::destroy_logical_region(LogicalRegion handle,
                                   const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
       "Illegal logical region deletion performed in leaf task %s (ID %lld)",
       get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::reset_equivalence_sets(LogicalRegion parent,
                                             LogicalRegion region,
                                             const std::set<FieldID> &fields)
    //--------------------------------------------------------------------------
    {
      // No-op
    }

    //--------------------------------------------------------------------------
    void LeafContext::get_local_field_set(const FieldSpace handle,
                                          const std::set<unsigned> &indexes,
                                          std::set<FieldID> &to_set) const
    //--------------------------------------------------------------------------
    {
      // Should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    void LeafContext::get_local_field_set(const FieldSpace handle,
                                          const std::set<unsigned> &indexes,
                                          std::vector<FieldID> &to_set) const
    //--------------------------------------------------------------------------
    {
      // Should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    void LeafContext::add_physical_region(const RegionRequirement &req,
          bool mapped, MapperID mid, MappingTagID tag, ApUserEvent &unmap_event,
          bool virtual_mapped, const InstanceSet &physical_instances)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(!unmap_event.exists());
#endif
      PhysicalRegionImpl *impl = new PhysicalRegionImpl(req, 
          RtEvent::NO_RT_EVENT, ApEvent::NO_AP_EVENT, 
          ApUserEvent::NO_AP_USER_EVENT, mapped, this, mid, tag, 
          true/*leaf region*/, virtual_mapped, false/*collective*/, runtime);
      physical_regions.emplace_back(PhysicalRegion(impl));
      if (mapped)
        impl->set_references(physical_instances, true/*safe*/);
    }

    //--------------------------------------------------------------------------
    Future LeafContext::execute_task(const TaskLauncher &launcher,
                                     std::vector<OutputRequirement> *outputs)
    //--------------------------------------------------------------------------
    {
      AutoProvenance provenance(launcher.provenance);
      if (launcher.enable_inlining)
      {
        if (launcher.predicate == Predicate::FALSE_PRED)
          return predicate_task_false(launcher, provenance);
        IndividualTask *task = runtime->get_available_individual_task(); 
        InnerContext *parent = owner_task->get_context();
        Future result =
          task->initialize_task(parent, launcher, provenance, 
              false/*track*/, false/*top level*/, false/*must epoch*/, outputs);
        inline_child_task(task);
        return result;
      }
      else
      {
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_EXECUTE_TASK_CALL,
          "Illegal execute task call performed in leaf task %s "
                       "(ID %lld)", get_task_name(), get_unique_id())
        return Future();
      }
    }

    //--------------------------------------------------------------------------
    FutureMap LeafContext::execute_index_space(
                                        const IndexTaskLauncher &launcher,
                                        std::vector<OutputRequirement> *outputs)
    //--------------------------------------------------------------------------
    {
      AutoProvenance provenance(launcher.provenance);
      if (!launcher.must_parallelism && launcher.enable_inlining)
      {
        IndexSpace launch_space = launcher.launch_space;
        if (!launch_space.exists())
          REPORT_LEGION_ERROR(ERROR_ILLEGAL_EXECUTE_INDEX_SPACE,
            "Illegal execute index space call performed in leaf task %s "
            "(ID %lld). All inline leaf task index space launches must "
            "specify a launch index space.", get_task_name(), get_unique_id())
        if (launcher.predicate == Predicate::FALSE_PRED)
          return predicate_index_task_false(++inlined_tasks, launch_space,
                                            launcher, provenance);
        IndexTask *task = runtime->get_available_index_task();
        InnerContext *parent = owner_task->get_context();
        FutureMap result = task->initialize_task(parent, launcher, launch_space,
                                           provenance, false/*track*/, outputs);
        inline_child_task(task);
        return result;
      }
      else
      {
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_EXECUTE_INDEX_SPACE,
          "Illegal execute index space call performed in leaf "
                       "task %s (ID %lld)", get_task_name(), get_unique_id())
        return FutureMap();
      }
    }

    //--------------------------------------------------------------------------
    Future LeafContext::execute_index_space(const IndexTaskLauncher &launcher,
                                        ReductionOpID redop, bool deterministic,
                                        std::vector<OutputRequirement> *outputs)
    //--------------------------------------------------------------------------
    {
      AutoProvenance provenance(launcher.provenance);
      if (!launcher.must_parallelism && launcher.enable_inlining)
      {
        IndexSpace launch_space = launcher.launch_space;
        if (!launch_space.exists())
          REPORT_LEGION_ERROR(ERROR_ILLEGAL_EXECUTE_INDEX_SPACE,
            "Illegal execute index space call performed in leaf task %s "
            "(ID %lld). All inline leaf task index space launches must "
            "specify a launch index space.", get_task_name(), get_unique_id())
        if (launcher.predicate == Predicate::FALSE_PRED)
          return predicate_index_task_reduce_false(launcher, launch_space,
                                                   redop, provenance);
        IndexTask *task = runtime->get_available_index_task();
        InnerContext *parent = owner_task->get_context();
        Future result = task->initialize_task(parent, launcher, launch_space,
                    provenance, redop, deterministic, false/*track*/, outputs);
        inline_child_task(task);
        return result;
      }
      else
      {
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_EXECUTE_INDEX_SPACE,
          "Illegal execute index space call performed in leaf "
                       "task %s (ID %lld)", get_task_name(), get_unique_id())
        return Future();
      }
    }

    //--------------------------------------------------------------------------
    Future LeafContext::reduce_future_map(const FutureMap &future_map,
                                        ReductionOpID redop, bool deterministic,
                                        MapperID mapper_id, MappingTagID tag,
                                        Provenance *provenance,
                                        Future initial_value)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_EXECUTE_INDEX_SPACE,
        "Illegal reduce future map call performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return Future();
    }

    //--------------------------------------------------------------------------
    FutureMap LeafContext::construct_future_map(IndexSpace domain,
                                const std::map<DomainPoint,UntypedBuffer> &data,
                                Provenance *provenance, bool collective,
                                ShardingID sid, bool implicit, bool internal,
                                bool check_space)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_EXECUTE_INDEX_SPACE,
        "Illegal construct future map call performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return FutureMap();
    }

    //--------------------------------------------------------------------------
    FutureMap LeafContext::construct_future_map(const Domain &domain,
                                const std::map<DomainPoint,UntypedBuffer> &data,
                                bool collective, ShardingID sid, bool implicit)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_EXECUTE_INDEX_SPACE,
        "Illegal construct future map call performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return FutureMap();
    }

    //--------------------------------------------------------------------------
    FutureMap LeafContext::construct_future_map(IndexSpace domain,
                                    const std::map<DomainPoint,Future> &futures,
                                    Provenance *provenance,
                                    bool internal, bool collective,
                                    ShardingID sid, bool implicit, 
                                    bool check_space)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_EXECUTE_INDEX_SPACE,
        "Illegal construct future map call performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return FutureMap();
    }

    //--------------------------------------------------------------------------
    FutureMap LeafContext::construct_future_map(const Domain &domain,
                                    const std::map<DomainPoint,Future> &futures,
                                    bool internal, bool collective,
                                    ShardingID sid, bool implicit)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_EXECUTE_INDEX_SPACE,
        "Illegal construct future map call performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return FutureMap();
    }

    //--------------------------------------------------------------------------
    FutureMap LeafContext::transform_future_map(const FutureMap &fm,
       IndexSpace new_domain, TransformFutureMapImpl::PointTransformFnptr fnptr,
       Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal transform future map call performed in leaf task %s (ID %lld)",
        get_task_name(), get_unique_id())
      return FutureMap();
    }

    //--------------------------------------------------------------------------
    FutureMap LeafContext::transform_future_map(const FutureMap &fm,
           IndexSpace new_domain, PointTransformFunctor *functor,
           bool own_func, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal transform future map call performed in leaf task %s (ID %lld)",
        get_task_name(), get_unique_id())
      return FutureMap();
    }

    //--------------------------------------------------------------------------
    PhysicalRegion LeafContext::map_region(const InlineLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_MAP_REGION,
        "Illegal map_region operation performed in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
      return PhysicalRegion();
    }

    //--------------------------------------------------------------------------
    ApEvent LeafContext::remap_region(const PhysicalRegion &region,
                                      Provenance *provenance, bool internal)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_REMAP_OPERATION,
        "Illegal remap operation performed in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
      return ApEvent::NO_AP_EVENT;
    }

    //--------------------------------------------------------------------------
    void LeafContext::unmap_region(PhysicalRegion region)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_UNMAP_OPERATION,
        "Illegal unmap operation performed in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::unmap_all_regions(bool external)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_UNMAP_OPERATION,
        "Illegal unmap_all_regions call performed in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::fill_fields(const FillLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_FILL_OPERATION_CALL,
        "Illegal fill operation call performed in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::fill_fields(const IndexFillLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_INDEX_FILL_OPERATION_CALL,
        "Illegal index fill operation call performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::discard_fields(const DiscardLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal discard operation call performed in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::issue_copy(const CopyLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_COPY_FILL_OPERATION_CALL,
        "Illegal copy operation call performed in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::issue_copy(const IndexCopyLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_INDEX_COPY_OPERATION,
        "Illegal index copy operation call performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::issue_acquire(const AcquireLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_ACQUIRE_OPERATION,
        "Illegal acquire operation performed in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::issue_release(const ReleaseLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_RELEASE_OPERATION,
        "Illegal release operation performed in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    PhysicalRegion LeafContext::attach_resource(const AttachLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_ATTACH_RESOURCE_OPERATION,
        "Illegal attach resource operation performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return PhysicalRegion();
    }

    //--------------------------------------------------------------------------
    ExternalResources LeafContext::attach_resources(
                                            const IndexAttachLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_ATTACH_RESOURCE_OPERATION,
        "Illegal attach resources operation performed in leaf "
                     "task %s (ID %lld)", get_task_name(), get_unique_id())
      return ExternalResources();
    }
    
    //--------------------------------------------------------------------------
    Future LeafContext::detach_resource(PhysicalRegion region, const bool flush,
                                   const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_DETACH_RESOURCE_OPERATION,
        "Illegal detach resource operation performed in leaf "
                      "task %s (ID %lld)", get_task_name(), get_unique_id())
      return Future();
    }

    //--------------------------------------------------------------------------
    Future LeafContext::detach_resources(ExternalResources resources,
                 const bool flush, const bool unordered, Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_DETACH_RESOURCE_OPERATION,
        "Illegal index detach resource operation performed in leaf "
                      "task %s (ID %lld)", get_task_name(), get_unique_id())
      return Future();
    }

    //--------------------------------------------------------------------------
    void LeafContext::progress_unordered_operations(bool end_task)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_DETACH_RESOURCE_OPERATION,
        "Illegal progress unordered operations performed in leaf "
                      "task %s (ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    FutureMap LeafContext::execute_must_epoch(const MustEpochLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_LEGION_EXECUTE_MUST_EPOCH,
        "Illegal Legion execute must epoch call in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
      return FutureMap();
    }

    //--------------------------------------------------------------------------
    Future LeafContext::issue_timing_measurement(const TimingLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_TIMING_MEASUREMENT,
        "Illegal timing measurement operation in leaf task %s"
                     "(ID %lld)", get_task_name(), get_unique_id())
      return Future();
    }

    //--------------------------------------------------------------------------
    Future LeafContext::select_tunable_value(const TunableLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
        "Illegal tunable value operation request in leaf task %s (ID %lld)",
        get_task_name(), get_unique_id())
      return Future();
    }

    //--------------------------------------------------------------------------
    Future LeafContext::issue_mapping_fence(Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_LEGION_MAPPING_FENCE_CALL,
        "Illegal legion mapping fence call in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
      return Future();
    }

    //--------------------------------------------------------------------------
    Future LeafContext::issue_execution_fence(Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_LEGION_EXECUTION_FENCE_CALL,
        "Illegal Legion execution fence call in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
      return Future();
    }

    //--------------------------------------------------------------------------
    void LeafContext::complete_frame(Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_LEGION_COMPLETE_FRAME_CALL,
        "Illegal Legion complete frame call in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    Predicate LeafContext::create_predicate(const Future &f,
                                            Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      if (f.impl == NULL)
        return Predicate::FALSE_PRED;
      f.impl->request_runtime_instance(owner_task, true/*eager*/);
      const RtEvent ready = f.impl->subscribe(); 
      if (ready.exists() && !ready.has_triggered())
        ready.wait();
      // Always eagerly evaluate predicates in leaf contexts
      const bool value = f.impl->get_boolean_value(this);
      if (value)
        return Predicate::TRUE_PRED;
      else
        return Predicate::FALSE_PRED;
    }

    //--------------------------------------------------------------------------
    Predicate LeafContext::predicate_not(const Predicate &p,
                                         Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      if (p == Predicate::TRUE_PRED)
        return Predicate::FALSE_PRED;
      else if (p == Predicate::FALSE_PRED)
        return Predicate::TRUE_PRED;
      else // should never get here, all predicates should be eagerly evaluated
        assert(false);  
      return Predicate::TRUE_PRED;
    }
    
    //--------------------------------------------------------------------------
    Predicate LeafContext::create_predicate(const PredicateLauncher &launcher)
    //--------------------------------------------------------------------------
    {
      if (launcher.predicates.empty())
        REPORT_LEGION_ERROR(ERROR_ILLEGAL_PREDICATE_CREATION,
          "Illegal predicate creation performed on a "
                      "set of empty previous predicates in task %s (ID %lld).",
                      get_task_name(), get_unique_id())
      else if (launcher.predicates.size() == 1)
        return launcher.predicates[0];
      if (launcher.and_op)
      {
        // Check for short circuit cases
        for (std::vector<Predicate>::const_iterator it = 
              launcher.predicates.begin(); it != 
              launcher.predicates.end(); it++)
        {
          if ((*it) == Predicate::FALSE_PRED)
            return Predicate::FALSE_PRED;
          else if ((*it) == Predicate::TRUE_PRED)
            continue;
          else // should never get here, 
            // all predicates should be eagerly evaluated
            assert(false);
        }
        return Predicate::TRUE_PRED;
      }
      else
      {
        // Check for short circuit cases
        for (std::vector<Predicate>::const_iterator it = 
              launcher.predicates.begin(); it != 
              launcher.predicates.end(); it++)
        {
          if ((*it) == Predicate::TRUE_PRED)
            return Predicate::TRUE_PRED;
          else if ((*it) == Predicate::FALSE_PRED)
            continue;
          else // should never get here, 
            // all predicates should be eagerly evaluated
            assert(false);
        }
        return Predicate::FALSE_PRED;
      }
    }

    //--------------------------------------------------------------------------
    Future LeafContext::get_predicate_future(const Predicate &p,
                                             Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      if (p == Predicate::TRUE_PRED)
      {
        Future result(new FutureImpl(this, runtime, true/*register*/,
              runtime->get_available_distributed_id(), provenance));
        const bool value = true;
        result.impl->set_local(&value, sizeof(value));
        return result;
      }
      else if (p == Predicate::FALSE_PRED)
      {
        Future result(new FutureImpl(this, runtime, true/*register*/,
              runtime->get_available_distributed_id(), provenance));
        const bool value = false;
        result.impl->set_local(&value, sizeof(value));
        return result;
      }
      else // should never get here, all predicates should be eagerly evaluated
        assert(false);
      return Future();
    }

    //--------------------------------------------------------------------------
    void LeafContext::begin_trace(TraceID tid, bool logical_only,
        bool static_trace, const std::set<RegionTreeID> *trees,
        bool deprecated, Provenance *provenance, bool from_application)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_LEGION_BEGIN_TRACE,
        "Illegal Legion begin trace call in leaf task %s "
                     "(ID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::end_trace(TraceID tid, bool deprecated,
                                Provenance *provenance, bool from_application)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_ILLEGAL_LEGION_END_TRACE,
        "Illegal Legion end trace call in leaf task %s (ID %lld)",
                     get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::record_previous_trace(LogicalTrace *trace)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(false);
#endif
      exit(ERROR_LEAF_TASK_VIOLATION);
    }

    //--------------------------------------------------------------------------
    void LeafContext::invalidate_trace_cache(
                                    LogicalTrace *trace, Operation *invalidator)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_LEGION
      assert(false);
#endif
      exit(ERROR_LEAF_TASK_VIOLATION);
    }

    //--------------------------------------------------------------------------
    void LeafContext::record_blocking_call(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    void LeafContext::issue_frame(FrameOp *frame, ApEvent frame_termination)
    //--------------------------------------------------------------------------
    {
      assert(false);
    }

    //--------------------------------------------------------------------------
    void LeafContext::perform_frame_issue(FrameOp *frame, ApEvent frame_term)
    //--------------------------------------------------------------------------
    {
      assert(false);
    }

    //--------------------------------------------------------------------------
    void LeafContext::finish_frame(ApEvent frame_termination)
    //--------------------------------------------------------------------------
    {
      assert(false);
    }

    //--------------------------------------------------------------------------
    void LeafContext::increment_outstanding(void)
    //--------------------------------------------------------------------------
    {
      assert(false);
    }

    //--------------------------------------------------------------------------
    void LeafContext::decrement_outstanding(void)
    //--------------------------------------------------------------------------
    {
      assert(false);
    }

    //--------------------------------------------------------------------------
    void LeafContext::increment_pending(void)
    //--------------------------------------------------------------------------
    {
      assert(false);
    }

    //--------------------------------------------------------------------------
    void LeafContext::decrement_pending(TaskOp *child)
    //--------------------------------------------------------------------------
    {
      assert(false);
    }

    //--------------------------------------------------------------------------
    void LeafContext::decrement_pending(bool need_deferral)
    //--------------------------------------------------------------------------
    {
      assert(false);
    }

    //--------------------------------------------------------------------------
    void LeafContext::increment_frame(void)
    //--------------------------------------------------------------------------
    {
      assert(false);
    }

    //--------------------------------------------------------------------------
    void LeafContext::decrement_frame(void)
    //--------------------------------------------------------------------------
    {
      assert(false);
    }

    //--------------------------------------------------------------------------
    InnerContext* LeafContext::find_top_context(InnerContext *previous)
    //--------------------------------------------------------------------------
    {
      assert(false);
      return NULL;
    }

    //--------------------------------------------------------------------------
    void LeafContext::initialize_region_tree_contexts(
                       const std::vector<RegionRequirement> &clone_requirements,
                       const LegionVector<VersionInfo> &version_infos,
                       const std::vector<ApUserEvent> &unmap_events)
    //--------------------------------------------------------------------------
    {
      // Nothing to do
    }

    //--------------------------------------------------------------------------
    void LeafContext::invalidate_region_tree_contexts(
                       const bool is_top_level_task, std::set<RtEvent> &applied,
                       const ShardMapping *mapping, ShardID source_shard)
    //--------------------------------------------------------------------------
    {
      // Nothing to do 
    }

    //--------------------------------------------------------------------------
    void LeafContext::end_task(const void *res, size_t res_size, bool owned,
                               PhysicalInstance deferred_result_instance,
                               FutureFunctor *callback_functor,
                               const Realm::ExternalInstanceResource *resource,
                      void (*freefunc)(const Realm::ExternalInstanceResource&),
                  const void *metadataptr, size_t metadatasize, ApEvent effects)
    //--------------------------------------------------------------------------
    {
      // No local regions or fields permitted in leaf tasks
      if (overhead_profiler != NULL)
      {
        const long long current = Realm::Clock::current_time_in_nanoseconds();
        const long long diff = current - 
          overhead_profiler->previous_profiling_time;
        overhead_profiler->application_time += diff;
      }
      if (Processor::get_executing_processor().exists())
      {
#ifdef DEBUG_LEGION
        assert(!effects.exists());
#endif
        effects = ApEvent(Processor::get_current_finish_event());
      }
      // No need to unmap the physical regions, they never had events
      TaskContext::end_task(res, res_size, owned, deferred_result_instance,
          callback_functor,resource,freefunc,metadataptr,metadatasize,effects);
    }

    //--------------------------------------------------------------------------
    void LeafContext::post_end_task(FutureInstance *instance,
                                    void *metadata, size_t metasize,
                                    FutureFunctor *callback_functor,
                                    bool own_callback_functor)
    //--------------------------------------------------------------------------
    {
      // Safe to cast to a single task here because this will never
      // be called while inlining an index space task
      // Handle the future result
      owner_task->handle_post_execution(instance, metadata, metasize,
          callback_functor, executing_processor, own_callback_functor);
      bool need_complete = false;
      bool need_commit = false;
      {
        AutoLock leaf(leaf_lock);
#ifdef DEBUG_LEGION
        assert(!task_executed);
#endif
        // Now that we know the last registration has taken place we
        // can mark that we are done executing
        task_executed = true;
        if (!children_complete_invoked)
        {
          need_complete = true;
          children_complete_invoked = true;
        }
        if (!children_commit_invoked)
        {
          need_commit = true;
          children_commit_invoked = true;
        }
      } 
      if (need_complete)
        owner_task->trigger_children_complete();
      if (need_commit)
        owner_task->trigger_children_committed();
    }

    //--------------------------------------------------------------------------
    void LeafContext::destroy_lock(Lock l)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
          "Illegal destroy lock performed in leaf task %s (UID %lld)",
          get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    Grant LeafContext::acquire_grant(const std::vector<LockRequest> &requests)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
          "Illegal acquire grant performed in leaf task %s (UID %lld)",
          get_task_name(), get_unique_id())
      return Grant();
    }

    //--------------------------------------------------------------------------
    void LeafContext::release_grant(Grant g)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
          "Illegal release grant performed in leaf task %s (UID %lld)",
          get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::destroy_phase_barrier(PhaseBarrier pb)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
          "Illegal destroy phase barrier performed in leaf task %s (UID %lld)",
          get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    DynamicCollective LeafContext::create_dynamic_collective(
                                       unsigned arrivals, ReductionOpID redop,
                                       const void *init_value, size_t init_size)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
          "Illegal create dynamic collective performed in leaf task %s "
          "(UID %lld)", get_task_name(), get_unique_id())
      return DynamicCollective();
    }

    //--------------------------------------------------------------------------
    void LeafContext::destroy_dynamic_collective(DynamicCollective dc)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
          "Illegal destroy dynamic collective performed in leaf task %s "
          "(UID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::arrive_dynamic_collective(DynamicCollective dc,
                                const void *buffer, size_t size, unsigned count)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
          "Illegal arrive dynamic collective performed in leaf task %s "
          "(UID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    void LeafContext::defer_dynamic_collective_arrival(DynamicCollective dc,
                                           const Future &future, unsigned count)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
          "Illegal defer dynamic collective performed in leaf task %s "
          "(UID %lld)", get_task_name(), get_unique_id())
    }

    //--------------------------------------------------------------------------
    Future LeafContext::get_dynamic_collective_result(DynamicCollective dc,
                                                      Provenance *provenance)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
          "Illegal get dynamic collective performed in leaf task %s (UID %lld)",
          get_task_name(), get_unique_id())
      return Future();
    }

    //--------------------------------------------------------------------------
    DynamicCollective LeafContext::advance_dynamic_collective(
                                                           DynamicCollective dc)
    //--------------------------------------------------------------------------
    {
      REPORT_LEGION_ERROR(ERROR_LEAF_TASK_VIOLATION,
          "Illegal advance dynamic collective performed in leaf task %s "
          "(UID %lld)", get_task_name(), get_unique_id())
      return DynamicCollective();
    }

    //--------------------------------------------------------------------------
    TaskPriority LeafContext::get_current_priority(void) const
    //--------------------------------------------------------------------------
    {
      assert(false);
      return 0;
    }

    //--------------------------------------------------------------------------
    void LeafContext::set_current_priority(TaskPriority priority)
    //--------------------------------------------------------------------------
    {
      assert(false);
    }

  };
};

// EOF

